Conformance test: not doing test setup.
I0220 18:07:55.982474   30262 e2e.go:224] Starting e2e run "71f5f6f5-353a-11e9-9f03-4261fb5d5f3f" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550686074 - Will randomize all specs
Will run 201 of 2161 specs

Feb 20 18:07:56.433: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:07:56.437: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 20 18:07:56.531: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 20 18:07:56.650: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 20 18:07:56.650: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 20 18:07:56.650: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 20 18:07:56.673: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 20 18:07:56.673: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 20 18:07:56.673: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 20 18:07:56.673: INFO: e2e test version: v1.13.3
Feb 20 18:07:56.688: INFO: kube-apiserver version: v1.13.3
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:07:56.688: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
Feb 20 18:07:57.401: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 20 18:07:57.452: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hg6qq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 18:07:57.641: INFO: Waiting up to 5m0s for pod "pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-hg6qq" to be "success or failure"
Feb 20 18:07:57.658: INFO: Pod "pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.110816ms
Feb 20 18:07:59.680: INFO: Pod "pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039039665s
Feb 20 18:08:01.704: INFO: Pod "pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063029863s
STEP: Saw pod success
Feb 20 18:08:01.704: INFO: Pod "pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:08:01.721: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:08:01.900: INFO: Waiting for pod pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:08:01.916: INFO: Pod pod-73a72b87-353a-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:08:01.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hg6qq" for this suite.
Feb 20 18:08:07.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:08:08.159: INFO: namespace: e2e-tests-emptydir-hg6qq, resource: bindings, ignored listing per whitelist
Feb 20 18:08:08.671: INFO: namespace e2e-tests-emptydir-hg6qq deletion completed in 6.737976059s

• [SLOW TEST:11.983 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:08:08.671: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-s7ctp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0220 18:08:19.639581   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:08:19.639: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:08:19.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-s7ctp" for this suite.
Feb 20 18:08:25.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:08:26.337: INFO: namespace: e2e-tests-gc-s7ctp, resource: bindings, ignored listing per whitelist
Feb 20 18:08:26.474: INFO: namespace e2e-tests-gc-s7ctp deletion completed in 6.818111444s

• [SLOW TEST:17.804 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:08:26.475: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-26cs6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 18:08:27.354: INFO: Waiting up to 5m0s for pod "pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-26cs6" to be "success or failure"
Feb 20 18:08:27.372: INFO: Pod "pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.630769ms
Feb 20 18:08:29.389: INFO: Pod "pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03537s
STEP: Saw pod success
Feb 20 18:08:29.390: INFO: Pod "pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:08:29.406: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:08:29.450: INFO: Waiting for pod pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:08:29.466: INFO: Pod pod-855cf4bd-353a-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:08:29.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-26cs6" for this suite.
Feb 20 18:08:35.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:08:35.650: INFO: namespace: e2e-tests-emptydir-26cs6, resource: bindings, ignored listing per whitelist
Feb 20 18:08:36.221: INFO: namespace e2e-tests-emptydir-26cs6 deletion completed in 6.738538735s

• [SLOW TEST:9.746 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:08:36.221: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-l4wdd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:08:37.055: INFO: Waiting up to 5m0s for pod "downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-l4wdd" to be "success or failure"
Feb 20 18:08:37.070: INFO: Pod "downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.491722ms
Feb 20 18:08:39.088: INFO: Pod "downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032807456s
Feb 20 18:08:41.105: INFO: Pod "downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049907697s
STEP: Saw pod success
Feb 20 18:08:41.105: INFO: Pod "downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:08:41.121: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:08:41.249: INFO: Waiting for pod downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:08:41.265: INFO: Pod downward-api-8b252ad2-353a-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:08:41.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-l4wdd" for this suite.
Feb 20 18:08:47.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:08:47.534: INFO: namespace: e2e-tests-downward-api-l4wdd, resource: bindings, ignored listing per whitelist
Feb 20 18:08:47.990: INFO: namespace e2e-tests-downward-api-l4wdd deletion completed in 6.708374889s

• [SLOW TEST:11.769 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:08:47.990: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bjhh7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-464p
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:08:48.891: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-464p" in namespace "e2e-tests-subpath-bjhh7" to be "success or failure"
Feb 20 18:08:48.906: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Pending", Reason="", readiness=false. Elapsed: 15.670274ms
Feb 20 18:08:50.924: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033074383s
Feb 20 18:08:52.941: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 4.050328281s
Feb 20 18:08:54.958: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 6.067139045s
Feb 20 18:08:56.974: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 8.08386212s
Feb 20 18:08:58.991: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 10.100606265s
Feb 20 18:09:01.008: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 12.117585459s
Feb 20 18:09:03.025: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 14.134479207s
Feb 20 18:09:05.042: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 16.151373556s
Feb 20 18:09:07.059: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 18.168057486s
Feb 20 18:09:09.076: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 20.185145648s
Feb 20 18:09:11.093: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Running", Reason="", readiness=false. Elapsed: 22.202836893s
Feb 20 18:09:13.111: INFO: Pod "pod-subpath-test-configmap-464p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.220388401s
STEP: Saw pod success
Feb 20 18:09:13.111: INFO: Pod "pod-subpath-test-configmap-464p" satisfied condition "success or failure"
Feb 20 18:09:13.127: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-subpath-test-configmap-464p container test-container-subpath-configmap-464p: <nil>
STEP: delete the pod
Feb 20 18:09:13.213: INFO: Waiting for pod pod-subpath-test-configmap-464p to disappear
Feb 20 18:09:13.229: INFO: Pod pod-subpath-test-configmap-464p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-464p
Feb 20 18:09:13.229: INFO: Deleting pod "pod-subpath-test-configmap-464p" in namespace "e2e-tests-subpath-bjhh7"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:09:13.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bjhh7" for this suite.
Feb 20 18:09:19.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:09:19.494: INFO: namespace: e2e-tests-subpath-bjhh7, resource: bindings, ignored listing per whitelist
Feb 20 18:09:20.005: INFO: namespace e2e-tests-subpath-bjhh7 deletion completed in 6.743120641s

• [SLOW TEST:32.015 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:09:20.005: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-8cp42
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 18:09:28.987: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:29.003: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:31.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:31.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:33.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:33.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:35.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:35.022: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:37.003: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:37.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:39.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:39.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:41.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:41.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:43.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:43.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:45.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:45.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:47.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:47.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:49.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:49.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:51.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:51.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:53.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:53.021: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:55.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:55.020: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:09:57.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:09:57.024: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:09:57.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8cp42" for this suite.
Feb 20 18:10:21.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:10:21.372: INFO: namespace: e2e-tests-container-lifecycle-hook-8cp42, resource: bindings, ignored listing per whitelist
Feb 20 18:10:21.785: INFO: namespace e2e-tests-container-lifecycle-hook-8cp42 deletion completed in 24.719364309s

• [SLOW TEST:61.780 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:10:21.786: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-lssv8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-pwl8m
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-5vqqh
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:10:29.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-lssv8" for this suite.
Feb 20 18:10:37.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:10:37.689: INFO: namespace: e2e-tests-namespaces-lssv8, resource: bindings, ignored listing per whitelist
Feb 20 18:10:38.180: INFO: namespace e2e-tests-namespaces-lssv8 deletion completed in 8.727858485s
STEP: Destroying namespace "e2e-tests-nsdeletetest-pwl8m" for this suite.
Feb 20 18:10:38.197: INFO: Namespace e2e-tests-nsdeletetest-pwl8m was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-5vqqh" for this suite.
Feb 20 18:10:44.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:10:44.547: INFO: namespace: e2e-tests-nsdeletetest-5vqqh, resource: bindings, ignored listing per whitelist
Feb 20 18:10:44.888: INFO: namespace e2e-tests-nsdeletetest-5vqqh deletion completed in 6.69000269s

• [SLOW TEST:23.102 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:10:44.888: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4xbjw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-d7dd7d45-353a-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:10:45.786: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-4xbjw" to be "success or failure"
Feb 20 18:10:45.802: INFO: Pod "pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.114847ms
Feb 20 18:10:47.820: INFO: Pod "pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034232676s
Feb 20 18:10:49.836: INFO: Pod "pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050556909s
STEP: Saw pod success
Feb 20 18:10:49.836: INFO: Pod "pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:10:49.853: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:10:49.931: INFO: Waiting for pod pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:10:49.947: INFO: Pod pod-projected-secrets-d7e00864-353a-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:10:49.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4xbjw" for this suite.
Feb 20 18:10:56.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:10:56.352: INFO: namespace: e2e-tests-projected-4xbjw, resource: bindings, ignored listing per whitelist
Feb 20 18:10:56.726: INFO: namespace e2e-tests-projected-4xbjw deletion completed in 6.762707602s

• [SLOW TEST:11.838 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:10:56.726: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-hgmgr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-hgmgr
Feb 20 18:10:59.579: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-hgmgr
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:10:59.595: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:14:59.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hgmgr" for this suite.
Feb 20 18:15:05.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:06.293: INFO: namespace: e2e-tests-container-probe-hgmgr, resource: bindings, ignored listing per whitelist
Feb 20 18:15:06.373: INFO: namespace e2e-tests-container-probe-hgmgr deletion completed in 6.694087762s

• [SLOW TEST:249.647 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:15:06.374: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lvhr8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 18:15:07.255: INFO: Waiting up to 5m0s for pod "pod-73b90854-353b-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-lvhr8" to be "success or failure"
Feb 20 18:15:07.270: INFO: Pod "pod-73b90854-353b-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.479424ms
Feb 20 18:15:09.287: INFO: Pod "pod-73b90854-353b-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032094482s
STEP: Saw pod success
Feb 20 18:15:09.287: INFO: Pod "pod-73b90854-353b-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:15:09.303: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-73b90854-353b-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:15:09.347: INFO: Waiting for pod pod-73b90854-353b-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:15:09.363: INFO: Pod pod-73b90854-353b-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:15:09.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lvhr8" for this suite.
Feb 20 18:15:15.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:15.523: INFO: namespace: e2e-tests-emptydir-lvhr8, resource: bindings, ignored listing per whitelist
Feb 20 18:15:16.041: INFO: namespace e2e-tests-emptydir-lvhr8 deletion completed in 6.661447674s

• [SLOW TEST:9.668 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:15:16.041: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-28nkt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-8ltm
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:15:16.885: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8ltm" in namespace "e2e-tests-subpath-28nkt" to be "success or failure"
Feb 20 18:15:16.900: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Pending", Reason="", readiness=false. Elapsed: 15.508831ms
Feb 20 18:15:18.917: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032605293s
Feb 20 18:15:20.934: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 4.049191958s
Feb 20 18:15:22.954: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 6.069190392s
Feb 20 18:15:24.971: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 8.086327645s
Feb 20 18:15:26.989: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 10.104025857s
Feb 20 18:15:29.010: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 12.125217809s
Feb 20 18:15:31.026: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 14.141743102s
Feb 20 18:15:33.042: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 16.157795999s
Feb 20 18:15:35.060: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 18.174907478s
Feb 20 18:15:37.076: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 20.191490883s
Feb 20 18:15:39.093: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Running", Reason="", readiness=false. Elapsed: 22.20867266s
Feb 20 18:15:41.110: INFO: Pod "pod-subpath-test-projected-8ltm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.225542188s
STEP: Saw pod success
Feb 20 18:15:41.110: INFO: Pod "pod-subpath-test-projected-8ltm" satisfied condition "success or failure"
Feb 20 18:15:41.127: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-subpath-test-projected-8ltm container test-container-subpath-projected-8ltm: <nil>
STEP: delete the pod
Feb 20 18:15:41.244: INFO: Waiting for pod pod-subpath-test-projected-8ltm to disappear
Feb 20 18:15:41.294: INFO: Pod pod-subpath-test-projected-8ltm no longer exists
STEP: Deleting pod pod-subpath-test-projected-8ltm
Feb 20 18:15:41.294: INFO: Deleting pod "pod-subpath-test-projected-8ltm" in namespace "e2e-tests-subpath-28nkt"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:15:41.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-28nkt" for this suite.
Feb 20 18:15:47.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:47.747: INFO: namespace: e2e-tests-subpath-28nkt, resource: bindings, ignored listing per whitelist
Feb 20 18:15:48.101: INFO: namespace e2e-tests-subpath-28nkt deletion completed in 6.69061714s

• [SLOW TEST:32.059 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:15:48.101: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-p6nd2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-8ca5005a-353b-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:15:49.083: INFO: Waiting up to 5m0s for pod "pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-p6nd2" to be "success or failure"
Feb 20 18:15:49.099: INFO: Pod "pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.689511ms
Feb 20 18:15:51.116: INFO: Pod "pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03244727s
STEP: Saw pod success
Feb 20 18:15:51.116: INFO: Pod "pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:15:51.131: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f container secret-env-test: <nil>
STEP: delete the pod
Feb 20 18:15:51.241: INFO: Waiting for pod pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:15:51.257: INFO: Pod pod-secrets-8ca79385-353b-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:15:51.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-p6nd2" for this suite.
Feb 20 18:15:57.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:57.798: INFO: namespace: e2e-tests-secrets-p6nd2, resource: bindings, ignored listing per whitelist
Feb 20 18:15:57.976: INFO: namespace e2e-tests-secrets-p6nd2 deletion completed in 6.701827972s

• [SLOW TEST:9.875 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:15:57.977: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-r5nz6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-r5nz6
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-r5nz6
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-r5nz6
Feb 20 18:15:58.879: INFO: Found 0 stateful pods, waiting for 1
Feb 20 18:16:08.930: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 20 18:16:08.946: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:16:09.752: INFO: stderr: ""
Feb 20 18:16:09.752: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:16:09.752: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:16:09.768: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 18:16:19.787: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:16:19.787: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:16:19.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999568s
Feb 20 18:16:20.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.940033782s
Feb 20 18:16:21.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.922651636s
Feb 20 18:16:22.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.905375305s
Feb 20 18:16:23.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.888437243s
Feb 20 18:16:24.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.871406224s
Feb 20 18:16:25.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.853858666s
Feb 20 18:16:27.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.836835031s
Feb 20 18:16:28.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.819460744s
Feb 20 18:16:29.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 802.547761ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-r5nz6
Feb 20 18:16:30.067: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:16:30.759: INFO: stderr: ""
Feb 20 18:16:30.759: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:16:30.759: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:16:30.759: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:16:31.460: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 18:16:31.460: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:16:31.460: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:16:31.460: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:16:32.153: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 18:16:32.153: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:16:32.153: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:16:32.169: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:16:32.169: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:16:32.169: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 20 18:16:32.186: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:16:32.880: INFO: stderr: ""
Feb 20 18:16:32.880: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:16:32.880: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:16:32.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:16:33.534: INFO: stderr: ""
Feb 20 18:16:33.534: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:16:33.534: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:16:33.534: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:16:34.205: INFO: stderr: ""
Feb 20 18:16:34.206: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:16:34.206: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:16:34.206: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:16:34.223: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 20 18:16:44.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:16:44.259: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:16:44.259: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:16:44.310: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:44.310: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:44.310: INFO: ss-1  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:44.310: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:44.310: INFO: 
Feb 20 18:16:44.310: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:16:45.327: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:45.327: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:45.327: INFO: ss-1  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:45.327: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:45.327: INFO: 
Feb 20 18:16:45.327: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:16:46.347: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:46.347: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:46.347: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:46.347: INFO: 
Feb 20 18:16:46.347: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:47.364: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:47.364: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:47.364: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:47.364: INFO: 
Feb 20 18:16:47.364: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:48.382: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:48.382: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:48.382: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:48.382: INFO: 
Feb 20 18:16:48.383: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:49.399: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:49.399: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:49.399: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:49.399: INFO: 
Feb 20 18:16:49.399: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:50.417: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:50.417: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:50.417: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:50.417: INFO: 
Feb 20 18:16:50.417: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:51.434: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:51.434: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:51.434: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:51.434: INFO: 
Feb 20 18:16:51.434: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:52.451: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:52.451: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:52.451: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:52.451: INFO: 
Feb 20 18:16:52.451: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:16:53.468: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:16:53.468: INFO: ss-0  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:58 +0000 UTC  }]
Feb 20 18:16:53.468: INFO: ss-2  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:16:19 +0000 UTC  }]
Feb 20 18:16:53.468: INFO: 
Feb 20 18:16:53.468: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-r5nz6
Feb 20 18:16:54.485: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:16:54.918: INFO: rc: 1
Feb 20 18:16:54.918: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0019875c0 exit status 1 <nil> <nil> true [0xc00000f800 0xc00000f818 0xc00000f850] [0xc00000f800 0xc00000f818 0xc00000f850] [0xc00000f810 0xc00000f848] [0x933040 0x933040] 0xc00169d740 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 20 18:17:04.918: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:05.079: INFO: rc: 1
Feb 20 18:17:05.079: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001889f20 exit status 1 <nil> <nil> true [0xc0009eb000 0xc0009eb018 0xc0009eb030] [0xc0009eb000 0xc0009eb018 0xc0009eb030] [0xc0009eb010 0xc0009eb028] [0x933040 0x933040] 0xc00188af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:17:15.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:15.211: INFO: rc: 1
Feb 20 18:17:15.211: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ab3bf0 exit status 1 <nil> <nil> true [0xc00052f8a0 0xc00052f8d0 0xc00052f8f0] [0xc00052f8a0 0xc00052f8d0 0xc00052f8f0] [0xc00052f8b0 0xc00052f8e0] [0x933040 0x933040] 0xc001a1d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:17:25.211: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:25.392: INFO: rc: 1
Feb 20 18:17:25.393: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000914270 exit status 1 <nil> <nil> true [0xc0001b6018 0xc0001b68e8 0xc0001b6a50] [0xc0001b6018 0xc0001b68e8 0xc0001b6a50] [0xc0001b68c0 0xc0001b6970] [0x933040 0x933040] 0xc001656360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:17:35.393: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:35.519: INFO: rc: 1
Feb 20 18:17:35.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736270 exit status 1 <nil> <nil> true [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4910 0xc0000d4e98] [0x933040 0x933040] 0xc0012c0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:17:45.519: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:45.666: INFO: rc: 1
Feb 20 18:17:45.666: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f50270 exit status 1 <nil> <nil> true [0xc00000e100 0xc00000e1d0 0xc00000e260] [0xc00000e100 0xc00000e1d0 0xc00000e260] [0xc00000e1c0 0xc00000e1e8] [0x933040 0x933040] 0xc000e12d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:17:55.667: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:17:55.855: INFO: rc: 1
Feb 20 18:17:55.855: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736510 exit status 1 <nil> <nil> true [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5190 0xc0000d5360] [0x933040 0x933040] 0xc0012c0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:05.855: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:05.986: INFO: rc: 1
Feb 20 18:18:05.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0007367b0 exit status 1 <nil> <nil> true [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d54d8 0xc0000d5598] [0x933040 0x933040] 0xc0012c0840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:15.986: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:16.160: INFO: rc: 1
Feb 20 18:18:16.160: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000914510 exit status 1 <nil> <nil> true [0xc0001b6a70 0xc0001b6e70 0xc0001b7208] [0xc0001b6a70 0xc0001b6e70 0xc0001b7208] [0xc0001b6ce8 0xc0001b71f8] [0x933040 0x933040] 0xc001656720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:26.160: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:26.379: INFO: rc: 1
Feb 20 18:18:26.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736a20 exit status 1 <nil> <nil> true [0xc0000d56a8 0xc0000d5768 0xc0000d5858] [0xc0000d56a8 0xc0000d5768 0xc0000d5858] [0xc0000d5758 0xc0000d57c8] [0x933040 0x933040] 0xc0012c13e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:36.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:36.535: INFO: rc: 1
Feb 20 18:18:36.535: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009b82d0 exit status 1 <nil> <nil> true [0xc00052e078 0xc00052e0f0 0xc00052e128] [0xc00052e078 0xc00052e0f0 0xc00052e128] [0xc00052e0e0 0xc00052e120] [0x933040 0x933040] 0xc000916660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:46.537: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:46.771: INFO: rc: 1
Feb 20 18:18:46.772: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009b8540 exit status 1 <nil> <nil> true [0xc00052e138 0xc00052e180 0xc00052e1c0] [0xc00052e138 0xc00052e180 0xc00052e1c0] [0xc00052e178 0xc00052e1b8] [0x933040 0x933040] 0xc000917860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:18:56.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:18:56.955: INFO: rc: 1
Feb 20 18:18:56.955: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009b87e0 exit status 1 <nil> <nil> true [0xc00052e1c8 0xc00052e1e0 0xc00052e208] [0xc00052e1c8 0xc00052e1e0 0xc00052e208] [0xc00052e1d8 0xc00052e200] [0x933040 0x933040] 0xc000f7e7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:06.955: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:07.094: INFO: rc: 1
Feb 20 18:19:07.095: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009147e0 exit status 1 <nil> <nil> true [0xc0001b7240 0xc0001b76f8 0xc0001b7798] [0xc0001b7240 0xc0001b76f8 0xc0001b7798] [0xc0001b7608 0xc0001b7768] [0x933040 0x933040] 0xc001656ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:17.095: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:17.263: INFO: rc: 1
Feb 20 18:19:17.263: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000914a80 exit status 1 <nil> <nil> true [0xc0001b77f0 0xc0001b7988 0xc0001b7ac0] [0xc0001b77f0 0xc0001b7988 0xc0001b7ac0] [0xc0001b7900 0xc0001b7aa8] [0x933040 0x933040] 0xc0016572c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:27.263: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:27.454: INFO: rc: 1
Feb 20 18:19:27.454: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009b8300 exit status 1 <nil> <nil> true [0xc00052e0d0 0xc00052e118 0xc00052e138] [0xc00052e0d0 0xc00052e118 0xc00052e138] [0xc00052e0f0 0xc00052e128] [0x933040 0x933040] 0xc000916660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:37.454: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:37.616: INFO: rc: 1
Feb 20 18:19:37.616: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f502a0 exit status 1 <nil> <nil> true [0xc00000e100 0xc00000e1d0 0xc00000e260] [0xc00000e100 0xc00000e1d0 0xc00000e260] [0xc00000e1c0 0xc00000e1e8] [0x933040 0x933040] 0xc000f7eb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:47.616: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:47.750: INFO: rc: 1
Feb 20 18:19:47.750: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009142a0 exit status 1 <nil> <nil> true [0xc0001b6000 0xc0001b68c0 0xc0001b6970] [0xc0001b6000 0xc0001b68c0 0xc0001b6970] [0xc0001b66f8 0xc0001b6950] [0x933040 0x933040] 0xc000e12d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:19:57.750: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:19:57.881: INFO: rc: 1
Feb 20 18:19:57.881: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0007362a0 exit status 1 <nil> <nil> true [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4910 0xc0000d4e98] [0x933040 0x933040] 0xc001656360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:07.882: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:08.109: INFO: rc: 1
Feb 20 18:20:08.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f50510 exit status 1 <nil> <nil> true [0xc00000e278 0xc00000e2a0 0xc00000e2b8] [0xc00000e278 0xc00000e2a0 0xc00000e2b8] [0xc00000e290 0xc00000e2b0] [0x933040 0x933040] 0xc000f7f080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:18.109: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:18.297: INFO: rc: 1
Feb 20 18:20:18.297: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f50780 exit status 1 <nil> <nil> true [0xc00000e2c0 0xc00000e2e0 0xc00000e300] [0xc00000e2c0 0xc00000e2e0 0xc00000e300] [0xc00000e2d8 0xc00000e2f8] [0x933040 0x933040] 0xc000f7f440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:28.298: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:28.548: INFO: rc: 1
Feb 20 18:20:28.548: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009145d0 exit status 1 <nil> <nil> true [0xc0001b6a50 0xc0001b6ce8 0xc0001b71f8] [0xc0001b6a50 0xc0001b6ce8 0xc0001b71f8] [0xc0001b6b60 0xc0001b6f58] [0x933040 0x933040] 0xc000e13320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:38.548: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:38.712: INFO: rc: 1
Feb 20 18:20:38.712: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736570 exit status 1 <nil> <nil> true [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5190 0xc0000d5360] [0x933040 0x933040] 0xc001656720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:48.712: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:48.929: INFO: rc: 1
Feb 20 18:20:48.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009148d0 exit status 1 <nil> <nil> true [0xc0001b7208 0xc0001b7608 0xc0001b7768] [0xc0001b7208 0xc0001b7608 0xc0001b7768] [0xc0001b7458 0xc0001b7738] [0x933040 0x933040] 0xc000e13860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:20:58.930: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:20:59.107: INFO: rc: 1
Feb 20 18:20:59.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f50c60 exit status 1 <nil> <nil> true [0xc00000e308 0xc00000e328 0xc00000e340] [0xc00000e308 0xc00000e328 0xc00000e340] [0xc00000e320 0xc00000e338] [0x933040 0x933040] 0xc0012c0060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:09.108: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:21:09.247: INFO: rc: 1
Feb 20 18:21:09.247: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736810 exit status 1 <nil> <nil> true [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d54d8 0xc0000d5598] [0x933040 0x933040] 0xc001656ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:19.247: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:21:19.435: INFO: rc: 1
Feb 20 18:21:19.435: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f51230 exit status 1 <nil> <nil> true [0xc00000e360 0xc00000e388 0xc00000e3b0] [0xc00000e360 0xc00000e388 0xc00000e3b0] [0xc00000e380 0xc00000e3a0] [0x933040 0x933040] 0xc0012c0360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:29.435: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:21:29.641: INFO: rc: 1
Feb 20 18:21:29.641: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000736840 exit status 1 <nil> <nil> true [0xc0001b77f0 0xc0001b7988 0xc0001b7ac0] [0xc0001b77f0 0xc0001b7988 0xc0001b7ac0] [0xc0001b7900 0xc0001b7aa8] [0x933040 0x933040] 0xc001656cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:39.641: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:21:39.808: INFO: rc: 1
Feb 20 18:21:39.808: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009b82d0 exit status 1 <nil> <nil> true [0xc00052e078 0xc00052e0f0 0xc00052e128] [0xc00052e078 0xc00052e0f0 0xc00052e128] [0xc00052e0e0 0xc00052e120] [0x933040 0x933040] 0xc000f7eb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:49.808: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:21:49.992: INFO: rc: 1
Feb 20 18:21:49.993: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000914270 exit status 1 <nil> <nil> true [0xc0001b6000 0xc0001b68c0 0xc0001b6970] [0xc0001b6000 0xc0001b68c0 0xc0001b6970] [0xc0001b66f8 0xc0001b6950] [0x933040 0x933040] 0xc000916660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:21:59.993: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-r5nz6 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:22:00.205: INFO: rc: 1
Feb 20 18:22:00.205: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 20 18:22:00.205: INFO: Scaling statefulset ss to 0
Feb 20 18:22:00.256: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 18:22:00.273: INFO: Deleting all statefulset in ns e2e-tests-statefulset-r5nz6
Feb 20 18:22:00.289: INFO: Scaling statefulset ss to 0
Feb 20 18:22:00.339: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:22:00.356: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:22:00.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-r5nz6" for this suite.
Feb 20 18:22:08.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:08.833: INFO: namespace: e2e-tests-statefulset-r5nz6, resource: bindings, ignored listing per whitelist
Feb 20 18:22:09.105: INFO: namespace e2e-tests-statefulset-r5nz6 deletion completed in 8.681529027s

• [SLOW TEST:371.128 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:22:09.106: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-kr9mh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:22:10.151: INFO: (0) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 108.175747ms)
Feb 20 18:22:10.170: INFO: (1) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.653012ms)
Feb 20 18:22:10.188: INFO: (2) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.721928ms)
Feb 20 18:22:10.206: INFO: (3) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.000063ms)
Feb 20 18:22:10.224: INFO: (4) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.749454ms)
Feb 20 18:22:10.242: INFO: (5) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.994235ms)
Feb 20 18:22:10.260: INFO: (6) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.796815ms)
Feb 20 18:22:10.278: INFO: (7) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.090224ms)
Feb 20 18:22:10.296: INFO: (8) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.101275ms)
Feb 20 18:22:10.314: INFO: (9) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.869451ms)
Feb 20 18:22:10.332: INFO: (10) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.142119ms)
Feb 20 18:22:10.351: INFO: (11) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.297057ms)
Feb 20 18:22:10.369: INFO: (12) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.558122ms)
Feb 20 18:22:10.388: INFO: (13) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.076507ms)
Feb 20 18:22:10.406: INFO: (14) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.900399ms)
Feb 20 18:22:10.424: INFO: (15) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.939612ms)
Feb 20 18:22:10.441: INFO: (16) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.746951ms)
Feb 20 18:22:10.459: INFO: (17) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.94105ms)
Feb 20 18:22:10.478: INFO: (18) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.214285ms)
Feb 20 18:22:10.496: INFO: (19) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.857086ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:22:10.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-kr9mh" for this suite.
Feb 20 18:22:16.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:17.000: INFO: namespace: e2e-tests-proxy-kr9mh, resource: bindings, ignored listing per whitelist
Feb 20 18:22:17.221: INFO: namespace e2e-tests-proxy-kr9mh deletion completed in 6.709303826s

• [SLOW TEST:8.116 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:22:17.222: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2mlnm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:22:18.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-2mlnm" to be "success or failure"
Feb 20 18:22:18.069: INFO: Pod "downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.52492ms
Feb 20 18:22:20.086: INFO: Pod "downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032408835s
STEP: Saw pod success
Feb 20 18:22:20.086: INFO: Pod "downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:22:20.102: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 18:22:20.150: INFO: Waiting for pod downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:22:20.166: INFO: Pod downwardapi-volume-747fb129-353c-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:22:20.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2mlnm" for this suite.
Feb 20 18:22:26.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:26.312: INFO: namespace: e2e-tests-projected-2mlnm, resource: bindings, ignored listing per whitelist
Feb 20 18:22:26.883: INFO: namespace e2e-tests-projected-2mlnm deletion completed in 6.70005183s

• [SLOW TEST:9.661 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:22:26.884: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6v6sc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 18:22:30.355: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7a47bf2c-353c-11e9-9f03-4261fb5d5f3f"
Feb 20 18:22:30.355: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7a47bf2c-353c-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-pods-6v6sc" to be "terminated due to deadline exceeded"
Feb 20 18:22:30.371: INFO: Pod "pod-update-activedeadlineseconds-7a47bf2c-353c-11e9-9f03-4261fb5d5f3f": Phase="Running", Reason="", readiness=true. Elapsed: 16.02591ms
Feb 20 18:22:32.388: INFO: Pod "pod-update-activedeadlineseconds-7a47bf2c-353c-11e9-9f03-4261fb5d5f3f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.032909932s
Feb 20 18:22:32.388: INFO: Pod "pod-update-activedeadlineseconds-7a47bf2c-353c-11e9-9f03-4261fb5d5f3f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:22:32.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6v6sc" for this suite.
Feb 20 18:22:38.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:38.941: INFO: namespace: e2e-tests-pods-6v6sc, resource: bindings, ignored listing per whitelist
Feb 20 18:22:39.120: INFO: namespace e2e-tests-pods-6v6sc deletion completed in 6.715347909s

• [SLOW TEST:12.236 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:22:39.120: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2f8p9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0220 18:23:20.052928   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:23:20.053: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:23:20.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2f8p9" for this suite.
Feb 20 18:23:26.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:23:26.469: INFO: namespace: e2e-tests-gc-2f8p9, resource: bindings, ignored listing per whitelist
Feb 20 18:23:26.723: INFO: namespace e2e-tests-gc-2f8p9 deletion completed in 6.653647193s

• [SLOW TEST:47.603 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:23:26.724: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-6qkx7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-wt89
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:23:27.582: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wt89" in namespace "e2e-tests-subpath-6qkx7" to be "success or failure"
Feb 20 18:23:27.598: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Pending", Reason="", readiness=false. Elapsed: 15.817615ms
Feb 20 18:23:29.617: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034167112s
Feb 20 18:23:31.634: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 4.051637707s
Feb 20 18:23:33.652: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 6.069148152s
Feb 20 18:23:35.668: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 8.086095083s
Feb 20 18:23:37.686: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 10.103205362s
Feb 20 18:23:39.703: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 12.120349729s
Feb 20 18:23:41.721: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 14.138170127s
Feb 20 18:23:43.738: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 16.155358423s
Feb 20 18:23:45.755: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 18.172627621s
Feb 20 18:23:47.775: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 20.192474134s
Feb 20 18:23:49.792: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Running", Reason="", readiness=false. Elapsed: 22.210130579s
Feb 20 18:23:51.810: INFO: Pod "pod-subpath-test-downwardapi-wt89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.227744328s
STEP: Saw pod success
Feb 20 18:23:51.810: INFO: Pod "pod-subpath-test-downwardapi-wt89" satisfied condition "success or failure"
Feb 20 18:23:51.826: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-subpath-test-downwardapi-wt89 container test-container-subpath-downwardapi-wt89: <nil>
STEP: delete the pod
Feb 20 18:23:51.871: INFO: Waiting for pod pod-subpath-test-downwardapi-wt89 to disappear
Feb 20 18:23:51.887: INFO: Pod pod-subpath-test-downwardapi-wt89 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wt89
Feb 20 18:23:51.887: INFO: Deleting pod "pod-subpath-test-downwardapi-wt89" in namespace "e2e-tests-subpath-6qkx7"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:23:51.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6qkx7" for this suite.
Feb 20 18:23:57.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:23:58.475: INFO: namespace: e2e-tests-subpath-6qkx7, resource: bindings, ignored listing per whitelist
Feb 20 18:23:58.595: INFO: namespace e2e-tests-subpath-6qkx7 deletion completed in 6.674213275s

• [SLOW TEST:31.871 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:23:58.595: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-szjc6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 18:23:59.449: INFO: Waiting up to 5m0s for pod "pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-szjc6" to be "success or failure"
Feb 20 18:23:59.465: INFO: Pod "pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.904875ms
Feb 20 18:24:01.482: INFO: Pod "pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032476982s
STEP: Saw pod success
Feb 20 18:24:01.482: INFO: Pod "pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:24:01.498: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:24:01.635: INFO: Waiting for pod pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:24:01.695: INFO: Pod pod-b0ef8076-353c-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:24:01.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-szjc6" for this suite.
Feb 20 18:24:07.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:24:08.137: INFO: namespace: e2e-tests-emptydir-szjc6, resource: bindings, ignored listing per whitelist
Feb 20 18:24:08.407: INFO: namespace e2e-tests-emptydir-szjc6 deletion completed in 6.695847991s

• [SLOW TEST:9.813 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:24:08.408: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gblrq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-gblrq
Feb 20 18:24:11.282: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-gblrq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:24:11.298: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:28:11.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gblrq" for this suite.
Feb 20 18:28:17.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:28:17.995: INFO: namespace: e2e-tests-container-probe-gblrq, resource: bindings, ignored listing per whitelist
Feb 20 18:28:18.091: INFO: namespace e2e-tests-container-probe-gblrq deletion completed in 6.702612527s

• [SLOW TEST:249.684 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:28:18.092: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2hkzl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4bab8a7f-353d-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 18:28:19.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-2hkzl" to be "success or failure"
Feb 20 18:28:19.083: INFO: Pod "pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.44974ms
Feb 20 18:28:21.105: INFO: Pod "pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038008308s
Feb 20 18:28:23.125: INFO: Pod "pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057619241s
STEP: Saw pod success
Feb 20 18:28:23.125: INFO: Pod "pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:28:23.141: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:28:23.189: INFO: Waiting for pod pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:28:23.205: INFO: Pod pod-projected-configmaps-4bae0c8b-353d-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:28:23.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2hkzl" for this suite.
Feb 20 18:28:29.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:28:29.939: INFO: namespace: e2e-tests-projected-2hkzl, resource: bindings, ignored listing per whitelist
Feb 20 18:28:29.939: INFO: namespace e2e-tests-projected-2hkzl deletion completed in 6.717918864s

• [SLOW TEST:11.848 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:28:29.939: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-czxgp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 18:28:30.866: INFO: Number of nodes with available pods: 0
Feb 20 18:28:30.866: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:31.900: INFO: Number of nodes with available pods: 0
Feb 20 18:28:31.900: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:32.900: INFO: Number of nodes with available pods: 0
Feb 20 18:28:32.900: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:33.899: INFO: Number of nodes with available pods: 0
Feb 20 18:28:33.899: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:34.901: INFO: Number of nodes with available pods: 2
Feb 20 18:28:34.901: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 20 18:28:34.986: INFO: Number of nodes with available pods: 1
Feb 20 18:28:34.986: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:36.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:36.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:37.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:37.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:38.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:38.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:39.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:39.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:40.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:40.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:41.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:41.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:42.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:42.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:43.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:43.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:44.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:44.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:45.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:45.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:46.022: INFO: Number of nodes with available pods: 1
Feb 20 18:28:46.022: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:47.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:47.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:48.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:48.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:49.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:49.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:50.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:50.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:51.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:51.021: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:52.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:52.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:53.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:53.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:54.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:54.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:55.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:55.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:56.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:56.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:57.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:57.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:58.020: INFO: Number of nodes with available pods: 1
Feb 20 18:28:58.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:28:59.019: INFO: Number of nodes with available pods: 1
Feb 20 18:28:59.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:00.023: INFO: Number of nodes with available pods: 1
Feb 20 18:29:00.023: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:01.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:01.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:02.020: INFO: Number of nodes with available pods: 1
Feb 20 18:29:02.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:03.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:03.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:04.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:04.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:05.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:05.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:06.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:06.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:07.020: INFO: Number of nodes with available pods: 1
Feb 20 18:29:07.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:08.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:08.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:09.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:09.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:10.023: INFO: Number of nodes with available pods: 1
Feb 20 18:29:10.023: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:11.020: INFO: Number of nodes with available pods: 1
Feb 20 18:29:11.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:12.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:12.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:13.020: INFO: Number of nodes with available pods: 1
Feb 20 18:29:13.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:14.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:14.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:15.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:15.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:16.024: INFO: Number of nodes with available pods: 1
Feb 20 18:29:16.024: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:17.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:17.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:18.021: INFO: Number of nodes with available pods: 1
Feb 20 18:29:18.021: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:19.019: INFO: Number of nodes with available pods: 1
Feb 20 18:29:19.019: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:20.020: INFO: Number of nodes with available pods: 1
Feb 20 18:29:20.020: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:29:21.019: INFO: Number of nodes with available pods: 2
Feb 20 18:29:21.019: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-czxgp, will wait for the garbage collector to delete the pods
Feb 20 18:29:21.121: INFO: Deleting DaemonSet.extensions daemon-set took: 18.949085ms
Feb 20 18:29:21.221: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.247259ms
Feb 20 18:29:59.638: INFO: Number of nodes with available pods: 0
Feb 20 18:29:59.638: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 18:29:59.655: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-czxgp/daemonsets","resourceVersion":"5781"},"items":null}

Feb 20 18:29:59.671: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-czxgp/pods","resourceVersion":"5781"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:29:59.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-czxgp" for this suite.
Feb 20 18:30:05.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:30:06.039: INFO: namespace: e2e-tests-daemonsets-czxgp, resource: bindings, ignored listing per whitelist
Feb 20 18:30:06.440: INFO: namespace e2e-tests-daemonsets-czxgp deletion completed in 6.702945582s

• [SLOW TEST:96.501 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:30:06.440: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2flxk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:30:07.332: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:08.506: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 18:30:08.506: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 20 18:30:08.537: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 20 18:30:08.579: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 20 18:30:08.587: INFO: scanned /root for discovery docs: <nil>
Feb 20 18:30:08.587: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:19.761: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 18:30:19.761: INFO: stdout: "Created e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4\nScaling up e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 20 18:30:19.761: INFO: stdout: "Created e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4\nScaling up e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 20 18:30:19.761: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:19.931: INFO: stderr: ""
Feb 20 18:30:19.931: INFO: stdout: "e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw e2e-test-nginx-rc-g9xmb "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 20 18:30:24.932: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:25.102: INFO: stderr: ""
Feb 20 18:30:25.102: INFO: stdout: "e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw e2e-test-nginx-rc-g9xmb "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 20 18:30:30.102: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:30.294: INFO: stderr: ""
Feb 20 18:30:30.294: INFO: stdout: "e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw "
Feb 20 18:30:30.294: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:30.460: INFO: stderr: ""
Feb 20 18:30:30.460: INFO: stdout: "true"
Feb 20 18:30:30.460: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:30.620: INFO: stderr: ""
Feb 20 18:30:30.620: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 20 18:30:30.620: INFO: e2e-test-nginx-rc-c4d14e9040ce86742a907132203a71c4-hdqpw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 20 18:30:30.620: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-2flxk'
Feb 20 18:30:30.779: INFO: stderr: ""
Feb 20 18:30:30.779: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:30:30.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2flxk" for this suite.
Feb 20 18:30:36.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:30:37.115: INFO: namespace: e2e-tests-kubectl-2flxk, resource: bindings, ignored listing per whitelist
Feb 20 18:30:37.454: INFO: namespace e2e-tests-kubectl-2flxk deletion completed in 6.65875861s

• [SLOW TEST:31.014 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:30:37.455: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-vn4tm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-xz29
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:30:38.384: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xz29" in namespace "e2e-tests-subpath-vn4tm" to be "success or failure"
Feb 20 18:30:38.400: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Pending", Reason="", readiness=false. Elapsed: 16.090474ms
Feb 20 18:30:40.417: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033085987s
Feb 20 18:30:42.434: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 4.05045819s
Feb 20 18:30:44.451: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 6.067394269s
Feb 20 18:30:46.468: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 8.084458594s
Feb 20 18:30:48.485: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 10.101539505s
Feb 20 18:30:50.503: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 12.119332563s
Feb 20 18:30:52.520: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 14.136346913s
Feb 20 18:30:54.537: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 16.153598902s
Feb 20 18:30:56.554: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 18.170678177s
Feb 20 18:30:58.572: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 20.187863618s
Feb 20 18:31:00.589: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Running", Reason="", readiness=false. Elapsed: 22.205550021s
Feb 20 18:31:02.607: INFO: Pod "pod-subpath-test-secret-xz29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.222854899s
STEP: Saw pod success
Feb 20 18:31:02.607: INFO: Pod "pod-subpath-test-secret-xz29" satisfied condition "success or failure"
Feb 20 18:31:02.624: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-subpath-test-secret-xz29 container test-container-subpath-secret-xz29: <nil>
STEP: delete the pod
Feb 20 18:31:02.670: INFO: Waiting for pod pod-subpath-test-secret-xz29 to disappear
Feb 20 18:31:02.686: INFO: Pod pod-subpath-test-secret-xz29 no longer exists
STEP: Deleting pod pod-subpath-test-secret-xz29
Feb 20 18:31:02.686: INFO: Deleting pod "pod-subpath-test-secret-xz29" in namespace "e2e-tests-subpath-vn4tm"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:31:02.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-vn4tm" for this suite.
Feb 20 18:31:08.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:31:08.833: INFO: namespace: e2e-tests-subpath-vn4tm, resource: bindings, ignored listing per whitelist
Feb 20 18:31:09.380: INFO: namespace e2e-tests-subpath-vn4tm deletion completed in 6.660558188s

• [SLOW TEST:31.925 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:31:09.380: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-m7qz5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b1b68c06-353d-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:31:10.267: INFO: Waiting up to 5m0s for pod "pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-m7qz5" to be "success or failure"
Feb 20 18:31:10.292: INFO: Pod "pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.843337ms
Feb 20 18:31:12.309: INFO: Pod "pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041868076s
STEP: Saw pod success
Feb 20 18:31:12.309: INFO: Pod "pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:31:12.326: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:31:12.370: INFO: Waiting for pod pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:31:12.386: INFO: Pod pod-secrets-b1b91088-353d-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:31:12.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m7qz5" for this suite.
Feb 20 18:31:18.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:31:18.957: INFO: namespace: e2e-tests-secrets-m7qz5, resource: bindings, ignored listing per whitelist
Feb 20 18:31:19.118: INFO: namespace e2e-tests-secrets-m7qz5 deletion completed in 6.715564282s

• [SLOW TEST:9.738 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:31:19.118: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zd8b8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 18:31:20.194: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zd8b8'
Feb 20 18:31:20.682: INFO: stderr: ""
Feb 20 18:31:20.682: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 18:31:21.699: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:21.699: INFO: Found 0 / 1
Feb 20 18:31:22.699: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:22.699: INFO: Found 0 / 1
Feb 20 18:31:23.700: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:23.700: INFO: Found 0 / 1
Feb 20 18:31:24.700: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:24.700: INFO: Found 1 / 1
Feb 20 18:31:24.700: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 20 18:31:24.716: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:24.716: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 18:31:24.716: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml patch pod redis-master-7rf8w --namespace=e2e-tests-kubectl-zd8b8 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 20 18:31:24.902: INFO: stderr: ""
Feb 20 18:31:24.902: INFO: stdout: "pod/redis-master-7rf8w patched\n"
STEP: checking annotations
Feb 20 18:31:24.919: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:31:24.919: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:31:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zd8b8" for this suite.
Feb 20 18:31:46.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:31:47.324: INFO: namespace: e2e-tests-kubectl-zd8b8, resource: bindings, ignored listing per whitelist
Feb 20 18:31:47.644: INFO: namespace e2e-tests-kubectl-zd8b8 deletion completed in 22.708817069s

• [SLOW TEST:28.526 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:31:47.645: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-xvxr7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:31:50.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-xvxr7" for this suite.
Feb 20 18:32:42.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:32:42.897: INFO: namespace: e2e-tests-kubelet-test-xvxr7, resource: bindings, ignored listing per whitelist
Feb 20 18:32:43.323: INFO: namespace e2e-tests-kubelet-test-xvxr7 deletion completed in 52.700647567s

• [SLOW TEST:55.678 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:32:43.323: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-ncdjj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 18:32:54.473508   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:32:54.473: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:32:54.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-ncdjj" for this suite.
Feb 20 18:33:02.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:33:03.166: INFO: namespace: e2e-tests-gc-ncdjj, resource: bindings, ignored listing per whitelist
Feb 20 18:33:03.166: INFO: namespace e2e-tests-gc-ncdjj deletion completed in 8.676587597s

• [SLOW TEST:19.843 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:33:03.166: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bzhsd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-vbck
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:33:04.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vbck" in namespace "e2e-tests-subpath-bzhsd" to be "success or failure"
Feb 20 18:33:04.102: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024593ms
Feb 20 18:33:06.118: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032540614s
Feb 20 18:33:08.135: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 4.049345036s
Feb 20 18:33:10.152: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 6.066100901s
Feb 20 18:33:12.168: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 8.082435531s
Feb 20 18:33:14.185: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 10.099463283s
Feb 20 18:33:16.203: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 12.116798773s
Feb 20 18:33:18.220: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 14.133882672s
Feb 20 18:33:20.237: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 16.151349336s
Feb 20 18:33:22.254: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 18.168642781s
Feb 20 18:33:24.271: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 20.185405241s
Feb 20 18:33:26.288: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Running", Reason="", readiness=false. Elapsed: 22.202688539s
Feb 20 18:33:28.306: INFO: Pod "pod-subpath-test-configmap-vbck": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.219987086s
STEP: Saw pod success
Feb 20 18:33:28.306: INFO: Pod "pod-subpath-test-configmap-vbck" satisfied condition "success or failure"
Feb 20 18:33:28.322: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-subpath-test-configmap-vbck container test-container-subpath-configmap-vbck: <nil>
STEP: delete the pod
Feb 20 18:33:28.425: INFO: Waiting for pod pod-subpath-test-configmap-vbck to disappear
Feb 20 18:33:28.441: INFO: Pod pod-subpath-test-configmap-vbck no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vbck
Feb 20 18:33:28.441: INFO: Deleting pod "pod-subpath-test-configmap-vbck" in namespace "e2e-tests-subpath-bzhsd"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:33:28.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bzhsd" for this suite.
Feb 20 18:33:36.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:33:36.876: INFO: namespace: e2e-tests-subpath-bzhsd, resource: bindings, ignored listing per whitelist
Feb 20 18:33:37.133: INFO: namespace e2e-tests-subpath-bzhsd deletion completed in 8.658856507s

• [SLOW TEST:33.966 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:33:37.133: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-dpn7z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:33:38.012: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 20 18:33:38.044: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dpn7z/daemonsets","resourceVersion":"6516"},"items":null}

Feb 20 18:33:38.061: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dpn7z/pods","resourceVersion":"6516"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:33:38.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dpn7z" for this suite.
Feb 20 18:33:44.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:33:44.706: INFO: namespace: e2e-tests-daemonsets-dpn7z, resource: bindings, ignored listing per whitelist
Feb 20 18:33:44.785: INFO: namespace e2e-tests-daemonsets-dpn7z deletion completed in 6.658120049s

S [SKIPPING] [7.652 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 20 18:33:38.012: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:33:44.785: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2mczr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 18:33:45.646: INFO: Waiting up to 5m0s for pod "pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-2mczr" to be "success or failure"
Feb 20 18:33:45.661: INFO: Pod "pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.561442ms
Feb 20 18:33:47.678: INFO: Pod "pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.032485007s
Feb 20 18:33:49.695: INFO: Pod "pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049032196s
STEP: Saw pod success
Feb 20 18:33:49.695: INFO: Pod "pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:33:49.711: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:33:49.754: INFO: Waiting for pod pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:33:49.770: INFO: Pod pod-0e55eada-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:33:49.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2mczr" for this suite.
Feb 20 18:33:55.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:33:56.287: INFO: namespace: e2e-tests-emptydir-2mczr, resource: bindings, ignored listing per whitelist
Feb 20 18:33:56.447: INFO: namespace e2e-tests-emptydir-2mczr deletion completed in 6.660102013s

• [SLOW TEST:11.662 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:33:56.448: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sltbc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 18:33:59.976: INFO: Successfully updated pod "annotationupdate15504076-353e-11e9-9f03-4261fb5d5f3f"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:34:02.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sltbc" for this suite.
Feb 20 18:34:26.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:26.398: INFO: namespace: e2e-tests-projected-sltbc, resource: bindings, ignored listing per whitelist
Feb 20 18:34:26.783: INFO: namespace e2e-tests-projected-sltbc deletion completed in 24.740743089s

• [SLOW TEST:30.336 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:34:26.783: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-pp9k2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:34:27.665: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 18:34:29.698: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 18:34:29.793: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-pp9k2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pp9k2/deployments/test-cleanup-deployment,UID:28a00c7b-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:6683,Generation:1,CreationTimestamp:2019-02-20 18:34:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[{Progressing True 2019-02-20 18:34:29 +0000 UTC 2019-02-20 18:34:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-cleanup-deployment-7dbbfcf846"}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 18:34:29.809: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-pp9k2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pp9k2/replicasets/test-cleanup-deployment-7dbbfcf846,UID:28a6c6ae-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:6688,Generation:1,CreationTimestamp:2019-02-20 18:34:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 28a00c7b-353e-11e9-9fe1-e2ab716e6bf8 0xc001e4b7b7 0xc001e4b7b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:34:29.809: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 20 18:34:29.809: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-pp9k2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pp9k2/replicasets/test-cleanup-controller,UID:276031c7-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:6681,Generation:1,CreationTimestamp:2019-02-20 18:34:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 28a00c7b-353e-11e9-9fe1-e2ab716e6bf8 0xc001e4b6ef 0xc001e4b700}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 18:34:29.826: INFO: Pod "test-cleanup-controller-7zwkr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-7zwkr,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-pp9k2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pp9k2/pods/test-cleanup-controller-7zwkr,UID:2761e011-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:6677,Generation:0,CreationTimestamp:2019-02-20 18:34:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 276031c7-353e-11e9-9fe1-e2ab716e6bf8 0xc001f0802f 0xc001f08050}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cqhqc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cqhqc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cqhqc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f080b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f080d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.44,StartTime:2019-02-20 18:34:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:34:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://3884e9061e0d71eaaa81c3be46f9753759347cea89817b482c159d25d8dc203c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:34:29.826: INFO: Pod "test-cleanup-deployment-7dbbfcf846-jxtcr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-jxtcr,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-pp9k2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pp9k2/pods/test-cleanup-deployment-7dbbfcf846-jxtcr,UID:28a72df3-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:6690,Generation:0,CreationTimestamp:2019-02-20 18:34:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 28a6c6ae-353e-11e9-9fe1-e2ab716e6bf8 0xc001f08197 0xc001f08198}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cqhqc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cqhqc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cqhqc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f08200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f08220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC ContainersNotReady containers with unready status: [redis]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC ContainersNotReady containers with unready status: [redis]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:34:29 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:34:29 +0000 UTC,ContainerStatuses:[{redis {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 gcr.io/kubernetes-e2e-test-images/redis:1.0  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:34:29.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-pp9k2" for this suite.
Feb 20 18:34:35.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:36.320: INFO: namespace: e2e-tests-deployment-pp9k2, resource: bindings, ignored listing per whitelist
Feb 20 18:34:36.542: INFO: namespace e2e-tests-deployment-pp9k2 deletion completed in 6.700104584s

• [SLOW TEST:9.759 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:34:36.543: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xvhpg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 20 18:34:37.331: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix931004558/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:34:37.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xvhpg" for this suite.
Feb 20 18:34:43.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:43.775: INFO: namespace: e2e-tests-kubectl-xvhpg, resource: bindings, ignored listing per whitelist
Feb 20 18:34:44.065: INFO: namespace e2e-tests-kubectl-xvhpg deletion completed in 6.660191244s

• [SLOW TEST:7.522 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:34:44.065: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-j984x
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-31b16a28-353e-11e9-9f03-4261fb5d5f3f
STEP: Creating secret with name s-test-opt-upd-31b16a6b-353e-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-31b16a28-353e-11e9-9f03-4261fb5d5f3f
STEP: Updating secret s-test-opt-upd-31b16a6b-353e-11e9-9f03-4261fb5d5f3f
STEP: Creating secret with name s-test-opt-create-31b16a89-353e-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:34:49.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j984x" for this suite.
Feb 20 18:35:13.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:14.095: INFO: namespace: e2e-tests-secrets-j984x, resource: bindings, ignored listing per whitelist
Feb 20 18:35:14.191: INFO: namespace e2e-tests-secrets-j984x deletion completed in 24.704869572s

• [SLOW TEST:30.126 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:35:14.191: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6ssn6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:35:15.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-6ssn6" to be "success or failure"
Feb 20 18:35:15.083: INFO: Pod "downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.20306ms
Feb 20 18:35:17.100: INFO: Pod "downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033339639s
Feb 20 18:35:19.117: INFO: Pod "downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050257726s
STEP: Saw pod success
Feb 20 18:35:19.117: INFO: Pod "downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:35:19.133: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 18:35:19.213: INFO: Waiting for pod downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:35:19.229: INFO: Pod downwardapi-volume-43a258b4-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:35:19.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6ssn6" for this suite.
Feb 20 18:35:25.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:25.377: INFO: namespace: e2e-tests-projected-6ssn6, resource: bindings, ignored listing per whitelist
Feb 20 18:35:25.946: INFO: namespace e2e-tests-projected-6ssn6 deletion completed in 6.69989331s

• [SLOW TEST:11.755 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:35:25.946: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-n6fh7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 20 18:35:27.315: INFO: created pod pod-service-account-defaultsa
Feb 20 18:35:27.315: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 20 18:35:27.332: INFO: created pod pod-service-account-mountsa
Feb 20 18:35:27.332: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 20 18:35:27.349: INFO: created pod pod-service-account-nomountsa
Feb 20 18:35:27.349: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 20 18:35:27.366: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 20 18:35:27.366: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 20 18:35:27.382: INFO: created pod pod-service-account-mountsa-mountspec
Feb 20 18:35:27.383: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 20 18:35:27.399: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 20 18:35:27.399: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 20 18:35:27.415: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 20 18:35:27.415: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 20 18:35:27.432: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 20 18:35:27.432: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 20 18:35:27.448: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 20 18:35:27.448: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:35:27.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-n6fh7" for this suite.
Feb 20 18:35:35.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:35.761: INFO: namespace: e2e-tests-svcaccounts-n6fh7, resource: bindings, ignored listing per whitelist
Feb 20 18:35:36.216: INFO: namespace e2e-tests-svcaccounts-n6fh7 deletion completed in 8.750681828s

• [SLOW TEST:10.269 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:35:36.216: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-2bxt9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 18:35:37.030: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 18:35:37.067: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 18:35:37.084: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx before test
Feb 20 18:35:37.109: INFO: vpn-shoot-7888565cb8-lbjc9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 18:35:37.109: INFO: blackbox-exporter-d6c46f9fc-rkmrl from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 18:35:37.109: INFO: addons-kubernetes-dashboard-6579b646c5-dxmmz from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 18:35:37.109: INFO: kube-proxy-t5t2g from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:35:37.109: INFO: addons-nginx-ingress-controller-d74bfff57-dl2zg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 18:35:37.109: INFO: calico-node-9z44j from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:35:37.109: INFO: coredns-67df79bbdd-pj6m9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container coredns ready: true, restart count 0
Feb 20 18:35:37.109: INFO: addons-kube-lego-69bbdc96b6-mzrzg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 18:35:37.109: INFO: node-exporter-sqlkd from kube-system started at 2019-02-20 17:53:30 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:35:37.109: INFO: metrics-server-778b8c55bd-rltst from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 18:35:37.109: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-x8q7r from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.109: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 18:35:37.109: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf before test
Feb 20 18:35:37.160: INFO: calico-node-m6q9z from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.160: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:35:37.160: INFO: kube-proxy-xgmss from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.160: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:35:37.160: INFO: node-exporter-jrf9r from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:35:37.161: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15852647f6914178], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:35:38.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2bxt9" for this suite.
Feb 20 18:35:44.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:44.801: INFO: namespace: e2e-tests-sched-pred-2bxt9, resource: bindings, ignored listing per whitelist
Feb 20 18:35:44.959: INFO: namespace e2e-tests-sched-pred-2bxt9 deletion completed in 6.695496686s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:8.744 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:35:44.960: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-xtg8d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:35:47.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-xtg8d" for this suite.
Feb 20 18:36:27.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:28.073: INFO: namespace: e2e-tests-kubelet-test-xtg8d, resource: bindings, ignored listing per whitelist
Feb 20 18:36:28.642: INFO: namespace e2e-tests-kubelet-test-xtg8d deletion completed in 40.697825555s

• [SLOW TEST:43.682 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:36:28.642: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gpnbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 20 18:36:29.431: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-gpnbp'
Feb 20 18:36:29.830: INFO: stderr: ""
Feb 20 18:36:29.830: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 20 18:36:30.847: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:36:30.847: INFO: Found 0 / 1
Feb 20 18:36:31.848: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:36:31.848: INFO: Found 1 / 1
Feb 20 18:36:31.848: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 18:36:31.864: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:36:31.864: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 20 18:36:31.864: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp'
Feb 20 18:36:32.063: INFO: stderr: ""
Feb 20 18:36:32.063: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 18:36:30.903 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 18:36:30.903 # Server started, Redis version 3.2.12\n1:M 20 Feb 18:36:30.904 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 18:36:30.904 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 20 18:36:32.063: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp --tail=1'
Feb 20 18:36:32.266: INFO: stderr: ""
Feb 20 18:36:32.266: INFO: stdout: "1:M 20 Feb 18:36:30.904 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 20 18:36:32.266: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp --limit-bytes=1'
Feb 20 18:36:32.445: INFO: stderr: ""
Feb 20 18:36:32.445: INFO: stdout: " "
STEP: exposing timestamps
Feb 20 18:36:32.445: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp --tail=1 --timestamps'
Feb 20 18:36:32.712: INFO: stderr: ""
Feb 20 18:36:32.712: INFO: stdout: "2019-02-20T18:36:30.904396883Z 1:M 20 Feb 18:36:30.904 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 20 18:36:35.213: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp --since=1s'
Feb 20 18:36:35.421: INFO: stderr: ""
Feb 20 18:36:35.421: INFO: stdout: ""
Feb 20 18:36:35.421: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-sscgh redis-master --namespace=e2e-tests-kubectl-gpnbp --since=24h'
Feb 20 18:36:35.711: INFO: stderr: ""
Feb 20 18:36:35.712: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 18:36:30.903 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 18:36:30.903 # Server started, Redis version 3.2.12\n1:M 20 Feb 18:36:30.904 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 18:36:30.904 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 20 18:36:35.712: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gpnbp'
Feb 20 18:36:35.877: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:36:35.877: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 20 18:36:35.877: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-gpnbp'
Feb 20 18:36:36.065: INFO: stderr: "No resources found.\n"
Feb 20 18:36:36.065: INFO: stdout: ""
Feb 20 18:36:36.065: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-gpnbp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 18:36:36.206: INFO: stderr: ""
Feb 20 18:36:36.206: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:36:36.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gpnbp" for this suite.
Feb 20 18:36:42.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:42.519: INFO: namespace: e2e-tests-kubectl-gpnbp, resource: bindings, ignored listing per whitelist
Feb 20 18:36:42.917: INFO: namespace e2e-tests-kubectl-gpnbp deletion completed in 6.694824866s

• [SLOW TEST:14.275 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:36:42.918: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-vvmf5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 20 18:36:43.847: INFO: Waiting up to 5m0s for pod "var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-var-expansion-vvmf5" to be "success or failure"
Feb 20 18:36:43.862: INFO: Pod "var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.620502ms
Feb 20 18:36:45.879: INFO: Pod "var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032641275s
STEP: Saw pod success
Feb 20 18:36:45.880: INFO: Pod "var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:36:45.896: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:36:45.937: INFO: Waiting for pod var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:36:45.955: INFO: Pod var-expansion-788d3b33-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:36:45.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-vvmf5" for this suite.
Feb 20 18:36:52.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:52.497: INFO: namespace: e2e-tests-var-expansion-vvmf5, resource: bindings, ignored listing per whitelist
Feb 20 18:36:52.626: INFO: namespace e2e-tests-var-expansion-vvmf5 deletion completed in 6.654368616s

• [SLOW TEST:9.708 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:36:52.627: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-dszks
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:36:53.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-dszks" for this suite.
Feb 20 18:36:59.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:59.729: INFO: namespace: e2e-tests-services-dszks, resource: bindings, ignored listing per whitelist
Feb 20 18:37:00.262: INFO: namespace e2e-tests-services-dszks deletion completed in 6.70210404s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:7.636 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:37:00.263: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qt49z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:37:01.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-qt49z" to be "success or failure"
Feb 20 18:37:01.166: INFO: Pod "downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.645609ms
Feb 20 18:37:03.183: INFO: Pod "downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03253053s
Feb 20 18:37:05.200: INFO: Pod "downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049632404s
STEP: Saw pod success
Feb 20 18:37:05.200: INFO: Pod "downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:37:05.216: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 18:37:05.261: INFO: Waiting for pod downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:37:05.277: INFO: Pod downwardapi-volume-82dd8e00-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:37:05.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qt49z" for this suite.
Feb 20 18:37:11.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:11.679: INFO: namespace: e2e-tests-projected-qt49z, resource: bindings, ignored listing per whitelist
Feb 20 18:37:12.010: INFO: namespace e2e-tests-projected-qt49z deletion completed in 6.71681581s

• [SLOW TEST:11.748 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:37:12.010: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-pmjtg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 18:37:15.454: INFO: Successfully updated pod "pod-update-89d7a51e-353e-11e9-9f03-4261fb5d5f3f"
STEP: verifying the updated pod is in kubernetes
Feb 20 18:37:15.486: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:37:15.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-pmjtg" for this suite.
Feb 20 18:37:39.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:39.614: INFO: namespace: e2e-tests-pods-pmjtg, resource: bindings, ignored listing per whitelist
Feb 20 18:37:40.209: INFO: namespace e2e-tests-pods-pmjtg deletion completed in 24.70737605s

• [SLOW TEST:28.199 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:37:40.210: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-cnzw6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:37:41.030: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:37:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-cnzw6" for this suite.
Feb 20 18:37:49.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:50.367: INFO: namespace: e2e-tests-init-container-cnzw6, resource: bindings, ignored listing per whitelist
Feb 20 18:37:50.733: INFO: namespace e2e-tests-init-container-cnzw6 deletion completed in 6.813139401s

• [SLOW TEST:10.524 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:37:50.734: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-rnhx5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-rnhx5
Feb 20 18:37:55.684: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-rnhx5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:37:55.701: INFO: Initial restart count of pod liveness-http is 0
Feb 20 18:38:15.887: INFO: Restart count of pod e2e-tests-container-probe-rnhx5/liveness-http is now 1 (20.18653783s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:38:15.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rnhx5" for this suite.
Feb 20 18:38:21.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:38:22.191: INFO: namespace: e2e-tests-container-probe-rnhx5, resource: bindings, ignored listing per whitelist
Feb 20 18:38:22.627: INFO: namespace e2e-tests-container-probe-rnhx5 deletion completed in 6.702395202s

• [SLOW TEST:31.893 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:38:22.627: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-jblws
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-jblws/secret-test-b3ebd990-353e-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:38:23.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-jblws" to be "success or failure"
Feb 20 18:38:23.492: INFO: Pod "pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.137382ms
Feb 20 18:38:25.510: INFO: Pod "pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038697802s
Feb 20 18:38:27.528: INFO: Pod "pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057278566s
STEP: Saw pod success
Feb 20 18:38:27.528: INFO: Pod "pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:38:27.545: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f container env-test: <nil>
STEP: delete the pod
Feb 20 18:38:27.624: INFO: Waiting for pod pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:38:27.640: INFO: Pod pod-configmaps-b3ee7bca-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:38:27.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jblws" for this suite.
Feb 20 18:38:33.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:38:34.089: INFO: namespace: e2e-tests-secrets-jblws, resource: bindings, ignored listing per whitelist
Feb 20 18:38:34.314: INFO: namespace e2e-tests-secrets-jblws deletion completed in 6.657321511s

• [SLOW TEST:11.687 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:38:34.314: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-c8mkg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 18:38:35.135: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 18:38:35.168: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 18:38:35.184: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx before test
Feb 20 18:38:35.208: INFO: calico-node-9z44j from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:38:35.208: INFO: coredns-67df79bbdd-pj6m9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container coredns ready: true, restart count 0
Feb 20 18:38:35.208: INFO: addons-kube-lego-69bbdc96b6-mzrzg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 18:38:35.208: INFO: addons-nginx-ingress-controller-d74bfff57-dl2zg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 18:38:35.208: INFO: node-exporter-sqlkd from kube-system started at 2019-02-20 17:53:30 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:38:35.208: INFO: metrics-server-778b8c55bd-rltst from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 18:38:35.208: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-x8q7r from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 18:38:35.208: INFO: vpn-shoot-7888565cb8-lbjc9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 18:38:35.208: INFO: blackbox-exporter-d6c46f9fc-rkmrl from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 18:38:35.208: INFO: addons-kubernetes-dashboard-6579b646c5-dxmmz from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 18:38:35.208: INFO: kube-proxy-t5t2g from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.208: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:38:35.208: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf before test
Feb 20 18:38:35.254: INFO: calico-node-m6q9z from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.255: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:38:35.255: INFO: kube-proxy-xgmss from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.255: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:38:35.255: INFO: node-exporter-jrf9r from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:38:35.255: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
STEP: verifying the node has the label node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
Feb 20 18:38:35.359: INFO: Pod addons-kube-lego-69bbdc96b6-mzrzg requesting resource cpu=20m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod addons-kubernetes-dashboard-6579b646c5-dxmmz requesting resource cpu=50m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod addons-nginx-ingress-controller-d74bfff57-dl2zg requesting resource cpu=100m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-x8q7r requesting resource cpu=0m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod blackbox-exporter-d6c46f9fc-rkmrl requesting resource cpu=5m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod calico-node-9z44j requesting resource cpu=100m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod calico-node-m6q9z requesting resource cpu=100m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
Feb 20 18:38:35.359: INFO: Pod coredns-67df79bbdd-pj6m9 requesting resource cpu=50m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod kube-proxy-t5t2g requesting resource cpu=20m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod kube-proxy-xgmss requesting resource cpu=20m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
Feb 20 18:38:35.359: INFO: Pod metrics-server-778b8c55bd-rltst requesting resource cpu=20m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod node-exporter-jrf9r requesting resource cpu=5m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
Feb 20 18:38:35.359: INFO: Pod node-exporter-sqlkd requesting resource cpu=5m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
Feb 20 18:38:35.359: INFO: Pod vpn-shoot-7888565cb8-lbjc9 requesting resource cpu=50m on Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f.1585267171404fca], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-c8mkg/filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f to shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f.15852671a09fd58d], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f.15852671b7c137eb], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f.15852671bb38128b], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb07b4dc-353e-11e9-9f03-4261fb5d5f3f.15852671c47adc49], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f.15852671723d1b77], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-c8mkg/filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f to shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f.15852671a6f13f4b], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f.15852671bdeb92c8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f.15852671c0f323dc], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0ab974-353e-11e9-9f03-4261fb5d5f3f.15852671ca4f27a2], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15852672666ec9a5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:38:40.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-c8mkg" for this suite.
Feb 20 18:38:48.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:38:49.249: INFO: namespace: e2e-tests-sched-pred-c8mkg, resource: bindings, ignored listing per whitelist
Feb 20 18:38:49.329: INFO: namespace e2e-tests-sched-pred-c8mkg deletion completed in 8.697809436s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:15.015 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:38:49.329: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-6s4bx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 20 18:38:52.217: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-c3d611ef-353e-11e9-9f03-4261fb5d5f3f,GenerateName:,Namespace:e2e-tests-events-6s4bx,SelfLink:/api/v1/namespaces/e2e-tests-events-6s4bx/pods/send-events-c3d611ef-353e-11e9-9f03-4261fb5d5f3f,UID:c3d6f1dd-353e-11e9-9fe1-e2ab716e6bf8,ResourceVersion:7607,Generation:0,CreationTimestamp:2019-02-20 18:38:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 134172071,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.64/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bhdjt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bhdjt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-bhdjt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ac8990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ac89c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:38:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:38:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:38:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:38:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.64,StartTime:2019-02-20 18:38:50 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-20 18:38:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://002f1b6052869ab533e005ca5ae7883a208cb6a3f7ca2bb583a3d3f091934b51}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 20 18:38:54.234: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 20 18:38:56.251: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:38:56.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-6s4bx" for this suite.
Feb 20 18:39:36.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:39:36.557: INFO: namespace: e2e-tests-events-6s4bx, resource: bindings, ignored listing per whitelist
Feb 20 18:39:36.941: INFO: namespace e2e-tests-events-6s4bx deletion completed in 40.656786326s

• [SLOW TEST:47.612 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:39:36.942: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-m22hn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8wbmr in namespace e2e-tests-proxy-m22hn
I0220 18:39:37.769229   30262 runners.go:184] Created replication controller with name: proxy-service-8wbmr, namespace: e2e-tests-proxy-m22hn, replica count: 1
I0220 18:39:38.819673   30262 runners.go:184] proxy-service-8wbmr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 18:39:39.820269   30262 runners.go:184] proxy-service-8wbmr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 18:39:40.820614   30262 runners.go:184] proxy-service-8wbmr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:39:41.820940   30262 runners.go:184] proxy-service-8wbmr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 18:39:41.837: INFO: setup took 4.105916846s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 20 18:39:41.903: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 66.20966ms)
Feb 20 18:39:41.903: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 65.991299ms)
Feb 20 18:39:41.903: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 66.25755ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 66.259314ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 66.215273ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 66.542419ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 66.48467ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 66.278362ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 66.456224ms)
Feb 20 18:39:41.904: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 66.238471ms)
Feb 20 18:39:41.913: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 75.131219ms)
Feb 20 18:39:41.913: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 75.203201ms)
Feb 20 18:39:41.913: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 75.541065ms)
Feb 20 18:39:41.953: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 115.716812ms)
Feb 20 18:39:41.953: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 115.826888ms)
Feb 20 18:39:42.980: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 1.142437988s)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.720014ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.715501ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.903323ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.944482ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.907661ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.109229ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.891899ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.946703ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.021184ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.956573ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.249486ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.429348ms)
Feb 20 18:39:42.999: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.80603ms)
Feb 20 18:39:43.000: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.971897ms)
Feb 20 18:39:43.000: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 19.942229ms)
Feb 20 18:39:43.000: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.92589ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.881404ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.960491ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.927122ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.022998ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.990978ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.980632ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.109789ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.112006ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.068897ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.511785ms)
Feb 20 18:39:43.019: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.457034ms)
Feb 20 18:39:43.020: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.440405ms)
Feb 20 18:39:43.020: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.855833ms)
Feb 20 18:39:43.020: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 20.279174ms)
Feb 20 18:39:43.020: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.313397ms)
Feb 20 18:39:43.020: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 20.467905ms)
Feb 20 18:39:43.039: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 17.986823ms)
Feb 20 18:39:43.039: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.286574ms)
Feb 20 18:39:43.039: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.298619ms)
Feb 20 18:39:43.039: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.176117ms)
Feb 20 18:39:43.039: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.393377ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.640101ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 19.130966ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.719454ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.181192ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.876838ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.181965ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.530083ms)
Feb 20 18:39:43.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.780258ms)
Feb 20 18:39:43.041: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.832227ms)
Feb 20 18:39:43.041: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.287107ms)
Feb 20 18:39:43.042: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 21.171978ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.72052ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 18.974082ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.41682ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 18.858877ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.30872ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.107642ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 18.791708ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 18.652509ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.880435ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.561102ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 19.371117ms)
Feb 20 18:39:43.061: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.021207ms)
Feb 20 18:39:43.103: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 60.085128ms)
Feb 20 18:39:43.103: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 60.76032ms)
Feb 20 18:39:43.103: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 60.176472ms)
Feb 20 18:39:43.103: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 60.627197ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.744561ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.81986ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.747209ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.876387ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.984423ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.012719ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.954352ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.960257ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.879422ms)
Feb 20 18:39:43.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.03936ms)
Feb 20 18:39:43.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 20.398231ms)
Feb 20 18:39:43.165: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 61.461103ms)
Feb 20 18:39:43.165: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 61.501106ms)
Feb 20 18:39:43.165: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 61.493725ms)
Feb 20 18:39:43.165: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 61.563021ms)
Feb 20 18:39:43.165: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 61.576381ms)
Feb 20 18:39:43.184: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.414815ms)
Feb 20 18:39:43.184: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.444607ms)
Feb 20 18:39:43.184: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.634525ms)
Feb 20 18:39:43.184: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 19.744742ms)
Feb 20 18:39:43.184: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.677863ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.741589ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 19.599986ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.802894ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.853708ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.70696ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.881852ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.628222ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.702423ms)
Feb 20 18:39:43.185: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 20.32497ms)
Feb 20 18:39:43.186: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 21.099241ms)
Feb 20 18:39:43.186: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 21.249812ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 20.105101ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.656843ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 19.97819ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.748609ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 20.162955ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 20.447782ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 20.306649ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 20.087855ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.93516ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 20.271302ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 20.414867ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 20.650606ms)
Feb 20 18:39:43.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 20.232167ms)
Feb 20 18:39:43.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 20.941473ms)
Feb 20 18:39:43.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.671962ms)
Feb 20 18:39:43.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 20.643089ms)
Feb 20 18:39:43.226: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.443348ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.472836ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.681841ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.514727ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.516721ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.661281ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.83702ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.638555ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.702005ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 18.611864ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.66069ms)
Feb 20 18:39:43.227: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.427808ms)
Feb 20 18:39:43.228: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.993358ms)
Feb 20 18:39:43.228: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.955658ms)
Feb 20 18:39:43.228: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.173108ms)
Feb 20 18:39:43.228: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.940788ms)
Feb 20 18:39:43.246: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.106894ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.460147ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.276684ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.40773ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.307567ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.665253ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.428993ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.475833ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.44998ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.553312ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.659509ms)
Feb 20 18:39:43.247: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 18.824616ms)
Feb 20 18:39:43.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 18.975412ms)
Feb 20 18:39:43.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.138795ms)
Feb 20 18:39:43.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.637531ms)
Feb 20 18:39:43.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 19.958769ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 17.542657ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.492667ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.050112ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.346982ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.807218ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 17.907467ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.018396ms)
Feb 20 18:39:43.268: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.047066ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 17.747034ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 18.324574ms)
Feb 20 18:39:43.267: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 17.845344ms)
Feb 20 18:39:43.268: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.25503ms)
Feb 20 18:39:43.308: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 59.724604ms)
Feb 20 18:39:43.308: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 59.505671ms)
Feb 20 18:39:43.308: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 59.634281ms)
Feb 20 18:39:43.309: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 60.019047ms)
Feb 20 18:39:43.328: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.296773ms)
Feb 20 18:39:43.328: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.688423ms)
Feb 20 18:39:43.328: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.384357ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 19.818093ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.809614ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.645364ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.907736ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 19.999668ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.830248ms)
Feb 20 18:39:43.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.996338ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 48.114831ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 48.083438ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 47.788143ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 48.152723ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 48.038886ms)
Feb 20 18:39:43.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 47.758729ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.780789ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.830413ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.36631ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 19.685936ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.457478ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 19.237767ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 19.24069ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.371549ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.112682ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.420021ms)
Feb 20 18:39:43.377: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 19.902974ms)
Feb 20 18:39:43.378: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.972743ms)
Feb 20 18:39:43.378: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 20.574687ms)
Feb 20 18:39:43.378: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.885965ms)
Feb 20 18:39:43.378: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 20.550783ms)
Feb 20 18:39:43.378: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 21.38629ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.065064ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.103183ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.254336ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.106626ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.105393ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.373813ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.177333ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.309406ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.264506ms)
Feb 20 18:39:43.397: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.245722ms)
Feb 20 18:39:43.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.072202ms)
Feb 20 18:39:43.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.206257ms)
Feb 20 18:39:43.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 19.209371ms)
Feb 20 18:39:43.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.816937ms)
Feb 20 18:39:43.399: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.991068ms)
Feb 20 18:39:43.399: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.968626ms)
Feb 20 18:39:43.418: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.508984ms)
Feb 20 18:39:43.418: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.782943ms)
Feb 20 18:39:43.418: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.219642ms)
Feb 20 18:39:43.418: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 19.033643ms)
Feb 20 18:39:43.418: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.362777ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 20.118607ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 19.286019ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.634663ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.55002ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 19.757355ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 20.181577ms)
Feb 20 18:39:43.419: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 19.922795ms)
Feb 20 18:39:43.420: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 20.508332ms)
Feb 20 18:39:43.420: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 20.119458ms)
Feb 20 18:39:43.420: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 20.200142ms)
Feb 20 18:39:43.420: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 20.705889ms)
Feb 20 18:39:43.438: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.227593ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 17.688905ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 17.884272ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.539196ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.725922ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 17.850866ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.501624ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.882171ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.423092ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.268141ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 18.37489ms)
Feb 20 18:39:43.439: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.353202ms)
Feb 20 18:39:43.440: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 18.963576ms)
Feb 20 18:39:43.440: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.870529ms)
Feb 20 18:39:43.440: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.193955ms)
Feb 20 18:39:43.440: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 18.883029ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.577428ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.464382ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.88976ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 19.164786ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.368785ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 18.684914ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.341343ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.900307ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.492782ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 19.341288ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 19.305274ms)
Feb 20 18:39:43.459: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.026509ms)
Feb 20 18:39:43.500: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 59.510781ms)
Feb 20 18:39:43.501: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 60.29516ms)
Feb 20 18:39:43.501: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 59.766229ms)
Feb 20 18:39:43.501: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 59.948112ms)
Feb 20 18:39:43.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 17.968754ms)
Feb 20 18:39:43.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.326048ms)
Feb 20 18:39:43.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.575069ms)
Feb 20 18:39:43.519: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.195067ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 17.990471ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.927776ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.481415ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.552032ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.114997ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.422085ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.876755ms)
Feb 20 18:39:43.520: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 19.146404ms)
Feb 20 18:39:43.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 19.57974ms)
Feb 20 18:39:43.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.949177ms)
Feb 20 18:39:43.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 20.660692ms)
Feb 20 18:39:43.521: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.072371ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 17.510783ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 17.236492ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 17.486466ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.24499ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.038014ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 17.597898ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 17.444183ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 17.52004ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.240775ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.157383ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 17.711084ms)
Feb 20 18:39:43.540: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.404792ms)
Feb 20 18:39:43.541: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 19.108402ms)
Feb 20 18:39:43.542: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 19.933262ms)
Feb 20 18:39:43.542: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.021011ms)
Feb 20 18:39:43.542: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 20.295577ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 17.991778ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:160/proxy/: foo (200; 18.008978ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.051187ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:460/proxy/: tls baz (200; 18.302756ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:1080/proxy/... (200; 18.386995ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq:1080/proxy/rewri... (200; 18.198769ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:462/proxy/: tls qux (200; 18.284685ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname2/proxy/: tls qux (200; 18.962066ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/proxy-service-8wbmr-5vjmq/proxy/rewriteme"... (200; 18.349286ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/https:proxy-service-8wbmr:tlsportname1/proxy/: tls baz (200; 18.380797ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m22hn/pods/https:proxy-service-8wbmr-5vjmq:443/proxy/... (200; 18.742072ms)
Feb 20 18:39:43.561: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/pods/http:proxy-service-8wbmr-5vjmq:162/proxy/: bar (200; 18.860586ms)
Feb 20 18:39:43.563: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname2/proxy/: bar (200; 20.140353ms)
Feb 20 18:39:43.563: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname1/proxy/: foo (200; 20.355388ms)
Feb 20 18:39:43.563: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/http:proxy-service-8wbmr:portname2/proxy/: bar (200; 20.591849ms)
Feb 20 18:39:43.563: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m22hn/services/proxy-service-8wbmr:portname1/proxy/: foo (200; 20.81743ms)
STEP: deleting ReplicationController proxy-service-8wbmr in namespace e2e-tests-proxy-m22hn, will wait for the garbage collector to delete the pods
Feb 20 18:39:43.648: INFO: Deleting ReplicationController proxy-service-8wbmr took: 18.558589ms
Feb 20 18:39:43.748: INFO: Terminating ReplicationController proxy-service-8wbmr pods took: 100.225896ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:39:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-m22hn" for this suite.
Feb 20 18:39:51.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:39:52.280: INFO: namespace: e2e-tests-proxy-m22hn, resource: bindings, ignored listing per whitelist
Feb 20 18:39:52.314: INFO: namespace e2e-tests-proxy-m22hn deletion completed in 6.64804876s

• [SLOW TEST:15.372 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:39:52.314: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2x4p6
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 20 18:39:53.153: INFO: Waiting up to 5m0s for pod "pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-2x4p6" to be "success or failure"
Feb 20 18:39:53.170: INFO: Pod "pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.363828ms
Feb 20 18:39:55.187: INFO: Pod "pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034348174s
STEP: Saw pod success
Feb 20 18:39:55.187: INFO: Pod "pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:39:55.204: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:39:55.246: INFO: Waiting for pod pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:39:55.261: INFO: Pod pod-e96321dd-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:39:55.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2x4p6" for this suite.
Feb 20 18:40:01.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:01.878: INFO: namespace: e2e-tests-emptydir-2x4p6, resource: bindings, ignored listing per whitelist
Feb 20 18:40:01.957: INFO: namespace e2e-tests-emptydir-2x4p6 deletion completed in 6.680177946s

• [SLOW TEST:9.643 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:40:01.958: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4q6mq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-ef2b8fb4-353e-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 18:40:02.872: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-4q6mq" to be "success or failure"
Feb 20 18:40:02.887: INFO: Pod "pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.794003ms
Feb 20 18:40:04.904: INFO: Pod "pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03227497s
Feb 20 18:40:06.921: INFO: Pod "pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049138718s
STEP: Saw pod success
Feb 20 18:40:06.921: INFO: Pod "pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:40:06.937: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:40:06.983: INFO: Waiting for pod pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:40:06.998: INFO: Pod pod-projected-configmaps-ef2e08f2-353e-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:40:06.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4q6mq" for this suite.
Feb 20 18:40:13.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:13.720: INFO: namespace: e2e-tests-projected-4q6mq, resource: bindings, ignored listing per whitelist
Feb 20 18:40:13.799: INFO: namespace e2e-tests-projected-4q6mq deletion completed in 6.784178652s

• [SLOW TEST:11.842 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:40:13.800: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-9htfx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 18:40:18.788: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:40:18.804: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:40:20.807: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:40:20.825: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:40:22.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:40:22.822: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:40:24.806: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:40:24.823: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:40:26.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:40:26.821: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:40:26.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-9htfx" for this suite.
Feb 20 18:40:50.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:51.554: INFO: namespace: e2e-tests-container-lifecycle-hook-9htfx, resource: bindings, ignored listing per whitelist
Feb 20 18:40:51.570: INFO: namespace e2e-tests-container-lifecycle-hook-9htfx deletion completed in 24.652243767s

• [SLOW TEST:37.771 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:40:51.570: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-wdl27
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:40:52.428: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:40:56.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-wdl27" for this suite.
Feb 20 18:41:20.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:20.598: INFO: namespace: e2e-tests-init-container-wdl27, resource: bindings, ignored listing per whitelist
Feb 20 18:41:21.114: INFO: namespace e2e-tests-init-container-wdl27 deletion completed in 24.772674346s

• [SLOW TEST:29.544 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:41:21.115: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dxqfz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 20 18:41:24.362: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-1e7ec41f-353f-11e9-9f03-4261fb5d5f3f", GenerateName:"", Namespace:"e2e-tests-pods-dxqfz", SelfLink:"/api/v1/namespaces/e2e-tests-pods-dxqfz/pods/pod-submit-remove-1e7ec41f-353f-11e9-9f03-4261fb5d5f3f", UID:"1e865be0-353f-11e9-9fe1-e2ab716e6bf8", ResourceVersion:"8051", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686284882, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"234683532"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.71/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9x9vr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001f35880), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9x9vr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001edeaf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007b4660), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001edeb30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001edeb50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001edeb58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001edeb5c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686284882, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686284883, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686284883, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686284882, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.2", PodIP:"100.96.1.71", StartTime:(*v1.Time)(0xc0021c5640), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0021c56e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://4aaddf5656f9ba95fbbfdb6695b5c769cdb35e5141b015c3086b48560123ecd9"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 20 18:41:29.402: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:41:29.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dxqfz" for this suite.
Feb 20 18:41:35.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:35.649: INFO: namespace: e2e-tests-pods-dxqfz, resource: bindings, ignored listing per whitelist
Feb 20 18:41:36.220: INFO: namespace e2e-tests-pods-dxqfz deletion completed in 6.78174831s

• [SLOW TEST:15.105 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:41:36.220: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b46c5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 18:41:37.053: INFO: Waiting up to 5m0s for pod "pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-b46c5" to be "success or failure"
Feb 20 18:41:37.096: INFO: Pod "pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.034723ms
Feb 20 18:41:39.113: INFO: Pod "pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059901023s
STEP: Saw pod success
Feb 20 18:41:39.113: INFO: Pod "pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:41:39.130: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:41:39.172: INFO: Waiting for pod pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:41:39.188: INFO: Pod pod-2750fd6d-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:41:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b46c5" for this suite.
Feb 20 18:41:45.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:45.604: INFO: namespace: e2e-tests-emptydir-b46c5, resource: bindings, ignored listing per whitelist
Feb 20 18:41:45.928: INFO: namespace e2e-tests-emptydir-b46c5 deletion completed in 6.723509837s

• [SLOW TEST:9.709 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:41:45.929: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-24fs6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-2d2857ef-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:41:46.870: INFO: Waiting up to 5m0s for pod "pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-24fs6" to be "success or failure"
Feb 20 18:41:46.886: INFO: Pod "pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.047985ms
Feb 20 18:41:48.903: INFO: Pod "pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032835746s
Feb 20 18:41:50.920: INFO: Pod "pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049925815s
STEP: Saw pod success
Feb 20 18:41:50.920: INFO: Pod "pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:41:50.936: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:41:51.022: INFO: Waiting for pod pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:41:51.038: INFO: Pod pod-secrets-2d2ae455-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:41:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-24fs6" for this suite.
Feb 20 18:41:59.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:59.747: INFO: namespace: e2e-tests-secrets-24fs6, resource: bindings, ignored listing per whitelist
Feb 20 18:41:59.835: INFO: namespace e2e-tests-secrets-24fs6 deletion completed in 8.778302848s

• [SLOW TEST:13.906 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:41:59.835: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qr29l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:42:00.632: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml version --client'
Feb 20 18:42:00.691: INFO: stderr: ""
Feb 20 18:42:00.691: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T18:04:32Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 20 18:42:00.706: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-qr29l'
Feb 20 18:42:02.338: INFO: stderr: ""
Feb 20 18:42:02.338: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 20 18:42:02.338: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-qr29l'
Feb 20 18:42:02.609: INFO: stderr: ""
Feb 20 18:42:02.609: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 18:42:03.626: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:42:03.626: INFO: Found 0 / 1
Feb 20 18:42:04.626: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:42:04.626: INFO: Found 1 / 1
Feb 20 18:42:04.626: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 18:42:04.643: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:42:04.643: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 18:42:04.643: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe pod redis-master-l8psl --namespace=e2e-tests-kubectl-qr29l'
Feb 20 18:42:04.831: INFO: stderr: ""
Feb 20 18:42:04.831: INFO: stdout: "Name:               redis-master-l8psl\nNamespace:          e2e-tests-kubectl-qr29l\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf/10.250.0.2\nStart Time:         Wed, 20 Feb 2019 18:42:02 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.74/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.74\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9368f4d25fdf3f32f99bce09147d6e6d86afbf24661823b5378bcf769211a342\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 20 Feb 2019 18:42:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rrs7n (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rrs7n:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rrs7n\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                              Message\n  ----    ------     ----  ----                                                              -------\n  Normal  Scheduled  2s    default-scheduler                                                 Successfully assigned e2e-tests-kubectl-qr29l/redis-master-l8psl to shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf\n  Normal  Pulled     1s    kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Created container\n  Normal  Started    1s    kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf  Started container\n"
Feb 20 18:42:04.831: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-qr29l'
Feb 20 18:42:05.034: INFO: stderr: ""
Feb 20 18:42:05.034: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-qr29l\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-l8psl\n"
Feb 20 18:42:05.035: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-qr29l'
Feb 20 18:42:05.276: INFO: stderr: ""
Feb 20 18:42:05.276: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-qr29l\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.70.18.33\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.74:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 20 18:42:05.293: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx'
Feb 20 18:42:05.566: INFO: stderr: ""
Feb 20 18:42:05.566: INFO: stdout: "Name:               shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west1\n                    failure-domain.beta.kubernetes.io/zone=europe-west1-b\n                    kubernetes.io/hostname=shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.3/32\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 20 Feb 2019 17:53:29 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 20 Feb 2019 17:53:49 +0000   Wed, 20 Feb 2019 17:53:49 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 20 Feb 2019 18:42:04 +0000   Wed, 20 Feb 2019 17:53:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 20 Feb 2019 18:42:04 +0000   Wed, 20 Feb 2019 17:53:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 20 Feb 2019 18:42:04 +0000   Wed, 20 Feb 2019 17:53:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 20 Feb 2019 18:42:04 +0000   Wed, 20 Feb 2019 17:53:49 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.0.3\n  ExternalIP:   35.195.205.186\n  InternalDNS:  shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx.c.sap-se-gcp-scp-k8s-dev.internal\n  Hostname:     shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx.c.sap-se-gcp-scp-k8s-dev.internal\nCapacity:\n attachable-volumes-gce-pd:  64\n cpu:                        4\n ephemeral-storage:          17897500Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     15396260Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  64\n cpu:                        3920m\n ephemeral-storage:          17410687987\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     13299108Ki\n pods:                       110\nSystem Info:\n Machine ID:                 93aa45afe39d541eef25b4585069280f\n System UUID:                93AA45AF-E39D-541E-EF25-B4585069280F\n Boot ID:                    a614938e-13c8-4a8d-9523-36ce146a8530\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     100.96.0.0/24\nProviderID:                  gce://sap-se-gcp-scp-k8s-dev/europe-west1-b/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kube-lego-69bbdc96b6-mzrzg                                 20m (0%)      50m (1%)    8Mi (0%)         32Mi (0%)      49m\n  kube-system                addons-kubernetes-dashboard-6579b646c5-dxmmz                      50m (1%)      100m (2%)   50Mi (0%)        256Mi (1%)     49m\n  kube-system                addons-nginx-ingress-controller-d74bfff57-dl2zg                   100m (2%)     2 (51%)     100Mi (0%)       800Mi (6%)     49m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-x8q7r    0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                blackbox-exporter-d6c46f9fc-rkmrl                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      49m\n  kube-system                calico-node-9z44j                                                 100m (2%)     500m (12%)  100Mi (0%)       700Mi (5%)     48m\n  kube-system                coredns-67df79bbdd-pj6m9                                          50m (1%)      100m (2%)   15Mi (0%)        100Mi (0%)     49m\n  kube-system                kube-proxy-t5t2g                                                  20m (0%)      900m (22%)  64Mi (0%)        200Mi (1%)     48m\n  kube-system                metrics-server-778b8c55bd-rltst                                   20m (0%)      80m (2%)    100Mi (0%)       400Mi (3%)     49m\n  kube-system                node-exporter-sqlkd                                               5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)      48m\n  kube-system                vpn-shoot-7888565cb8-lbjc9                                        50m (1%)      100m (2%)   50Mi (0%)        100Mi (0%)     49m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        420m (10%)  3855m (98%)\n  memory                     502Mi (3%)  2673Mi (20%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-gce-pd  0           0\nEvents:\n  Type    Reason                   Age   From                                                                 Message\n  ----    ------                   ----  ----                                                                 -------\n  Normal  Starting                 48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Starting kubelet.\n  Normal  NodeHasSufficientMemory  48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Updated Node Allocatable limit across pods\n  Normal  Starting                 48m   kube-proxy, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx  Starting kube-proxy.\n  Normal  NodeReady                48m   kubelet, shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx     Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx status is now: NodeReady\n"
Feb 20 18:42:05.566: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe namespace e2e-tests-kubectl-qr29l'
Feb 20 18:42:05.801: INFO: stderr: ""
Feb 20 18:42:05.801: INFO: stdout: "Name:         e2e-tests-kubectl-qr29l\nLabels:       e2e-framework=kubectl\n              e2e-run=71f5f6f5-353a-11e9-9f03-4261fb5d5f3f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:42:05.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qr29l" for this suite.
Feb 20 18:42:29.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:42:29.984: INFO: namespace: e2e-tests-kubectl-qr29l, resource: bindings, ignored listing per whitelist
Feb 20 18:42:30.480: INFO: namespace e2e-tests-kubectl-qr29l deletion completed in 24.662164042s

• [SLOW TEST:30.645 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:42:30.480: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-m6jzt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-47ae408d-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 18:42:31.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-m6jzt" to be "success or failure"
Feb 20 18:42:31.383: INFO: Pod "pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.668165ms
Feb 20 18:42:33.400: INFO: Pod "pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032250025s
Feb 20 18:42:35.417: INFO: Pod "pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049090586s
STEP: Saw pod success
Feb 20 18:42:35.417: INFO: Pod "pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:42:35.433: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:42:35.478: INFO: Waiting for pod pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:42:35.494: INFO: Pod pod-configmaps-47b0b911-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:42:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-m6jzt" for this suite.
Feb 20 18:42:41.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:42:41.840: INFO: namespace: e2e-tests-configmap-m6jzt, resource: bindings, ignored listing per whitelist
Feb 20 18:42:42.223: INFO: namespace e2e-tests-configmap-m6jzt deletion completed in 6.712198548s

• [SLOW TEST:11.743 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:42:42.223: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-8kdkx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8kdkx.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8kdkx.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8kdkx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8kdkx.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8kdkx.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8kdkx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 18:42:58.416: INFO: DNS probes using e2e-tests-dns-8kdkx/dns-test-4ea6dbb9-353f-11e9-9f03-4261fb5d5f3f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:42:58.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8kdkx" for this suite.
Feb 20 18:43:04.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:43:04.858: INFO: namespace: e2e-tests-dns-8kdkx, resource: bindings, ignored listing per whitelist
Feb 20 18:43:05.147: INFO: namespace e2e-tests-dns-8kdkx deletion completed in 6.694457544s

• [SLOW TEST:22.924 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:43:05.148: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-xdkvh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:44:05.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-xdkvh" for this suite.
Feb 20 18:44:30.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:44:30.618: INFO: namespace: e2e-tests-container-probe-xdkvh, resource: bindings, ignored listing per whitelist
Feb 20 18:44:30.651: INFO: namespace e2e-tests-container-probe-xdkvh deletion completed in 24.652502123s

• [SLOW TEST:85.504 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:44:30.652: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-xsxnx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 18:44:31.437: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 18:44:31.469: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 18:44:31.484: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx before test
Feb 20 18:44:31.510: INFO: node-exporter-sqlkd from kube-system started at 2019-02-20 17:53:30 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:44:31.510: INFO: metrics-server-778b8c55bd-rltst from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 18:44:31.510: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-x8q7r from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 18:44:31.510: INFO: vpn-shoot-7888565cb8-lbjc9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 18:44:31.510: INFO: blackbox-exporter-d6c46f9fc-rkmrl from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 18:44:31.510: INFO: addons-kubernetes-dashboard-6579b646c5-dxmmz from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 18:44:31.510: INFO: kube-proxy-t5t2g from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:44:31.510: INFO: addons-nginx-ingress-controller-d74bfff57-dl2zg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 18:44:31.510: INFO: calico-node-9z44j from kube-system started at 2019-02-20 17:53:29 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:44:31.510: INFO: coredns-67df79bbdd-pj6m9 from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container coredns ready: true, restart count 0
Feb 20 18:44:31.510: INFO: addons-kube-lego-69bbdc96b6-mzrzg from kube-system started at 2019-02-20 17:53:49 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.510: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 18:44:31.510: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf before test
Feb 20 18:44:31.603: INFO: kube-proxy-xgmss from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.603: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:44:31.603: INFO: node-exporter-jrf9r from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.603: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:44:31.603: INFO: calico-node-m6q9z from kube-system started at 2019-02-20 17:53:35 +0000 UTC (1 container statuses recorded)
Feb 20 18:44:31.603: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-909c7e3a-353f-11e9-9f03-4261fb5d5f3f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-909c7e3a-353f-11e9-9f03-4261fb5d5f3f off the node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf
STEP: verifying the node doesn't have the label kubernetes.io/e2e-909c7e3a-353f-11e9-9f03-4261fb5d5f3f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:44:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-xsxnx" for this suite.
Feb 20 18:44:47.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:44:48.336: INFO: namespace: e2e-tests-sched-pred-xsxnx, resource: bindings, ignored listing per whitelist
Feb 20 18:44:48.509: INFO: namespace e2e-tests-sched-pred-xsxnx deletion completed in 12.651819621s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:17.857 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:44:48.509: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-22nc4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 20 18:44:53.482: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:53.482: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:54.079: INFO: Exec stderr: ""
Feb 20 18:44:54.079: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:54.079: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:54.590: INFO: Exec stderr: ""
Feb 20 18:44:54.590: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:54.590: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:55.104: INFO: Exec stderr: ""
Feb 20 18:44:55.104: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:55.104: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:55.611: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 20 18:44:55.611: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:55.611: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:56.116: INFO: Exec stderr: ""
Feb 20 18:44:56.116: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:56.116: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:56.663: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 20 18:44:56.663: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:56.664: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:57.230: INFO: Exec stderr: ""
Feb 20 18:44:57.230: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:57.230: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:57.794: INFO: Exec stderr: ""
Feb 20 18:44:57.794: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:57.794: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:58.381: INFO: Exec stderr: ""
Feb 20 18:44:58.381: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-22nc4 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:44:58.381: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:44:58.919: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:44:58.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-22nc4" for this suite.
Feb 20 18:45:48.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:45:49.538: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-22nc4, resource: bindings, ignored listing per whitelist
Feb 20 18:45:49.726: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-22nc4 deletion completed in 50.788886903s

• [SLOW TEST:61.217 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:45:49.726: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-s2sgg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-be69620a-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 18:45:50.565: INFO: Waiting up to 5m0s for pod "pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-s2sgg" to be "success or failure"
Feb 20 18:45:50.583: INFO: Pod "pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.04132ms
Feb 20 18:45:52.600: INFO: Pod "pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034285234s
STEP: Saw pod success
Feb 20 18:45:52.600: INFO: Pod "pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:45:52.616: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:45:52.660: INFO: Waiting for pod pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:45:52.677: INFO: Pod pod-configmaps-be6be6b4-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:45:52.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-s2sgg" for this suite.
Feb 20 18:45:58.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:45:59.264: INFO: namespace: e2e-tests-configmap-s2sgg, resource: bindings, ignored listing per whitelist
Feb 20 18:45:59.389: INFO: namespace e2e-tests-configmap-s2sgg deletion completed in 6.695581684s

• [SLOW TEST:9.663 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:45:59.389: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-bvn9b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:46:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-bvn9b" for this suite.
Feb 20 18:46:10.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:46:10.996: INFO: namespace: e2e-tests-kubelet-test-bvn9b, resource: bindings, ignored listing per whitelist
Feb 20 18:46:11.060: INFO: namespace e2e-tests-kubelet-test-bvn9b deletion completed in 6.658205448s

• [SLOW TEST:11.671 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:46:11.060: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-t6f65
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:46:11.930: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-t6f65'
Feb 20 18:46:12.213: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 18:46:12.213: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 20 18:46:12.229: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-t6f65'
Feb 20 18:46:12.394: INFO: stderr: ""
Feb 20 18:46:12.394: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:46:12.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t6f65" for this suite.
Feb 20 18:46:18.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:46:18.782: INFO: namespace: e2e-tests-kubectl-t6f65, resource: bindings, ignored listing per whitelist
Feb 20 18:46:19.081: INFO: namespace e2e-tests-kubectl-t6f65 deletion completed in 6.669977686s

• [SLOW TEST:8.021 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:46:19.081: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-vt8sq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f
Feb 20 18:46:19.995: INFO: Pod name my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f: Found 1 pods out of 1
Feb 20 18:46:19.996: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f" are running
Feb 20 18:46:22.028: INFO: Pod "my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f-4r549" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 18:46:19 +0000 UTC Reason: Message:}])
Feb 20 18:46:22.028: INFO: Trying to dial the pod
Feb 20 18:46:27.186: INFO: Controller my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f: Got expected result from replica 1 [my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f-4r549]: "my-hostname-basic-cff0e2aa-353f-11e9-9f03-4261fb5d5f3f-4r549", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:46:27.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-vt8sq" for this suite.
Feb 20 18:46:35.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:46:35.383: INFO: namespace: e2e-tests-replication-controller-vt8sq, resource: bindings, ignored listing per whitelist
Feb 20 18:46:35.863: INFO: namespace e2e-tests-replication-controller-vt8sq deletion completed in 8.659305568s

• [SLOW TEST:16.782 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:46:35.863: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-8bht7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-8bht7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-8bht7 to expose endpoints map[]
Feb 20 18:46:36.878: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-8bht7 exposes endpoints map[] (15.757951ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-8bht7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-8bht7 to expose endpoints map[pod1:[100]]
Feb 20 18:46:38.994: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-8bht7 exposes endpoints map[pod1:[100]] (2.097215875s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-8bht7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-8bht7 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 20 18:46:41.158: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-8bht7 exposes endpoints map[pod1:[100] pod2:[101]] (2.146096973s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-8bht7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-8bht7 to expose endpoints map[pod2:[101]]
Feb 20 18:46:41.207: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-8bht7 exposes endpoints map[pod2:[101]] (31.264995ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-8bht7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-8bht7 to expose endpoints map[]
Feb 20 18:46:41.239: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-8bht7 exposes endpoints map[] (15.492869ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:46:41.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-8bht7" for this suite.
Feb 20 18:46:47.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:46:47.520: INFO: namespace: e2e-tests-services-8bht7, resource: bindings, ignored listing per whitelist
Feb 20 18:46:47.982: INFO: namespace e2e-tests-services-8bht7 deletion completed in 6.704142385s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:12.119 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:46:47.983: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-69p9j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-e128fad4-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating secret with name secret-projected-all-test-volume-e128fabc-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 20 18:46:48.881: INFO: Waiting up to 5m0s for pod "projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-69p9j" to be "success or failure"
Feb 20 18:46:48.898: INFO: Pod "projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.301047ms
Feb 20 18:46:50.914: INFO: Pod "projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032569442s
STEP: Saw pod success
Feb 20 18:46:50.914: INFO: Pod "projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:46:50.930: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 20 18:46:51.023: INFO: Waiting for pod projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:46:51.039: INFO: Pod projected-volume-e128fa89-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:46:51.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-69p9j" for this suite.
Feb 20 18:46:57.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:46:57.551: INFO: namespace: e2e-tests-projected-69p9j, resource: bindings, ignored listing per whitelist
Feb 20 18:46:57.754: INFO: namespace e2e-tests-projected-69p9j deletion completed in 6.697783355s

• [SLOW TEST:9.771 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:46:57.754: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-t2lhp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:46:58.565: INFO: Waiting up to 5m0s for pod "downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-t2lhp" to be "success or failure"
Feb 20 18:46:58.581: INFO: Pod "downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.617914ms
Feb 20 18:47:00.598: INFO: Pod "downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032934151s
STEP: Saw pod success
Feb 20 18:47:00.598: INFO: Pod "downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:47:00.614: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:47:00.657: INFO: Waiting for pod downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:47:00.673: INFO: Pod downward-api-e6f3cf19-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:47:00.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-t2lhp" for this suite.
Feb 20 18:47:06.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:47:06.909: INFO: namespace: e2e-tests-downward-api-t2lhp, resource: bindings, ignored listing per whitelist
Feb 20 18:47:07.389: INFO: namespace e2e-tests-downward-api-t2lhp deletion completed in 6.699508272s

• [SLOW TEST:9.635 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:47:07.389: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-t7lnx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-ecba290f-353f-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:47:08.270: INFO: Waiting up to 5m0s for pod "pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-t7lnx" to be "success or failure"
Feb 20 18:47:08.286: INFO: Pod "pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.822003ms
Feb 20 18:47:10.303: INFO: Pod "pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033539599s
STEP: Saw pod success
Feb 20 18:47:10.303: INFO: Pod "pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:47:10.319: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:47:10.362: INFO: Waiting for pod pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:47:10.377: INFO: Pod pod-secrets-ecbcab1a-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:47:10.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t7lnx" for this suite.
Feb 20 18:47:16.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:47:16.736: INFO: namespace: e2e-tests-secrets-t7lnx, resource: bindings, ignored listing per whitelist
Feb 20 18:47:17.118: INFO: namespace e2e-tests-secrets-t7lnx deletion completed in 6.724238338s

• [SLOW TEST:9.729 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:47:17.119: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nprw7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:47:17.959: INFO: Waiting up to 5m0s for pod "downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-nprw7" to be "success or failure"
Feb 20 18:47:17.975: INFO: Pod "downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.854503ms
Feb 20 18:47:19.992: INFO: Pod "downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032648446s
STEP: Saw pod success
Feb 20 18:47:19.992: INFO: Pod "downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:47:20.009: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:47:20.053: INFO: Waiting for pod downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:47:20.069: INFO: Pod downward-api-f2831828-353f-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:47:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nprw7" for this suite.
Feb 20 18:47:26.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:47:26.197: INFO: namespace: e2e-tests-downward-api-nprw7, resource: bindings, ignored listing per whitelist
Feb 20 18:47:26.739: INFO: namespace e2e-tests-downward-api-nprw7 deletion completed in 6.65406038s

• [SLOW TEST:9.621 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:47:26.740: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6qckl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:47:27.637: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml version'
Feb 20 18:47:27.844: INFO: stderr: ""
Feb 20 18:47:27.844: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T18:04:32Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:47:27.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6qckl" for this suite.
Feb 20 18:47:33.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:47:34.474: INFO: namespace: e2e-tests-kubectl-6qckl, resource: bindings, ignored listing per whitelist
Feb 20 18:47:34.554: INFO: namespace e2e-tests-kubectl-6qckl deletion completed in 6.69273127s

• [SLOW TEST:7.815 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:47:34.555: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-6kcxf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:47:35.440: INFO: PodSpec: initContainers in spec.initContainers
Feb 20 18:48:20.007: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fcf17547-353f-11e9-9f03-4261fb5d5f3f", GenerateName:"", Namespace:"e2e-tests-init-container-6kcxf", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-6kcxf/pods/pod-init-fcf17547-353f-11e9-9f03-4261fb5d5f3f", UID:"fcf23cf4-353f-11e9-9fe1-e2ab716e6bf8", ResourceVersion:"9291", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686285255, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"440523657", "name":"foo"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.90/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-m5xvf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0017e60c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m5xvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m5xvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m5xvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0003436f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000f7f0e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000343800)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000343840)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000343848), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00034384c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285255, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285255, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285255, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285255, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.2", PodIP:"100.96.1.90", StartTime:(*v1.Time)(0xc000f02460), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00034a230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00034a2a0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://617fdb62bab38ede56496956ccc46ae62cc96fabce1ece78cb9f2f77185347fc"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f024a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f02480), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:48:20.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-6kcxf" for this suite.
Feb 20 18:48:44.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:48:44.466: INFO: namespace: e2e-tests-init-container-6kcxf, resource: bindings, ignored listing per whitelist
Feb 20 18:48:44.752: INFO: namespace e2e-tests-init-container-6kcxf deletion completed in 24.728623525s

• [SLOW TEST:70.198 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:48:44.753: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-ptrvl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ptrvl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 18:48:45.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 18:49:12.043: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.92:8080/dial?request=hostName&protocol=udp&host=100.96.0.26&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ptrvl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:49:12.043: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:49:12.586: INFO: Waiting for endpoints: map[]
Feb 20 18:49:12.603: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.92:8080/dial?request=hostName&protocol=udp&host=100.96.1.91&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ptrvl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:49:12.603: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:49:13.138: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:49:13.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ptrvl" for this suite.
Feb 20 18:49:37.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:49:37.626: INFO: namespace: e2e-tests-pod-network-test-ptrvl, resource: bindings, ignored listing per whitelist
Feb 20 18:49:37.820: INFO: namespace e2e-tests-pod-network-test-ptrvl deletion completed in 24.664826439s

• [SLOW TEST:53.067 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:49:37.820: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vf2ls
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-465f8187-3540-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:49:38.672: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-vf2ls" to be "success or failure"
Feb 20 18:49:38.688: INFO: Pod "pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.924435ms
Feb 20 18:49:40.705: INFO: Pod "pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032952813s
STEP: Saw pod success
Feb 20 18:49:40.706: INFO: Pod "pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:49:40.722: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:49:40.766: INFO: Waiting for pod pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:49:40.783: INFO: Pod pod-projected-secrets-4662426a-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:49:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vf2ls" for this suite.
Feb 20 18:49:46.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:49:46.998: INFO: namespace: e2e-tests-projected-vf2ls, resource: bindings, ignored listing per whitelist
Feb 20 18:49:47.489: INFO: namespace e2e-tests-projected-vf2ls deletion completed in 6.68928911s

• [SLOW TEST:9.669 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:49:47.489: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-np4rb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 20 18:49:48.354: INFO: Waiting up to 5m0s for pod "client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-containers-np4rb" to be "success or failure"
Feb 20 18:49:48.370: INFO: Pod "client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.132013ms
Feb 20 18:49:50.387: INFO: Pod "client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03354253s
Feb 20 18:49:52.405: INFO: Pod "client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050817401s
STEP: Saw pod success
Feb 20 18:49:52.405: INFO: Pod "client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:49:52.422: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:49:52.471: INFO: Waiting for pod client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:49:52.487: INFO: Pod client-containers-4c278e96-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:49:52.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-np4rb" for this suite.
Feb 20 18:49:58.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:49:58.956: INFO: namespace: e2e-tests-containers-np4rb, resource: bindings, ignored listing per whitelist
Feb 20 18:49:59.210: INFO: namespace e2e-tests-containers-np4rb deletion completed in 6.706432101s

• [SLOW TEST:11.721 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:49:59.210: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fk6x4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:50:00.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-fk6x4" to be "success or failure"
Feb 20 18:50:00.080: INFO: Pod "downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.047756ms
Feb 20 18:50:02.097: INFO: Pod "downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03406088s
STEP: Saw pod success
Feb 20 18:50:02.097: INFO: Pod "downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:50:02.113: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 18:50:02.157: INFO: Waiting for pod downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:50:02.173: INFO: Pod downwardapi-volume-5321f1bd-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:50:02.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fk6x4" for this suite.
Feb 20 18:50:10.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:50:10.729: INFO: namespace: e2e-tests-projected-fk6x4, resource: bindings, ignored listing per whitelist
Feb 20 18:50:10.907: INFO: namespace e2e-tests-projected-fk6x4 deletion completed in 8.717228678s

• [SLOW TEST:11.697 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:50:10.907: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-ksqpm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-ksqpm
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 20 18:50:11.886: INFO: Found 1 stateful pods, waiting for 3
Feb 20 18:50:21.906: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:50:21.906: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:50:21.906: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:50:21.957: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ksqpm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:50:22.711: INFO: stderr: ""
Feb 20 18:50:22.711: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:50:22.711: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 18:50:32.823: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 20 18:50:32.875: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ksqpm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:50:33.540: INFO: stderr: ""
Feb 20 18:50:33.540: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:50:33.540: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:50:43.641: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:50:43.641: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:50:43.641: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:50:43.641: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:50:53.701: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:50:53.701: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:50:53.701: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:51:03.675: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:51:03.675: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:51:13.675: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:51:13.675: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 20 18:51:23.675: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ksqpm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:51:24.406: INFO: stderr: ""
Feb 20 18:51:24.406: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:51:24.406: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:51:24.483: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 20 18:51:24.533: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ksqpm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:51:25.289: INFO: stderr: ""
Feb 20 18:51:25.289: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:51:25.289: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:51:35.395: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:51:35.395: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:51:35.395: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:51:45.429: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:51:45.429: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:51:45.429: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:51:55.429: INFO: Waiting for StatefulSet e2e-tests-statefulset-ksqpm/ss2 to complete update
Feb 20 18:51:55.429: INFO: Waiting for Pod e2e-tests-statefulset-ksqpm/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 18:52:15.429: INFO: Deleting all statefulset in ns e2e-tests-statefulset-ksqpm
Feb 20 18:52:15.445: INFO: Scaling statefulset ss2 to 0
Feb 20 18:52:35.512: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:52:35.529: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:52:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-ksqpm" for this suite.
Feb 20 18:52:41.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:52:42.337: INFO: namespace: e2e-tests-statefulset-ksqpm, resource: bindings, ignored listing per whitelist
Feb 20 18:52:42.337: INFO: namespace e2e-tests-statefulset-ksqpm deletion completed in 6.741130116s

• [SLOW TEST:151.430 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:52:42.337: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-lcfmv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:52:43.142: INFO: Creating ReplicaSet my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f
Feb 20 18:52:43.176: INFO: Pod name my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f: Found 1 pods out of 1
Feb 20 18:52:43.176: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f" is running
Feb 20 18:52:45.211: INFO: Pod "my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f-9gf4w" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 18:52:43 +0000 UTC Reason: Message:}])
Feb 20 18:52:45.211: INFO: Trying to dial the pod
Feb 20 18:52:50.346: INFO: Controller my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f: Got expected result from replica 1 [my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f-9gf4w]: "my-hostname-basic-b458c824-3540-11e9-9f03-4261fb5d5f3f-9gf4w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:52:50.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-lcfmv" for this suite.
Feb 20 18:52:56.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:52:57.027: INFO: namespace: e2e-tests-replicaset-lcfmv, resource: bindings, ignored listing per whitelist
Feb 20 18:52:57.059: INFO: namespace e2e-tests-replicaset-lcfmv deletion completed in 6.695953018s

• [SLOW TEST:14.722 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:52:57.060: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-k877w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 20 18:52:57.954: INFO: Waiting up to 5m0s for pod "client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-containers-k877w" to be "success or failure"
Feb 20 18:52:57.969: INFO: Pod "client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.753869ms
Feb 20 18:52:59.986: INFO: Pod "client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032009723s
STEP: Saw pod success
Feb 20 18:52:59.986: INFO: Pod "client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:53:00.002: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:53:00.046: INFO: Waiting for pod client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:53:00.062: INFO: Pod client-containers-bd2a39f4-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:53:00.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-k877w" for this suite.
Feb 20 18:53:08.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:08.374: INFO: namespace: e2e-tests-containers-k877w, resource: bindings, ignored listing per whitelist
Feb 20 18:53:08.775: INFO: namespace e2e-tests-containers-k877w deletion completed in 8.695810969s

• [SLOW TEST:11.715 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:53:08.775: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5z8tr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-5z8tr/configmap-test-c4324224-3540-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 18:53:09.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-5z8tr" to be "success or failure"
Feb 20 18:53:09.796: INFO: Pod "pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.210432ms
Feb 20 18:53:11.812: INFO: Pod "pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044126377s
STEP: Saw pod success
Feb 20 18:53:11.812: INFO: Pod "pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:53:11.827: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f container env-test: <nil>
STEP: delete the pod
Feb 20 18:53:11.868: INFO: Waiting for pod pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:53:11.884: INFO: Pod pod-configmaps-c434ca8c-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:53:11.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5z8tr" for this suite.
Feb 20 18:53:17.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:18.507: INFO: namespace: e2e-tests-configmap-5z8tr, resource: bindings, ignored listing per whitelist
Feb 20 18:53:18.619: INFO: namespace e2e-tests-configmap-5z8tr deletion completed in 6.718055672s

• [SLOW TEST:9.844 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:53:18.619: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-z6zn9
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-ca0cf56a-3540-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:53:21.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-z6zn9" for this suite.
Feb 20 18:53:45.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:46.293: INFO: namespace: e2e-tests-configmap-z6zn9, resource: bindings, ignored listing per whitelist
Feb 20 18:53:46.410: INFO: namespace e2e-tests-configmap-z6zn9 deletion completed in 24.661314883s

• [SLOW TEST:27.791 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:53:46.410: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-4jc7g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 18:53:47.354: INFO: Waiting up to 5m0s for pod "pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-4jc7g" to be "success or failure"
Feb 20 18:53:47.371: INFO: Pod "pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.425177ms
Feb 20 18:53:49.388: INFO: Pod "pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033670843s
STEP: Saw pod success
Feb 20 18:53:49.388: INFO: Pod "pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:53:49.404: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:53:49.445: INFO: Waiting for pod pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:53:49.461: INFO: Pod pod-da9c154a-3540-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:53:49.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4jc7g" for this suite.
Feb 20 18:53:55.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:56.001: INFO: namespace: e2e-tests-emptydir-4jc7g, resource: bindings, ignored listing per whitelist
Feb 20 18:53:56.190: INFO: namespace e2e-tests-emptydir-4jc7g deletion completed in 6.712072628s

• [SLOW TEST:9.781 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:53:56.191: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-hjzdh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:53:57.133: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:54:01.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hjzdh" for this suite.
Feb 20 18:54:47.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:54:48.359: INFO: namespace: e2e-tests-pods-hjzdh, resource: bindings, ignored listing per whitelist
Feb 20 18:54:48.407: INFO: namespace e2e-tests-pods-hjzdh deletion completed in 46.792928392s

• [SLOW TEST:52.216 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:54:48.407: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jszvx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 20 18:54:49.233: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml api-versions'
Feb 20 18:54:49.610: INFO: stderr: ""
Feb 20 18:54:49.610: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:54:49.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jszvx" for this suite.
Feb 20 18:54:55.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:54:55.878: INFO: namespace: e2e-tests-kubectl-jszvx, resource: bindings, ignored listing per whitelist
Feb 20 18:54:56.392: INFO: namespace e2e-tests-kubectl-jszvx deletion completed in 6.763964913s

• [SLOW TEST:7.985 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:54:56.392: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zs5tl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-044636a1-3541-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:54:57.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-zs5tl" to be "success or failure"
Feb 20 18:54:57.288: INFO: Pod "pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.598163ms
Feb 20 18:54:59.304: INFO: Pod "pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032017475s
STEP: Saw pod success
Feb 20 18:54:59.304: INFO: Pod "pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:54:59.320: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:54:59.365: INFO: Waiting for pod pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:54:59.381: INFO: Pod pod-projected-secrets-0448bad8-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:54:59.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zs5tl" for this suite.
Feb 20 18:55:05.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:55:05.637: INFO: namespace: e2e-tests-projected-zs5tl, resource: bindings, ignored listing per whitelist
Feb 20 18:55:06.136: INFO: namespace e2e-tests-projected-zs5tl deletion completed in 6.738824542s

• [SLOW TEST:9.744 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:55:06.137: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-w9qlz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 18:55:07.054: INFO: Waiting up to 5m0s for pod "pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-w9qlz" to be "success or failure"
Feb 20 18:55:07.070: INFO: Pod "pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.995807ms
Feb 20 18:55:09.087: INFO: Pod "pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033468105s
STEP: Saw pod success
Feb 20 18:55:09.087: INFO: Pod "pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:55:09.103: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 18:55:09.145: INFO: Waiting for pod pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:55:09.161: INFO: Pod pod-0a1d5943-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:55:09.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-w9qlz" for this suite.
Feb 20 18:55:17.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:55:17.827: INFO: namespace: e2e-tests-emptydir-w9qlz, resource: bindings, ignored listing per whitelist
Feb 20 18:55:17.860: INFO: namespace e2e-tests-emptydir-w9qlz deletion completed in 8.661283568s

• [SLOW TEST:11.723 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:55:17.860: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-ngfhb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:55:18.739: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:55:20.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-ngfhb" for this suite.
Feb 20 18:56:01.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:56:01.545: INFO: namespace: e2e-tests-pods-ngfhb, resource: bindings, ignored listing per whitelist
Feb 20 18:56:01.674: INFO: namespace e2e-tests-pods-ngfhb deletion completed in 40.658858192s

• [SLOW TEST:43.814 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:56:01.674: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-pcn25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:56:12.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-pcn25" for this suite.
Feb 20 18:56:36.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:56:37.261: INFO: namespace: e2e-tests-replication-controller-pcn25, resource: bindings, ignored listing per whitelist
Feb 20 18:56:37.310: INFO: namespace e2e-tests-replication-controller-pcn25 deletion completed in 24.662837492s

• [SLOW TEST:35.636 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:56:37.310: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-c5jlx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-c5jlx
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 18:56:38.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 18:57:02.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.30:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-c5jlx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:57:02.423: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:57:02.973: INFO: Found all expected endpoints: [netserver-0]
Feb 20 18:57:02.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.112:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-c5jlx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:57:02.990: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:57:03.482: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:57:03.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-c5jlx" for this suite.
Feb 20 18:57:27.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:57:27.793: INFO: namespace: e2e-tests-pod-network-test-c5jlx, resource: bindings, ignored listing per whitelist
Feb 20 18:57:28.162: INFO: namespace e2e-tests-pod-network-test-c5jlx deletion completed in 24.663039298s

• [SLOW TEST:50.852 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:57:28.162: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vrj2r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-5ec2025c-3541-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 18:57:29.078: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-vrj2r" to be "success or failure"
Feb 20 18:57:29.094: INFO: Pod "pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.869132ms
Feb 20 18:57:31.111: INFO: Pod "pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032873522s
STEP: Saw pod success
Feb 20 18:57:31.111: INFO: Pod "pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 18:57:31.128: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:57:31.177: INFO: Waiting for pod pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 18:57:31.193: INFO: Pod pod-projected-secrets-5ec49312-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:57:31.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vrj2r" for this suite.
Feb 20 18:57:37.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:57:37.730: INFO: namespace: e2e-tests-projected-vrj2r, resource: bindings, ignored listing per whitelist
Feb 20 18:57:37.925: INFO: namespace e2e-tests-projected-vrj2r deletion completed in 6.714193548s

• [SLOW TEST:9.763 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:57:37.925: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6dvxn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 20 18:57:38.736: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:57:38.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6dvxn" for this suite.
Feb 20 18:57:45.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:57:45.719: INFO: namespace: e2e-tests-kubectl-6dvxn, resource: bindings, ignored listing per whitelist
Feb 20 18:57:45.752: INFO: namespace e2e-tests-kubectl-6dvxn deletion completed in 6.799747793s

• [SLOW TEST:7.827 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:57:45.753: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bs54x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:57:46.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bs54x" for this suite.
Feb 20 18:58:10.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:11.274: INFO: namespace: e2e-tests-pods-bs54x, resource: bindings, ignored listing per whitelist
Feb 20 18:58:11.385: INFO: namespace e2e-tests-pods-bs54x deletion completed in 24.698409242s

• [SLOW TEST:25.632 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:58:11.385: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-cnvh2
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:58:12.251: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:58:13.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-cnvh2" for this suite.
Feb 20 18:58:19.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:19.712: INFO: namespace: e2e-tests-custom-resource-definition-cnvh2, resource: bindings, ignored listing per whitelist
Feb 20 18:58:19.776: INFO: namespace e2e-tests-custom-resource-definition-cnvh2 deletion completed in 6.730077144s

• [SLOW TEST:8.391 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:58:19.776: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-7x2fb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:58:20.633: INFO: Creating deployment "test-recreate-deployment"
Feb 20 18:58:20.649: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 20 18:58:20.682: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 20 18:58:20.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285900, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285900, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285900, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686285900, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:58:22.715: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 20 18:58:22.749: INFO: Updating deployment test-recreate-deployment
Feb 20 18:58:22.749: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 18:58:22.901: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-7x2fb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7x2fb/deployments/test-recreate-deployment,UID:7d827225-3541-11e9-9fe1-e2ab716e6bf8,ResourceVersion:11142,Generation:2,CreationTimestamp:2019-02-20 18:58:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-20 18:58:22 +0000 UTC 2019-02-20 18:58:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 18:58:22 +0000 UTC 2019-02-20 18:58:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 18:58:22.918: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-7x2fb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7x2fb/replicasets/test-recreate-deployment-697fbf54bf,UID:7ec8189b-3541-11e9-9fe1-e2ab716e6bf8,ResourceVersion:11141,Generation:1,CreationTimestamp:2019-02-20 18:58:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 7d827225-3541-11e9-9fe1-e2ab716e6bf8 0xc00180cd87 0xc00180cd88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:58:22.918: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 20 18:58:22.918: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-7x2fb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7x2fb/replicasets/test-recreate-deployment-5dfdcc846d,UID:7d83919a-3541-11e9-9fe1-e2ab716e6bf8,ResourceVersion:11133,Generation:2,CreationTimestamp:2019-02-20 18:58:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 7d827225-3541-11e9-9fe1-e2ab716e6bf8 0xc00180ccc7 0xc00180ccc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:58:22.935: INFO: Pod "test-recreate-deployment-697fbf54bf-c9wqd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-c9wqd,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-7x2fb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-7x2fb/pods/test-recreate-deployment-697fbf54bf-c9wqd,UID:7ec88ba0-3541-11e9-9fe1-e2ab716e6bf8,ResourceVersion:11140,Generation:0,CreationTimestamp:2019-02-20 18:58:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 7ec8189b-3541-11e9-9fe1-e2ab716e6bf8 0xc00180d5d7 0xc00180d5d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4zmh2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4zmh2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4zmh2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180d640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180d660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:58:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:58:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:58:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:58:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:58:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:58:22.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-7x2fb" for this suite.
Feb 20 18:58:29.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:29.335: INFO: namespace: e2e-tests-deployment-7x2fb, resource: bindings, ignored listing per whitelist
Feb 20 18:58:29.604: INFO: namespace e2e-tests-deployment-7x2fb deletion completed in 6.65199039s

• [SLOW TEST:9.827 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 18:58:29.604: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-tpt7z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:58:30.513: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 18:58:30.562: INFO: Number of nodes with available pods: 0
Feb 20 18:58:30.562: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:58:31.596: INFO: Number of nodes with available pods: 0
Feb 20 18:58:31.596: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 18:58:32.596: INFO: Number of nodes with available pods: 1
Feb 20 18:58:32.596: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf is running more than one daemon pod
Feb 20 18:58:33.596: INFO: Number of nodes with available pods: 2
Feb 20 18:58:33.596: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 20 18:58:33.696: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:33.696: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:34.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:34.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:35.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:35.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:36.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:36.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:37.732: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:37.732: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:38.731: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:38.731: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:39.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:39.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:40.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:40.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:41.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:41.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:42.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:42.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:43.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:43.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:44.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:44.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:45.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:45.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:46.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:46.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:47.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:47.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:48.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:48.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:49.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:49.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:50.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:50.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:51.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:51.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:52.732: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:52.732: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:53.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:53.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:54.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:54.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:55.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:55.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:56.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:56.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:57.735: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:57.735: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:58.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:58.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:59.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:58:59.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:00.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:00.729: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:01.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:01.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:02.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:02.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:03.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:03.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:04.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:04.730: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:05.731: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:05.731: INFO: Wrong image for pod: daemon-set-t7brv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:05.731: INFO: Pod daemon-set-t7brv is not available
Feb 20 18:59:06.732: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:06.732: INFO: Pod daemon-set-g7m4l is not available
Feb 20 18:59:07.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:07.730: INFO: Pod daemon-set-g7m4l is not available
Feb 20 18:59:08.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:08.729: INFO: Pod daemon-set-g7m4l is not available
Feb 20 18:59:09.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:10.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:11.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:12.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:13.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:14.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:15.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:16.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:17.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:18.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:19.731: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:20.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:21.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:22.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:23.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:24.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:25.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:26.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:27.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:28.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:29.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:30.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:31.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:32.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:33.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:34.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:35.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:36.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:37.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:38.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:39.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:40.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:40.730: INFO: Pod daemon-set-btdq6 is not available
Feb 20 18:59:41.729: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:41.730: INFO: Pod daemon-set-btdq6 is not available
Feb 20 18:59:42.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:42.730: INFO: Pod daemon-set-btdq6 is not available
Feb 20 18:59:43.732: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:43.732: INFO: Pod daemon-set-btdq6 is not available
Feb 20 18:59:44.730: INFO: Wrong image for pod: daemon-set-btdq6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 18:59:44.730: INFO: Pod daemon-set-btdq6 is not available
Feb 20 18:59:45.736: INFO: Pod daemon-set-w2wm6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 20 18:59:45.786: INFO: Number of nodes with available pods: 1
Feb 20 18:59:45.786: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf is running more than one daemon pod
Feb 20 18:59:46.823: INFO: Number of nodes with available pods: 1
Feb 20 18:59:46.823: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf is running more than one daemon pod
Feb 20 18:59:47.825: INFO: Number of nodes with available pods: 2
Feb 20 18:59:47.825: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tpt7z, will wait for the garbage collector to delete the pods
Feb 20 18:59:47.994: INFO: Deleting DaemonSet.extensions daemon-set took: 18.613335ms
Feb 20 18:59:48.094: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.340143ms
Feb 20 18:59:51.511: INFO: Number of nodes with available pods: 0
Feb 20 18:59:51.511: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 18:59:51.527: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tpt7z/daemonsets","resourceVersion":"11391"},"items":null}

Feb 20 18:59:51.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tpt7z/pods","resourceVersion":"11391"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 18:59:51.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tpt7z" for this suite.
Feb 20 18:59:59.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:59:59.756: INFO: namespace: e2e-tests-daemonsets-tpt7z, resource: bindings, ignored listing per whitelist
Feb 20 19:00:00.306: INFO: namespace e2e-tests-daemonsets-tpt7z deletion completed in 8.696617805s

• [SLOW TEST:90.702 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:00:00.306: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-2q62c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b978a323-3541-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:00:01.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-2q62c" to be "success or failure"
Feb 20 19:00:01.286: INFO: Pod "pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.463761ms
Feb 20 19:00:03.307: INFO: Pod "pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036823604s
STEP: Saw pod success
Feb 20 19:00:03.307: INFO: Pod "pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:00:03.323: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:00:03.370: INFO: Waiting for pod pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:00:03.386: INFO: Pod pod-configmaps-b97b292d-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:00:03.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2q62c" for this suite.
Feb 20 19:00:09.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:00:09.904: INFO: namespace: e2e-tests-configmap-2q62c, resource: bindings, ignored listing per whitelist
Feb 20 19:00:10.127: INFO: namespace e2e-tests-configmap-2q62c deletion completed in 6.722792268s

• [SLOW TEST:9.820 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:00:10.127: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-t2lqv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:00:13.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-t2lqv" for this suite.
Feb 20 19:00:19.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:00:19.755: INFO: namespace: e2e-tests-emptydir-wrapper-t2lqv, resource: bindings, ignored listing per whitelist
Feb 20 19:00:20.022: INFO: namespace e2e-tests-emptydir-wrapper-t2lqv deletion completed in 6.815519763s

• [SLOW TEST:9.896 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:00:20.023: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-ch9xc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 20 19:00:20.851: INFO: Waiting up to 5m0s for pod "client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-containers-ch9xc" to be "success or failure"
Feb 20 19:00:20.867: INFO: Pod "client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.616747ms
Feb 20 19:00:22.884: INFO: Pod "client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032331516s
STEP: Saw pod success
Feb 20 19:00:22.884: INFO: Pod "client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:00:22.900: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:00:22.943: INFO: Waiting for pod client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:00:22.960: INFO: Pod client-containers-c52714d9-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:00:22.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ch9xc" for this suite.
Feb 20 19:00:29.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:00:29.293: INFO: namespace: e2e-tests-containers-ch9xc, resource: bindings, ignored listing per whitelist
Feb 20 19:00:29.679: INFO: namespace e2e-tests-containers-ch9xc deletion completed in 6.703112456s

• [SLOW TEST:9.657 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:00:29.680: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-htk9x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-htk9x
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-htk9x
STEP: Deleting pre-stop pod
Feb 20 19:00:41.772: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:00:41.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-htk9x" for this suite.
Feb 20 19:01:21.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:01:22.526: INFO: namespace: e2e-tests-prestop-htk9x, resource: bindings, ignored listing per whitelist
Feb 20 19:01:22.943: INFO: namespace e2e-tests-prestop-htk9x deletion completed in 41.133599371s

• [SLOW TEST:53.263 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:01:22.943: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wxvjm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 20 19:01:24.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml cluster-info'
Feb 20 19:01:24.925: INFO: stderr: ""
Feb 20 19:01:24.925: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:01:24.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wxvjm" for this suite.
Feb 20 19:01:30.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:01:31.307: INFO: namespace: e2e-tests-kubectl-wxvjm, resource: bindings, ignored listing per whitelist
Feb 20 19:01:31.738: INFO: namespace e2e-tests-kubectl-wxvjm deletion completed in 6.795359897s

• [SLOW TEST:8.794 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:01:31.738: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bxw5l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 19:01:32.658: INFO: Waiting up to 5m0s for pod "pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-bxw5l" to be "success or failure"
Feb 20 19:01:32.674: INFO: Pod "pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.787ms
Feb 20 19:01:34.692: INFO: Pod "pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033945838s
STEP: Saw pod success
Feb 20 19:01:34.692: INFO: Pod "pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:01:34.708: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:01:34.754: INFO: Waiting for pod pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:01:34.770: INFO: Pod pod-eff3c2dd-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:01:34.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bxw5l" for this suite.
Feb 20 19:01:40.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:01:41.355: INFO: namespace: e2e-tests-emptydir-bxw5l, resource: bindings, ignored listing per whitelist
Feb 20 19:01:41.450: INFO: namespace e2e-tests-emptydir-bxw5l deletion completed in 6.663590083s

• [SLOW TEST:9.712 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:01:41.450: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-vxkxl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 20 19:01:42.883: INFO: Waiting up to 5m0s for pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh" in namespace "e2e-tests-svcaccounts-vxkxl" to be "success or failure"
Feb 20 19:01:42.898: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh": Phase="Pending", Reason="", readiness=false. Elapsed: 15.918169ms
Feb 20 19:01:44.924: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041255992s
Feb 20 19:01:46.941: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05841908s
STEP: Saw pod success
Feb 20 19:01:46.941: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh" satisfied condition "success or failure"
Feb 20 19:01:46.957: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh container token-test: <nil>
STEP: delete the pod
Feb 20 19:01:47.001: INFO: Waiting for pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh to disappear
Feb 20 19:01:47.018: INFO: Pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-wclvh no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 20 19:01:47.035: INFO: Waiting up to 5m0s for pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7" in namespace "e2e-tests-svcaccounts-vxkxl" to be "success or failure"
Feb 20 19:01:47.051: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.838938ms
Feb 20 19:01:49.067: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032271573s
STEP: Saw pod success
Feb 20 19:01:49.067: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7" satisfied condition "success or failure"
Feb 20 19:01:49.085: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7 container root-ca-test: <nil>
STEP: delete the pod
Feb 20 19:01:49.137: INFO: Waiting for pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7 to disappear
Feb 20 19:01:49.154: INFO: Pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-cmgp7 no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 20 19:01:49.171: INFO: Waiting up to 5m0s for pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t" in namespace "e2e-tests-svcaccounts-vxkxl" to be "success or failure"
Feb 20 19:01:49.187: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t": Phase="Pending", Reason="", readiness=false. Elapsed: 16.290374ms
Feb 20 19:01:51.204: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032844957s
STEP: Saw pod success
Feb 20 19:01:51.204: INFO: Pod "pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t" satisfied condition "success or failure"
Feb 20 19:01:51.221: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t container namespace-test: <nil>
STEP: delete the pod
Feb 20 19:01:51.263: INFO: Waiting for pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t to disappear
Feb 20 19:01:51.279: INFO: Pod pod-service-account-f60bf0e1-3541-11e9-9f03-4261fb5d5f3f-tfk2t no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:01:51.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-vxkxl" for this suite.
Feb 20 19:01:57.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:01:57.637: INFO: namespace: e2e-tests-svcaccounts-vxkxl, resource: bindings, ignored listing per whitelist
Feb 20 19:01:58.190: INFO: namespace e2e-tests-svcaccounts-vxkxl deletion completed in 6.893521438s

• [SLOW TEST:16.739 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:01:58.191: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2xnq5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-ffafaa6c-3541-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:01:59.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-2xnq5" to be "success or failure"
Feb 20 19:01:59.088: INFO: Pod "pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.696848ms
Feb 20 19:02:01.104: INFO: Pod "pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032572043s
STEP: Saw pod success
Feb 20 19:02:01.105: INFO: Pod "pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:02:01.121: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:02:01.164: INFO: Waiting for pod pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:02:01.179: INFO: Pod pod-projected-secrets-ffb23969-3541-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:02:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2xnq5" for this suite.
Feb 20 19:02:07.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:02:07.899: INFO: namespace: e2e-tests-projected-2xnq5, resource: bindings, ignored listing per whitelist
Feb 20 19:02:07.946: INFO: namespace e2e-tests-projected-2xnq5 deletion completed in 6.749724275s

• [SLOW TEST:9.755 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:02:07.946: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lzs7s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 19:02:08.854: INFO: Waiting up to 5m0s for pod "downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-lzs7s" to be "success or failure"
Feb 20 19:02:08.870: INFO: Pod "downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803761ms
Feb 20 19:02:10.887: INFO: Pod "downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032530935s
STEP: Saw pod success
Feb 20 19:02:10.887: INFO: Pod "downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:02:10.903: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:02:10.945: INFO: Waiting for pod downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:02:10.961: INFO: Pod downward-api-0586e7a1-3542-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:02:10.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lzs7s" for this suite.
Feb 20 19:02:19.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:02:19.208: INFO: namespace: e2e-tests-downward-api-lzs7s, resource: bindings, ignored listing per whitelist
Feb 20 19:02:19.728: INFO: namespace e2e-tests-downward-api-lzs7s deletion completed in 8.750122615s

• [SLOW TEST:11.782 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:02:19.728: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-5b4x8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-5b4x8
I0220 19:02:20.549346   30262 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-5b4x8, replica count: 1
I0220 19:02:21.599689   30262 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 19:02:22.599896   30262 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 19:02:22.721: INFO: Created: latency-svc-cgrpm
Feb 20 19:02:22.728: INFO: Got endpoints: latency-svc-cgrpm [28.27127ms]
Feb 20 19:02:22.794: INFO: Created: latency-svc-7q8lm
Feb 20 19:02:22.799: INFO: Created: latency-svc-wqtzd
Feb 20 19:02:22.894: INFO: Created: latency-svc-xvwcc
Feb 20 19:02:22.895: INFO: Got endpoints: latency-svc-wqtzd [166.620952ms]
Feb 20 19:02:22.896: INFO: Got endpoints: latency-svc-7q8lm [167.591031ms]
Feb 20 19:02:22.900: INFO: Got endpoints: latency-svc-xvwcc [171.822031ms]
Feb 20 19:02:22.901: INFO: Created: latency-svc-8gwm9
Feb 20 19:02:22.906: INFO: Got endpoints: latency-svc-8gwm9 [177.633762ms]
Feb 20 19:02:22.907: INFO: Created: latency-svc-gjv45
Feb 20 19:02:22.911: INFO: Created: latency-svc-cn6cc
Feb 20 19:02:22.912: INFO: Got endpoints: latency-svc-gjv45 [183.205932ms]
Feb 20 19:02:22.994: INFO: Got endpoints: latency-svc-cn6cc [265.527599ms]
Feb 20 19:02:22.996: INFO: Created: latency-svc-fp2bt
Feb 20 19:02:22.997: INFO: Got endpoints: latency-svc-fp2bt [268.486112ms]
Feb 20 19:02:23.002: INFO: Created: latency-svc-gbtnv
Feb 20 19:02:23.008: INFO: Created: latency-svc-cg8n5
Feb 20 19:02:23.008: INFO: Got endpoints: latency-svc-gbtnv [278.61719ms]
Feb 20 19:02:23.009: INFO: Got endpoints: latency-svc-cg8n5 [279.358431ms]
Feb 20 19:02:23.100: INFO: Created: latency-svc-9wv8v
Feb 20 19:02:23.104: INFO: Got endpoints: latency-svc-9wv8v [374.737069ms]
Feb 20 19:02:23.104: INFO: Created: latency-svc-dpbrt
Feb 20 19:02:23.109: INFO: Got endpoints: latency-svc-dpbrt [379.815392ms]
Feb 20 19:02:23.110: INFO: Created: latency-svc-llp4f
Feb 20 19:02:23.114: INFO: Got endpoints: latency-svc-llp4f [384.503609ms]
Feb 20 19:02:23.115: INFO: Created: latency-svc-jmlmm
Feb 20 19:02:23.116: INFO: Got endpoints: latency-svc-jmlmm [386.847077ms]
Feb 20 19:02:23.121: INFO: Created: latency-svc-ttg7s
Feb 20 19:02:23.122: INFO: Got endpoints: latency-svc-ttg7s [392.476054ms]
Feb 20 19:02:23.125: INFO: Created: latency-svc-x6ts9
Feb 20 19:02:23.130: INFO: Created: latency-svc-jkggg
Feb 20 19:02:23.137: INFO: Created: latency-svc-bbjct
Feb 20 19:02:23.141: INFO: Created: latency-svc-ccmmr
Feb 20 19:02:23.145: INFO: Created: latency-svc-vrm57
Feb 20 19:02:23.149: INFO: Created: latency-svc-j2b2q
Feb 20 19:02:23.153: INFO: Created: latency-svc-scjnz
Feb 20 19:02:23.158: INFO: Created: latency-svc-tq57q
Feb 20 19:02:23.175: INFO: Created: latency-svc-q78mw
Feb 20 19:02:23.203: INFO: Created: latency-svc-gcz9m
Feb 20 19:02:23.204: INFO: Created: latency-svc-vvs8c
Feb 20 19:02:23.204: INFO: Got endpoints: latency-svc-jkggg [308.478644ms]
Feb 20 19:02:23.204: INFO: Got endpoints: latency-svc-bbjct [308.011232ms]
Feb 20 19:02:23.204: INFO: Got endpoints: latency-svc-vrm57 [297.745147ms]
Feb 20 19:02:23.204: INFO: Got endpoints: latency-svc-ccmmr [304.316433ms]
Feb 20 19:02:23.204: INFO: Got endpoints: latency-svc-x6ts9 [474.565668ms]
Feb 20 19:02:23.205: INFO: Got endpoints: latency-svc-j2b2q [292.947129ms]
Feb 20 19:02:23.205: INFO: Got endpoints: latency-svc-scjnz [210.360877ms]
Feb 20 19:02:23.205: INFO: Got endpoints: latency-svc-tq57q [207.436442ms]
Feb 20 19:02:23.205: INFO: Got endpoints: latency-svc-q78mw [197.338265ms]
Feb 20 19:02:23.205: INFO: Got endpoints: latency-svc-vvs8c [196.61196ms]
Feb 20 19:02:23.215: INFO: Created: latency-svc-7qnwk
Feb 20 19:02:23.216: INFO: Created: latency-svc-c7dvx
Feb 20 19:02:23.216: INFO: Got endpoints: latency-svc-c7dvx [106.30219ms]
Feb 20 19:02:23.216: INFO: Got endpoints: latency-svc-gcz9m [111.736484ms]
Feb 20 19:02:23.219: INFO: Created: latency-svc-bkwrl
Feb 20 19:02:23.229: INFO: Created: latency-svc-sbtw4
Feb 20 19:02:23.233: INFO: Created: latency-svc-q678s
Feb 20 19:02:23.237: INFO: Created: latency-svc-vtpjx
Feb 20 19:02:23.243: INFO: Created: latency-svc-mb4mr
Feb 20 19:02:23.254: INFO: Created: latency-svc-2zvgc
Feb 20 19:02:23.254: INFO: Created: latency-svc-q68cl
Feb 20 19:02:23.281: INFO: Created: latency-svc-vmlhh
Feb 20 19:02:23.293: INFO: Got endpoints: latency-svc-sbtw4 [170.9268ms]
Feb 20 19:02:23.293: INFO: Got endpoints: latency-svc-bkwrl [176.948378ms]
Feb 20 19:02:23.294: INFO: Got endpoints: latency-svc-7qnwk [179.569256ms]
Feb 20 19:02:23.294: INFO: Got endpoints: latency-svc-vtpjx [89.777658ms]
Feb 20 19:02:23.295: INFO: Got endpoints: latency-svc-q678s [91.238258ms]
Feb 20 19:02:23.302: INFO: Got endpoints: latency-svc-vmlhh [97.831769ms]
Feb 20 19:02:23.303: INFO: Got endpoints: latency-svc-q68cl [98.450256ms]
Feb 20 19:02:23.303: INFO: Got endpoints: latency-svc-mb4mr [98.89731ms]
Feb 20 19:02:23.303: INFO: Got endpoints: latency-svc-2zvgc [98.596189ms]
Feb 20 19:02:23.304: INFO: Created: latency-svc-bmdcg
Feb 20 19:02:23.310: INFO: Created: latency-svc-2n6lz
Feb 20 19:02:23.310: INFO: Got endpoints: latency-svc-bmdcg [105.696974ms]
Feb 20 19:02:23.313: INFO: Created: latency-svc-g6nzt
Feb 20 19:02:23.317: INFO: Created: latency-svc-z6slh
Feb 20 19:02:23.323: INFO: Created: latency-svc-4ng8s
Feb 20 19:02:23.331: INFO: Created: latency-svc-xdzwq
Feb 20 19:02:23.331: INFO: Created: latency-svc-wnvbq
Feb 20 19:02:23.335: INFO: Created: latency-svc-6sjjk
Feb 20 19:02:23.340: INFO: Created: latency-svc-kn78b
Feb 20 19:02:23.344: INFO: Created: latency-svc-r85cb
Feb 20 19:02:23.349: INFO: Created: latency-svc-qwzh9
Feb 20 19:02:23.357: INFO: Created: latency-svc-vchkm
Feb 20 19:02:23.357: INFO: Created: latency-svc-mptbw
Feb 20 19:02:23.362: INFO: Created: latency-svc-78psc
Feb 20 19:02:23.366: INFO: Created: latency-svc-lx8qw
Feb 20 19:02:23.371: INFO: Created: latency-svc-r4hh2
Feb 20 19:02:23.381: INFO: Got endpoints: latency-svc-z6slh [176.193837ms]
Feb 20 19:02:23.381: INFO: Got endpoints: latency-svc-4ng8s [165.778262ms]
Feb 20 19:02:23.381: INFO: Got endpoints: latency-svc-2n6lz [176.46622ms]
Feb 20 19:02:23.381: INFO: Got endpoints: latency-svc-wnvbq [165.557613ms]
Feb 20 19:02:23.381: INFO: Got endpoints: latency-svc-g6nzt [176.304389ms]
Feb 20 19:02:23.408: INFO: Got endpoints: latency-svc-xdzwq [115.249378ms]
Feb 20 19:02:23.409: INFO: Created: latency-svc-n6bx8
Feb 20 19:02:23.414: INFO: Created: latency-svc-rjpd5
Feb 20 19:02:23.419: INFO: Created: latency-svc-qq6nm
Feb 20 19:02:23.424: INFO: Created: latency-svc-7p4xk
Feb 20 19:02:23.429: INFO: Created: latency-svc-6pkk6
Feb 20 19:02:23.441: INFO: Created: latency-svc-ddcn8
Feb 20 19:02:23.453: INFO: Got endpoints: latency-svc-kn78b [159.158185ms]
Feb 20 19:02:23.474: INFO: Created: latency-svc-t66r9
Feb 20 19:02:23.503: INFO: Got endpoints: latency-svc-6sjjk [209.708916ms]
Feb 20 19:02:23.524: INFO: Created: latency-svc-swn59
Feb 20 19:02:23.553: INFO: Got endpoints: latency-svc-qwzh9 [258.363049ms]
Feb 20 19:02:23.575: INFO: Created: latency-svc-4vwsf
Feb 20 19:02:23.602: INFO: Got endpoints: latency-svc-r85cb [308.393723ms]
Feb 20 19:02:23.624: INFO: Created: latency-svc-j4fqg
Feb 20 19:02:23.654: INFO: Got endpoints: latency-svc-mptbw [350.97908ms]
Feb 20 19:02:23.676: INFO: Created: latency-svc-xg7g8
Feb 20 19:02:23.703: INFO: Got endpoints: latency-svc-vchkm [400.487415ms]
Feb 20 19:02:23.725: INFO: Created: latency-svc-mmkpb
Feb 20 19:02:23.753: INFO: Got endpoints: latency-svc-78psc [449.810423ms]
Feb 20 19:02:23.773: INFO: Created: latency-svc-mz5jf
Feb 20 19:02:23.803: INFO: Got endpoints: latency-svc-lx8qw [500.109422ms]
Feb 20 19:02:23.829: INFO: Created: latency-svc-s9s6m
Feb 20 19:02:23.853: INFO: Got endpoints: latency-svc-r4hh2 [542.812164ms]
Feb 20 19:02:23.874: INFO: Created: latency-svc-wmfst
Feb 20 19:02:23.903: INFO: Got endpoints: latency-svc-n6bx8 [521.516192ms]
Feb 20 19:02:23.924: INFO: Created: latency-svc-vnjwj
Feb 20 19:02:23.953: INFO: Got endpoints: latency-svc-rjpd5 [571.503413ms]
Feb 20 19:02:23.975: INFO: Created: latency-svc-r5hjd
Feb 20 19:02:24.004: INFO: Got endpoints: latency-svc-qq6nm [622.08492ms]
Feb 20 19:02:24.025: INFO: Created: latency-svc-zvjrh
Feb 20 19:02:24.053: INFO: Got endpoints: latency-svc-7p4xk [671.547272ms]
Feb 20 19:02:24.075: INFO: Created: latency-svc-4784d
Feb 20 19:02:24.103: INFO: Got endpoints: latency-svc-6pkk6 [721.148575ms]
Feb 20 19:02:24.124: INFO: Created: latency-svc-lf22k
Feb 20 19:02:24.153: INFO: Got endpoints: latency-svc-ddcn8 [744.717367ms]
Feb 20 19:02:24.175: INFO: Created: latency-svc-k56cv
Feb 20 19:02:24.203: INFO: Got endpoints: latency-svc-t66r9 [750.471558ms]
Feb 20 19:02:24.226: INFO: Created: latency-svc-7mcmh
Feb 20 19:02:24.253: INFO: Got endpoints: latency-svc-swn59 [749.979486ms]
Feb 20 19:02:24.274: INFO: Created: latency-svc-294pw
Feb 20 19:02:24.303: INFO: Got endpoints: latency-svc-4vwsf [749.498735ms]
Feb 20 19:02:24.324: INFO: Created: latency-svc-c8jhf
Feb 20 19:02:24.353: INFO: Got endpoints: latency-svc-j4fqg [750.841463ms]
Feb 20 19:02:24.374: INFO: Created: latency-svc-g67ph
Feb 20 19:02:24.403: INFO: Got endpoints: latency-svc-xg7g8 [749.930337ms]
Feb 20 19:02:24.425: INFO: Created: latency-svc-lvtmg
Feb 20 19:02:24.453: INFO: Got endpoints: latency-svc-mmkpb [749.846887ms]
Feb 20 19:02:24.475: INFO: Created: latency-svc-wfmxd
Feb 20 19:02:24.503: INFO: Got endpoints: latency-svc-mz5jf [750.62616ms]
Feb 20 19:02:24.525: INFO: Created: latency-svc-qrtfj
Feb 20 19:02:24.553: INFO: Got endpoints: latency-svc-s9s6m [749.934066ms]
Feb 20 19:02:24.575: INFO: Created: latency-svc-4mdrn
Feb 20 19:02:24.604: INFO: Got endpoints: latency-svc-wmfst [750.67346ms]
Feb 20 19:02:24.627: INFO: Created: latency-svc-cv7cp
Feb 20 19:02:24.658: INFO: Got endpoints: latency-svc-vnjwj [754.599996ms]
Feb 20 19:02:24.680: INFO: Created: latency-svc-m5z42
Feb 20 19:02:24.780: INFO: Got endpoints: latency-svc-zvjrh [776.829013ms]
Feb 20 19:02:24.781: INFO: Got endpoints: latency-svc-r5hjd [827.58462ms]
Feb 20 19:02:24.802: INFO: Created: latency-svc-4l29s
Feb 20 19:02:24.803: INFO: Got endpoints: latency-svc-4784d [749.913619ms]
Feb 20 19:02:24.810: INFO: Created: latency-svc-fvv8p
Feb 20 19:02:24.824: INFO: Created: latency-svc-5cc9r
Feb 20 19:02:24.853: INFO: Got endpoints: latency-svc-lf22k [749.872471ms]
Feb 20 19:02:24.875: INFO: Created: latency-svc-55bpk
Feb 20 19:02:24.903: INFO: Got endpoints: latency-svc-k56cv [749.756199ms]
Feb 20 19:02:24.924: INFO: Created: latency-svc-wfmmq
Feb 20 19:02:24.953: INFO: Got endpoints: latency-svc-7mcmh [749.799464ms]
Feb 20 19:02:24.976: INFO: Created: latency-svc-6xtmt
Feb 20 19:02:25.003: INFO: Got endpoints: latency-svc-294pw [750.093503ms]
Feb 20 19:02:25.024: INFO: Created: latency-svc-vz4pk
Feb 20 19:02:25.053: INFO: Got endpoints: latency-svc-c8jhf [750.401694ms]
Feb 20 19:02:25.076: INFO: Created: latency-svc-mvbrp
Feb 20 19:02:25.103: INFO: Got endpoints: latency-svc-g67ph [749.65258ms]
Feb 20 19:02:25.124: INFO: Created: latency-svc-sjbmm
Feb 20 19:02:25.153: INFO: Got endpoints: latency-svc-lvtmg [749.778796ms]
Feb 20 19:02:25.174: INFO: Created: latency-svc-bs4l4
Feb 20 19:02:25.203: INFO: Got endpoints: latency-svc-wfmxd [749.831413ms]
Feb 20 19:02:25.227: INFO: Created: latency-svc-mdzpn
Feb 20 19:02:25.253: INFO: Got endpoints: latency-svc-qrtfj [749.903527ms]
Feb 20 19:02:25.274: INFO: Created: latency-svc-dj7hz
Feb 20 19:02:25.303: INFO: Got endpoints: latency-svc-4mdrn [750.018721ms]
Feb 20 19:02:25.326: INFO: Created: latency-svc-mq2tx
Feb 20 19:02:25.353: INFO: Got endpoints: latency-svc-cv7cp [748.818313ms]
Feb 20 19:02:25.375: INFO: Created: latency-svc-lmqjf
Feb 20 19:02:25.403: INFO: Got endpoints: latency-svc-m5z42 [745.638251ms]
Feb 20 19:02:25.426: INFO: Created: latency-svc-xh7vv
Feb 20 19:02:25.454: INFO: Got endpoints: latency-svc-4l29s [673.074448ms]
Feb 20 19:02:25.475: INFO: Created: latency-svc-cs28d
Feb 20 19:02:25.503: INFO: Got endpoints: latency-svc-fvv8p [722.228918ms]
Feb 20 19:02:25.524: INFO: Created: latency-svc-r9b6q
Feb 20 19:02:25.553: INFO: Got endpoints: latency-svc-5cc9r [750.258702ms]
Feb 20 19:02:25.575: INFO: Created: latency-svc-wmg7g
Feb 20 19:02:25.603: INFO: Got endpoints: latency-svc-55bpk [750.14833ms]
Feb 20 19:02:25.625: INFO: Created: latency-svc-tbw4m
Feb 20 19:02:25.653: INFO: Got endpoints: latency-svc-wfmmq [749.935616ms]
Feb 20 19:02:25.676: INFO: Created: latency-svc-mx2tj
Feb 20 19:02:25.703: INFO: Got endpoints: latency-svc-6xtmt [749.896902ms]
Feb 20 19:02:25.724: INFO: Created: latency-svc-zrq4c
Feb 20 19:02:25.753: INFO: Got endpoints: latency-svc-vz4pk [749.436141ms]
Feb 20 19:02:25.777: INFO: Created: latency-svc-8mdvf
Feb 20 19:02:25.803: INFO: Got endpoints: latency-svc-mvbrp [749.522433ms]
Feb 20 19:02:25.825: INFO: Created: latency-svc-599bj
Feb 20 19:02:25.853: INFO: Got endpoints: latency-svc-sjbmm [749.710994ms]
Feb 20 19:02:25.875: INFO: Created: latency-svc-4fp8t
Feb 20 19:02:25.903: INFO: Got endpoints: latency-svc-bs4l4 [749.455783ms]
Feb 20 19:02:25.924: INFO: Created: latency-svc-4dlf9
Feb 20 19:02:25.953: INFO: Got endpoints: latency-svc-mdzpn [749.569937ms]
Feb 20 19:02:25.975: INFO: Created: latency-svc-9hh6j
Feb 20 19:02:26.002: INFO: Got endpoints: latency-svc-dj7hz [748.844858ms]
Feb 20 19:02:26.024: INFO: Created: latency-svc-7c65b
Feb 20 19:02:26.053: INFO: Got endpoints: latency-svc-mq2tx [749.612502ms]
Feb 20 19:02:26.075: INFO: Created: latency-svc-9npn6
Feb 20 19:02:26.103: INFO: Got endpoints: latency-svc-lmqjf [749.906029ms]
Feb 20 19:02:26.126: INFO: Created: latency-svc-vqbtk
Feb 20 19:02:26.153: INFO: Got endpoints: latency-svc-xh7vv [749.597222ms]
Feb 20 19:02:26.175: INFO: Created: latency-svc-nxcqg
Feb 20 19:02:26.203: INFO: Got endpoints: latency-svc-cs28d [749.48839ms]
Feb 20 19:02:26.227: INFO: Created: latency-svc-nmv5w
Feb 20 19:02:26.253: INFO: Got endpoints: latency-svc-r9b6q [750.327363ms]
Feb 20 19:02:26.275: INFO: Created: latency-svc-pwpjc
Feb 20 19:02:26.303: INFO: Got endpoints: latency-svc-wmg7g [749.75061ms]
Feb 20 19:02:26.326: INFO: Created: latency-svc-28lgr
Feb 20 19:02:26.353: INFO: Got endpoints: latency-svc-tbw4m [750.030492ms]
Feb 20 19:02:26.375: INFO: Created: latency-svc-2tf88
Feb 20 19:02:26.403: INFO: Got endpoints: latency-svc-mx2tj [750.235337ms]
Feb 20 19:02:26.429: INFO: Created: latency-svc-pd5pj
Feb 20 19:02:26.453: INFO: Got endpoints: latency-svc-zrq4c [749.874032ms]
Feb 20 19:02:26.475: INFO: Created: latency-svc-pjw8b
Feb 20 19:02:26.503: INFO: Got endpoints: latency-svc-8mdvf [750.378725ms]
Feb 20 19:02:26.525: INFO: Created: latency-svc-pxhhb
Feb 20 19:02:26.553: INFO: Got endpoints: latency-svc-599bj [750.248719ms]
Feb 20 19:02:26.575: INFO: Created: latency-svc-jrmhp
Feb 20 19:02:26.603: INFO: Got endpoints: latency-svc-4fp8t [750.150577ms]
Feb 20 19:02:26.625: INFO: Created: latency-svc-7scrk
Feb 20 19:02:26.654: INFO: Got endpoints: latency-svc-4dlf9 [750.79124ms]
Feb 20 19:02:26.677: INFO: Created: latency-svc-7xv2m
Feb 20 19:02:26.704: INFO: Got endpoints: latency-svc-9hh6j [750.265077ms]
Feb 20 19:02:26.725: INFO: Created: latency-svc-rl7j6
Feb 20 19:02:26.754: INFO: Got endpoints: latency-svc-7c65b [751.736606ms]
Feb 20 19:02:26.779: INFO: Created: latency-svc-rrmwz
Feb 20 19:02:26.803: INFO: Got endpoints: latency-svc-9npn6 [749.963626ms]
Feb 20 19:02:26.824: INFO: Created: latency-svc-vcbts
Feb 20 19:02:26.853: INFO: Got endpoints: latency-svc-vqbtk [750.162974ms]
Feb 20 19:02:26.874: INFO: Created: latency-svc-k8jwd
Feb 20 19:02:26.903: INFO: Got endpoints: latency-svc-nxcqg [749.963497ms]
Feb 20 19:02:26.925: INFO: Created: latency-svc-jbch6
Feb 20 19:02:26.953: INFO: Got endpoints: latency-svc-nmv5w [749.852232ms]
Feb 20 19:02:26.974: INFO: Created: latency-svc-mvz6s
Feb 20 19:02:27.003: INFO: Got endpoints: latency-svc-pwpjc [749.662413ms]
Feb 20 19:02:27.027: INFO: Created: latency-svc-sngct
Feb 20 19:02:27.053: INFO: Got endpoints: latency-svc-28lgr [750.240889ms]
Feb 20 19:02:27.074: INFO: Created: latency-svc-wgs4v
Feb 20 19:02:27.103: INFO: Got endpoints: latency-svc-2tf88 [749.794379ms]
Feb 20 19:02:27.126: INFO: Created: latency-svc-tbjgl
Feb 20 19:02:27.157: INFO: Got endpoints: latency-svc-pd5pj [753.700886ms]
Feb 20 19:02:27.178: INFO: Created: latency-svc-tcqbj
Feb 20 19:02:27.203: INFO: Got endpoints: latency-svc-pjw8b [750.023706ms]
Feb 20 19:02:27.226: INFO: Created: latency-svc-hc9vf
Feb 20 19:02:27.254: INFO: Got endpoints: latency-svc-pxhhb [750.117044ms]
Feb 20 19:02:27.274: INFO: Created: latency-svc-sxs8r
Feb 20 19:02:27.303: INFO: Got endpoints: latency-svc-jrmhp [749.995381ms]
Feb 20 19:02:27.325: INFO: Created: latency-svc-h45fm
Feb 20 19:02:27.353: INFO: Got endpoints: latency-svc-7scrk [750.052686ms]
Feb 20 19:02:27.374: INFO: Created: latency-svc-f2h96
Feb 20 19:02:27.404: INFO: Got endpoints: latency-svc-7xv2m [749.426787ms]
Feb 20 19:02:27.425: INFO: Created: latency-svc-2bckc
Feb 20 19:02:27.453: INFO: Got endpoints: latency-svc-rl7j6 [749.722651ms]
Feb 20 19:02:27.475: INFO: Created: latency-svc-hs2fq
Feb 20 19:02:27.508: INFO: Got endpoints: latency-svc-rrmwz [754.329222ms]
Feb 20 19:02:27.530: INFO: Created: latency-svc-n2cnx
Feb 20 19:02:27.554: INFO: Got endpoints: latency-svc-vcbts [750.5535ms]
Feb 20 19:02:27.579: INFO: Created: latency-svc-vz9mq
Feb 20 19:02:27.604: INFO: Got endpoints: latency-svc-k8jwd [750.248604ms]
Feb 20 19:02:27.625: INFO: Created: latency-svc-mrzj5
Feb 20 19:02:27.653: INFO: Got endpoints: latency-svc-jbch6 [750.202004ms]
Feb 20 19:02:27.677: INFO: Created: latency-svc-49c2s
Feb 20 19:02:27.704: INFO: Got endpoints: latency-svc-mvz6s [750.562113ms]
Feb 20 19:02:27.726: INFO: Created: latency-svc-x9l4c
Feb 20 19:02:27.753: INFO: Got endpoints: latency-svc-sngct [750.113903ms]
Feb 20 19:02:27.785: INFO: Created: latency-svc-j2n9m
Feb 20 19:02:27.803: INFO: Got endpoints: latency-svc-wgs4v [749.41333ms]
Feb 20 19:02:27.824: INFO: Created: latency-svc-tvhrl
Feb 20 19:02:27.853: INFO: Got endpoints: latency-svc-tbjgl [750.073291ms]
Feb 20 19:02:27.875: INFO: Created: latency-svc-4226s
Feb 20 19:02:27.903: INFO: Got endpoints: latency-svc-tcqbj [745.700644ms]
Feb 20 19:02:27.925: INFO: Created: latency-svc-m9mpq
Feb 20 19:02:27.953: INFO: Got endpoints: latency-svc-hc9vf [750.095164ms]
Feb 20 19:02:27.975: INFO: Created: latency-svc-hbv78
Feb 20 19:02:28.003: INFO: Got endpoints: latency-svc-sxs8r [749.529854ms]
Feb 20 19:02:28.024: INFO: Created: latency-svc-kmp9z
Feb 20 19:02:28.053: INFO: Got endpoints: latency-svc-h45fm [749.789339ms]
Feb 20 19:02:28.075: INFO: Created: latency-svc-zdxnw
Feb 20 19:02:28.103: INFO: Got endpoints: latency-svc-f2h96 [749.64403ms]
Feb 20 19:02:28.125: INFO: Created: latency-svc-fzmqh
Feb 20 19:02:28.153: INFO: Got endpoints: latency-svc-2bckc [749.725893ms]
Feb 20 19:02:28.175: INFO: Created: latency-svc-6cswk
Feb 20 19:02:28.215: INFO: Got endpoints: latency-svc-hs2fq [761.357084ms]
Feb 20 19:02:28.236: INFO: Created: latency-svc-thb6l
Feb 20 19:02:28.253: INFO: Got endpoints: latency-svc-n2cnx [744.53899ms]
Feb 20 19:02:28.274: INFO: Created: latency-svc-2t7n9
Feb 20 19:02:28.311: INFO: Got endpoints: latency-svc-vz9mq [757.468634ms]
Feb 20 19:02:28.332: INFO: Created: latency-svc-z9vxc
Feb 20 19:02:28.358: INFO: Got endpoints: latency-svc-mrzj5 [754.939558ms]
Feb 20 19:02:28.400: INFO: Created: latency-svc-546rc
Feb 20 19:02:28.497: INFO: Got endpoints: latency-svc-x9l4c [793.171828ms]
Feb 20 19:02:28.497: INFO: Got endpoints: latency-svc-49c2s [843.803277ms]
Feb 20 19:02:28.513: INFO: Got endpoints: latency-svc-j2n9m [759.637467ms]
Feb 20 19:02:28.604: INFO: Created: latency-svc-4r66d
Feb 20 19:02:28.604: INFO: Got endpoints: latency-svc-tvhrl [800.62511ms]
Feb 20 19:02:28.604: INFO: Created: latency-svc-qmkvp
Feb 20 19:02:28.604: INFO: Got endpoints: latency-svc-4226s [750.2207ms]
Feb 20 19:02:28.608: INFO: Created: latency-svc-tnfwd
Feb 20 19:02:28.624: INFO: Created: latency-svc-z4zff
Feb 20 19:02:28.629: INFO: Created: latency-svc-xdlgt
Feb 20 19:02:28.653: INFO: Got endpoints: latency-svc-m9mpq [750.000337ms]
Feb 20 19:02:28.674: INFO: Created: latency-svc-bdxv2
Feb 20 19:02:28.703: INFO: Got endpoints: latency-svc-hbv78 [749.963188ms]
Feb 20 19:02:28.729: INFO: Created: latency-svc-mbfvq
Feb 20 19:02:28.753: INFO: Got endpoints: latency-svc-kmp9z [749.987806ms]
Feb 20 19:02:28.775: INFO: Created: latency-svc-jvt5w
Feb 20 19:02:28.803: INFO: Got endpoints: latency-svc-zdxnw [749.944483ms]
Feb 20 19:02:28.831: INFO: Created: latency-svc-cjxmz
Feb 20 19:02:28.853: INFO: Got endpoints: latency-svc-fzmqh [749.924287ms]
Feb 20 19:02:28.875: INFO: Created: latency-svc-dpt4l
Feb 20 19:02:28.903: INFO: Got endpoints: latency-svc-6cswk [749.881181ms]
Feb 20 19:02:28.925: INFO: Created: latency-svc-nl8sx
Feb 20 19:02:28.953: INFO: Got endpoints: latency-svc-thb6l [738.246437ms]
Feb 20 19:02:28.974: INFO: Created: latency-svc-w7zm8
Feb 20 19:02:29.003: INFO: Got endpoints: latency-svc-2t7n9 [750.086646ms]
Feb 20 19:02:29.024: INFO: Created: latency-svc-pzwsk
Feb 20 19:02:29.053: INFO: Got endpoints: latency-svc-z9vxc [741.974475ms]
Feb 20 19:02:29.074: INFO: Created: latency-svc-ftw69
Feb 20 19:02:29.103: INFO: Got endpoints: latency-svc-546rc [744.569299ms]
Feb 20 19:02:29.125: INFO: Created: latency-svc-4v78m
Feb 20 19:02:29.153: INFO: Got endpoints: latency-svc-qmkvp [656.168488ms]
Feb 20 19:02:29.174: INFO: Created: latency-svc-8gs77
Feb 20 19:02:29.203: INFO: Got endpoints: latency-svc-4r66d [706.105449ms]
Feb 20 19:02:29.226: INFO: Created: latency-svc-vm922
Feb 20 19:02:29.253: INFO: Got endpoints: latency-svc-tnfwd [740.480184ms]
Feb 20 19:02:29.280: INFO: Created: latency-svc-qghl5
Feb 20 19:02:29.303: INFO: Got endpoints: latency-svc-z4zff [699.698488ms]
Feb 20 19:02:29.326: INFO: Created: latency-svc-h6jzc
Feb 20 19:02:29.353: INFO: Got endpoints: latency-svc-xdlgt [749.513097ms]
Feb 20 19:02:29.375: INFO: Created: latency-svc-x6rj6
Feb 20 19:02:29.403: INFO: Got endpoints: latency-svc-bdxv2 [750.513962ms]
Feb 20 19:02:29.425: INFO: Created: latency-svc-vqgdj
Feb 20 19:02:29.453: INFO: Got endpoints: latency-svc-mbfvq [749.862819ms]
Feb 20 19:02:29.477: INFO: Created: latency-svc-86s46
Feb 20 19:02:29.503: INFO: Got endpoints: latency-svc-jvt5w [750.105293ms]
Feb 20 19:02:29.525: INFO: Created: latency-svc-rwkx2
Feb 20 19:02:29.557: INFO: Got endpoints: latency-svc-cjxmz [753.304208ms]
Feb 20 19:02:29.578: INFO: Created: latency-svc-9sq5d
Feb 20 19:02:29.603: INFO: Got endpoints: latency-svc-dpt4l [750.341774ms]
Feb 20 19:02:29.624: INFO: Created: latency-svc-d5wf9
Feb 20 19:02:29.653: INFO: Got endpoints: latency-svc-nl8sx [749.877151ms]
Feb 20 19:02:29.674: INFO: Created: latency-svc-9t6zl
Feb 20 19:02:29.703: INFO: Got endpoints: latency-svc-w7zm8 [749.996803ms]
Feb 20 19:02:29.724: INFO: Created: latency-svc-mrfcg
Feb 20 19:02:29.753: INFO: Got endpoints: latency-svc-pzwsk [750.039617ms]
Feb 20 19:02:29.775: INFO: Created: latency-svc-b28lh
Feb 20 19:02:29.803: INFO: Got endpoints: latency-svc-ftw69 [749.861861ms]
Feb 20 19:02:29.829: INFO: Created: latency-svc-hwnqz
Feb 20 19:02:29.853: INFO: Got endpoints: latency-svc-4v78m [750.104619ms]
Feb 20 19:02:29.875: INFO: Created: latency-svc-jb6zb
Feb 20 19:02:29.903: INFO: Got endpoints: latency-svc-8gs77 [749.397872ms]
Feb 20 19:02:29.923: INFO: Created: latency-svc-j9kft
Feb 20 19:02:29.953: INFO: Got endpoints: latency-svc-vm922 [749.769219ms]
Feb 20 19:02:29.982: INFO: Created: latency-svc-cjcjf
Feb 20 19:02:30.004: INFO: Got endpoints: latency-svc-qghl5 [750.25997ms]
Feb 20 19:02:30.029: INFO: Created: latency-svc-rvl7w
Feb 20 19:02:30.053: INFO: Got endpoints: latency-svc-h6jzc [749.261422ms]
Feb 20 19:02:30.077: INFO: Created: latency-svc-bgxnn
Feb 20 19:02:30.104: INFO: Got endpoints: latency-svc-x6rj6 [750.532376ms]
Feb 20 19:02:30.125: INFO: Created: latency-svc-ph8fv
Feb 20 19:02:30.153: INFO: Got endpoints: latency-svc-vqgdj [749.543448ms]
Feb 20 19:02:30.174: INFO: Created: latency-svc-674c7
Feb 20 19:02:30.203: INFO: Got endpoints: latency-svc-86s46 [749.761168ms]
Feb 20 19:02:30.224: INFO: Created: latency-svc-2m8g5
Feb 20 19:02:30.253: INFO: Got endpoints: latency-svc-rwkx2 [750.012065ms]
Feb 20 19:02:30.275: INFO: Created: latency-svc-hn5w2
Feb 20 19:02:30.303: INFO: Got endpoints: latency-svc-9sq5d [746.606252ms]
Feb 20 19:02:30.325: INFO: Created: latency-svc-sd564
Feb 20 19:02:30.353: INFO: Got endpoints: latency-svc-d5wf9 [749.783593ms]
Feb 20 19:02:30.375: INFO: Created: latency-svc-jxt95
Feb 20 19:02:30.403: INFO: Got endpoints: latency-svc-9t6zl [749.771682ms]
Feb 20 19:02:30.424: INFO: Created: latency-svc-22j87
Feb 20 19:02:30.453: INFO: Got endpoints: latency-svc-mrfcg [749.9314ms]
Feb 20 19:02:30.475: INFO: Created: latency-svc-2zggm
Feb 20 19:02:30.503: INFO: Got endpoints: latency-svc-b28lh [749.253649ms]
Feb 20 19:02:30.525: INFO: Created: latency-svc-m65f9
Feb 20 19:02:30.553: INFO: Got endpoints: latency-svc-hwnqz [750.055781ms]
Feb 20 19:02:30.574: INFO: Created: latency-svc-pg2zv
Feb 20 19:02:30.604: INFO: Got endpoints: latency-svc-jb6zb [750.059568ms]
Feb 20 19:02:30.653: INFO: Got endpoints: latency-svc-j9kft [750.695363ms]
Feb 20 19:02:30.703: INFO: Got endpoints: latency-svc-cjcjf [750.061907ms]
Feb 20 19:02:30.753: INFO: Got endpoints: latency-svc-rvl7w [749.395282ms]
Feb 20 19:02:30.803: INFO: Got endpoints: latency-svc-bgxnn [750.097466ms]
Feb 20 19:02:30.853: INFO: Got endpoints: latency-svc-ph8fv [749.655423ms]
Feb 20 19:02:30.903: INFO: Got endpoints: latency-svc-674c7 [750.183128ms]
Feb 20 19:02:30.953: INFO: Got endpoints: latency-svc-2m8g5 [750.295514ms]
Feb 20 19:02:31.005: INFO: Got endpoints: latency-svc-hn5w2 [751.238329ms]
Feb 20 19:02:31.053: INFO: Got endpoints: latency-svc-sd564 [750.056076ms]
Feb 20 19:02:31.103: INFO: Got endpoints: latency-svc-jxt95 [750.097907ms]
Feb 20 19:02:31.153: INFO: Got endpoints: latency-svc-22j87 [750.160336ms]
Feb 20 19:02:31.203: INFO: Got endpoints: latency-svc-2zggm [750.244457ms]
Feb 20 19:02:31.253: INFO: Got endpoints: latency-svc-m65f9 [750.523755ms]
Feb 20 19:02:31.304: INFO: Got endpoints: latency-svc-pg2zv [750.904625ms]
Feb 20 19:02:31.304: INFO: Latencies: [89.777658ms 91.238258ms 97.831769ms 98.450256ms 98.596189ms 98.89731ms 105.696974ms 106.30219ms 111.736484ms 115.249378ms 159.158185ms 165.557613ms 165.778262ms 166.620952ms 167.591031ms 170.9268ms 171.822031ms 176.193837ms 176.304389ms 176.46622ms 176.948378ms 177.633762ms 179.569256ms 183.205932ms 196.61196ms 197.338265ms 207.436442ms 209.708916ms 210.360877ms 258.363049ms 265.527599ms 268.486112ms 278.61719ms 279.358431ms 292.947129ms 297.745147ms 304.316433ms 308.011232ms 308.393723ms 308.478644ms 350.97908ms 374.737069ms 379.815392ms 384.503609ms 386.847077ms 392.476054ms 400.487415ms 449.810423ms 474.565668ms 500.109422ms 521.516192ms 542.812164ms 571.503413ms 622.08492ms 656.168488ms 671.547272ms 673.074448ms 699.698488ms 706.105449ms 721.148575ms 722.228918ms 738.246437ms 740.480184ms 741.974475ms 744.53899ms 744.569299ms 744.717367ms 745.638251ms 745.700644ms 746.606252ms 748.818313ms 748.844858ms 749.253649ms 749.261422ms 749.395282ms 749.397872ms 749.41333ms 749.426787ms 749.436141ms 749.455783ms 749.48839ms 749.498735ms 749.513097ms 749.522433ms 749.529854ms 749.543448ms 749.569937ms 749.597222ms 749.612502ms 749.64403ms 749.65258ms 749.655423ms 749.662413ms 749.710994ms 749.722651ms 749.725893ms 749.75061ms 749.756199ms 749.761168ms 749.769219ms 749.771682ms 749.778796ms 749.783593ms 749.789339ms 749.794379ms 749.799464ms 749.831413ms 749.846887ms 749.852232ms 749.861861ms 749.862819ms 749.872471ms 749.874032ms 749.877151ms 749.881181ms 749.896902ms 749.903527ms 749.906029ms 749.913619ms 749.924287ms 749.930337ms 749.9314ms 749.934066ms 749.935616ms 749.944483ms 749.963188ms 749.963497ms 749.963626ms 749.979486ms 749.987806ms 749.995381ms 749.996803ms 750.000337ms 750.012065ms 750.018721ms 750.023706ms 750.030492ms 750.039617ms 750.052686ms 750.055781ms 750.056076ms 750.059568ms 750.061907ms 750.073291ms 750.086646ms 750.093503ms 750.095164ms 750.097466ms 750.097907ms 750.104619ms 750.105293ms 750.113903ms 750.117044ms 750.14833ms 750.150577ms 750.160336ms 750.162974ms 750.183128ms 750.202004ms 750.2207ms 750.235337ms 750.240889ms 750.244457ms 750.248604ms 750.248719ms 750.258702ms 750.25997ms 750.265077ms 750.295514ms 750.327363ms 750.341774ms 750.378725ms 750.401694ms 750.471558ms 750.513962ms 750.523755ms 750.532376ms 750.5535ms 750.562113ms 750.62616ms 750.67346ms 750.695363ms 750.79124ms 750.841463ms 750.904625ms 751.238329ms 751.736606ms 753.304208ms 753.700886ms 754.329222ms 754.599996ms 754.939558ms 757.468634ms 759.637467ms 761.357084ms 776.829013ms 793.171828ms 800.62511ms 827.58462ms 843.803277ms]
Feb 20 19:02:31.304: INFO: 50 %ile: 749.771682ms
Feb 20 19:02:31.304: INFO: 90 %ile: 750.67346ms
Feb 20 19:02:31.304: INFO: 99 %ile: 827.58462ms
Feb 20 19:02:31.304: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:02:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-5b4x8" for this suite.
Feb 20 19:02:47.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:02:47.730: INFO: namespace: e2e-tests-svc-latency-5b4x8, resource: bindings, ignored listing per whitelist
Feb 20 19:02:48.015: INFO: namespace e2e-tests-svc-latency-5b4x8 deletion completed in 16.694691289s

• [SLOW TEST:28.287 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:02:48.015: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-n9gf9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-1d60e497-3542-11e9-9f03-4261fb5d5f3f
STEP: Creating configMap with name cm-test-opt-upd-1d60e4d8-3542-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1d60e497-3542-11e9-9f03-4261fb5d5f3f
STEP: Updating configmap cm-test-opt-upd-1d60e4d8-3542-11e9-9f03-4261fb5d5f3f
STEP: Creating configMap with name cm-test-opt-create-1d60e4ee-3542-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:04:22.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n9gf9" for this suite.
Feb 20 19:04:46.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:04:46.718: INFO: namespace: e2e-tests-projected-n9gf9, resource: bindings, ignored listing per whitelist
Feb 20 19:04:47.288: INFO: namespace e2e-tests-projected-n9gf9 deletion completed in 24.768989216s

• [SLOW TEST:119.273 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:04:47.289: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tksd9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 19:04:52.764: INFO: Successfully updated pod "labelsupdate647a83ed-3542-11e9-9f03-4261fb5d5f3f"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:04:54.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tksd9" for this suite.
Feb 20 19:05:16.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:05:17.125: INFO: namespace: e2e-tests-downward-api-tksd9, resource: bindings, ignored listing per whitelist
Feb 20 19:05:17.657: INFO: namespace e2e-tests-downward-api-tksd9 deletion completed in 22.743800653s

• [SLOW TEST:30.369 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:05:17.658: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qkvwq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:05:20.835: INFO: Waiting up to 5m0s for pod "client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-pods-qkvwq" to be "success or failure"
Feb 20 19:05:20.851: INFO: Pod "client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.938928ms
Feb 20 19:05:22.868: INFO: Pod "client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032405003s
STEP: Saw pod success
Feb 20 19:05:22.868: INFO: Pod "client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:05:22.884: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f container env3cont: <nil>
STEP: delete the pod
Feb 20 19:05:22.929: INFO: Waiting for pod client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:05:22.946: INFO: Pod client-envvars-77f546d0-3542-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:05:22.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qkvwq" for this suite.
Feb 20 19:06:07.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:06:07.396: INFO: namespace: e2e-tests-pods-qkvwq, resource: bindings, ignored listing per whitelist
Feb 20 19:06:07.627: INFO: namespace e2e-tests-pods-qkvwq deletion completed in 44.665144273s

• [SLOW TEST:49.970 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:06:07.627: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-b7qqd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9466363c-3542-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:06:08.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-b7qqd" to be "success or failure"
Feb 20 19:06:08.589: INFO: Pod "pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487311ms
Feb 20 19:06:10.606: INFO: Pod "pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033891307s
STEP: Saw pod success
Feb 20 19:06:10.606: INFO: Pod "pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:06:10.623: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:06:10.669: INFO: Waiting for pod pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:06:10.685: INFO: Pod pod-projected-configmaps-9468c6b8-3542-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:06:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b7qqd" for this suite.
Feb 20 19:06:16.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:06:17.132: INFO: namespace: e2e-tests-projected-b7qqd, resource: bindings, ignored listing per whitelist
Feb 20 19:06:17.371: INFO: namespace e2e-tests-projected-b7qqd deletion completed in 6.668370903s

• [SLOW TEST:9.743 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:06:17.371: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-dhnvm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0220 19:06:18.811703   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 19:06:18.811: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:06:18.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dhnvm" for this suite.
Feb 20 19:06:24.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:06:25.112: INFO: namespace: e2e-tests-gc-dhnvm, resource: bindings, ignored listing per whitelist
Feb 20 19:06:25.531: INFO: namespace e2e-tests-gc-dhnvm deletion completed in 6.702346491s

• [SLOW TEST:8.160 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:06:25.531: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-85fqv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:06:28.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-85fqv" for this suite.
Feb 20 19:07:08.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:07:08.624: INFO: namespace: e2e-tests-kubelet-test-85fqv, resource: bindings, ignored listing per whitelist
Feb 20 19:07:09.145: INFO: namespace e2e-tests-kubelet-test-85fqv deletion completed in 40.698998134s

• [SLOW TEST:43.614 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:07:09.145: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-kqjnl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 19:07:14.113: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:14.129: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:16.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:16.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:18.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:18.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:20.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:20.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:22.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:22.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:24.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:24.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:26.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:26.147: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:28.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:28.147: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:30.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:30.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:32.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:32.147: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:34.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:34.146: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:07:36.129: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:07:36.147: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:07:36.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-kqjnl" for this suite.
Feb 20 19:07:58.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:07:58.455: INFO: namespace: e2e-tests-container-lifecycle-hook-kqjnl, resource: bindings, ignored listing per whitelist
Feb 20 19:07:58.822: INFO: namespace e2e-tests-container-lifecycle-hook-kqjnl deletion completed in 22.659237617s

• [SLOW TEST:49.677 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:07:58.823: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mtp8m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:07:59.732: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-mtp8m'
Feb 20 19:08:00.036: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 19:08:00.036: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 20 19:08:02.071: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-mtp8m'
Feb 20 19:08:02.229: INFO: stderr: ""
Feb 20 19:08:02.230: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:08:02.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mtp8m" for this suite.
Feb 20 19:08:24.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:08:24.881: INFO: namespace: e2e-tests-kubectl-mtp8m, resource: bindings, ignored listing per whitelist
Feb 20 19:08:24.943: INFO: namespace e2e-tests-kubectl-mtp8m deletion completed in 22.696666557s

• [SLOW TEST:26.120 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:08:24.943: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-l94wr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 19:08:30.108: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 19:08:30.124: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 19:08:32.125: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 19:08:32.142: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 19:08:34.125: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 19:08:34.142: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:08:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-l94wr" for this suite.
Feb 20 19:08:58.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:08:58.338: INFO: namespace: e2e-tests-container-lifecycle-hook-l94wr, resource: bindings, ignored listing per whitelist
Feb 20 19:08:58.824: INFO: namespace e2e-tests-container-lifecycle-hook-l94wr deletion completed in 24.664371817s

• [SLOW TEST:33.881 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:08:58.824: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4j46s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:08:59.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-4j46s" to be "success or failure"
Feb 20 19:08:59.769: INFO: Pod "downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.888415ms
Feb 20 19:09:01.786: INFO: Pod "downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032923375s
STEP: Saw pod success
Feb 20 19:09:01.786: INFO: Pod "downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:09:01.802: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:09:01.846: INFO: Waiting for pod downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:09:01.862: INFO: Pod downwardapi-volume-fa710790-3542-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:09:01.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4j46s" for this suite.
Feb 20 19:09:07.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:09:08.327: INFO: namespace: e2e-tests-projected-4j46s, resource: bindings, ignored listing per whitelist
Feb 20 19:09:08.534: INFO: namespace e2e-tests-projected-4j46s deletion completed in 6.65546904s

• [SLOW TEST:9.710 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:09:08.534: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-nnx2k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:09:32.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-nnx2k" for this suite.
Feb 20 19:09:38.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:09:38.632: INFO: namespace: e2e-tests-container-runtime-nnx2k, resource: bindings, ignored listing per whitelist
Feb 20 19:09:38.955: INFO: namespace e2e-tests-container-runtime-nnx2k deletion completed in 6.715510024s

• [SLOW TEST:30.421 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:09:38.955: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-mjjhp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:09:39.830: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 20 19:09:39.863: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 19:09:41.896: INFO: Creating deployment "test-rolling-update-deployment"
Feb 20 19:09:41.913: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 20 19:09:41.945: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Feb 20 19:09:43.979: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 20 19:09:43.995: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:09:44.044: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-mjjhp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mjjhp/deployments/test-rolling-update-deployment,UID:1392fa58-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14303,Generation:1,CreationTimestamp:2019-02-20 19:09:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 19:09:41 +0000 UTC 2019-02-20 19:09:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 19:09:43 +0000 UTC 2019-02-20 19:09:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 19:09:44.062: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-mjjhp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mjjhp/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:13951c18-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14295,Generation:1,CreationTimestamp:2019-02-20 19:09:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1392fa58-3543-11e9-9fe1-e2ab716e6bf8 0xc001898ed7 0xc001898ed8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 19:09:44.062: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 20 19:09:44.062: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-mjjhp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mjjhp/replicasets/test-rolling-update-controller,UID:1257abb3-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14302,Generation:2,CreationTimestamp:2019-02-20 19:09:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1392fa58-3543-11e9-9fe1-e2ab716e6bf8 0xc001898d87 0xc001898d88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:09:44.079: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-sjgcr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-sjgcr,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-mjjhp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-mjjhp/pods/test-rolling-update-deployment-68b55d7bc6-sjgcr,UID:13958b4b-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14294,Generation:0,CreationTimestamp:2019-02-20 19:09:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.148/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 13951c18-3543-11e9-9fe1-e2ab716e6bf8 0xc0019702c7 0xc0019702c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dqgf7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dqgf7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dqgf7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001970410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001970440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:09:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:09:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:09:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:09:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.148,StartTime:2019-02-20 19:09:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 19:09:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://342b789684c52cefe586050f8f3f012be0b3789324013b91dc0be8d1f83e2093}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:09:44.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-mjjhp" for this suite.
Feb 20 19:09:52.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:09:52.273: INFO: namespace: e2e-tests-deployment-mjjhp, resource: bindings, ignored listing per whitelist
Feb 20 19:09:52.797: INFO: namespace e2e-tests-deployment-mjjhp deletion completed in 8.700909807s

• [SLOW TEST:13.842 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:09:52.798: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-896gt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 20 19:09:53.763: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:09:53.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-896gt" for this suite.
Feb 20 19:10:01.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:10:01.941: INFO: namespace: e2e-tests-replication-controller-896gt, resource: bindings, ignored listing per whitelist
Feb 20 19:10:02.487: INFO: namespace e2e-tests-replication-controller-896gt deletion completed in 8.659197058s

• [SLOW TEST:9.689 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:10:02.487: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-nlvpr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-nlvpr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-nlvpr to expose endpoints map[]
Feb 20 19:10:03.383: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-nlvpr exposes endpoints map[] (16.05402ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-nlvpr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-nlvpr to expose endpoints map[pod1:[80]]
Feb 20 19:10:05.512: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-nlvpr exposes endpoints map[pod1:[80]] (2.100079725s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-nlvpr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-nlvpr to expose endpoints map[pod2:[80] pod1:[80]]
Feb 20 19:10:07.677: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-nlvpr exposes endpoints map[pod1:[80] pod2:[80]] (2.147367118s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-nlvpr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-nlvpr to expose endpoints map[pod2:[80]]
Feb 20 19:10:07.727: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-nlvpr exposes endpoints map[pod2:[80]] (31.478429ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-nlvpr
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-nlvpr to expose endpoints map[]
Feb 20 19:10:07.761: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-nlvpr exposes endpoints map[] (16.003162ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:10:07.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-nlvpr" for this suite.
Feb 20 19:10:31.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:10:31.906: INFO: namespace: e2e-tests-services-nlvpr, resource: bindings, ignored listing per whitelist
Feb 20 19:10:32.465: INFO: namespace e2e-tests-services-nlvpr deletion completed in 24.656649585s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:29.978 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:10:32.466: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-tgdhr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:10:33.339: INFO: Creating deployment "nginx-deployment"
Feb 20 19:10:33.355: INFO: Waiting for observed generation 1
Feb 20 19:10:35.390: INFO: Waiting for all required pods to come up
Feb 20 19:10:35.410: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 20 19:10:37.443: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 20 19:10:37.476: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 20 19:10:37.510: INFO: Updating deployment nginx-deployment
Feb 20 19:10:37.510: INFO: Waiting for observed generation 2
Feb 20 19:10:39.542: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 20 19:10:39.559: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 20 19:10:39.576: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 19:10:39.625: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 20 19:10:39.625: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 20 19:10:39.641: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 19:10:39.674: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 20 19:10:39.674: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 20 19:10:39.707: INFO: Updating deployment nginx-deployment
Feb 20 19:10:39.707: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 20 19:10:39.743: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 20 19:10:41.775: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:10:41.807: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tgdhr/deployments/nginx-deployment,UID:323c848d-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14715,Generation:3,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[{Available False 2019-02-20 19:10:39 +0000 UTC 2019-02-20 19:10:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 19:10:41 +0000 UTC 2019-02-20 19:10:33 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:9,CollisionCount:nil,},}

Feb 20 19:10:41.825: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tgdhr/replicasets/nginx-deployment-65bbdb5f8,UID:34b6df42-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14695,Generation:3,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 323c848d-3543-11e9-9fe1-e2ab716e6bf8 0xc0021d8a27 0xc0021d8a28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:10:41.825: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 20 19:10:41.825: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tgdhr/replicasets/nginx-deployment-555b55d965,UID:323de604-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14714,Generation:3,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 323c848d-3543-11e9-9fe1-e2ab716e6bf8 0xc0021d8957 0xc0021d8958}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[],},}
Feb 20 19:10:41.843: INFO: Pod "nginx-deployment-555b55d965-4958z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4958z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-4958z,UID:36078722-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14713,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.162/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc0021d9be7 0xc0021d9be8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d9c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d9c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.162,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://83ffe6acb0725f04f9c66bc86484cafc2d50a868e763ba56504b4fa23cb1937d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.843: INFO: Pod "nginx-deployment-555b55d965-6bdhd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6bdhd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-6bdhd,UID:361116a0-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14709,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.167/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc0021d9d80 0xc0021d9d81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d9de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d9e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-6d75j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6d75j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-6d75j,UID:3242052b-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14569,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.156/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc0021d9f50 0xc0021d9f51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d9fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d9fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.156,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7f7fedd6a6b524a259ac9e4ac1971ef96d2b9024dbfd9dd854a8c91882196ae5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-86b6k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-86b6k,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-86b6k,UID:36138557-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14712,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.168/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b720a0 0xc001b720a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-92z25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-92z25,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-92z25,UID:36138b39-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14708,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.42/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72500 0xc001b72501}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-c4m6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c4m6m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-c4m6m,UID:361119e0-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14702,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.40/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72640 0xc001b72641}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-fbsns" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-fbsns,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-fbsns,UID:3241fc56-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14564,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.154/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72940 0xc001b72941}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.154,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6830c339d2815f30168c0de3600d6dabd97ff2b4920ad2611103130cee2d8da7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-g6cwn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-g6cwn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-g6cwn,UID:36079883-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14644,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72b00 0xc001b72b01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.844: INFO: Pod "nginx-deployment-555b55d965-hcn8z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hcn8z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-hcn8z,UID:32420143-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14553,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.157/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72c40 0xc001b72c41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.157,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://1d2a73a6735141584540084327802883e90dd8200a6ddcbf40a1d5cad3df433a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-jcnpl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jcnpl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-jcnpl,UID:36139259-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14718,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.169/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b72d90 0xc001b72d91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b72df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b72e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-jxn7v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jxn7v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-jxn7v,UID:36137865-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14710,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.166/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73010 0xc001b73011}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-kvjrg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kvjrg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-kvjrg,UID:32418448-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14557,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.153/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73150 0xc001b73151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b731b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b731d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.153,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6327f71e4fb101bdbd53efef33062cde135ba52b9713bedd725d2cf83cb9955e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-l5jd2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-l5jd2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-l5jd2,UID:360705d2-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14721,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b732a0 0xc001b732a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.39,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2dd746b12d7d33a1148186c198cb44c1120aa4b04f4057122f6d48645d8c48e3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-lgc5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lgc5r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-lgc5r,UID:36138ea7-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14717,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.45/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b733f0 0xc001b733f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-mjssq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mjssq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-mjssq,UID:324171ee-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14541,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73530 0xc001b73531}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b735b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.34,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:34 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://9ffff40070ddb49287279099bfc72337d53d4ac37bb1f06307c6ee341eaf17a5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-r724r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-r724r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-r724r,UID:3241f21b-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14544,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73680 0xc001b73681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b736e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.36,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:34 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c710ca6d57df097403ce29232465002edb1d152639a54a065468ecdf24ec2635}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.845: INFO: Pod "nginx-deployment-555b55d965-v9fqs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v9fqs,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-v9fqs,UID:361118f5-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14703,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.164/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b737d0 0xc001b737d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-555b55d965-wc94r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wc94r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-wc94r,UID:3242e9da-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14567,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.158/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73910 0xc001b73911}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.158,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7ce99e1774948ce259bac3be733ab1b702452ca71038c708f90f2fc23f104f7b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-555b55d965-xnml6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xnml6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-xnml6,UID:361102d5-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14711,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73a60 0xc001b73a61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-555b55d965-z6k6m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z6k6m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-555b55d965-z6k6m,UID:3242ff75-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14538,Generation:0,CreationTimestamp:2019-02-20 19:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 323de604-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73ba0 0xc001b73ba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.35,StartTime:2019-02-20 19:10:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 19:10:34 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://1996e1b2cea04604d2088e8f2b64a47b860a3388c62e8623b88dbfc5ffeb91a7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-78q8p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-78q8p,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-78q8p,UID:34d3128f-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14618,Generation:0,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.160/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73cf0 0xc001b73cf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-7mxs6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7mxs6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-7mxs6,UID:3611931b-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14683,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc001b73ec0 0xc001b73ec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b73f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b73fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-9tq6z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-9tq6z,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-9tq6z,UID:3610facf-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14671,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236c070 0xc00236c071}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236c1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236c1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-fhqmc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-fhqmc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-fhqmc,UID:36119382-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14720,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236c290 0xc00236c291}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236c310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236c330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-gw5z4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gw5z4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-gw5z4,UID:3610ee44-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14701,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.163/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236c400 0xc00236c401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236c480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236c4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.846: INFO: Pod "nginx-deployment-65bbdb5f8-jpkzx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jpkzx,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-jpkzx,UID:34b7f1fc-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14616,Generation:0,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236c5a0 0xc00236c5a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236c610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236c630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-kf82j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kf82j,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-kf82j,UID:36119c90-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14706,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.41/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236c710 0xc00236c711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236cf90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236cfb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-ljfcn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ljfcn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-ljfcn,UID:36227d92-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14716,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236d080 0xc00236d081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236d180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236d1a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-r5ldk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-r5ldk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-r5ldk,UID:34d44ce0-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14619,Generation:0,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.161/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236d280 0xc00236d281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236dae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236db00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-txmzz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-txmzz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-txmzz,UID:36118308-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14707,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.165/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc00236dbd0 0xc00236dbd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ea6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ea700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-xcl6n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xcl6n,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-xcl6n,UID:34b76c05-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14690,Generation:0,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.159/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc0021ea8d0 0xc0021ea8d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ea940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ea960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.159,StartTime:2019-02-20 19:10:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-xh4cx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xh4cx,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-xh4cx,UID:34b7f221-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14663,Generation:0,CreationTimestamp:2019-02-20 19:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc0021eadf0 0xc0021eadf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021eae60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021eae80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.37,StartTime:2019-02-20 19:10:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 19:10:41.847: INFO: Pod "nginx-deployment-65bbdb5f8-zc9h7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zc9h7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tgdhr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tgdhr/pods/nginx-deployment-65bbdb5f8-zc9h7,UID:36076c36-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:14719,Generation:0,CreationTimestamp:2019-02-20 19:10:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.46/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 34b6df42-3543-11e9-9fe1-e2ab716e6bf8 0xc0021eb020 0xc0021eb021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s78f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s78f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-s78f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021eb090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021eb0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:10:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:10:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:10:41.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tgdhr" for this suite.
Feb 20 19:10:49.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:10:50.548: INFO: namespace: e2e-tests-deployment-tgdhr, resource: bindings, ignored listing per whitelist
Feb 20 19:10:50.647: INFO: namespace e2e-tests-deployment-tgdhr deletion completed in 8.780149313s

• [SLOW TEST:18.182 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:10:50.648: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kjg62
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-3d1463a5-3543-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:10:51.570: INFO: Waiting up to 5m0s for pod "pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-kjg62" to be "success or failure"
Feb 20 19:10:51.585: INFO: Pod "pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.673662ms
Feb 20 19:10:53.603: INFO: Pod "pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033108758s
Feb 20 19:10:55.620: INFO: Pod "pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049996616s
STEP: Saw pod success
Feb 20 19:10:55.620: INFO: Pod "pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:10:55.636: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:10:55.727: INFO: Waiting for pod pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:10:55.743: INFO: Pod pod-secrets-3d16f0e7-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:10:55.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kjg62" for this suite.
Feb 20 19:11:01.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:02.032: INFO: namespace: e2e-tests-secrets-kjg62, resource: bindings, ignored listing per whitelist
Feb 20 19:11:02.425: INFO: namespace e2e-tests-secrets-kjg62 deletion completed in 6.664945803s

• [SLOW TEST:11.777 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:11:02.425: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tf7c8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:11:03.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-tf7c8" to be "success or failure"
Feb 20 19:11:03.370: INFO: Pod "downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015342ms
Feb 20 19:11:05.387: INFO: Pod "downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032800054s
STEP: Saw pod success
Feb 20 19:11:05.387: INFO: Pod "downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:11:05.403: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:11:05.452: INFO: Waiting for pod downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:11:05.468: INFO: Pod downwardapi-volume-441d1c19-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:11:05.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tf7c8" for this suite.
Feb 20 19:11:11.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:11.648: INFO: namespace: e2e-tests-projected-tf7c8, resource: bindings, ignored listing per whitelist
Feb 20 19:11:12.244: INFO: namespace e2e-tests-projected-tf7c8 deletion completed in 6.759128802s

• [SLOW TEST:9.819 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:11:12.244: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4hmn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:11:13.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-4hmn9" to be "success or failure"
Feb 20 19:11:13.085: INFO: Pod "downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.856799ms
Feb 20 19:11:15.102: INFO: Pod "downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037685979s
STEP: Saw pod success
Feb 20 19:11:15.102: INFO: Pod "downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:11:15.118: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:11:15.166: INFO: Waiting for pod downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:11:15.183: INFO: Pod downwardapi-volume-49e641b6-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:11:15.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4hmn9" for this suite.
Feb 20 19:11:23.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:23.498: INFO: namespace: e2e-tests-downward-api-4hmn9, resource: bindings, ignored listing per whitelist
Feb 20 19:11:24.138: INFO: namespace e2e-tests-downward-api-4hmn9 deletion completed in 8.938249297s

• [SLOW TEST:11.894 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:11:24.138: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-5fd8p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:11:25.107: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"51122199-3543-11e9-9fe1-e2ab716e6bf8", Controller:(*bool)(0xc0026a1936), BlockOwnerDeletion:(*bool)(0xc0026a1937)}}
Feb 20 19:11:25.125: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"510c87c4-3543-11e9-9fe1-e2ab716e6bf8", Controller:(*bool)(0xc0023eb986), BlockOwnerDeletion:(*bool)(0xc0023eb987)}}
Feb 20 19:11:25.142: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"510f1001-3543-11e9-9fe1-e2ab716e6bf8", Controller:(*bool)(0xc0026a1b5e), BlockOwnerDeletion:(*bool)(0xc0026a1b5f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:11:30.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5fd8p" for this suite.
Feb 20 19:11:36.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:36.743: INFO: namespace: e2e-tests-gc-5fd8p, resource: bindings, ignored listing per whitelist
Feb 20 19:11:36.857: INFO: namespace e2e-tests-gc-5fd8p deletion completed in 6.664355071s

• [SLOW TEST:12.719 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:11:36.858: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-54lgn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 19:11:43.957111   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 19:11:43.957: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:11:43.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-54lgn" for this suite.
Feb 20 19:11:50.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:50.496: INFO: namespace: e2e-tests-gc-54lgn, resource: bindings, ignored listing per whitelist
Feb 20 19:11:50.643: INFO: namespace e2e-tests-gc-54lgn deletion completed in 6.669892928s

• [SLOW TEST:13.785 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:11:50.643: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-r68n7
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-60d9eaa8-3543-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-60d9eaa8-3543-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:11:55.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-r68n7" for this suite.
Feb 20 19:12:09.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:09.925: INFO: namespace: e2e-tests-configmap-r68n7, resource: bindings, ignored listing per whitelist
Feb 20 19:12:10.578: INFO: namespace e2e-tests-configmap-r68n7 deletion completed in 14.794411934s

• [SLOW TEST:19.935 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:12:10.578: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-j85rk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:12:11.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-j85rk" to be "success or failure"
Feb 20 19:12:11.469: INFO: Pod "downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.49187ms
Feb 20 19:12:13.486: INFO: Pod "downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032887354s
STEP: Saw pod success
Feb 20 19:12:13.486: INFO: Pod "downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:12:13.503: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:12:13.549: INFO: Waiting for pod downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:12:13.566: INFO: Pod downwardapi-volume-6cb4481d-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:12:13.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j85rk" for this suite.
Feb 20 19:12:21.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:22.030: INFO: namespace: e2e-tests-downward-api-j85rk, resource: bindings, ignored listing per whitelist
Feb 20 19:12:22.238: INFO: namespace e2e-tests-downward-api-j85rk deletion completed in 8.655675693s

• [SLOW TEST:11.660 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:12:22.239: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-h9vlw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-739e176a-3543-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:12:23.069: INFO: Waiting up to 5m0s for pod "pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-h9vlw" to be "success or failure"
Feb 20 19:12:23.085: INFO: Pod "pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03904ms
Feb 20 19:12:25.101: INFO: Pod "pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032346334s
STEP: Saw pod success
Feb 20 19:12:25.101: INFO: Pod "pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:12:25.117: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:12:25.162: INFO: Waiting for pod pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:12:25.177: INFO: Pod pod-configmaps-73a0a9f4-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:12:25.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h9vlw" for this suite.
Feb 20 19:12:31.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:31.664: INFO: namespace: e2e-tests-configmap-h9vlw, resource: bindings, ignored listing per whitelist
Feb 20 19:12:31.920: INFO: namespace e2e-tests-configmap-h9vlw deletion completed in 6.725197181s

• [SLOW TEST:9.681 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:12:31.920: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-8rjmp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 20 19:12:32.758: INFO: Waiting up to 5m0s for pod "var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-var-expansion-8rjmp" to be "success or failure"
Feb 20 19:12:32.774: INFO: Pod "var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.750354ms
Feb 20 19:12:34.790: INFO: Pod "var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032572402s
STEP: Saw pod success
Feb 20 19:12:34.791: INFO: Pod "var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:12:34.807: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:12:34.851: INFO: Waiting for pod var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:12:34.867: INFO: Pod var-expansion-7967052d-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:12:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-8rjmp" for this suite.
Feb 20 19:12:40.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:41.535: INFO: namespace: e2e-tests-var-expansion-8rjmp, resource: bindings, ignored listing per whitelist
Feb 20 19:12:41.585: INFO: namespace e2e-tests-var-expansion-8rjmp deletion completed in 6.696981835s

• [SLOW TEST:9.665 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:12:41.585: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4t2f9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-7f2ec23f-3543-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:12:42.472: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-4t2f9" to be "success or failure"
Feb 20 19:12:42.487: INFO: Pod "pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.662954ms
Feb 20 19:12:44.504: INFO: Pod "pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032450757s
STEP: Saw pod success
Feb 20 19:12:44.504: INFO: Pod "pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:12:44.520: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:12:44.618: INFO: Waiting for pod pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:12:44.634: INFO: Pod pod-projected-configmaps-7f314d12-3543-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:12:44.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4t2f9" for this suite.
Feb 20 19:12:50.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:51.147: INFO: namespace: e2e-tests-projected-4t2f9, resource: bindings, ignored listing per whitelist
Feb 20 19:12:51.347: INFO: namespace e2e-tests-projected-4t2f9 deletion completed in 6.696632284s

• [SLOW TEST:9.762 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:12:51.347: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-df4f6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-df4f6
Feb 20 19:12:54.283: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-df4f6
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 19:12:54.300: INFO: Initial restart count of pod liveness-exec is 0
Feb 20 19:13:44.753: INFO: Restart count of pod e2e-tests-container-probe-df4f6/liveness-exec is now 1 (50.452994301s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:13:44.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-df4f6" for this suite.
Feb 20 19:13:50.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:13:51.213: INFO: namespace: e2e-tests-container-probe-df4f6, resource: bindings, ignored listing per whitelist
Feb 20 19:13:51.454: INFO: namespace e2e-tests-container-probe-df4f6 deletion completed in 6.65809566s

• [SLOW TEST:60.106 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:13:51.454: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-2kfhw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:13:52.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2kfhw" for this suite.
Feb 20 19:13:58.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:13:58.517: INFO: namespace: e2e-tests-kubelet-test-2kfhw, resource: bindings, ignored listing per whitelist
Feb 20 19:13:58.989: INFO: namespace e2e-tests-kubelet-test-2kfhw deletion completed in 6.69882563s

• [SLOW TEST:7.535 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:13:58.989: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-7rmfp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0220 19:14:29.966467   30262 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 19:14:29.966: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:14:29.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7rmfp" for this suite.
Feb 20 19:14:36.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:14:36.137: INFO: namespace: e2e-tests-gc-7rmfp, resource: bindings, ignored listing per whitelist
Feb 20 19:14:36.676: INFO: namespace e2e-tests-gc-7rmfp deletion completed in 6.692289043s

• [SLOW TEST:37.687 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:14:36.676: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6bnt6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 20 19:14:37.537: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml --namespace=e2e-tests-kubectl-6bnt6 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 20 19:14:40.625: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 20 19:14:40.625: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:14:42.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6bnt6" for this suite.
Feb 20 19:14:48.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:14:49.352: INFO: namespace: e2e-tests-kubectl-6bnt6, resource: bindings, ignored listing per whitelist
Feb 20 19:14:49.416: INFO: namespace e2e-tests-kubectl-6bnt6 deletion completed in 6.739474959s

• [SLOW TEST:12.739 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:14:49.416: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-dlfcr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 20 19:14:50.294: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15723,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:14:50.295: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15723,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 20 19:15:00.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15743,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 19:15:00.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15743,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 20 19:15:10.363: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15763,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:15:10.363: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15763,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 20 19:15:20.382: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15784,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:15:20.382: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-a,UID:cb624c49-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15784,Generation:0,CreationTimestamp:2019-02-20 19:14:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 20 19:15:30.402: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-b,UID:e349ee5f-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15804,Generation:0,CreationTimestamp:2019-02-20 19:15:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:15:30.402: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-b,UID:e349ee5f-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15804,Generation:0,CreationTimestamp:2019-02-20 19:15:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 20 19:15:40.421: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-b,UID:e349ee5f-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15824,Generation:0,CreationTimestamp:2019-02-20 19:15:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:15:40.421: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-dlfcr,SelfLink:/api/v1/namespaces/e2e-tests-watch-dlfcr/configmaps/e2e-watch-test-configmap-b,UID:e349ee5f-3543-11e9-9fe1-e2ab716e6bf8,ResourceVersion:15824,Generation:0,CreationTimestamp:2019-02-20 19:15:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:15:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-dlfcr" for this suite.
Feb 20 19:15:56.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:15:56.749: INFO: namespace: e2e-tests-watch-dlfcr, resource: bindings, ignored listing per whitelist
Feb 20 19:15:57.106: INFO: namespace e2e-tests-watch-dlfcr deletion completed in 6.666110122s

• [SLOW TEST:67.690 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:15:57.106: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-xb2st
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-74d9w
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 20 19:16:07.802: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-6br78
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:16:24.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-xb2st" for this suite.
Feb 20 19:16:30.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:30.919: INFO: namespace: e2e-tests-namespaces-xb2st, resource: bindings, ignored listing per whitelist
Feb 20 19:16:31.431: INFO: namespace e2e-tests-namespaces-xb2st deletion completed in 6.662203322s
STEP: Destroying namespace "e2e-tests-nsdeletetest-74d9w" for this suite.
Feb 20 19:16:31.448: INFO: Namespace e2e-tests-nsdeletetest-74d9w was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-6br78" for this suite.
Feb 20 19:16:37.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:37.790: INFO: namespace: e2e-tests-nsdeletetest-6br78, resource: bindings, ignored listing per whitelist
Feb 20 19:16:38.115: INFO: namespace e2e-tests-nsdeletetest-6br78 deletion completed in 6.667302912s

• [SLOW TEST:41.009 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:16:38.115: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-t7wqb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-t7wqb
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-t7wqb
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-t7wqb
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-t7wqb
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-t7wqb
Feb 20 19:16:43.283: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-t7wqb, name: ss-0, uid: 0ebad207-3544-11e9-9fe1-e2ab716e6bf8, status phase: Pending. Waiting for statefulset controller to delete.
Feb 20 19:16:43.295: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-t7wqb, name: ss-0, uid: 0ebad207-3544-11e9-9fe1-e2ab716e6bf8, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 19:16:43.333: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-t7wqb, name: ss-0, uid: 0ebad207-3544-11e9-9fe1-e2ab716e6bf8, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 19:16:43.335: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-t7wqb
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-t7wqb
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-t7wqb and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:16:47.408: INFO: Deleting all statefulset in ns e2e-tests-statefulset-t7wqb
Feb 20 19:16:47.424: INFO: Scaling statefulset ss to 0
Feb 20 19:17:27.491: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:17:27.509: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:17:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-t7wqb" for this suite.
Feb 20 19:17:33.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:34.250: INFO: namespace: e2e-tests-statefulset-t7wqb, resource: bindings, ignored listing per whitelist
Feb 20 19:17:34.282: INFO: namespace e2e-tests-statefulset-t7wqb deletion completed in 6.703143269s

• [SLOW TEST:56.167 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:17:34.283: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-hfcql
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 20 19:17:35.247: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-hfcql" to be "success or failure"
Feb 20 19:17:35.263: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 15.663519ms
Feb 20 19:17:37.280: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032466911s
STEP: Saw pod success
Feb 20 19:17:37.280: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 20 19:17:37.296: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 20 19:17:37.342: INFO: Waiting for pod pod-host-path-test to disappear
Feb 20 19:17:37.358: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:17:37.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-hfcql" for this suite.
Feb 20 19:17:43.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:43.782: INFO: namespace: e2e-tests-hostpath-hfcql, resource: bindings, ignored listing per whitelist
Feb 20 19:17:44.090: INFO: namespace e2e-tests-hostpath-hfcql deletion completed in 6.71526007s

• [SLOW TEST:9.808 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:17:44.090: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-zclsz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:17:44.980: INFO: (0) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.009467ms)
Feb 20 19:17:45.023: INFO: (1) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.141411ms)
Feb 20 19:17:45.041: INFO: (2) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.49005ms)
Feb 20 19:17:45.060: INFO: (3) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.298204ms)
Feb 20 19:17:45.079: INFO: (4) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.613663ms)
Feb 20 19:17:45.097: INFO: (5) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.500446ms)
Feb 20 19:17:45.115: INFO: (6) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.610091ms)
Feb 20 19:17:45.133: INFO: (7) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.251619ms)
Feb 20 19:17:45.152: INFO: (8) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.569497ms)
Feb 20 19:17:45.170: INFO: (9) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.379338ms)
Feb 20 19:17:45.189: INFO: (10) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.910473ms)
Feb 20 19:17:45.208: INFO: (11) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.415422ms)
Feb 20 19:17:45.226: INFO: (12) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.404414ms)
Feb 20 19:17:45.245: INFO: (13) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.722704ms)
Feb 20 19:17:45.264: INFO: (14) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.898622ms)
Feb 20 19:17:45.283: INFO: (15) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.683699ms)
Feb 20 19:17:45.301: INFO: (16) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.126666ms)
Feb 20 19:17:45.319: INFO: (17) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.504001ms)
Feb 20 19:17:45.338: INFO: (18) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.086591ms)
Feb 20 19:17:45.356: INFO: (19) /api/v1/nodes/shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.405378ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:17:45.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zclsz" for this suite.
Feb 20 19:17:51.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:51.978: INFO: namespace: e2e-tests-proxy-zclsz, resource: bindings, ignored listing per whitelist
Feb 20 19:17:52.061: INFO: namespace e2e-tests-proxy-zclsz deletion completed in 6.687844461s

• [SLOW TEST:7.970 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:17:52.061: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-65qxw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 19:17:52.939: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:17:56.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-65qxw" for this suite.
Feb 20 19:18:02.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:18:02.479: INFO: namespace: e2e-tests-init-container-65qxw, resource: bindings, ignored listing per whitelist
Feb 20 19:18:02.943: INFO: namespace e2e-tests-init-container-65qxw deletion completed in 6.699988728s

• [SLOW TEST:10.882 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:18:02.943: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-md96d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 20 19:18:03.738: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:04.180: INFO: stderr: ""
Feb 20 19:18:04.180: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:18:04.180: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:04.357: INFO: stderr: ""
Feb 20 19:18:04.357: INFO: stdout: "update-demo-nautilus-5xqpf update-demo-nautilus-tmtfx "
Feb 20 19:18:04.358: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-5xqpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:04.507: INFO: stderr: ""
Feb 20 19:18:04.507: INFO: stdout: ""
Feb 20 19:18:04.507: INFO: update-demo-nautilus-5xqpf is created but not running
Feb 20 19:18:09.507: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:09.679: INFO: stderr: ""
Feb 20 19:18:09.679: INFO: stdout: "update-demo-nautilus-5xqpf update-demo-nautilus-tmtfx "
Feb 20 19:18:09.679: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-5xqpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:09.893: INFO: stderr: ""
Feb 20 19:18:09.893: INFO: stdout: "true"
Feb 20 19:18:09.893: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-5xqpf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:10.091: INFO: stderr: ""
Feb 20 19:18:10.091: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:18:10.091: INFO: validating pod update-demo-nautilus-5xqpf
Feb 20 19:18:10.214: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:18:10.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:18:10.214: INFO: update-demo-nautilus-5xqpf is verified up and running
Feb 20 19:18:10.214: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-tmtfx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:10.350: INFO: stderr: ""
Feb 20 19:18:10.350: INFO: stdout: "true"
Feb 20 19:18:10.350: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-tmtfx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:10.514: INFO: stderr: ""
Feb 20 19:18:10.514: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:18:10.514: INFO: validating pod update-demo-nautilus-tmtfx
Feb 20 19:18:10.618: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:18:10.618: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:18:10.618: INFO: update-demo-nautilus-tmtfx is verified up and running
STEP: rolling-update to new replication controller
Feb 20 19:18:10.622: INFO: scanned /root for discovery docs: <nil>
Feb 20 19:18:10.622: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:28.626: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 19:18:28.626: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:18:28.626: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:28.785: INFO: stderr: ""
Feb 20 19:18:28.785: INFO: stdout: "update-demo-kitten-gj6bb update-demo-kitten-zg8xr update-demo-nautilus-5xqpf "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 20 19:18:33.785: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:33.940: INFO: stderr: ""
Feb 20 19:18:33.940: INFO: stdout: "update-demo-kitten-gj6bb update-demo-kitten-zg8xr "
Feb 20 19:18:33.940: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-gj6bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:34.120: INFO: stderr: ""
Feb 20 19:18:34.120: INFO: stdout: "true"
Feb 20 19:18:34.120: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-gj6bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:34.271: INFO: stderr: ""
Feb 20 19:18:34.271: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 19:18:34.271: INFO: validating pod update-demo-kitten-gj6bb
Feb 20 19:18:34.375: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 19:18:34.375: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 19:18:34.375: INFO: update-demo-kitten-gj6bb is verified up and running
Feb 20 19:18:34.375: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-zg8xr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:34.556: INFO: stderr: ""
Feb 20 19:18:34.556: INFO: stdout: "true"
Feb 20 19:18:34.557: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-zg8xr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-md96d'
Feb 20 19:18:34.718: INFO: stderr: ""
Feb 20 19:18:34.718: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 19:18:34.718: INFO: validating pod update-demo-kitten-zg8xr
Feb 20 19:18:34.819: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 19:18:34.820: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 19:18:34.820: INFO: update-demo-kitten-zg8xr is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:18:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-md96d" for this suite.
Feb 20 19:18:58.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:18:59.278: INFO: namespace: e2e-tests-kubectl-md96d, resource: bindings, ignored listing per whitelist
Feb 20 19:18:59.556: INFO: namespace e2e-tests-kubectl-md96d deletion completed in 24.718809198s

• [SLOW TEST:56.613 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:18:59.556: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-kbd2z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 20 19:19:00.459: INFO: Waiting up to 5m0s for pod "var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-var-expansion-kbd2z" to be "success or failure"
Feb 20 19:19:00.475: INFO: Pod "var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.435296ms
Feb 20 19:19:02.492: INFO: Pod "var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033420659s
STEP: Saw pod success
Feb 20 19:19:02.492: INFO: Pod "var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:19:02.508: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:19:02.554: INFO: Waiting for pod var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:19:02.571: INFO: Pod var-expansion-607d7161-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:19:02.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-kbd2z" for this suite.
Feb 20 19:19:10.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:11.224: INFO: namespace: e2e-tests-var-expansion-kbd2z, resource: bindings, ignored listing per whitelist
Feb 20 19:19:11.335: INFO: namespace e2e-tests-var-expansion-kbd2z deletion completed in 8.74725138s

• [SLOW TEST:11.779 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:19:11.336: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-68566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 19:19:12.241: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:12.615: INFO: stderr: ""
Feb 20 19:19:12.615: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:19:12.615: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:12.807: INFO: stderr: ""
Feb 20 19:19:12.807: INFO: stdout: "update-demo-nautilus-6k7zj update-demo-nautilus-hwgct "
Feb 20 19:19:12.807: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-6k7zj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:12.988: INFO: stderr: ""
Feb 20 19:19:12.988: INFO: stdout: ""
Feb 20 19:19:12.988: INFO: update-demo-nautilus-6k7zj is created but not running
Feb 20 19:19:17.989: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:18.134: INFO: stderr: ""
Feb 20 19:19:18.134: INFO: stdout: "update-demo-nautilus-6k7zj update-demo-nautilus-hwgct "
Feb 20 19:19:18.134: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-6k7zj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:18.322: INFO: stderr: ""
Feb 20 19:19:18.322: INFO: stdout: "true"
Feb 20 19:19:18.322: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-6k7zj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:18.599: INFO: stderr: ""
Feb 20 19:19:18.599: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:19:18.599: INFO: validating pod update-demo-nautilus-6k7zj
Feb 20 19:19:18.633: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:19:18.633: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:19:18.633: INFO: update-demo-nautilus-6k7zj is verified up and running
Feb 20 19:19:18.633: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-hwgct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:18.781: INFO: stderr: ""
Feb 20 19:19:18.782: INFO: stdout: "true"
Feb 20 19:19:18.782: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-hwgct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:18.964: INFO: stderr: ""
Feb 20 19:19:18.964: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:19:18.964: INFO: validating pod update-demo-nautilus-hwgct
Feb 20 19:19:19.068: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:19:19.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:19:19.069: INFO: update-demo-nautilus-hwgct is verified up and running
STEP: using delete to clean up resources
Feb 20 19:19:19.069: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:19.266: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:19:19.266: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 19:19:19.266: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-68566'
Feb 20 19:19:19.472: INFO: stderr: "No resources found.\n"
Feb 20 19:19:19.473: INFO: stdout: ""
Feb 20 19:19:19.473: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-68566 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 19:19:19.631: INFO: stderr: ""
Feb 20 19:19:19.631: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:19:19.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-68566" for this suite.
Feb 20 19:19:43.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:43.843: INFO: namespace: e2e-tests-kubectl-68566, resource: bindings, ignored listing per whitelist
Feb 20 19:19:44.353: INFO: namespace e2e-tests-kubectl-68566 deletion completed in 24.704830401s

• [SLOW TEST:33.017 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:19:44.353: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-bm6tn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 20 19:19:45.333: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16741,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:19:45.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16742,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 19:19:45.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16743,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 20 19:19:55.452: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16764,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:19:55.452: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16765,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 20 19:19:55.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-bm6tn,SelfLink:/api/v1/namespaces/e2e-tests-watch-bm6tn/configmaps/e2e-watch-test-label-changed,UID:7b33b061-3544-11e9-9fe1-e2ab716e6bf8,ResourceVersion:16766,Generation:0,CreationTimestamp:2019-02-20 19:19:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:19:55.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-bm6tn" for this suite.
Feb 20 19:20:01.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:01.762: INFO: namespace: e2e-tests-watch-bm6tn, resource: bindings, ignored listing per whitelist
Feb 20 19:20:02.132: INFO: namespace e2e-tests-watch-bm6tn deletion completed in 6.663449638s

• [SLOW TEST:17.779 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:20:02.132: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-wrsdq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 20 19:20:05.120: INFO: Pod pod-hostip-85cca395-3544-11e9-9f03-4261fb5d5f3f has hostIP: 10.250.0.2
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:20:05.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wrsdq" for this suite.
Feb 20 19:20:29.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:29.299: INFO: namespace: e2e-tests-pods-wrsdq, resource: bindings, ignored listing per whitelist
Feb 20 19:20:29.794: INFO: namespace e2e-tests-pods-wrsdq deletion completed in 24.657766629s

• [SLOW TEST:27.662 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:20:29.795: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8cbws
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:20:30.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-8cbws" to be "success or failure"
Feb 20 19:20:30.773: INFO: Pod "downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.664825ms
Feb 20 19:20:32.790: INFO: Pod "downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032807456s
STEP: Saw pod success
Feb 20 19:20:32.790: INFO: Pod "downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:20:32.806: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:20:32.912: INFO: Waiting for pod downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:20:32.928: INFO: Pod downwardapi-volume-964ff952-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:20:32.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8cbws" for this suite.
Feb 20 19:20:38.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:39.058: INFO: namespace: e2e-tests-downward-api-8cbws, resource: bindings, ignored listing per whitelist
Feb 20 19:20:39.611: INFO: namespace e2e-tests-downward-api-8cbws deletion completed in 6.666191511s

• [SLOW TEST:9.816 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:20:39.611: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lwcq9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:20:40.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-lwcq9" to be "success or failure"
Feb 20 19:20:40.567: INFO: Pod "downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.39034ms
Feb 20 19:20:42.584: INFO: Pod "downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032415673s
Feb 20 19:20:44.601: INFO: Pod "downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049561445s
STEP: Saw pod success
Feb 20 19:20:44.601: INFO: Pod "downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:20:44.618: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:20:44.660: INFO: Waiting for pod downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:20:44.676: INFO: Pod downwardapi-volume-9c2689ce-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:20:44.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lwcq9" for this suite.
Feb 20 19:20:50.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:50.844: INFO: namespace: e2e-tests-downward-api-lwcq9, resource: bindings, ignored listing per whitelist
Feb 20 19:20:51.362: INFO: namespace e2e-tests-downward-api-lwcq9 deletion completed in 6.669770304s

• [SLOW TEST:11.751 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:20:51.363: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8ctpj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a31ff6df-3544-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:20:52.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-8ctpj" to be "success or failure"
Feb 20 19:20:52.286: INFO: Pod "pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.993303ms
Feb 20 19:20:54.302: INFO: Pod "pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032719639s
STEP: Saw pod success
Feb 20 19:20:54.303: INFO: Pod "pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:20:54.319: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:20:54.364: INFO: Waiting for pod pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:20:54.380: INFO: Pod pod-configmaps-a32283dd-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:20:54.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8ctpj" for this suite.
Feb 20 19:21:00.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:21:00.800: INFO: namespace: e2e-tests-configmap-8ctpj, resource: bindings, ignored listing per whitelist
Feb 20 19:21:01.100: INFO: namespace e2e-tests-configmap-8ctpj deletion completed in 6.703178939s

• [SLOW TEST:9.737 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:21:01.100: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qnt7l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:21:01.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-qnt7l" to be "success or failure"
Feb 20 19:21:01.974: INFO: Pod "downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.876108ms
Feb 20 19:21:03.991: INFO: Pod "downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.03396729s
Feb 20 19:21:06.007: INFO: Pod "downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0506038s
STEP: Saw pod success
Feb 20 19:21:06.007: INFO: Pod "downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:21:06.024: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:21:06.068: INFO: Waiting for pod downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:21:06.085: INFO: Pod downwardapi-volume-a8e89eef-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:21:06.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qnt7l" for this suite.
Feb 20 19:21:12.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:21:12.617: INFO: namespace: e2e-tests-projected-qnt7l, resource: bindings, ignored listing per whitelist
Feb 20 19:21:12.825: INFO: namespace e2e-tests-projected-qnt7l deletion completed in 6.723259428s

• [SLOW TEST:11.725 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:21:12.826: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hjts4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:21:13.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-hjts4" to be "success or failure"
Feb 20 19:21:13.675: INFO: Pod "downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.670857ms
Feb 20 19:21:15.693: INFO: Pod "downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032932528s
STEP: Saw pod success
Feb 20 19:21:15.693: INFO: Pod "downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:21:15.710: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:21:15.755: INFO: Waiting for pod downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:21:15.771: INFO: Pod downwardapi-volume-afe2559e-3544-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:21:15.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hjts4" for this suite.
Feb 20 19:21:21.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:21:22.244: INFO: namespace: e2e-tests-downward-api-hjts4, resource: bindings, ignored listing per whitelist
Feb 20 19:21:22.642: INFO: namespace e2e-tests-downward-api-hjts4 deletion completed in 6.854654702s

• [SLOW TEST:9.816 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:21:22.642: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-rnzzj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-rnzzj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 171.206.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.206.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.206.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.206.171_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-rnzzj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-rnzzj.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-rnzzj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-rnzzj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 171.206.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.206.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.206.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.206.171_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 19:21:25.969: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.012: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.031: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.050: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.069: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.087: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.105: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.125: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.521: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.539: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.558: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.576: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.595: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.612: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.636: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:26.654: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:27.048: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:32.067: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.104: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.122: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.140: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.158: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.195: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.592: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.611: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.630: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.648: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.667: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.685: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.703: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:32.723: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:33.076: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:37.067: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.103: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.121: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.139: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.157: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.195: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.593: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.611: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.629: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.648: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.666: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.684: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.702: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:37.720: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:38.073: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:42.067: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.103: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.123: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.140: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.158: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.193: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.630: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.648: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.666: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.685: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.705: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.724: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.743: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:42.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:43.156: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:47.067: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.109: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.127: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.145: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.163: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.181: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.198: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.217: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.613: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.632: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.650: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.669: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.688: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.706: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.725: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:47.743: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:48.098: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:52.067: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.104: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.122: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.139: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.158: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.196: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.634: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.652: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.672: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.691: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.709: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.727: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.745: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:52.764: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc from pod e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f: the server could not find the requested resource (get pods dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f)
Feb 20 19:21:53.159: INFO: Lookups using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj wheezy_udp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-rnzzj jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj jessie_udp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@dns-test-service.e2e-tests-dns-rnzzj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-rnzzj.svc]

Feb 20 19:21:58.800: INFO: DNS probes using e2e-tests-dns-rnzzj/dns-test-b5f06813-3544-11e9-9f03-4261fb5d5f3f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:21:58.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-rnzzj" for this suite.
Feb 20 19:22:04.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:22:05.368: INFO: namespace: e2e-tests-dns-rnzzj, resource: bindings, ignored listing per whitelist
Feb 20 19:22:05.545: INFO: namespace e2e-tests-dns-rnzzj deletion completed in 6.659951336s

• [SLOW TEST:42.904 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:22:05.547: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-m7vhq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 19:22:09.080: INFO: Successfully updated pod "annotationupdatecf5a4f24-3544-11e9-9f03-4261fb5d5f3f"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:22:11.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-m7vhq" for this suite.
Feb 20 19:22:33.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:22:33.849: INFO: namespace: e2e-tests-downward-api-m7vhq, resource: bindings, ignored listing per whitelist
Feb 20 19:22:33.865: INFO: namespace e2e-tests-downward-api-m7vhq deletion completed in 22.719906978s

• [SLOW TEST:28.319 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:22:33.866: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sghls
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:22:34.846: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-sghls'
Feb 20 19:22:35.096: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 19:22:35.097: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 20 19:22:35.114: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-sghls'
Feb 20 19:22:35.329: INFO: stderr: ""
Feb 20 19:22:35.329: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:22:35.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sghls" for this suite.
Feb 20 19:22:41.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:22:41.659: INFO: namespace: e2e-tests-kubectl-sghls, resource: bindings, ignored listing per whitelist
Feb 20 19:22:42.015: INFO: namespace e2e-tests-kubectl-sghls deletion completed in 6.668497445s

• [SLOW TEST:8.149 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:22:42.015: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ggkqw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 19:22:45.559: INFO: Successfully updated pod "labelsupdatee51abbae-3544-11e9-9f03-4261fb5d5f3f"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:22:47.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ggkqw" for this suite.
Feb 20 19:23:11.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:12.233: INFO: namespace: e2e-tests-projected-ggkqw, resource: bindings, ignored listing per whitelist
Feb 20 19:23:12.328: INFO: namespace e2e-tests-projected-ggkqw deletion completed in 24.701620932s

• [SLOW TEST:30.313 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:23:12.328: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fwsg9
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f72ee4ea-3544-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f72ee4ea-3544-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:23:17.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fwsg9" for this suite.
Feb 20 19:23:41.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:42.005: INFO: namespace: e2e-tests-projected-fwsg9, resource: bindings, ignored listing per whitelist
Feb 20 19:23:42.293: INFO: namespace e2e-tests-projected-fwsg9 deletion completed in 24.794177766s

• [SLOW TEST:29.965 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:23:42.293: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-5vxcf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:23:43.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-5vxcf" to be "success or failure"
Feb 20 19:23:43.171: INFO: Pod "downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029416ms
Feb 20 19:23:45.188: INFO: Pod "downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037607015s
STEP: Saw pod success
Feb 20 19:23:45.189: INFO: Pod "downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:23:45.205: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:23:45.249: INFO: Waiting for pod downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:23:45.265: INFO: Pod downwardapi-volume-08fcfb83-3545-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:23:45.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5vxcf" for this suite.
Feb 20 19:23:51.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:51.835: INFO: namespace: e2e-tests-downward-api-5vxcf, resource: bindings, ignored listing per whitelist
Feb 20 19:23:51.996: INFO: namespace e2e-tests-downward-api-5vxcf deletion completed in 6.712518701s

• [SLOW TEST:9.702 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:23:51.996: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-vhzdp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vhzdp
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 19:23:52.837: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 19:24:13.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.209:8080/dial?request=hostName&protocol=http&host=100.96.1.208&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-vhzdp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:24:13.119: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:24:13.695: INFO: Waiting for endpoints: map[]
Feb 20 19:24:13.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.209:8080/dial?request=hostName&protocol=http&host=100.96.0.59&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-vhzdp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:24:13.712: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:24:14.257: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:24:14.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vhzdp" for this suite.
Feb 20 19:24:38.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:24:38.373: INFO: namespace: e2e-tests-pod-network-test-vhzdp, resource: bindings, ignored listing per whitelist
Feb 20 19:24:38.995: INFO: namespace e2e-tests-pod-network-test-vhzdp deletion completed in 24.720490785s

• [SLOW TEST:46.999 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:24:38.995: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5rsmt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5rsmt
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-5rsmt
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-5rsmt
Feb 20 19:24:39.899: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 20 19:24:49.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 20 19:24:49.933: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:24:50.673: INFO: stderr: ""
Feb 20 19:24:50.673: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:24:50.673: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:24:50.690: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 19:25:00.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:25:00.709: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:25:00.774: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999556s
Feb 20 19:25:01.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983173896s
Feb 20 19:25:02.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.965862655s
Feb 20 19:25:03.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.948479882s
Feb 20 19:25:04.845: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.931133628s
Feb 20 19:25:05.863: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.912506172s
Feb 20 19:25:06.881: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.894790864s
Feb 20 19:25:07.898: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.876737274s
Feb 20 19:25:08.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.85954446s
Feb 20 19:25:09.934: INFO: Verifying statefulset ss doesn't scale past 1 for another 841.339001ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-5rsmt
Feb 20 19:25:10.951: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:25:11.593: INFO: stderr: ""
Feb 20 19:25:11.593: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:25:11.593: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:25:11.610: INFO: Found 1 stateful pods, waiting for 3
Feb 20 19:25:21.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:25:21.628: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:25:21.628: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 20 19:25:21.660: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:25:22.334: INFO: stderr: ""
Feb 20 19:25:22.335: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:25:22.335: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:25:22.335: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:25:23.056: INFO: stderr: ""
Feb 20 19:25:23.056: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:25:23.056: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:25:23.056: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:25:23.744: INFO: stderr: ""
Feb 20 19:25:23.744: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:25:23.744: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:25:23.744: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:25:23.761: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 20 19:25:33.802: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:25:33.802: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:25:33.802: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:25:33.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999459s
Feb 20 19:25:34.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983146129s
Feb 20 19:25:35.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965741998s
Feb 20 19:25:36.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94849153s
Feb 20 19:25:37.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931033416s
Feb 20 19:25:38.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.913655805s
Feb 20 19:25:39.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.896549141s
Feb 20 19:25:40.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.879268233s
Feb 20 19:25:41.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.861939805s
Feb 20 19:25:43.009: INFO: Verifying statefulset ss doesn't scale past 3 for another 844.239158ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-5rsmt
Feb 20 19:25:44.026: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:25:44.726: INFO: stderr: ""
Feb 20 19:25:44.726: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:25:44.726: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:25:44.726: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:25:45.444: INFO: stderr: ""
Feb 20 19:25:45.444: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:25:45.444: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:25:45.444: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:25:45.792: INFO: rc: 1
Feb 20 19:25:45.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000f505a0 exit status 1 <nil> <nil> true [0xc001422720 0xc001422738 0xc001422750] [0xc001422720 0xc001422738 0xc001422750] [0xc001422730 0xc001422748] [0x933040 0x933040] 0xc001bfca80 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 20 19:25:55.793: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:25:55.958: INFO: rc: 1
Feb 20 19:25:55.958: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f50a80 exit status 1 <nil> <nil> true [0xc001422758 0xc001422770 0xc001422788] [0xc001422758 0xc001422770 0xc001422788] [0xc001422768 0xc001422780] [0x933040 0x933040] 0xc001bfce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:05.958: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:06.105: INFO: rc: 1
Feb 20 19:26:06.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f50d80 exit status 1 <nil> <nil> true [0xc001422790 0xc0014227a8 0xc0014227c0] [0xc001422790 0xc0014227a8 0xc0014227c0] [0xc0014227a0 0xc0014227b8] [0x933040 0x933040] 0xc001bfd2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:16.106: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:16.264: INFO: rc: 1
Feb 20 19:26:16.264: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f51050 exit status 1 <nil> <nil> true [0xc0014227c8 0xc0014227e0 0xc0014227f8] [0xc0014227c8 0xc0014227e0 0xc0014227f8] [0xc0014227d8 0xc0014227f0] [0x933040 0x933040] 0xc001bfd6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:26.265: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:26.434: INFO: rc: 1
Feb 20 19:26:26.434: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f512f0 exit status 1 <nil> <nil> true [0xc001422800 0xc001422828 0xc001422840] [0xc001422800 0xc001422828 0xc001422840] [0xc001422810 0xc001422838] [0x933040 0x933040] 0xc001bfda40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:36.434: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:36.578: INFO: rc: 1
Feb 20 19:26:36.578: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b8450 exit status 1 <nil> <nil> true [0xc0001b66f8 0xc0001b6950 0xc0001b6b60] [0xc0001b66f8 0xc0001b6950 0xc0001b6b60] [0xc0001b68e8 0xc0001b6a50] [0x933040 0x933040] 0xc0018b4480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:46.578: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:46.717: INFO: rc: 1
Feb 20 19:26:46.717: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000bb2300 exit status 1 <nil> <nil> true [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4910 0xc0000d4e98] [0x933040 0x933040] 0xc0013ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:26:56.717: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:26:56.878: INFO: rc: 1
Feb 20 19:26:56.878: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd39e0 exit status 1 <nil> <nil> true [0xc001422010 0xc001422028 0xc001422040] [0xc001422010 0xc001422028 0xc001422040] [0xc001422020 0xc001422038] [0x933040 0x933040] 0xc0018a6300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:06.878: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:07.058: INFO: rc: 1
Feb 20 19:27:07.058: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b89f0 exit status 1 <nil> <nil> true [0xc0001b6ce8 0xc0001b71f8 0xc0001b7458] [0xc0001b6ce8 0xc0001b71f8 0xc0001b7458] [0xc0001b6f58 0xc0001b7240] [0x933040 0x933040] 0xc0018b4780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:17.058: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:17.193: INFO: rc: 1
Feb 20 19:27:17.193: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3c80 exit status 1 <nil> <nil> true [0xc001422048 0xc001422060 0xc001422078] [0xc001422048 0xc001422060 0xc001422078] [0xc001422058 0xc001422070] [0x933040 0x933040] 0xc0018a6600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:27.193: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:27.431: INFO: rc: 1
Feb 20 19:27:27.431: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000bb27b0 exit status 1 <nil> <nil> true [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5190 0xc0000d5360] [0x933040 0x933040] 0xc0013bb080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:37.431: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:37.615: INFO: rc: 1
Feb 20 19:27:37.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001274270 exit status 1 <nil> <nil> true [0xc0009ea020 0xc0009ea0d0 0xc0009ea128] [0xc0009ea020 0xc0009ea0d0 0xc0009ea128] [0xc0009ea0c0 0xc0009ea110] [0x933040 0x933040] 0xc001d74de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:47.616: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:47.756: INFO: rc: 1
Feb 20 19:27:47.756: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b8cf0 exit status 1 <nil> <nil> true [0xc0001b7608 0xc0001b7768 0xc0001b78c0] [0xc0001b7608 0xc0001b7768 0xc0001b78c0] [0xc0001b7738 0xc0001b77f0] [0x933040 0x933040] 0xc0018b4a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:27:57.756: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:27:57.980: INFO: rc: 1
Feb 20 19:27:57.980: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3f50 exit status 1 <nil> <nil> true [0xc001422080 0xc001422098 0xc0014220b0] [0xc001422080 0xc001422098 0xc0014220b0] [0xc001422090 0xc0014220a8] [0x933040 0x933040] 0xc0018a6900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:07.980: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:08.142: INFO: rc: 1
Feb 20 19:28:08.142: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012745a0 exit status 1 <nil> <nil> true [0xc0009ea130 0xc0009ea188 0xc0009ea1a0] [0xc0009ea130 0xc0009ea188 0xc0009ea1a0] [0xc0009ea148 0xc0009ea198] [0x933040 0x933040] 0xc001d750e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:18.142: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:18.314: INFO: rc: 1
Feb 20 19:28:18.314: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c502d0 exit status 1 <nil> <nil> true [0xc0014220b8 0xc0014220d0 0xc0014220e8] [0xc0014220b8 0xc0014220d0 0xc0014220e8] [0xc0014220c8 0xc0014220e0] [0x933040 0x933040] 0xc0018a6c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:28.314: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:28.496: INFO: rc: 1
Feb 20 19:28:28.496: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001274870 exit status 1 <nil> <nil> true [0xc0009ea1a8 0xc0009ea1c0 0xc0009ea1f0] [0xc0009ea1a8 0xc0009ea1c0 0xc0009ea1f0] [0xc0009ea1b8 0xc0009ea1d8] [0x933040 0x933040] 0xc001d753e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:38.499: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:38.766: INFO: rc: 1
Feb 20 19:28:38.766: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000bb2a50 exit status 1 <nil> <nil> true [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d5450 0xc0000d54f0 0xc0000d5678] [0xc0000d54d8 0xc0000d5598] [0x933040 0x933040] 0xc0013bb3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:48.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:48.924: INFO: rc: 1
Feb 20 19:28:48.925: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3a10 exit status 1 <nil> <nil> true [0xc0009ea090 0xc0009ea0f0 0xc0009ea130] [0xc0009ea090 0xc0009ea0f0 0xc0009ea130] [0xc0009ea0d0 0xc0009ea128] [0x933040 0x933040] 0xc001d74cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:28:58.925: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:28:59.110: INFO: rc: 1
Feb 20 19:28:59.110: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b8480 exit status 1 <nil> <nil> true [0xc0001b6018 0xc0001b68e8 0xc0001b6a50] [0xc0001b6018 0xc0001b68e8 0xc0001b6a50] [0xc0001b68c0 0xc0001b6970] [0x933040 0x933040] 0xc0018b45a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:09.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:29:09.294: INFO: rc: 1
Feb 20 19:29:09.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b8a50 exit status 1 <nil> <nil> true [0xc0001b6b60 0xc0001b6f58 0xc0001b7240] [0xc0001b6b60 0xc0001b6f58 0xc0001b7240] [0xc0001b6e70 0xc0001b7208] [0x933040 0x933040] 0xc0018b48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:19.294: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:29:19.451: INFO: rc: 1
Feb 20 19:29:19.451: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012742a0 exit status 1 <nil> <nil> true [0xc001422010 0xc001422028 0xc001422040] [0xc001422010 0xc001422028 0xc001422040] [0xc001422020 0xc001422038] [0x933040 0x933040] 0xc0018a6300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:29.451: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:29:29.631: INFO: rc: 1
Feb 20 19:29:29.632: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001274540 exit status 1 <nil> <nil> true [0xc001422048 0xc001422060 0xc001422078] [0xc001422048 0xc001422060 0xc001422078] [0xc001422058 0xc001422070] [0x933040 0x933040] 0xc0018a6600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:39.632: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:29:39.786: INFO: rc: 1
Feb 20 19:29:39.786: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c50420 exit status 1 <nil> <nil> true [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4270 0xc0000d4a98 0xc0000d4f88] [0xc0000d4910 0xc0000d4e98] [0x933040 0x933040] 0xc0013ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:49.786: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:29:49.917: INFO: rc: 1
Feb 20 19:29:49.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0009b8d50 exit status 1 <nil> <nil> true [0xc0001b7458 0xc0001b7738 0xc0001b77f0] [0xc0001b7458 0xc0001b7738 0xc0001b77f0] [0xc0001b76f8 0xc0001b7798] [0x933040 0x933040] 0xc0018b4ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:29:59.917: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:00.099: INFO: rc: 1
Feb 20 19:30:00.099: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3d10 exit status 1 <nil> <nil> true [0xc0009ea138 0xc0009ea190 0xc0009ea1a8] [0xc0009ea138 0xc0009ea190 0xc0009ea1a8] [0xc0009ea188 0xc0009ea1a0] [0x933040 0x933040] 0xc001d74fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:30:10.099: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:10.233: INFO: rc: 1
Feb 20 19:30:10.233: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c50780 exit status 1 <nil> <nil> true [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5058 0xc0000d51c8 0xc0000d53f8] [0xc0000d5190 0xc0000d5360] [0x933040 0x933040] 0xc0013bb080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:30:20.233: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:20.367: INFO: rc: 1
Feb 20 19:30:20.367: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000bb2060 exit status 1 <nil> <nil> true [0xc0009ea1b0 0xc0009ea1c8 0xc0009ea1f8] [0xc0009ea1b0 0xc0009ea1c8 0xc0009ea1f8] [0xc0009ea1c0 0xc0009ea1f0] [0x933040 0x933040] 0xc001d752c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:30:30.367: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:30.556: INFO: rc: 1
Feb 20 19:30:30.556: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000bb2390 exit status 1 <nil> <nil> true [0xc0009ea200 0xc0009ea240 0xc0009ea268] [0xc0009ea200 0xc0009ea240 0xc0009ea268] [0xc0009ea220 0xc0009ea260] [0x933040 0x933040] 0xc001d755c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:30:40.556: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:40.720: INFO: rc: 1
Feb 20 19:30:40.720: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012748a0 exit status 1 <nil> <nil> true [0xc001422080 0xc001422098 0xc0014220b0] [0xc001422080 0xc001422098 0xc0014220b0] [0xc001422090 0xc0014220a8] [0x933040 0x933040] 0xc0018a6900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:30:50.720: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5rsmt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:50.879: INFO: rc: 1
Feb 20 19:30:50.879: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 20 19:30:50.879: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:30:50.930: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5rsmt
Feb 20 19:30:50.946: INFO: Scaling statefulset ss to 0
Feb 20 19:30:50.996: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:30:51.012: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:30:51.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5rsmt" for this suite.
Feb 20 19:30:57.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:30:57.290: INFO: namespace: e2e-tests-statefulset-5rsmt, resource: bindings, ignored listing per whitelist
Feb 20 19:30:57.745: INFO: namespace e2e-tests-statefulset-5rsmt deletion completed in 6.665778638s

• [SLOW TEST:378.750 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:30:57.745: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x88jf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:30:58.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-x88jf" to be "success or failure"
Feb 20 19:30:58.670: INFO: Pod "downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.377549ms
Feb 20 19:31:00.686: INFO: Pod "downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03203271s
STEP: Saw pod success
Feb 20 19:31:00.686: INFO: Pod "downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:31:00.703: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:31:00.749: INFO: Waiting for pod downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:31:00.765: INFO: Pod downwardapi-volume-0c915f18-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:31:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x88jf" for this suite.
Feb 20 19:31:06.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:31:07.394: INFO: namespace: e2e-tests-projected-x88jf, resource: bindings, ignored listing per whitelist
Feb 20 19:31:07.441: INFO: namespace e2e-tests-projected-x88jf deletion completed in 6.658169347s

• [SLOW TEST:9.695 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:31:07.441: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ck8ff
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-1258a2e1-3546-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:31:08.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-ck8ff" to be "success or failure"
Feb 20 19:31:08.474: INFO: Pod "pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 108.23208ms
Feb 20 19:31:10.491: INFO: Pod "pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.12521507s
STEP: Saw pod success
Feb 20 19:31:10.491: INFO: Pod "pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:31:10.507: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:31:10.553: INFO: Waiting for pod pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:31:10.569: INFO: Pod pod-projected-configmaps-125b35ea-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:31:10.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ck8ff" for this suite.
Feb 20 19:31:16.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:31:16.800: INFO: namespace: e2e-tests-projected-ck8ff, resource: bindings, ignored listing per whitelist
Feb 20 19:31:17.248: INFO: namespace e2e-tests-projected-ck8ff deletion completed in 6.662264667s

• [SLOW TEST:9.807 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:31:17.248: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qjvsn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:31:18.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-qjvsn" to be "success or failure"
Feb 20 19:31:18.268: INFO: Pod "downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.804015ms
Feb 20 19:31:20.294: INFO: Pod "downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042614942s
STEP: Saw pod success
Feb 20 19:31:20.294: INFO: Pod "downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:31:20.311: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:31:20.502: INFO: Waiting for pod downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:31:20.596: INFO: Pod downwardapi-volume-183fb53e-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:31:20.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qjvsn" for this suite.
Feb 20 19:31:26.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:31:26.801: INFO: namespace: e2e-tests-downward-api-qjvsn, resource: bindings, ignored listing per whitelist
Feb 20 19:31:27.458: INFO: namespace e2e-tests-downward-api-qjvsn deletion completed in 6.753297136s

• [SLOW TEST:10.209 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:31:27.458: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mf6ww
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:31:28.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-mf6ww" to be "success or failure"
Feb 20 19:31:28.373: INFO: Pod "downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.647334ms
Feb 20 19:31:30.390: INFO: Pod "downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03290286s
STEP: Saw pod success
Feb 20 19:31:30.390: INFO: Pod "downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:31:30.406: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:31:30.451: INFO: Waiting for pod downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:31:30.467: INFO: Pod downwardapi-volume-1e45a5c8-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:31:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mf6ww" for this suite.
Feb 20 19:31:36.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:31:36.973: INFO: namespace: e2e-tests-projected-mf6ww, resource: bindings, ignored listing per whitelist
Feb 20 19:31:37.248: INFO: namespace e2e-tests-projected-mf6ww deletion completed in 6.764445307s

• [SLOW TEST:9.790 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:31:37.248: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-b24pn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-b24pn
Feb 20 19:31:40.086: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-b24pn
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 19:31:40.102: INFO: Initial restart count of pod liveness-http is 0
Feb 20 19:31:54.236: INFO: Restart count of pod e2e-tests-container-probe-b24pn/liveness-http is now 1 (14.134142999s elapsed)
Feb 20 19:32:14.409: INFO: Restart count of pod e2e-tests-container-probe-b24pn/liveness-http is now 2 (34.307004418s elapsed)
Feb 20 19:32:34.581: INFO: Restart count of pod e2e-tests-container-probe-b24pn/liveness-http is now 3 (54.479056283s elapsed)
Feb 20 19:32:54.751: INFO: Restart count of pod e2e-tests-container-probe-b24pn/liveness-http is now 4 (1m14.649372117s elapsed)
Feb 20 19:34:05.455: INFO: Restart count of pod e2e-tests-container-probe-b24pn/liveness-http is now 5 (2m25.353132904s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:34:05.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b24pn" for this suite.
Feb 20 19:34:11.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:34:12.134: INFO: namespace: e2e-tests-container-probe-b24pn, resource: bindings, ignored listing per whitelist
Feb 20 19:34:12.197: INFO: namespace e2e-tests-container-probe-b24pn deletion completed in 6.705733483s

• [SLOW TEST:154.949 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:34:12.197: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pr8th
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 20 19:34:13.256: INFO: Waiting up to 5m0s for pod "pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-pr8th" to be "success or failure"
Feb 20 19:34:13.272: INFO: Pod "pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.737559ms
Feb 20 19:34:15.289: INFO: Pod "pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032802955s
Feb 20 19:34:17.307: INFO: Pod "pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050562981s
STEP: Saw pod success
Feb 20 19:34:17.307: INFO: Pod "pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:34:17.323: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:34:17.413: INFO: Waiting for pod pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:34:17.429: INFO: Pod pod-808f2b55-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:34:17.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pr8th" for this suite.
Feb 20 19:34:23.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:34:23.713: INFO: namespace: e2e-tests-emptydir-pr8th, resource: bindings, ignored listing per whitelist
Feb 20 19:34:24.152: INFO: namespace e2e-tests-emptydir-pr8th deletion completed in 6.70554373s

• [SLOW TEST:11.955 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:34:24.152: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bmwgs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-8788ecb8-3546-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:34:24.975: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-bmwgs" to be "success or failure"
Feb 20 19:34:24.992: INFO: Pod "pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.78417ms
Feb 20 19:34:27.074: INFO: Pod "pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.098949115s
STEP: Saw pod success
Feb 20 19:34:27.074: INFO: Pod "pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:34:27.090: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:34:27.135: INFO: Waiting for pod pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:34:27.151: INFO: Pod pod-projected-secrets-878b74b8-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:34:27.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bmwgs" for this suite.
Feb 20 19:34:33.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:34:33.334: INFO: namespace: e2e-tests-projected-bmwgs, resource: bindings, ignored listing per whitelist
Feb 20 19:34:33.842: INFO: namespace e2e-tests-projected-bmwgs deletion completed in 6.674130215s

• [SLOW TEST:9.690 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:34:33.842: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vb64x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 20 19:34:34.749: INFO: Waiting up to 5m0s for pod "client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-containers-vb64x" to be "success or failure"
Feb 20 19:34:34.765: INFO: Pod "client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.463975ms
Feb 20 19:34:36.782: INFO: Pod "client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.032419965s
Feb 20 19:34:38.798: INFO: Pod "client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04916726s
STEP: Saw pod success
Feb 20 19:34:38.798: INFO: Pod "client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:34:38.815: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:34:38.858: INFO: Waiting for pod client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:34:38.874: INFO: Pod client-containers-8d5eeb8e-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:34:38.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vb64x" for this suite.
Feb 20 19:34:44.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:34:45.521: INFO: namespace: e2e-tests-containers-vb64x, resource: bindings, ignored listing per whitelist
Feb 20 19:34:45.552: INFO: namespace e2e-tests-containers-vb64x deletion completed in 6.660889383s

• [SLOW TEST:11.710 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:34:45.553: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2fj9c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9459283d-3546-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:34:46.473: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-2fj9c" to be "success or failure"
Feb 20 19:34:46.491: INFO: Pod "pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.983881ms
Feb 20 19:34:48.509: INFO: Pod "pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036366784s
STEP: Saw pod success
Feb 20 19:34:48.509: INFO: Pod "pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:34:48.526: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:34:48.603: INFO: Waiting for pod pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:34:48.620: INFO: Pod pod-projected-configmaps-945bb375-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:34:48.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2fj9c" for this suite.
Feb 20 19:34:54.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:34:55.339: INFO: namespace: e2e-tests-projected-2fj9c, resource: bindings, ignored listing per whitelist
Feb 20 19:34:55.339: INFO: namespace e2e-tests-projected-2fj9c deletion completed in 6.702247312s

• [SLOW TEST:9.786 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:34:55.339: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-84fg5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:34:56.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-84fg5" to be "success or failure"
Feb 20 19:34:56.296: INFO: Pod "downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.10929ms
Feb 20 19:34:58.312: INFO: Pod "downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056442361s
Feb 20 19:35:00.329: INFO: Pod "downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073633775s
STEP: Saw pod success
Feb 20 19:35:00.329: INFO: Pod "downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:35:00.346: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:35:00.392: INFO: Waiting for pod downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:35:00.455: INFO: Pod downwardapi-volume-9a3070b8-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:35:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-84fg5" for this suite.
Feb 20 19:35:06.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:35:07.115: INFO: namespace: e2e-tests-downward-api-84fg5, resource: bindings, ignored listing per whitelist
Feb 20 19:35:07.276: INFO: namespace e2e-tests-downward-api-84fg5 deletion completed in 6.803573756s

• [SLOW TEST:11.937 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:35:07.276: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p8t8f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:35:08.137: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p8t8f'
Feb 20 19:35:09.253: INFO: stderr: ""
Feb 20 19:35:09.253: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 20 19:35:14.303: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p8t8f -o json'
Feb 20 19:35:14.449: INFO: stderr: ""
Feb 20 19:35:14.449: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.222/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-20T19:35:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-p8t8f\",\n        \"resourceVersion\": \"19155\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-p8t8f/pods/e2e-test-nginx-pod\",\n        \"uid\": \"a1ef4ec7-3546-11e9-9fe1-e2ab716e6bf8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rwjqx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rwjqx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rwjqx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T19:35:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T19:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T19:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T19:35:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://570c4d05b2bd998ef4269a9a46c1338a2dcf45ca8576d9d54646f8abf2874e26\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-20T19:35:10Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.222\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-20T19:35:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 20 19:35:14.449: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-p8t8f'
Feb 20 19:35:14.740: INFO: stderr: ""
Feb 20 19:35:14.740: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 20 19:35:14.757: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p8t8f'
Feb 20 19:35:25.324: INFO: stderr: ""
Feb 20 19:35:25.324: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:35:25.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p8t8f" for this suite.
Feb 20 19:35:31.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:35:31.570: INFO: namespace: e2e-tests-kubectl-p8t8f, resource: bindings, ignored listing per whitelist
Feb 20 19:35:32.043: INFO: namespace e2e-tests-kubectl-p8t8f deletion completed in 6.700380431s

• [SLOW TEST:24.767 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:35:32.043: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mgnpf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 19:35:32.871: INFO: Waiting up to 5m0s for pod "downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-mgnpf" to be "success or failure"
Feb 20 19:35:32.888: INFO: Pod "downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.225171ms
Feb 20 19:35:34.904: INFO: Pod "downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033163517s
STEP: Saw pod success
Feb 20 19:35:34.905: INFO: Pod "downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:35:34.921: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:35:35.016: INFO: Waiting for pod downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:35:35.032: INFO: Pod downward-api-b003ae8a-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:35:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mgnpf" for this suite.
Feb 20 19:35:41.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:35:41.314: INFO: namespace: e2e-tests-downward-api-mgnpf, resource: bindings, ignored listing per whitelist
Feb 20 19:35:41.722: INFO: namespace e2e-tests-downward-api-mgnpf deletion completed in 6.673311979s

• [SLOW TEST:9.679 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:35:41.722: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mzgcf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 20 19:35:42.534: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:42.908: INFO: stderr: ""
Feb 20 19:35:42.908: INFO: stdout: "pod/pause created\n"
Feb 20 19:35:42.908: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 20 19:35:42.908: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-mzgcf" to be "running and ready"
Feb 20 19:35:42.924: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.969426ms
Feb 20 19:35:44.941: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.032911541s
Feb 20 19:35:44.941: INFO: Pod "pause" satisfied condition "running and ready"
Feb 20 19:35:44.941: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 20 19:35:44.941: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:45.126: INFO: stderr: ""
Feb 20 19:35:45.126: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 20 19:35:45.126: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:45.274: INFO: stderr: ""
Feb 20 19:35:45.274: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 20 19:35:45.274: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:45.469: INFO: stderr: ""
Feb 20 19:35:45.469: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 20 19:35:45.470: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:45.713: INFO: stderr: ""
Feb 20 19:35:45.713: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 20 19:35:45.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:45.882: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:35:45.882: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 20 19:35:45.882: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-mzgcf'
Feb 20 19:35:46.109: INFO: stderr: "No resources found.\n"
Feb 20 19:35:46.109: INFO: stdout: ""
Feb 20 19:35:46.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-mzgcf -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 19:35:46.265: INFO: stderr: ""
Feb 20 19:35:46.265: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:35:46.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mzgcf" for this suite.
Feb 20 19:35:52.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:35:52.953: INFO: namespace: e2e-tests-kubectl-mzgcf, resource: bindings, ignored listing per whitelist
Feb 20 19:35:53.018: INFO: namespace e2e-tests-kubectl-mzgcf deletion completed in 6.735359259s

• [SLOW TEST:11.296 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:35:53.018: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hcm7p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-bc941eee-3546-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:35:53.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-hcm7p" to be "success or failure"
Feb 20 19:35:53.982: INFO: Pod "pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.359005ms
Feb 20 19:35:55.999: INFO: Pod "pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032269227s
Feb 20 19:35:58.016: INFO: Pod "pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049207904s
STEP: Saw pod success
Feb 20 19:35:58.016: INFO: Pod "pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:35:58.032: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:35:58.077: INFO: Waiting for pod pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:35:58.093: INFO: Pod pod-configmaps-bc96a8a5-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:35:58.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hcm7p" for this suite.
Feb 20 19:36:04.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:04.800: INFO: namespace: e2e-tests-configmap-hcm7p, resource: bindings, ignored listing per whitelist
Feb 20 19:36:04.911: INFO: namespace e2e-tests-configmap-hcm7p deletion completed in 6.800907238s

• [SLOW TEST:11.893 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:36:04.911: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-44n8m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:36:05.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-downward-api-44n8m" to be "success or failure"
Feb 20 19:36:05.866: INFO: Pod "downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.894748ms
Feb 20 19:36:07.883: INFO: Pod "downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032508266s
STEP: Saw pod success
Feb 20 19:36:07.883: INFO: Pod "downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:36:07.899: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f container client-container: <nil>
STEP: delete the pod
Feb 20 19:36:07.946: INFO: Waiting for pod downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:36:07.962: INFO: Pod downwardapi-volume-c3abe230-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:36:07.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-44n8m" for this suite.
Feb 20 19:36:14.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:14.392: INFO: namespace: e2e-tests-downward-api-44n8m, resource: bindings, ignored listing per whitelist
Feb 20 19:36:14.779: INFO: namespace e2e-tests-downward-api-44n8m deletion completed in 6.799845101s

• [SLOW TEST:9.868 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:36:14.779: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kmdxg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c983f73c-3546-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:36:15.672: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-projected-kmdxg" to be "success or failure"
Feb 20 19:36:15.724: INFO: Pod "pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 51.538033ms
Feb 20 19:36:17.741: INFO: Pod "pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.068452224s
STEP: Saw pod success
Feb 20 19:36:17.741: INFO: Pod "pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:36:17.757: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:36:17.803: INFO: Waiting for pod pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:36:17.819: INFO: Pod pod-projected-configmaps-c9868a78-3546-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:36:17.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kmdxg" for this suite.
Feb 20 19:36:25.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:26.382: INFO: namespace: e2e-tests-projected-kmdxg, resource: bindings, ignored listing per whitelist
Feb 20 19:36:26.575: INFO: namespace e2e-tests-projected-kmdxg deletion completed in 8.739432439s

• [SLOW TEST:11.796 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:36:26.576: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-wgqnd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:36:47.502: INFO: Container started at 2019-02-20 19:36:28 +0000 UTC, pod became ready at 2019-02-20 19:36:46 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:36:47.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wgqnd" for this suite.
Feb 20 19:37:09.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:37:09.783: INFO: namespace: e2e-tests-container-probe-wgqnd, resource: bindings, ignored listing per whitelist
Feb 20 19:37:10.195: INFO: namespace e2e-tests-container-probe-wgqnd deletion completed in 22.675531308s

• [SLOW TEST:43.619 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:37:10.195: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jgwdr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 19:37:11.136: INFO: namespace e2e-tests-kubectl-jgwdr
Feb 20 19:37:11.136: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-jgwdr'
Feb 20 19:37:11.560: INFO: stderr: ""
Feb 20 19:37:11.560: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 19:37:12.577: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:37:12.577: INFO: Found 0 / 1
Feb 20 19:37:13.577: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:37:13.577: INFO: Found 1 / 1
Feb 20 19:37:13.577: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 19:37:13.600: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:37:13.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 19:37:13.600: INFO: wait on redis-master startup in e2e-tests-kubectl-jgwdr 
Feb 20 19:37:13.600: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs redis-master-wdb5h redis-master --namespace=e2e-tests-kubectl-jgwdr'
Feb 20 19:37:13.899: INFO: stderr: ""
Feb 20 19:37:13.899: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 19:37:12.642 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 19:37:12.642 # Server started, Redis version 3.2.12\n1:M 20 Feb 19:37:12.642 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 19:37:12.642 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 20 19:37:13.899: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-jgwdr'
Feb 20 19:37:14.136: INFO: stderr: ""
Feb 20 19:37:14.136: INFO: stdout: "service/rm2 exposed\n"
Feb 20 19:37:14.152: INFO: Service rm2 in namespace e2e-tests-kubectl-jgwdr found.
STEP: exposing service
Feb 20 19:37:16.187: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-jgwdr'
Feb 20 19:37:16.372: INFO: stderr: ""
Feb 20 19:37:16.372: INFO: stdout: "service/rm3 exposed\n"
Feb 20 19:37:16.389: INFO: Service rm3 in namespace e2e-tests-kubectl-jgwdr found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:37:18.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jgwdr" for this suite.
Feb 20 19:37:42.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:37:42.683: INFO: namespace: e2e-tests-kubectl-jgwdr, resource: bindings, ignored listing per whitelist
Feb 20 19:37:43.149: INFO: namespace e2e-tests-kubectl-jgwdr deletion completed in 24.710762196s

• [SLOW TEST:32.954 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:37:43.149: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vw4xr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 19:37:44.036: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:44.404: INFO: stderr: ""
Feb 20 19:37:44.404: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:37:44.404: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:44.599: INFO: stderr: ""
Feb 20 19:37:44.599: INFO: stdout: "update-demo-nautilus-lsz7n update-demo-nautilus-rflh4 "
Feb 20 19:37:44.599: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lsz7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:44.758: INFO: stderr: ""
Feb 20 19:37:44.758: INFO: stdout: ""
Feb 20 19:37:44.758: INFO: update-demo-nautilus-lsz7n is created but not running
Feb 20 19:37:49.758: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:49.942: INFO: stderr: ""
Feb 20 19:37:49.942: INFO: stdout: "update-demo-nautilus-lsz7n update-demo-nautilus-rflh4 "
Feb 20 19:37:49.942: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lsz7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:50.105: INFO: stderr: ""
Feb 20 19:37:50.105: INFO: stdout: "true"
Feb 20 19:37:50.105: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lsz7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:50.302: INFO: stderr: ""
Feb 20 19:37:50.302: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:37:50.302: INFO: validating pod update-demo-nautilus-lsz7n
Feb 20 19:37:50.403: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:37:50.403: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:37:50.403: INFO: update-demo-nautilus-lsz7n is verified up and running
Feb 20 19:37:50.403: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:50.593: INFO: stderr: ""
Feb 20 19:37:50.594: INFO: stdout: "true"
Feb 20 19:37:50.594: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:50.761: INFO: stderr: ""
Feb 20 19:37:50.761: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:37:50.761: INFO: validating pod update-demo-nautilus-rflh4
Feb 20 19:37:50.864: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:37:50.864: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:37:50.864: INFO: update-demo-nautilus-rflh4 is verified up and running
STEP: scaling down the replication controller
Feb 20 19:37:50.870: INFO: scanned /root for discovery docs: <nil>
Feb 20 19:37:50.870: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:51.177: INFO: stderr: ""
Feb 20 19:37:51.177: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:37:51.177: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:51.353: INFO: stderr: ""
Feb 20 19:37:51.353: INFO: stdout: "update-demo-nautilus-lsz7n update-demo-nautilus-rflh4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 19:37:56.354: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:37:56.610: INFO: stderr: ""
Feb 20 19:37:56.610: INFO: stdout: "update-demo-nautilus-lsz7n update-demo-nautilus-rflh4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 19:38:01.610: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:01.797: INFO: stderr: ""
Feb 20 19:38:01.797: INFO: stdout: "update-demo-nautilus-rflh4 "
Feb 20 19:38:01.797: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:01.980: INFO: stderr: ""
Feb 20 19:38:01.980: INFO: stdout: "true"
Feb 20 19:38:01.980: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:02.155: INFO: stderr: ""
Feb 20 19:38:02.155: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:38:02.155: INFO: validating pod update-demo-nautilus-rflh4
Feb 20 19:38:02.176: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:38:02.176: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:38:02.176: INFO: update-demo-nautilus-rflh4 is verified up and running
STEP: scaling up the replication controller
Feb 20 19:38:02.181: INFO: scanned /root for discovery docs: <nil>
Feb 20 19:38:02.181: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:02.388: INFO: stderr: ""
Feb 20 19:38:02.388: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 19:38:02.388: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:02.560: INFO: stderr: ""
Feb 20 19:38:02.560: INFO: stdout: "update-demo-nautilus-rflh4 update-demo-nautilus-wrdv2 "
Feb 20 19:38:02.560: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:02.753: INFO: stderr: ""
Feb 20 19:38:02.753: INFO: stdout: "true"
Feb 20 19:38:02.753: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:02.941: INFO: stderr: ""
Feb 20 19:38:02.941: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:38:02.941: INFO: validating pod update-demo-nautilus-rflh4
Feb 20 19:38:02.960: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:38:02.960: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:38:02.960: INFO: update-demo-nautilus-rflh4 is verified up and running
Feb 20 19:38:02.960: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-wrdv2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:03.116: INFO: stderr: ""
Feb 20 19:38:03.116: INFO: stdout: ""
Feb 20 19:38:03.116: INFO: update-demo-nautilus-wrdv2 is created but not running
Feb 20 19:38:08.117: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:08.341: INFO: stderr: ""
Feb 20 19:38:08.341: INFO: stdout: "update-demo-nautilus-rflh4 update-demo-nautilus-wrdv2 "
Feb 20 19:38:08.341: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:08.577: INFO: stderr: ""
Feb 20 19:38:08.577: INFO: stdout: "true"
Feb 20 19:38:08.577: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-rflh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:08.779: INFO: stderr: ""
Feb 20 19:38:08.779: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:38:08.779: INFO: validating pod update-demo-nautilus-rflh4
Feb 20 19:38:08.799: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:38:08.799: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:38:08.799: INFO: update-demo-nautilus-rflh4 is verified up and running
Feb 20 19:38:08.799: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-wrdv2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:09.006: INFO: stderr: ""
Feb 20 19:38:09.006: INFO: stdout: "true"
Feb 20 19:38:09.006: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-wrdv2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:09.193: INFO: stderr: ""
Feb 20 19:38:09.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 19:38:09.193: INFO: validating pod update-demo-nautilus-wrdv2
Feb 20 19:38:09.295: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 19:38:09.295: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 19:38:09.295: INFO: update-demo-nautilus-wrdv2 is verified up and running
STEP: using delete to clean up resources
Feb 20 19:38:09.295: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:09.469: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:38:09.469: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 19:38:09.469: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-vw4xr'
Feb 20 19:38:09.644: INFO: stderr: "No resources found.\n"
Feb 20 19:38:09.644: INFO: stdout: ""
Feb 20 19:38:09.644: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-vw4xr -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 19:38:09.810: INFO: stderr: ""
Feb 20 19:38:09.810: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:38:09.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vw4xr" for this suite.
Feb 20 19:38:15.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:38:16.147: INFO: namespace: e2e-tests-kubectl-vw4xr, resource: bindings, ignored listing per whitelist
Feb 20 19:38:16.609: INFO: namespace e2e-tests-kubectl-vw4xr deletion completed in 6.782201231s

• [SLOW TEST:33.459 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:38:16.609: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-rdm2f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rdm2f
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 19:38:17.433: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 19:38:33.716: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.0.63 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rdm2f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:38:33.716: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:38:35.221: INFO: Found all expected endpoints: [netserver-0]
Feb 20 19:38:35.238: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.1.231 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rdm2f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:38:35.238: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:38:36.744: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:38:36.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rdm2f" for this suite.
Feb 20 19:39:00.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:01.344: INFO: namespace: e2e-tests-pod-network-test-rdm2f, resource: bindings, ignored listing per whitelist
Feb 20 19:39:01.472: INFO: namespace e2e-tests-pod-network-test-rdm2f deletion completed in 24.711024811s

• [SLOW TEST:44.863 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:39:01.472: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sbtxg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:39:02.366: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 19:39:04.399: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 20 19:39:06.416: INFO: Creating deployment "test-rollover-deployment"
Feb 20 19:39:06.449: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 20 19:39:08.483: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 20 19:39:08.516: INFO: Ensure that both replica sets have 1 created replica
Feb 20 19:39:08.549: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 20 19:39:08.583: INFO: Updating deployment test-rollover-deployment
Feb 20 19:39:08.583: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 20 19:39:10.615: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 20 19:39:10.654: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 20 19:39:10.686: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 19:39:10.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288350, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:39:12.720: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 19:39:12.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288350, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:39:14.724: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 19:39:14.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288350, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:39:16.721: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 19:39:16.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288350, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:39:18.720: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 19:39:18.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288350, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686288346, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:39:20.719: INFO: 
Feb 20 19:39:20.720: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:39:20.768: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-sbtxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sbtxg/deployments/test-rollover-deployment,UID:2f4f74e3-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19960,Generation:2,CreationTimestamp:2019-02-20 19:39:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 19:39:06 +0000 UTC 2019-02-20 19:39:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 19:39:20 +0000 UTC 2019-02-20 19:39:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 19:39:20.785: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-sbtxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sbtxg/replicasets/test-rollover-deployment-6b7f9d6597,UID:3098012e-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19953,Generation:2,CreationTimestamp:2019-02-20 19:39:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2f4f74e3-3547-11e9-9fe1-e2ab716e6bf8 0xc001f900d7 0xc001f900d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 19:39:20.785: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 20 19:39:20.785: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-sbtxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sbtxg/replicasets/test-rollover-controller,UID:2ce04b66-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19959,Generation:2,CreationTimestamp:2019-02-20 19:39:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2f4f74e3-3547-11e9-9fe1-e2ab716e6bf8 0xc001c69f47 0xc001c69f48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:39:20.785: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-sbtxg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sbtxg/replicasets/test-rollover-deployment-6586df867b,UID:2f5156d3-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19920,Generation:2,CreationTimestamp:2019-02-20 19:39:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2f4f74e3-3547-11e9-9fe1-e2ab716e6bf8 0xc001f90007 0xc001f90008}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:39:20.803: INFO: Pod "test-rollover-deployment-6b7f9d6597-nfjbk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-nfjbk,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-sbtxg,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sbtxg/pods/test-rollover-deployment-6b7f9d6597-nfjbk,UID:30a6ee03-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19931,Generation:0,CreationTimestamp:2019-02-20 19:39:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.235/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 3098012e-3547-11e9-9fe1-e2ab716e6bf8 0xc001f90e67 0xc001f90e68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rx4ww {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rx4ww,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rx4ww true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f90ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f90ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:39:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:39:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:39:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:39:08 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.235,StartTime:2019-02-20 19:39:08 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 19:39:09 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0bb14fa1a8a20b061f5caec33530b25747cd280d8b44c77cc1f9739d4724dd7d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:39:20.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sbtxg" for this suite.
Feb 20 19:39:26.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:27.299: INFO: namespace: e2e-tests-deployment-sbtxg, resource: bindings, ignored listing per whitelist
Feb 20 19:39:27.473: INFO: namespace e2e-tests-deployment-sbtxg deletion completed in 6.654424866s

• [SLOW TEST:26.001 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:39:27.473: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-5gmcj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 20 19:39:28.451: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-5gmcj,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gmcj/configmaps/e2e-watch-test-resource-version,UID:3c5ff6b7-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19996,Generation:0,CreationTimestamp:2019-02-20 19:39:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:39:28.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-5gmcj,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gmcj/configmaps/e2e-watch-test-resource-version,UID:3c5ff6b7-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:19997,Generation:0,CreationTimestamp:2019-02-20 19:39:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:39:28.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-5gmcj" for this suite.
Feb 20 19:39:34.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:35.161: INFO: namespace: e2e-tests-watch-5gmcj, resource: bindings, ignored listing per whitelist
Feb 20 19:39:35.162: INFO: namespace e2e-tests-watch-5gmcj deletion completed in 6.694135008s

• [SLOW TEST:7.688 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:39:35.162: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-422b2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 19:39:36.166: INFO: Number of nodes with available pods: 0
Feb 20 19:39:36.166: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:39:37.199: INFO: Number of nodes with available pods: 0
Feb 20 19:39:37.199: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:39:38.200: INFO: Number of nodes with available pods: 2
Feb 20 19:39:38.200: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 20 19:39:38.285: INFO: Number of nodes with available pods: 1
Feb 20 19:39:38.285: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf is running more than one daemon pod
Feb 20 19:39:39.319: INFO: Number of nodes with available pods: 1
Feb 20 19:39:39.319: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf is running more than one daemon pod
Feb 20 19:39:40.319: INFO: Number of nodes with available pods: 2
Feb 20 19:39:40.319: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-422b2, will wait for the garbage collector to delete the pods
Feb 20 19:39:40.435: INFO: Deleting DaemonSet.extensions daemon-set took: 18.565424ms
Feb 20 19:39:40.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.465194ms
Feb 20 19:40:25.351: INFO: Number of nodes with available pods: 0
Feb 20 19:40:25.351: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 19:40:25.367: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-422b2/daemonsets","resourceVersion":"20159"},"items":null}

Feb 20 19:40:25.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-422b2/pods","resourceVersion":"20159"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:40:25.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-422b2" for this suite.
Feb 20 19:40:31.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:40:32.034: INFO: namespace: e2e-tests-daemonsets-422b2, resource: bindings, ignored listing per whitelist
Feb 20 19:40:32.341: INFO: namespace e2e-tests-daemonsets-422b2 deletion completed in 6.893780978s

• [SLOW TEST:57.179 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:40:32.341: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-84cm4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 19:40:33.151: INFO: Waiting up to 5m0s for pod "pod-62feda16-3547-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-84cm4" to be "success or failure"
Feb 20 19:40:33.167: INFO: Pod "pod-62feda16-3547-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.483777ms
Feb 20 19:40:35.183: INFO: Pod "pod-62feda16-3547-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03219176s
STEP: Saw pod success
Feb 20 19:40:35.183: INFO: Pod "pod-62feda16-3547-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:40:35.200: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-62feda16-3547-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:40:35.241: INFO: Waiting for pod pod-62feda16-3547-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:40:35.257: INFO: Pod pod-62feda16-3547-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:40:35.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-84cm4" for this suite.
Feb 20 19:40:43.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:40:43.703: INFO: namespace: e2e-tests-emptydir-84cm4, resource: bindings, ignored listing per whitelist
Feb 20 19:40:43.923: INFO: namespace e2e-tests-emptydir-84cm4 deletion completed in 8.648906244s

• [SLOW TEST:11.582 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:40:43.923: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nn9l6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:40:44.740: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-nn9l6'
Feb 20 19:40:45.008: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 19:40:45.008: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 20 19:40:45.040: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-djhnl]
Feb 20 19:40:45.040: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-djhnl" in namespace "e2e-tests-kubectl-nn9l6" to be "running and ready"
Feb 20 19:40:45.056: INFO: Pod "e2e-test-nginx-rc-djhnl": Phase="Pending", Reason="", readiness=false. Elapsed: 15.884844ms
Feb 20 19:40:47.073: INFO: Pod "e2e-test-nginx-rc-djhnl": Phase="Running", Reason="", readiness=true. Elapsed: 2.033227869s
Feb 20 19:40:47.073: INFO: Pod "e2e-test-nginx-rc-djhnl" satisfied condition "running and ready"
Feb 20 19:40:47.073: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-djhnl]
Feb 20 19:40:47.073: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-nn9l6'
Feb 20 19:40:47.312: INFO: stderr: ""
Feb 20 19:40:47.312: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 20 19:40:47.313: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-nn9l6'
Feb 20 19:40:47.475: INFO: stderr: ""
Feb 20 19:40:47.475: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:40:47.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nn9l6" for this suite.
Feb 20 19:41:11.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:11.999: INFO: namespace: e2e-tests-kubectl-nn9l6, resource: bindings, ignored listing per whitelist
Feb 20 19:41:12.142: INFO: namespace e2e-tests-kubectl-nn9l6 deletion completed in 24.65069489s

• [SLOW TEST:28.219 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:41:12.142: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6xrz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-7ac911a1-3547-11e9-9f03-4261fb5d5f3f
STEP: Creating secret with name s-test-opt-upd-7ac912c8-3547-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7ac911a1-3547-11e9-9f03-4261fb5d5f3f
STEP: Updating secret s-test-opt-upd-7ac912c8-3547-11e9-9f03-4261fb5d5f3f
STEP: Creating secret with name s-test-opt-create-7ac912e3-3547-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:41:17.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6xrz9" for this suite.
Feb 20 19:41:41.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:42.056: INFO: namespace: e2e-tests-projected-6xrz9, resource: bindings, ignored listing per whitelist
Feb 20 19:41:42.249: INFO: namespace e2e-tests-projected-6xrz9 deletion completed in 24.699493737s

• [SLOW TEST:30.107 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:41:42.250: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4jh6z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:41:43.135: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-4jh6z'
Feb 20 19:41:43.372: INFO: stderr: ""
Feb 20 19:41:43.372: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 20 19:41:43.389: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-4jh6z'
Feb 20 19:41:46.301: INFO: stderr: ""
Feb 20 19:41:46.301: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:41:46.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4jh6z" for this suite.
Feb 20 19:41:52.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:52.921: INFO: namespace: e2e-tests-kubectl-4jh6z, resource: bindings, ignored listing per whitelist
Feb 20 19:41:53.017: INFO: namespace e2e-tests-kubectl-4jh6z deletion completed in 6.697147088s

• [SLOW TEST:10.767 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:41:53.017: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-djql9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-7btgx
STEP: Creating secret with name secret-test-9336afd7-3547-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:41:54.369: INFO: Waiting up to 5m0s for pod "pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-djql9" to be "success or failure"
Feb 20 19:41:54.385: INFO: Pod "pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.033702ms
Feb 20 19:41:56.402: INFO: Pod "pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033095508s
Feb 20 19:41:58.419: INFO: Pod "pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050114592s
STEP: Saw pod success
Feb 20 19:41:58.419: INFO: Pod "pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:41:58.436: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:41:58.516: INFO: Waiting for pod pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:41:58.532: INFO: Pod pod-secrets-9367a00f-3547-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:41:58.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-djql9" for this suite.
Feb 20 19:42:04.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:42:04.963: INFO: namespace: e2e-tests-secrets-djql9, resource: bindings, ignored listing per whitelist
Feb 20 19:42:05.218: INFO: namespace e2e-tests-secrets-djql9 deletion completed in 6.669141265s
STEP: Destroying namespace "e2e-tests-secret-namespace-7btgx" for this suite.
Feb 20 19:42:13.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:42:13.791: INFO: namespace: e2e-tests-secret-namespace-7btgx, resource: bindings, ignored listing per whitelist
Feb 20 19:42:14.217: INFO: namespace e2e-tests-secret-namespace-7btgx deletion completed in 8.998911721s

• [SLOW TEST:21.200 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:42:14.218: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kl564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 20 19:42:15.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kl564,SelfLink:/api/v1/namespaces/e2e-tests-watch-kl564/configmaps/e2e-watch-test-watch-closed,UID:9fbe8acb-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:20510,Generation:0,CreationTimestamp:2019-02-20 19:42:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:42:15.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kl564,SelfLink:/api/v1/namespaces/e2e-tests-watch-kl564/configmaps/e2e-watch-test-watch-closed,UID:9fbe8acb-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:20511,Generation:0,CreationTimestamp:2019-02-20 19:42:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 20 19:42:15.166: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kl564,SelfLink:/api/v1/namespaces/e2e-tests-watch-kl564/configmaps/e2e-watch-test-watch-closed,UID:9fbe8acb-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:20512,Generation:0,CreationTimestamp:2019-02-20 19:42:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:42:15.166: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kl564,SelfLink:/api/v1/namespaces/e2e-tests-watch-kl564/configmaps/e2e-watch-test-watch-closed,UID:9fbe8acb-3547-11e9-9fe1-e2ab716e6bf8,ResourceVersion:20513,Generation:0,CreationTimestamp:2019-02-20 19:42:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:42:15.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kl564" for this suite.
Feb 20 19:42:21.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:42:21.756: INFO: namespace: e2e-tests-watch-kl564, resource: bindings, ignored listing per whitelist
Feb 20 19:42:21.899: INFO: namespace e2e-tests-watch-kl564 deletion completed in 6.715968139s

• [SLOW TEST:7.681 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:42:21.899: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-87wzl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 19:42:22.853: INFO: Waiting up to 5m0s for pod "pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-87wzl" to be "success or failure"
Feb 20 19:42:22.869: INFO: Pod "pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.66388ms
Feb 20 19:42:24.886: INFO: Pod "pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032590978s
STEP: Saw pod success
Feb 20 19:42:24.886: INFO: Pod "pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:42:24.902: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:42:24.946: INFO: Waiting for pod pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:42:24.962: INFO: Pod pod-a461a3f9-3547-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:42:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-87wzl" for this suite.
Feb 20 19:42:31.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:42:31.172: INFO: namespace: e2e-tests-emptydir-87wzl, resource: bindings, ignored listing per whitelist
Feb 20 19:42:31.692: INFO: namespace e2e-tests-emptydir-87wzl deletion completed in 6.713403781s

• [SLOW TEST:9.793 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:42:31.692: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-v747g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 20 19:42:36.655: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:42:36.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-v747g" for this suite.
Feb 20 19:43:00.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:01.343: INFO: namespace: e2e-tests-replicaset-v747g, resource: bindings, ignored listing per whitelist
Feb 20 19:43:01.424: INFO: namespace e2e-tests-replicaset-v747g deletion completed in 24.702919568s

• [SLOW TEST:29.732 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:43:01.424: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bg856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 19:43:02.347: INFO: Waiting up to 5m0s for pod "pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-emptydir-bg856" to be "success or failure"
Feb 20 19:43:02.362: INFO: Pod "pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.58557ms
Feb 20 19:43:04.379: INFO: Pod "pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032103376s
STEP: Saw pod success
Feb 20 19:43:04.379: INFO: Pod "pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:43:04.395: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f container test-container: <nil>
STEP: delete the pod
Feb 20 19:43:04.512: INFO: Waiting for pod pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:43:04.528: INFO: Pod pod-bbec3a5f-3547-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:43:04.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bg856" for this suite.
Feb 20 19:43:10.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:11.202: INFO: namespace: e2e-tests-emptydir-bg856, resource: bindings, ignored listing per whitelist
Feb 20 19:43:11.249: INFO: namespace e2e-tests-emptydir-bg856 deletion completed in 6.704200308s

• [SLOW TEST:9.825 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:43:11.249: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-mjz2k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:43:12.128: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 20 19:43:12.161: INFO: Number of nodes with available pods: 0
Feb 20 19:43:12.161: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 20 19:43:12.227: INFO: Number of nodes with available pods: 0
Feb 20 19:43:12.227: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:13.244: INFO: Number of nodes with available pods: 0
Feb 20 19:43:13.244: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:14.243: INFO: Number of nodes with available pods: 1
Feb 20 19:43:14.243: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 20 19:43:14.310: INFO: Number of nodes with available pods: 0
Feb 20 19:43:14.310: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 20 19:43:14.343: INFO: Number of nodes with available pods: 0
Feb 20 19:43:14.343: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:15.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:15.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:16.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:16.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:17.397: INFO: Number of nodes with available pods: 0
Feb 20 19:43:17.397: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:18.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:18.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:19.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:19.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:20.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:20.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:21.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:21.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:22.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:22.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:23.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:23.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:24.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:24.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:25.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:25.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:26.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:26.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:27.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:27.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:28.359: INFO: Number of nodes with available pods: 0
Feb 20 19:43:28.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:29.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:29.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:30.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:30.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:31.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:31.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:32.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:32.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:33.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:33.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:34.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:34.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:35.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:35.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:36.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:36.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:37.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:37.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:38.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:38.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:39.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:39.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:40.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:40.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:41.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:41.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:42.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:42.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:43.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:43.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:44.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:44.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:45.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:45.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:46.361: INFO: Number of nodes with available pods: 0
Feb 20 19:43:46.361: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:47.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:47.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:48.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:48.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:49.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:49.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:50.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:50.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:51.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:51.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:52.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:52.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:53.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:53.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:54.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:54.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:55.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:55.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:56.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:56.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:57.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:57.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:58.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:58.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:43:59.360: INFO: Number of nodes with available pods: 0
Feb 20 19:43:59.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:44:00.360: INFO: Number of nodes with available pods: 0
Feb 20 19:44:00.360: INFO: Node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-5bxcx is running more than one daemon pod
Feb 20 19:44:01.359: INFO: Number of nodes with available pods: 1
Feb 20 19:44:01.360: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-mjz2k, will wait for the garbage collector to delete the pods
Feb 20 19:44:01.480: INFO: Deleting DaemonSet.extensions daemon-set took: 18.767483ms
Feb 20 19:44:01.580: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.328279ms
Feb 20 19:44:39.696: INFO: Number of nodes with available pods: 0
Feb 20 19:44:39.696: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 19:44:39.713: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mjz2k/daemonsets","resourceVersion":"20916"},"items":null}

Feb 20 19:44:39.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mjz2k/pods","resourceVersion":"20916"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:44:39.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mjz2k" for this suite.
Feb 20 19:44:45.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:45.929: INFO: namespace: e2e-tests-daemonsets-mjz2k, resource: bindings, ignored listing per whitelist
Feb 20 19:44:46.564: INFO: namespace e2e-tests-daemonsets-mjz2k deletion completed in 6.750624026s

• [SLOW TEST:95.315 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:44:46.564: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8dm8n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 20 19:44:47.432: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 20 19:44:47.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:47.798: INFO: stderr: ""
Feb 20 19:44:47.798: INFO: stdout: "service/redis-slave created\n"
Feb 20 19:44:47.798: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 20 19:44:47.798: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:48.106: INFO: stderr: ""
Feb 20 19:44:48.106: INFO: stdout: "service/redis-master created\n"
Feb 20 19:44:48.106: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 20 19:44:48.106: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:48.384: INFO: stderr: ""
Feb 20 19:44:48.384: INFO: stdout: "service/frontend created\n"
Feb 20 19:44:48.384: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 20 19:44:48.384: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:48.697: INFO: stderr: ""
Feb 20 19:44:48.697: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 20 19:44:48.697: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 20 19:44:48.697: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:49.125: INFO: stderr: ""
Feb 20 19:44:49.125: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 20 19:44:49.125: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 20 19:44:49.125: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:44:49.415: INFO: stderr: ""
Feb 20 19:44:49.415: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 20 19:44:49.415: INFO: Waiting for all frontend pods to be Running.
Feb 20 19:45:09.466: INFO: Waiting for frontend to serve content.
Feb 20 19:45:14.544: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 20 19:45:19.646: INFO: Trying to add a new entry to the guestbook.
Feb 20 19:45:19.695: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 20 19:45:19.797: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.157: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.157: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 19:45:21.157: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.325: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.325: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 19:45:21.325: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.492: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.493: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 19:45:21.493: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.652: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.652: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 19:45:21.653: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.804: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.804: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 19:45:21.804: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-za9xu.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8dm8n'
Feb 20 19:45:21.957: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 19:45:21.957: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:45:21.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8dm8n" for this suite.
Feb 20 19:46:02.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:46:02.693: INFO: namespace: e2e-tests-kubectl-8dm8n, resource: bindings, ignored listing per whitelist
Feb 20 19:46:02.778: INFO: namespace e2e-tests-kubectl-8dm8n deletion completed in 40.804421447s

• [SLOW TEST:76.214 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:46:02.779: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-shzj9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-27fd5eed-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:46:03.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-shzj9" to be "success or failure"
Feb 20 19:46:03.686: INFO: Pod "pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.560079ms
Feb 20 19:46:05.702: INFO: Pod "pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031817612s
Feb 20 19:46:07.719: INFO: Pod "pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049025039s
STEP: Saw pod success
Feb 20 19:46:07.719: INFO: Pod "pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:46:07.735: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:46:07.781: INFO: Waiting for pod pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:46:07.797: INFO: Pod pod-configmaps-27ffeddf-3548-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:46:07.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-shzj9" for this suite.
Feb 20 19:46:13.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:46:14.499: INFO: namespace: e2e-tests-configmap-shzj9, resource: bindings, ignored listing per whitelist
Feb 20 19:46:14.515: INFO: namespace e2e-tests-configmap-shzj9 deletion completed in 6.701684428s

• [SLOW TEST:11.737 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:46:14.516: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ckszq
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-2ef99830-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating configMap with name cm-test-opt-upd-2ef99c71-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2ef99830-3548-11e9-9f03-4261fb5d5f3f
STEP: Updating configmap cm-test-opt-upd-2ef99c71-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating configMap with name cm-test-opt-create-2ef99d81-3548-11e9-9f03-4261fb5d5f3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:47:45.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ckszq" for this suite.
Feb 20 19:48:09.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:48:09.457: INFO: namespace: e2e-tests-configmap-ckszq, resource: bindings, ignored listing per whitelist
Feb 20 19:48:09.732: INFO: namespace e2e-tests-configmap-ckszq deletion completed in 24.674832761s

• [SLOW TEST:115.216 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:48:09.732: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-zwkhj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-zwkhj
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 20 19:48:10.693: INFO: Found 1 stateful pods, waiting for 3
Feb 20 19:48:20.711: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:48:20.711: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:48:20.711: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 19:48:20.805: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 20 19:48:20.881: INFO: Updating stateful set ss2
Feb 20 19:48:20.913: INFO: Waiting for Pod e2e-tests-statefulset-zwkhj/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 20 19:48:31.006: INFO: Found 2 stateful pods, waiting for 3
Feb 20 19:48:41.024: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:48:41.024: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:48:41.024: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 20 19:48:41.100: INFO: Updating stateful set ss2
Feb 20 19:48:41.135: INFO: Waiting for Pod e2e-tests-statefulset-zwkhj/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 19:48:51.215: INFO: Updating stateful set ss2
Feb 20 19:48:51.254: INFO: Waiting for StatefulSet e2e-tests-statefulset-zwkhj/ss2 to complete update
Feb 20 19:48:51.254: INFO: Waiting for Pod e2e-tests-statefulset-zwkhj/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:49:01.289: INFO: Deleting all statefulset in ns e2e-tests-statefulset-zwkhj
Feb 20 19:49:01.305: INFO: Scaling statefulset ss2 to 0
Feb 20 19:49:21.374: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:49:21.390: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:49:21.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-zwkhj" for this suite.
Feb 20 19:49:29.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:29.963: INFO: namespace: e2e-tests-statefulset-zwkhj, resource: bindings, ignored listing per whitelist
Feb 20 19:49:30.161: INFO: namespace e2e-tests-statefulset-zwkhj deletion completed in 8.703845143s

• [SLOW TEST:80.429 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:49:30.162: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-4js6d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a39c6b6e-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:49:31.076: INFO: Waiting up to 5m0s for pod "pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-4js6d" to be "success or failure"
Feb 20 19:49:31.099: INFO: Pod "pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 23.054103ms
Feb 20 19:49:33.116: INFO: Pod "pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040101615s
STEP: Saw pod success
Feb 20 19:49:33.116: INFO: Pod "pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:49:33.132: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:49:33.181: INFO: Waiting for pod pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:49:33.197: INFO: Pod pod-secrets-a39f349f-3548-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:49:33.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4js6d" for this suite.
Feb 20 19:49:39.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:39.532: INFO: namespace: e2e-tests-secrets-4js6d, resource: bindings, ignored listing per whitelist
Feb 20 19:49:39.958: INFO: namespace e2e-tests-secrets-4js6d deletion completed in 6.744356504s

• [SLOW TEST:9.796 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:49:39.958: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-b54lx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-b54lx/configmap-test-a973003e-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume configMaps
Feb 20 19:49:40.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-configmap-b54lx" to be "success or failure"
Feb 20 19:49:40.883: INFO: Pod "pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.000825ms
Feb 20 19:49:42.900: INFO: Pod "pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03242151s
STEP: Saw pod success
Feb 20 19:49:42.900: INFO: Pod "pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:49:42.916: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f container env-test: <nil>
STEP: delete the pod
Feb 20 19:49:42.960: INFO: Waiting for pod pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:49:42.976: INFO: Pod pod-configmaps-a9758337-3548-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:49:42.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b54lx" for this suite.
Feb 20 19:49:49.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:49.485: INFO: namespace: e2e-tests-configmap-b54lx, resource: bindings, ignored listing per whitelist
Feb 20 19:49:49.644: INFO: namespace e2e-tests-configmap-b54lx deletion completed in 6.650101575s

• [SLOW TEST:9.686 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:49:49.644: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-vwf6z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-af2c5edf-3548-11e9-9f03-4261fb5d5f3f
STEP: Creating a pod to test consume secrets
Feb 20 19:49:50.469: INFO: Waiting up to 5m0s for pod "pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f" in namespace "e2e-tests-secrets-vwf6z" to be "success or failure"
Feb 20 19:49:50.485: INFO: Pod "pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.474149ms
Feb 20 19:49:52.502: INFO: Pod "pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032296377s
STEP: Saw pod success
Feb 20 19:49:52.502: INFO: Pod "pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f" satisfied condition "success or failure"
Feb 20 19:49:52.518: INFO: Trying to get logs from node shoot--it--pub-gcp-za9xu-cpu-worker-z1-6fdc8fdccc-84qmf pod pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:49:52.564: INFO: Waiting for pod pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f to disappear
Feb 20 19:49:52.580: INFO: Pod pod-secrets-af2ed5a9-3548-11e9-9f03-4261fb5d5f3f no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:49:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vwf6z" for this suite.
Feb 20 19:49:58.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:58.728: INFO: namespace: e2e-tests-secrets-vwf6z, resource: bindings, ignored listing per whitelist
Feb 20 19:49:59.252: INFO: namespace e2e-tests-secrets-vwf6z deletion completed in 6.654544323s

• [SLOW TEST:9.608 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 19:49:59.252: INFO: >>> kubeConfig: /tmp/build/03b5f99b/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-wsbrj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 20 19:50:01.015: INFO: Pod name wrapped-volume-race-b572ffdd-3548-11e9-9f03-4261fb5d5f3f: Found 1 pods out of 5
Feb 20 19:50:06.035: INFO: Pod name wrapped-volume-race-b572ffdd-3548-11e9-9f03-4261fb5d5f3f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b572ffdd-3548-11e9-9f03-4261fb5d5f3f in namespace e2e-tests-emptydir-wrapper-wsbrj, will wait for the garbage collector to delete the pods
Feb 20 19:50:12.237: INFO: Deleting ReplicationController wrapped-volume-race-b572ffdd-3548-11e9-9f03-4261fb5d5f3f took: 18.670708ms
Feb 20 19:50:12.337: INFO: Terminating ReplicationController wrapped-volume-race-b572ffdd-3548-11e9-9f03-4261fb5d5f3f pods took: 100.349802ms
STEP: Creating RC which spawns configmap-volume pods
Feb 20 19:50:49.691: INFO: Pod name wrapped-volume-race-d2760605-3548-11e9-9f03-4261fb5d5f3f: Found 1 pods out of 5
Feb 20 19:50:54.710: INFO: Pod name wrapped-volume-race-d2760605-3548-11e9-9f03-4261fb5d5f3f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d2760605-3548-11e9-9f03-4261fb5d5f3f in namespace e2e-tests-emptydir-wrapper-wsbrj, will wait for the garbage collector to delete the pods
Feb 20 19:50:54.879: INFO: Deleting ReplicationController wrapped-volume-race-d2760605-3548-11e9-9f03-4261fb5d5f3f took: 19.293338ms
Feb 20 19:50:54.980: INFO: Terminating ReplicationController wrapped-volume-race-d2760605-3548-11e9-9f03-4261fb5d5f3f pods took: 100.461919ms
STEP: Creating RC which spawns configmap-volume pods
Feb 20 19:51:39.739: INFO: Pod name wrapped-volume-race-f049f465-3548-11e9-9f03-4261fb5d5f3f: Found 2 pods out of 5
Feb 20 19:51:44.761: INFO: Pod name wrapped-volume-race-f049f465-3548-11e9-9f03-4261fb5d5f3f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f049f465-3548-11e9-9f03-4261fb5d5f3f in namespace e2e-tests-emptydir-wrapper-wsbrj, will wait for the garbage collector to delete the pods
Feb 20 19:51:44.931: INFO: Deleting ReplicationController wrapped-volume-race-f049f465-3548-11e9-9f03-4261fb5d5f3f took: 19.476783ms
Feb 20 19:51:45.031: INFO: Terminating ReplicationController wrapped-volume-race-f049f465-3548-11e9-9f03-4261fb5d5f3f pods took: 100.26156ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 19:52:30.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-wsbrj" for this suite.
Feb 20 19:52:36.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:52:37.200: INFO: namespace: e2e-tests-emptydir-wrapper-wsbrj, resource: bindings, ignored listing per whitelist
Feb 20 19:52:37.345: INFO: namespace e2e-tests-emptydir-wrapper-wsbrj deletion completed in 6.70885177s

• [SLOW TEST:158.093 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSFeb 20 19:52:37.346: INFO: Running AfterSuite actions on all nodes
Feb 20 19:52:37.346: INFO: Running AfterSuite actions on node 1
Feb 20 19:52:37.346: INFO: Skipping dumping logs from cluster

Ran 200 of 2161 Specs in 6280.913 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Flaked | 0 Pending | 1961 Skipped PASS

Ginkgo ran 1 suite in 1h44m42.767022584s
Test Suite Passed
