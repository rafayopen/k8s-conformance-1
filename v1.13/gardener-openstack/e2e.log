Conformance test: not doing test setup.
I0228 07:32:02.212261   31734 e2e.go:224] Starting e2e run "f03bec75-3b2a-11e9-a616-82a6c0035a5d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551339121 - Will randomize all specs
Will run 201 of 2161 specs

Feb 28 07:32:02.412: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:32:02.414: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 28 07:32:02.443: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 28 07:32:02.491: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 28 07:32:02.491: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 28 07:32:02.491: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 28 07:32:02.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 28 07:32:02.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 28 07:32:02.503: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 28 07:32:02.503: INFO: e2e test version: v1.13.3
Feb 28 07:32:02.507: INFO: kube-apiserver version: v1.13.3
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:32:02.507: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
Feb 28 07:32:02.753: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 28 07:32:02.791: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cvp79
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-f0fd2f9f-3b2a-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:32:02.955: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-cvp79" to be "success or failure"
Feb 28 07:32:02.961: INFO: Pod "pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.859916ms
Feb 28 07:32:04.971: INFO: Pod "pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016143632s
Feb 28 07:32:06.977: INFO: Pod "pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022543719s
STEP: Saw pod success
Feb 28 07:32:06.977: INFO: Pod "pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:32:06.983: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:32:07.131: INFO: Waiting for pod pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:32:07.136: INFO: Pod pod-configmaps-f0fec4bc-3b2a-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:32:07.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cvp79" for this suite.
Feb 28 07:32:13.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:32:13.288: INFO: namespace: e2e-tests-configmap-cvp79, resource: bindings, ignored listing per whitelist
Feb 28 07:32:13.506: INFO: namespace e2e-tests-configmap-cvp79 deletion completed in 6.364600487s

• [SLOW TEST:10.999 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:32:13.506: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lhwfw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 07:32:13.993: INFO: Waiting up to 5m0s for pod "pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-lhwfw" to be "success or failure"
Feb 28 07:32:14.001: INFO: Pod "pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.860134ms
Feb 28 07:32:16.050: INFO: Pod "pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056206978s
Feb 28 07:32:18.057: INFO: Pod "pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063264464s
STEP: Saw pod success
Feb 28 07:32:18.057: INFO: Pod "pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:32:18.064: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:32:18.092: INFO: Waiting for pod pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:32:18.097: INFO: Pod pod-f7931df8-3b2a-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:32:18.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lhwfw" for this suite.
Feb 28 07:32:24.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:32:24.247: INFO: namespace: e2e-tests-emptydir-lhwfw, resource: bindings, ignored listing per whitelist
Feb 28 07:32:24.469: INFO: namespace e2e-tests-emptydir-lhwfw deletion completed in 6.365532008s

• [SLOW TEST:10.962 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:32:24.469: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-8xkqd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0228 07:32:55.478493   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:32:55.478: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:32:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8xkqd" for this suite.
Feb 28 07:33:01.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:01.732: INFO: namespace: e2e-tests-gc-8xkqd, resource: bindings, ignored listing per whitelist
Feb 28 07:33:01.763: INFO: namespace e2e-tests-gc-8xkqd deletion completed in 6.277071076s

• [SLOW TEST:37.294 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:33:01.763: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-bqff4
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:33:02.127: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:33:02.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-bqff4" for this suite.
Feb 28 07:33:08.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:08.842: INFO: namespace: e2e-tests-custom-resource-definition-bqff4, resource: bindings, ignored listing per whitelist
Feb 28 07:33:09.013: INFO: namespace e2e-tests-custom-resource-definition-bqff4 deletion completed in 6.289818052s

• [SLOW TEST:7.250 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:33:09.013: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-rmg9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-ln64
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 07:33:09.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ln64" in namespace "e2e-tests-subpath-rmg9t" to be "success or failure"
Feb 28 07:33:09.342: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280829ms
Feb 28 07:33:11.349: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012982716s
Feb 28 07:33:13.356: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019524343s
Feb 28 07:33:15.363: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 6.026659007s
Feb 28 07:33:17.368: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 8.032257324s
Feb 28 07:33:19.379: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 10.0429339s
Feb 28 07:33:21.387: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 12.051069325s
Feb 28 07:33:23.399: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 14.062922722s
Feb 28 07:33:25.408: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 16.07159833s
Feb 28 07:33:27.415: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 18.078771346s
Feb 28 07:33:29.420: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 20.083961541s
Feb 28 07:33:31.426: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 22.089840316s
Feb 28 07:33:33.436: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Running", Reason="", readiness=false. Elapsed: 24.099428536s
Feb 28 07:33:35.443: INFO: Pod "pod-subpath-test-downwardapi-ln64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.107285269s
STEP: Saw pod success
Feb 28 07:33:35.444: INFO: Pod "pod-subpath-test-downwardapi-ln64" satisfied condition "success or failure"
Feb 28 07:33:35.449: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-subpath-test-downwardapi-ln64 container test-container-subpath-downwardapi-ln64: <nil>
STEP: delete the pod
Feb 28 07:33:35.480: INFO: Waiting for pod pod-subpath-test-downwardapi-ln64 to disappear
Feb 28 07:33:35.484: INFO: Pod pod-subpath-test-downwardapi-ln64 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ln64
Feb 28 07:33:35.484: INFO: Deleting pod "pod-subpath-test-downwardapi-ln64" in namespace "e2e-tests-subpath-rmg9t"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:33:35.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-rmg9t" for this suite.
Feb 28 07:33:41.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:41.601: INFO: namespace: e2e-tests-subpath-rmg9t, resource: bindings, ignored listing per whitelist
Feb 28 07:33:41.790: INFO: namespace e2e-tests-subpath-rmg9t deletion completed in 6.294105695s

• [SLOW TEST:32.777 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:33:41.790: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-vh948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:33:42.322: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:33:45.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-vh948" for this suite.
Feb 28 07:33:51.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:51.849: INFO: namespace: e2e-tests-init-container-vh948, resource: bindings, ignored listing per whitelist
Feb 28 07:33:52.266: INFO: namespace e2e-tests-init-container-vh948 deletion completed in 6.597526489s

• [SLOW TEST:10.476 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:33:52.266: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-b8g9f
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-3274bf6f-3b2b-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:33:56.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b8g9f" for this suite.
Feb 28 07:34:18.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:19.182: INFO: namespace: e2e-tests-configmap-b8g9f, resource: bindings, ignored listing per whitelist
Feb 28 07:34:19.195: INFO: namespace e2e-tests-configmap-b8g9f deletion completed in 22.260447078s

• [SLOW TEST:26.928 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:34:19.195: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-z72qq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 07:34:19.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 07:34:19.556: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 07:34:19.561: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 before test
Feb 28 07:34:19.576: INFO: kube-proxy-q5pbs from kube-system started at 2019-02-28 07:18:44 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.576: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 28 07:34:19.576: INFO: calico-node-6z2pm from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.576: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:34:19.576: INFO: node-exporter-ffrzv from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.576: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:34:19.576: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw before test
Feb 28 07:34:19.714: INFO: addons-nginx-ingress-controller-7455744d9b-vs4bz from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 07:34:19.714: INFO: addons-kubernetes-dashboard-6579b646c5-hzhgc from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 07:34:19.714: INFO: blackbox-exporter-86f6cf4cb7-9tckp from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 07:34:19.714: INFO: kube-proxy-t74jn from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:34:19.714: INFO: node-exporter-4fg6q from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:34:19.714: INFO: calico-node-55zq6 from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:34:19.714: INFO: metrics-server-8d5b849b7-zdqrh from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 07:34:19.714: INFO: coredns-67df79bbdd-7xb2x from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container coredns ready: true, restart count 0
Feb 28 07:34:19.714: INFO: vpn-shoot-9899b5654-5z86s from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 07:34:19.714: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-c79ck from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 07:34:19.714: INFO: addons-kube-lego-69bbdc96b6-mrdv6 from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:34:19.714: INFO: 	Container kube-lego ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158776d65f86062f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:34:20.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-z72qq" for this suite.
Feb 28 07:34:26.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:27.173: INFO: namespace: e2e-tests-sched-pred-z72qq, resource: bindings, ignored listing per whitelist
Feb 28 07:34:27.211: INFO: namespace e2e-tests-sched-pred-z72qq deletion completed in 6.43238304s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:8.016 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:34:27.211: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pdrtv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 07:34:27.736: INFO: Waiting up to 5m0s for pod "pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-pdrtv" to be "success or failure"
Feb 28 07:34:27.744: INFO: Pod "pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638524ms
Feb 28 07:34:29.758: INFO: Pod "pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022271108s
STEP: Saw pod success
Feb 28 07:34:29.758: INFO: Pod "pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:34:29.765: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:34:29.793: INFO: Waiting for pod pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:34:29.801: INFO: Pod pod-474a8d62-3b2b-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:34:29.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pdrtv" for this suite.
Feb 28 07:34:35.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:36.258: INFO: namespace: e2e-tests-emptydir-pdrtv, resource: bindings, ignored listing per whitelist
Feb 28 07:34:36.266: INFO: namespace e2e-tests-emptydir-pdrtv deletion completed in 6.45752637s

• [SLOW TEST:9.055 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:34:36.266: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-52cg8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 28 07:34:36.982: INFO: Pod name wrapped-volume-race-4ccbd87b-3b2b-11e9-a616-82a6c0035a5d: Found 0 pods out of 5
Feb 28 07:34:42.016: INFO: Pod name wrapped-volume-race-4ccbd87b-3b2b-11e9-a616-82a6c0035a5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4ccbd87b-3b2b-11e9-a616-82a6c0035a5d in namespace e2e-tests-emptydir-wrapper-52cg8, will wait for the garbage collector to delete the pods
Feb 28 07:34:42.130: INFO: Deleting ReplicationController wrapped-volume-race-4ccbd87b-3b2b-11e9-a616-82a6c0035a5d took: 15.826355ms
Feb 28 07:34:42.230: INFO: Terminating ReplicationController wrapped-volume-race-4ccbd87b-3b2b-11e9-a616-82a6c0035a5d pods took: 100.291365ms
STEP: Creating RC which spawns configmap-volume pods
Feb 28 07:35:25.760: INFO: Pod name wrapped-volume-race-69de29a3-3b2b-11e9-a616-82a6c0035a5d: Found 0 pods out of 5
Feb 28 07:35:30.775: INFO: Pod name wrapped-volume-race-69de29a3-3b2b-11e9-a616-82a6c0035a5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-69de29a3-3b2b-11e9-a616-82a6c0035a5d in namespace e2e-tests-emptydir-wrapper-52cg8, will wait for the garbage collector to delete the pods
Feb 28 07:35:30.869: INFO: Deleting ReplicationController wrapped-volume-race-69de29a3-3b2b-11e9-a616-82a6c0035a5d took: 8.577734ms
Feb 28 07:35:30.970: INFO: Terminating ReplicationController wrapped-volume-race-69de29a3-3b2b-11e9-a616-82a6c0035a5d pods took: 100.232527ms
STEP: Creating RC which spawns configmap-volume pods
Feb 28 07:36:15.789: INFO: Pod name wrapped-volume-race-87b197e4-3b2b-11e9-a616-82a6c0035a5d: Found 0 pods out of 5
Feb 28 07:36:20.801: INFO: Pod name wrapped-volume-race-87b197e4-3b2b-11e9-a616-82a6c0035a5d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-87b197e4-3b2b-11e9-a616-82a6c0035a5d in namespace e2e-tests-emptydir-wrapper-52cg8, will wait for the garbage collector to delete the pods
Feb 28 07:36:20.891: INFO: Deleting ReplicationController wrapped-volume-race-87b197e4-3b2b-11e9-a616-82a6c0035a5d took: 8.139949ms
Feb 28 07:36:20.991: INFO: Terminating ReplicationController wrapped-volume-race-87b197e4-3b2b-11e9-a616-82a6c0035a5d pods took: 100.311703ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:06.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-52cg8" for this suite.
Feb 28 07:37:12.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:12.489: INFO: namespace: e2e-tests-emptydir-wrapper-52cg8, resource: bindings, ignored listing per whitelist
Feb 28 07:37:12.970: INFO: namespace e2e-tests-emptydir-wrapper-52cg8 deletion completed in 6.573062586s

• [SLOW TEST:156.704 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:37:12.970: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gh6xf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 07:37:16.301: INFO: Successfully updated pod "labelsupdateaa3efedf-3b2b-11e9-a616-82a6c0035a5d"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:20.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gh6xf" for this suite.
Feb 28 07:37:42.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:42.555: INFO: namespace: e2e-tests-projected-gh6xf, resource: bindings, ignored listing per whitelist
Feb 28 07:37:42.785: INFO: namespace e2e-tests-projected-gh6xf deletion completed in 22.430364391s

• [SLOW TEST:29.815 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:37:42.786: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-sghnm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 07:37:45.785: INFO: Successfully updated pod "pod-update-bbd0eb1b-3b2b-11e9-a616-82a6c0035a5d"
STEP: verifying the updated pod is in kubernetes
Feb 28 07:37:45.808: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:37:45.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-sghnm" for this suite.
Feb 28 07:38:07.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:08.350: INFO: namespace: e2e-tests-pods-sghnm, resource: bindings, ignored listing per whitelist
Feb 28 07:38:08.408: INFO: namespace e2e-tests-pods-sghnm deletion completed in 22.591961906s

• [SLOW TEST:25.623 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:38:08.409: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6w5dl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-cb53ddfb-3b2b-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:38:09.259: INFO: Waiting up to 5m0s for pod "pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-6w5dl" to be "success or failure"
Feb 28 07:38:09.265: INFO: Pod "pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.904843ms
Feb 28 07:38:11.272: INFO: Pod "pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012957689s
STEP: Saw pod success
Feb 28 07:38:11.273: INFO: Pod "pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:38:11.280: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d container secret-env-test: <nil>
STEP: delete the pod
Feb 28 07:38:11.313: INFO: Waiting for pod pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:38:11.320: INFO: Pod pod-secrets-cb551f96-3b2b-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:38:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6w5dl" for this suite.
Feb 28 07:38:17.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:17.379: INFO: namespace: e2e-tests-secrets-6w5dl, resource: bindings, ignored listing per whitelist
Feb 28 07:38:17.736: INFO: namespace e2e-tests-secrets-6w5dl deletion completed in 6.405631036s

• [SLOW TEST:9.327 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:38:17.737: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-wh9xx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:38:26.264: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:26.269: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:28.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:28.276: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:30.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:30.277: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:32.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:32.280: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:34.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:34.278: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:36.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:36.277: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:38.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:38.275: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:40.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:40.276: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:42.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:42.275: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:38:44.269: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:38:44.278: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:38:44.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-wh9xx" for this suite.
Feb 28 07:39:06.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:06.741: INFO: namespace: e2e-tests-container-lifecycle-hook-wh9xx, resource: bindings, ignored listing per whitelist
Feb 28 07:39:06.826: INFO: namespace e2e-tests-container-lifecycle-hook-wh9xx deletion completed in 22.518895612s

• [SLOW TEST:49.090 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:39:06.826: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-tqhqt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 07:39:07.235: INFO: Waiting up to 5m0s for pod "pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-tqhqt" to be "success or failure"
Feb 28 07:39:07.241: INFO: Pod "pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02397ms
Feb 28 07:39:09.252: INFO: Pod "pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016435015s
STEP: Saw pod success
Feb 28 07:39:09.252: INFO: Pod "pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:39:09.259: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:39:09.285: INFO: Waiting for pod pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:39:09.290: INFO: Pod pod-ede32f83-3b2b-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:39:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tqhqt" for this suite.
Feb 28 07:39:15.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:15.522: INFO: namespace: e2e-tests-emptydir-tqhqt, resource: bindings, ignored listing per whitelist
Feb 28 07:39:15.570: INFO: namespace e2e-tests-emptydir-tqhqt deletion completed in 6.271751597s

• [SLOW TEST:8.744 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:39:15.571: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-2vgms
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 07:39:19.983: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:19.992: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:21.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:22.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:23.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:24.229: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:25.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:26.003: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:27.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:27.998: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:29.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:30.003: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:31.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:32.003: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:33.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:34.009: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:35.994: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:36.007: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:37.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:38.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:39.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:40.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:41.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:42.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:43.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:44.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:39:45.992: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:39:46.001: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:39:46.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-2vgms" for this suite.
Feb 28 07:40:08.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:08.396: INFO: namespace: e2e-tests-container-lifecycle-hook-2vgms, resource: bindings, ignored listing per whitelist
Feb 28 07:40:08.506: INFO: namespace e2e-tests-container-lifecycle-hook-2vgms deletion completed in 22.492821654s

• [SLOW TEST:52.935 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:40:08.506: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-b4nqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-b4nqr
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-b4nqr
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-b4nqr
Feb 28 07:40:08.892: INFO: Found 0 stateful pods, waiting for 1
Feb 28 07:40:18.905: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 28 07:40:18.913: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:40:19.542: INFO: stderr: ""
Feb 28 07:40:19.542: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:40:19.542: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:40:19.552: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 07:40:29.557: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:40:29.557: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:40:29.581: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:29.581: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:29.581: INFO: 
Feb 28 07:40:29.581: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 28 07:40:30.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995979719s
Feb 28 07:40:31.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988694277s
Feb 28 07:40:32.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977280349s
Feb 28 07:40:33.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969474485s
Feb 28 07:40:34.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96141919s
Feb 28 07:40:35.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95525805s
Feb 28 07:40:36.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948093176s
Feb 28 07:40:37.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940467356s
Feb 28 07:40:38.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 934.307016ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-b4nqr
Feb 28 07:40:39.655: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:40:40.175: INFO: stderr: ""
Feb 28 07:40:40.175: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:40:40.175: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:40:40.175: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:40:40.709: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 07:40:40.709: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:40:40.709: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:40:40.709: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:40:41.308: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 07:40:41.308: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:40:41.308: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:40:41.315: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:40:41.315: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:40:41.315: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 28 07:40:41.322: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:40:41.926: INFO: stderr: ""
Feb 28 07:40:41.926: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:40:41.926: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:40:41.926: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:40:42.498: INFO: stderr: ""
Feb 28 07:40:42.498: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:40:42.498: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:40:42.498: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:40:43.062: INFO: stderr: ""
Feb 28 07:40:43.062: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:40:43.062: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:40:43.062: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:40:43.069: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 28 07:40:53.296: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:40:53.296: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:40:53.296: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:40:53.550: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:53.550: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:53.550: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:53.550: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:53.550: INFO: 
Feb 28 07:40:53.550: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:54.566: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:54.566: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:54.566: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:54.566: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:54.566: INFO: 
Feb 28 07:40:54.566: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:55.582: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:55.582: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:55.582: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:55.582: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:55.582: INFO: 
Feb 28 07:40:55.582: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:56.604: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:56.604: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:56.604: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:56.604: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:56.604: INFO: 
Feb 28 07:40:56.604: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:57.619: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:57.619: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:57.619: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:57.619: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:57.619: INFO: 
Feb 28 07:40:57.619: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:58.628: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:58.628: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:58.628: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:58.628: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:58.628: INFO: 
Feb 28 07:40:58.628: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:40:59.636: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:40:59.636: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:40:59.636: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:59.637: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:40:59.637: INFO: 
Feb 28 07:40:59.637: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:41:00.643: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:41:00.643: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:41:00.643: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:00.643: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:00.643: INFO: 
Feb 28 07:41:00.643: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:41:01.654: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:41:01.654: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:41:01.654: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:01.654: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:01.654: INFO: 
Feb 28 07:41:01.654: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 07:41:02.662: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 07:41:02.662: INFO: ss-0  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:08 +0000 UTC  }]
Feb 28 07:41:02.662: INFO: ss-1  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:02.662: INFO: ss-2  shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:40:29 +0000 UTC  }]
Feb 28 07:41:02.662: INFO: 
Feb 28 07:41:02.662: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-b4nqr
Feb 28 07:41:03.672: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:03.903: INFO: rc: 1
Feb 28 07:41:03.903: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000c0a840 exit status 1 <nil> <nil> true [0xc000712990 0xc0007129d8 0xc0007129f8] [0xc000712990 0xc0007129d8 0xc0007129f8] [0xc0007129c8 0xc0007129f0] [0x933040 0x933040] 0xc001a37620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 07:41:13.903: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:13.993: INFO: rc: 1
Feb 28 07:41:13.993: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000c0aba0 exit status 1 <nil> <nil> true [0xc000712a08 0xc000712a78 0xc000712ad8] [0xc000712a08 0xc000712a78 0xc000712ad8] [0xc000712a70 0xc000712ad0] [0x933040 0x933040] 0xc001a37920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:41:23.993: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:24.083: INFO: rc: 1
Feb 28 07:41:24.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000c0ae70 exit status 1 <nil> <nil> true [0xc000712af0 0xc000712b40 0xc000712c00] [0xc000712af0 0xc000712b40 0xc000712c00] [0xc000712b20 0xc000712b90] [0x933040 0x933040] 0xc001a37c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:41:34.084: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:34.185: INFO: rc: 1
Feb 28 07:41:34.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ad1b30 exit status 1 <nil> <nil> true [0xc0008501e8 0xc000850200 0xc000850218] [0xc0008501e8 0xc000850200 0xc000850218] [0xc0008501f8 0xc000850210] [0x933040 0x933040] 0xc001bdf2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:41:44.186: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:44.278: INFO: rc: 1
Feb 28 07:41:44.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017269f0 exit status 1 <nil> <nil> true [0xc000161200 0xc000161298 0xc000161348] [0xc000161200 0xc000161298 0xc000161348] [0xc000161288 0xc000161340] [0x933040 0x933040] 0xc0019f1a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:41:54.278: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:41:54.401: INFO: rc: 1
Feb 28 07:41:54.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000c0b380 exit status 1 <nil> <nil> true [0xc000712c20 0xc000712c58 0xc000712c78] [0xc000712c20 0xc000712c58 0xc000712c78] [0xc000712c40 0xc000712c70] [0x933040 0x933040] 0xc0007942a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:04.401: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:04.488: INFO: rc: 1
Feb 28 07:42:04.488: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ad1dd0 exit status 1 <nil> <nil> true [0xc000850220 0xc000850238 0xc000850268] [0xc000850220 0xc000850238 0xc000850268] [0xc000850230 0xc000850260] [0x933040 0x933040] 0xc001bdf5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:14.489: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:14.615: INFO: rc: 1
Feb 28 07:42:14.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00143c090 exit status 1 <nil> <nil> true [0xc000850270 0xc0008502b8 0xc0008502f0] [0xc000850270 0xc0008502b8 0xc0008502f0] [0xc0008502a0 0xc0008502e8] [0x933040 0x933040] 0xc001bdf8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:24.616: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:24.753: INFO: rc: 1
Feb 28 07:42:24.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e270 exit status 1 <nil> <nil> true [0xc000160138 0xc0001602d8 0xc0001604f0] [0xc000160138 0xc0001602d8 0xc0001604f0] [0xc000160238 0xc000160468] [0x933040 0x933040] 0xc00194e1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:34.753: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:34.882: INFO: rc: 1
Feb 28 07:42:34.882: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e510 exit status 1 <nil> <nil> true [0xc000160500 0xc000160668 0xc0001608c8] [0xc000160500 0xc000160668 0xc0001608c8] [0xc000160598 0xc000160868] [0x933040 0x933040] 0xc00194e4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:44.882: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:44.977: INFO: rc: 1
Feb 28 07:42:44.977: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ce62d0 exit status 1 <nil> <nil> true [0xc000850008 0xc000850020 0xc000850048] [0xc000850008 0xc000850020 0xc000850048] [0xc000850018 0xc000850030] [0x933040 0x933040] 0xc001a36240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:42:54.977: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:42:55.060: INFO: rc: 1
Feb 28 07:42:55.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ce65a0 exit status 1 <nil> <nil> true [0xc000850060 0xc000850078 0xc000850090] [0xc000850060 0xc000850078 0xc000850090] [0xc000850070 0xc000850088] [0x933040 0x933040] 0xc001a36540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:05.060: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:05.246: INFO: rc: 1
Feb 28 07:43:05.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e7b0 exit status 1 <nil> <nil> true [0xc000160900 0xc000160c30 0xc000160da0] [0xc000160900 0xc000160c30 0xc000160da0] [0xc000160c20 0xc000160d28] [0x933040 0x933040] 0xc00194e7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:15.247: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:15.325: INFO: rc: 1
Feb 28 07:43:15.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d2300 exit status 1 <nil> <nil> true [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a248 0xc00038a2f8] [0x933040 0x933040] 0xc0019f0c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:25.325: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:25.410: INFO: rc: 1
Feb 28 07:43:25.410: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136eab0 exit status 1 <nil> <nil> true [0xc000160de8 0xc000160e30 0xc000160e78] [0xc000160de8 0xc000160e30 0xc000160e78] [0xc000160e18 0xc000160e60] [0x933040 0x933040] 0xc00194eae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:35.410: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:35.493: INFO: rc: 1
Feb 28 07:43:35.493: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136ed50 exit status 1 <nil> <nil> true [0xc000160e88 0xc000160f78 0xc000160ff8] [0xc000160e88 0xc000160f78 0xc000160ff8] [0xc000160f68 0xc000160f90] [0x933040 0x933040] 0xc00194ee40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:45.493: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:45.631: INFO: rc: 1
Feb 28 07:43:45.631: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ce6870 exit status 1 <nil> <nil> true [0xc000850098 0xc0008500b0 0xc0008500f8] [0xc000850098 0xc0008500b0 0xc0008500f8] [0xc0008500a8 0xc0008500e0] [0x933040 0x933040] 0xc001a36840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:43:55.631: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:43:55.713: INFO: rc: 1
Feb 28 07:43:55.713: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ce6c90 exit status 1 <nil> <nil> true [0xc000850100 0xc000850150 0xc000850178] [0xc000850100 0xc000850150 0xc000850178] [0xc000850138 0xc000850170] [0x933040 0x933040] 0xc001a36b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:05.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:05.802: INFO: rc: 1
Feb 28 07:44:05.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014a82d0 exit status 1 <nil> <nil> true [0xc000712000 0xc000712020 0xc000712060] [0xc000712000 0xc000712020 0xc000712060] [0xc000712018 0xc000712030] [0x933040 0x933040] 0xc001bde7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:15.802: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:15.937: INFO: rc: 1
Feb 28 07:44:15.937: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ce70b0 exit status 1 <nil> <nil> true [0xc000850180 0xc000850198 0xc0008501c0] [0xc000850180 0xc000850198 0xc0008501c0] [0xc000850190 0xc0008501a8] [0x933040 0x933040] 0xc001a36ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:25.942: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:26.029: INFO: rc: 1
Feb 28 07:44:26.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d2000 exit status 1 <nil> <nil> true [0xc000712088 0xc0007120c8 0xc000712128] [0xc000712088 0xc0007120c8 0xc000712128] [0xc0007120c0 0xc0007120e8] [0x933040 0x933040] 0xc0019f09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:36.029: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:36.158: INFO: rc: 1
Feb 28 07:44:36.158: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d2330 exit status 1 <nil> <nil> true [0xc000850008 0xc000850020 0xc000850048] [0xc000850008 0xc000850020 0xc000850048] [0xc000850018 0xc000850030] [0x933040 0x933040] 0xc0019f0cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:46.159: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:46.283: INFO: rc: 1
Feb 28 07:44:46.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d2600 exit status 1 <nil> <nil> true [0xc000850060 0xc000850078 0xc000850090] [0xc000850060 0xc000850078 0xc000850090] [0xc000850070 0xc000850088] [0x933040 0x933040] 0xc0019f0fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:44:56.285: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:44:56.362: INFO: rc: 1
Feb 28 07:44:56.362: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e2a0 exit status 1 <nil> <nil> true [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a248 0xc00038a2f8] [0x933040 0x933040] 0xc001a361e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:06.362: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:06.447: INFO: rc: 1
Feb 28 07:45:06.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014a8690 exit status 1 <nil> <nil> true [0xc000160118 0xc000160238 0xc000160468] [0xc000160118 0xc000160238 0xc000160468] [0xc000160140 0xc0001603e8] [0x933040 0x933040] 0xc00194e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:16.447: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:16.572: INFO: rc: 1
Feb 28 07:45:16.572: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014a8990 exit status 1 <nil> <nil> true [0xc0001604f0 0xc000160598 0xc000160868] [0xc0001604f0 0xc000160598 0xc000160868] [0xc000160528 0xc0001606e8] [0x933040 0x933040] 0xc00194e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:26.572: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:26.651: INFO: rc: 1
Feb 28 07:45:26.651: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d28a0 exit status 1 <nil> <nil> true [0xc000850098 0xc0008500b0 0xc0008500f8] [0xc000850098 0xc0008500b0 0xc0008500f8] [0xc0008500a8 0xc0008500e0] [0x933040 0x933040] 0xc0019f12c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:36.651: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:36.726: INFO: rc: 1
Feb 28 07:45:36.726: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013d2ba0 exit status 1 <nil> <nil> true [0xc000850100 0xc000850150 0xc000850178] [0xc000850100 0xc000850150 0xc000850178] [0xc000850138 0xc000850170] [0x933040 0x933040] 0xc0019f15c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:46.727: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:46.919: INFO: rc: 1
Feb 28 07:45:46.919: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e570 exit status 1 <nil> <nil> true [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a4b8 0xc00038a5e8] [0x933040 0x933040] 0xc001a364e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:45:56.920: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:57.000: INFO: rc: 1
Feb 28 07:45:57.000: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00136e840 exit status 1 <nil> <nil> true [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a6c0 0xc00038a798] [0x933040 0x933040] 0xc001a367e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 07:46:07.000: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-b4nqr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:46:07.078: INFO: rc: 1
Feb 28 07:46:07.079: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 28 07:46:07.079: INFO: Scaling statefulset ss to 0
Feb 28 07:46:07.113: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:46:07.117: INFO: Deleting all statefulset in ns e2e-tests-statefulset-b4nqr
Feb 28 07:46:07.122: INFO: Scaling statefulset ss to 0
Feb 28 07:46:07.135: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:46:07.140: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:07.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-b4nqr" for this suite.
Feb 28 07:46:13.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:13.189: INFO: namespace: e2e-tests-statefulset-b4nqr, resource: bindings, ignored listing per whitelist
Feb 28 07:46:13.358: INFO: namespace e2e-tests-statefulset-b4nqr deletion completed in 6.197397886s

• [SLOW TEST:364.851 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:13.358: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zsrdp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 28 07:46:13.633: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zsrdp'
Feb 28 07:46:14.489: INFO: stderr: ""
Feb 28 07:46:14.489: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 28 07:46:15.498: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:46:15.498: INFO: Found 0 / 1
Feb 28 07:46:16.498: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:46:16.498: INFO: Found 0 / 1
Feb 28 07:46:17.495: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:46:17.495: INFO: Found 1 / 1
Feb 28 07:46:17.495: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:46:17.499: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:46:17.499: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 28 07:46:17.499: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp'
Feb 28 07:46:17.661: INFO: stderr: ""
Feb 28 07:46:17.661: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:46:16.890 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:46:16.890 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:46:16.890 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:46:16.890 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 28 07:46:17.661: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp --tail=1'
Feb 28 07:46:17.772: INFO: stderr: ""
Feb 28 07:46:17.772: INFO: stdout: "1:M 28 Feb 07:46:16.890 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 28 07:46:17.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp --limit-bytes=1'
Feb 28 07:46:17.880: INFO: stderr: ""
Feb 28 07:46:17.880: INFO: stdout: " "
STEP: exposing timestamps
Feb 28 07:46:17.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp --tail=1 --timestamps'
Feb 28 07:46:17.988: INFO: stderr: ""
Feb 28 07:46:17.988: INFO: stdout: "2019-02-28T07:46:16.890498592Z 1:M 28 Feb 07:46:16.890 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 28 07:46:20.488: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp --since=1s'
Feb 28 07:46:20.609: INFO: stderr: ""
Feb 28 07:46:20.609: INFO: stdout: ""
Feb 28 07:46:20.609: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-6jcvb redis-master --namespace=e2e-tests-kubectl-zsrdp --since=24h'
Feb 28 07:46:20.749: INFO: stderr: ""
Feb 28 07:46:20.749: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:46:16.890 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:46:16.890 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:46:16.890 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:46:16.890 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 28 07:46:20.749: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zsrdp'
Feb 28 07:46:20.848: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:46:20.848: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 28 07:46:20.849: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-zsrdp'
Feb 28 07:46:20.937: INFO: stderr: "No resources found.\n"
Feb 28 07:46:20.937: INFO: stdout: ""
Feb 28 07:46:20.937: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-zsrdp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:46:21.033: INFO: stderr: ""
Feb 28 07:46:21.033: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:21.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zsrdp" for this suite.
Feb 28 07:46:27.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:27.162: INFO: namespace: e2e-tests-kubectl-zsrdp, resource: bindings, ignored listing per whitelist
Feb 28 07:46:27.302: INFO: namespace e2e-tests-kubectl-zsrdp deletion completed in 6.263585713s

• [SLOW TEST:13.944 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:27.302: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mbvkb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:46:27.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-mbvkb" to be "success or failure"
Feb 28 07:46:27.601: INFO: Pod "downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168045ms
Feb 28 07:46:29.607: INFO: Pod "downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010301459s
STEP: Saw pod success
Feb 28 07:46:29.607: INFO: Pod "downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:46:29.612: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:46:29.638: INFO: Waiting for pod downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:46:29.642: INFO: Pod downwardapi-volume-f45d4a9f-3b2c-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:29.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mbvkb" for this suite.
Feb 28 07:46:35.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:35.724: INFO: namespace: e2e-tests-projected-mbvkb, resource: bindings, ignored listing per whitelist
Feb 28 07:46:35.889: INFO: namespace e2e-tests-projected-mbvkb deletion completed in 6.241479878s

• [SLOW TEST:8.587 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:35.889: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gmtrb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-f99ea495-3b2c-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:46:36.417: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-gmtrb" to be "success or failure"
Feb 28 07:46:36.422: INFO: Pod "pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939138ms
Feb 28 07:46:38.428: INFO: Pod "pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011029526s
STEP: Saw pod success
Feb 28 07:46:38.429: INFO: Pod "pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:46:38.434: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:46:38.459: INFO: Waiting for pod pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:46:38.465: INFO: Pod pod-projected-secrets-f99f842f-3b2c-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:46:38.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gmtrb" for this suite.
Feb 28 07:46:44.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:44.565: INFO: namespace: e2e-tests-projected-gmtrb, resource: bindings, ignored listing per whitelist
Feb 28 07:46:44.698: INFO: namespace e2e-tests-projected-gmtrb deletion completed in 6.227610975s

• [SLOW TEST:8.809 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:46:44.698: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nxvsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:47:08.983: INFO: Container started at 2019-02-28 07:46:47 +0000 UTC, pod became ready at 2019-02-28 07:47:08 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:47:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nxvsr" for this suite.
Feb 28 07:47:31.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:31.229: INFO: namespace: e2e-tests-container-probe-nxvsr, resource: bindings, ignored listing per whitelist
Feb 28 07:47:31.357: INFO: namespace e2e-tests-container-probe-nxvsr deletion completed in 22.369538402s

• [SLOW TEST:46.659 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:47:31.357: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-wnz47
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:47:31.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wnz47" for this suite.
Feb 28 07:48:01.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:01.843: INFO: namespace: e2e-tests-pods-wnz47, resource: bindings, ignored listing per whitelist
Feb 28 07:48:01.843: INFO: namespace e2e-tests-pods-wnz47 deletion completed in 30.207943264s

• [SLOW TEST:30.486 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:01.844: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kpsm8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:48:02.086: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-kpsm8" to be "success or failure"
Feb 28 07:48:02.089: INFO: Pod "downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.539671ms
Feb 28 07:48:04.097: INFO: Pod "downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011010487s
STEP: Saw pod success
Feb 28 07:48:04.097: INFO: Pod "downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:48:04.103: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:48:04.158: INFO: Waiting for pod downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:48:04.170: INFO: Pod downwardapi-volume-2cafa9d9-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:04.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kpsm8" for this suite.
Feb 28 07:48:10.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:10.319: INFO: namespace: e2e-tests-projected-kpsm8, resource: bindings, ignored listing per whitelist
Feb 28 07:48:10.453: INFO: namespace e2e-tests-projected-kpsm8 deletion completed in 6.27754008s

• [SLOW TEST:8.610 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:10.453: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cr826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-31d22fe8-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:48:10.706: INFO: Waiting up to 5m0s for pod "pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-cr826" to be "success or failure"
Feb 28 07:48:10.711: INFO: Pod "pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.11795ms
Feb 28 07:48:12.718: INFO: Pod "pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011397832s
STEP: Saw pod success
Feb 28 07:48:12.718: INFO: Pod "pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:48:12.721: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:48:12.741: INFO: Waiting for pod pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:48:12.745: INFO: Pod pod-configmaps-31d2faea-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:12.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cr826" for this suite.
Feb 28 07:48:18.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:18.893: INFO: namespace: e2e-tests-configmap-cr826, resource: bindings, ignored listing per whitelist
Feb 28 07:48:18.984: INFO: namespace e2e-tests-configmap-cr826 deletion completed in 6.233534533s

• [SLOW TEST:8.531 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:18.984: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8rv2n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 28 07:48:19.235: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml --namespace=e2e-tests-kubectl-8rv2n run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 28 07:48:21.041: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 28 07:48:21.041: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:23.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8rv2n" for this suite.
Feb 28 07:48:29.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:29.191: INFO: namespace: e2e-tests-kubectl-8rv2n, resource: bindings, ignored listing per whitelist
Feb 28 07:48:29.228: INFO: namespace e2e-tests-kubectl-8rv2n deletion completed in 6.174101879s

• [SLOW TEST:10.244 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:29.228: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8vtfw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:48:29.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-8vtfw" to be "success or failure"
Feb 28 07:48:29.492: INFO: Pod "downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487063ms
Feb 28 07:48:31.497: INFO: Pod "downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008612437s
STEP: Saw pod success
Feb 28 07:48:31.497: INFO: Pod "downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:48:31.501: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:48:31.523: INFO: Waiting for pod downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:48:31.527: INFO: Pod downwardapi-volume-3d050313-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:31.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8vtfw" for this suite.
Feb 28 07:48:37.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:37.603: INFO: namespace: e2e-tests-projected-8vtfw, resource: bindings, ignored listing per whitelist
Feb 28 07:48:37.772: INFO: namespace e2e-tests-projected-8vtfw deletion completed in 6.240625073s

• [SLOW TEST:8.544 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:37.772: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-9j2vt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0228 07:48:48.069090   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:48:48.069: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:48.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9j2vt" for this suite.
Feb 28 07:48:54.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:54.240: INFO: namespace: e2e-tests-gc-9j2vt, resource: bindings, ignored listing per whitelist
Feb 28 07:48:54.315: INFO: namespace e2e-tests-gc-9j2vt deletion completed in 6.24058342s

• [SLOW TEST:16.543 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:48:54.315: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-8k4x4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:48:54.596: INFO: (0) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.911321ms)
Feb 28 07:48:54.636: INFO: (1) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 39.911101ms)
Feb 28 07:48:54.643: INFO: (2) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.933058ms)
Feb 28 07:48:54.650: INFO: (3) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.078502ms)
Feb 28 07:48:54.656: INFO: (4) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.945393ms)
Feb 28 07:48:54.662: INFO: (5) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.901351ms)
Feb 28 07:48:54.670: INFO: (6) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.157942ms)
Feb 28 07:48:54.676: INFO: (7) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.726039ms)
Feb 28 07:48:54.684: INFO: (8) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.381664ms)
Feb 28 07:48:54.691: INFO: (9) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.709533ms)
Feb 28 07:48:54.699: INFO: (10) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.269161ms)
Feb 28 07:48:54.708: INFO: (11) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.818701ms)
Feb 28 07:48:54.716: INFO: (12) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.810782ms)
Feb 28 07:48:54.724: INFO: (13) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.761245ms)
Feb 28 07:48:54.731: INFO: (14) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.954458ms)
Feb 28 07:48:54.738: INFO: (15) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.366512ms)
Feb 28 07:48:54.745: INFO: (16) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.677233ms)
Feb 28 07:48:54.752: INFO: (17) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.420888ms)
Feb 28 07:48:54.763: INFO: (18) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.912369ms)
Feb 28 07:48:54.772: INFO: (19) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.267098ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:48:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-8k4x4" for this suite.
Feb 28 07:49:00.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:00.821: INFO: namespace: e2e-tests-proxy-8k4x4, resource: bindings, ignored listing per whitelist
Feb 28 07:49:00.992: INFO: namespace e2e-tests-proxy-8k4x4 deletion completed in 6.212241918s

• [SLOW TEST:6.676 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:49:00.992: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-snbc2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-bfl5t
STEP: Creating secret with name secret-test-4ff34e00-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:49:01.405: INFO: Waiting up to 5m0s for pod "pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-snbc2" to be "success or failure"
Feb 28 07:49:01.410: INFO: Pod "pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47624ms
Feb 28 07:49:03.415: INFO: Pod "pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009999392s
STEP: Saw pod success
Feb 28 07:49:03.415: INFO: Pod "pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:49:03.420: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:49:03.444: INFO: Waiting for pod pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:49:03.448: INFO: Pod pod-secrets-500ac33a-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:49:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-snbc2" for this suite.
Feb 28 07:49:09.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:09.594: INFO: namespace: e2e-tests-secrets-snbc2, resource: bindings, ignored listing per whitelist
Feb 28 07:49:09.666: INFO: namespace e2e-tests-secrets-snbc2 deletion completed in 6.213361874s
STEP: Destroying namespace "e2e-tests-secret-namespace-bfl5t" for this suite.
Feb 28 07:49:15.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:15.805: INFO: namespace: e2e-tests-secret-namespace-bfl5t, resource: bindings, ignored listing per whitelist
Feb 28 07:49:15.827: INFO: namespace e2e-tests-secret-namespace-bfl5t deletion completed in 6.160945028s

• [SLOW TEST:14.836 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:49:15.828: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-bkjdz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 28 07:49:16.077: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-bkjdz" to be "success or failure"
Feb 28 07:49:16.082: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492336ms
Feb 28 07:49:18.087: INFO: Pod "pod-host-path-test": Phase="Running", Reason="", readiness=false. Elapsed: 2.009788497s
Feb 28 07:49:20.094: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016968779s
STEP: Saw pod success
Feb 28 07:49:20.094: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 28 07:49:20.098: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 28 07:49:20.120: INFO: Waiting for pod pod-host-path-test to disappear
Feb 28 07:49:20.124: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:49:20.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-bkjdz" for this suite.
Feb 28 07:49:26.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:26.269: INFO: namespace: e2e-tests-hostpath-bkjdz, resource: bindings, ignored listing per whitelist
Feb 28 07:49:26.289: INFO: namespace e2e-tests-hostpath-bkjdz deletion completed in 6.16014197s

• [SLOW TEST:10.462 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:49:26.290: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-jghzt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:49:26.581: INFO: (0) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.344936ms)
Feb 28 07:49:26.620: INFO: (1) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 39.067655ms)
Feb 28 07:49:26.627: INFO: (2) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.914765ms)
Feb 28 07:49:26.634: INFO: (3) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.512193ms)
Feb 28 07:49:26.640: INFO: (4) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.131889ms)
Feb 28 07:49:26.646: INFO: (5) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.004075ms)
Feb 28 07:49:26.652: INFO: (6) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.926001ms)
Feb 28 07:49:26.658: INFO: (7) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.358533ms)
Feb 28 07:49:26.665: INFO: (8) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.42914ms)
Feb 28 07:49:26.671: INFO: (9) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.666339ms)
Feb 28 07:49:26.678: INFO: (10) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.839117ms)
Feb 28 07:49:26.685: INFO: (11) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.371997ms)
Feb 28 07:49:26.691: INFO: (12) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.180744ms)
Feb 28 07:49:26.698: INFO: (13) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.451375ms)
Feb 28 07:49:26.704: INFO: (14) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.344673ms)
Feb 28 07:49:26.711: INFO: (15) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.910416ms)
Feb 28 07:49:26.717: INFO: (16) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.096462ms)
Feb 28 07:49:26.724: INFO: (17) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.952081ms)
Feb 28 07:49:26.731: INFO: (18) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.514393ms)
Feb 28 07:49:26.737: INFO: (19) /api/v1/nodes/shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.414752ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:49:26.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-jghzt" for this suite.
Feb 28 07:49:32.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:32.823: INFO: namespace: e2e-tests-proxy-jghzt, resource: bindings, ignored listing per whitelist
Feb 28 07:49:32.968: INFO: namespace e2e-tests-proxy-jghzt deletion completed in 6.226384374s

• [SLOW TEST:6.679 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:49:32.968: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-zvd6k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-zvd6k
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 28 07:49:33.231: INFO: Found 0 stateful pods, waiting for 3
Feb 28 07:49:43.243: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:49:43.243: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:49:43.243: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 07:49:43.279: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 28 07:49:53.319: INFO: Updating stateful set ss2
Feb 28 07:49:53.327: INFO: Waiting for Pod e2e-tests-statefulset-zvd6k/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 07:50:03.338: INFO: Waiting for Pod e2e-tests-statefulset-zvd6k/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 28 07:50:13.365: INFO: Found 2 stateful pods, waiting for 3
Feb 28 07:50:23.373: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:50:23.373: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:50:23.373: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 28 07:50:23.407: INFO: Updating stateful set ss2
Feb 28 07:50:23.429: INFO: Waiting for Pod e2e-tests-statefulset-zvd6k/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 07:50:33.460: INFO: Updating stateful set ss2
Feb 28 07:50:33.472: INFO: Waiting for StatefulSet e2e-tests-statefulset-zvd6k/ss2 to complete update
Feb 28 07:50:33.472: INFO: Waiting for Pod e2e-tests-statefulset-zvd6k/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 07:50:43.481: INFO: Waiting for StatefulSet e2e-tests-statefulset-zvd6k/ss2 to complete update
Feb 28 07:50:43.481: INFO: Waiting for Pod e2e-tests-statefulset-zvd6k/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:50:53.482: INFO: Deleting all statefulset in ns e2e-tests-statefulset-zvd6k
Feb 28 07:50:53.485: INFO: Scaling statefulset ss2 to 0
Feb 28 07:51:23.507: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:51:23.513: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-zvd6k" for this suite.
Feb 28 07:51:29.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:29.734: INFO: namespace: e2e-tests-statefulset-zvd6k, resource: bindings, ignored listing per whitelist
Feb 28 07:51:29.779: INFO: namespace e2e-tests-statefulset-zvd6k deletion completed in 6.239297005s

• [SLOW TEST:116.810 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:29.779: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vckpm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:51:30.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-vckpm" to be "success or failure"
Feb 28 07:51:30.088: INFO: Pod "downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 53.279636ms
Feb 28 07:51:32.095: INFO: Pod "downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059694563s
STEP: Saw pod success
Feb 28 07:51:32.095: INFO: Pod "downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:51:32.100: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:51:32.130: INFO: Waiting for pod downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:51:32.134: INFO: Pod downwardapi-volume-a8a1a5e6-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:32.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vckpm" for this suite.
Feb 28 07:51:38.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:38.203: INFO: namespace: e2e-tests-projected-vckpm, resource: bindings, ignored listing per whitelist
Feb 28 07:51:38.415: INFO: namespace e2e-tests-projected-vckpm deletion completed in 6.276539115s

• [SLOW TEST:8.636 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:38.415: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-wlnz4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 07:51:38.644: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 07:51:38.655: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 07:51:38.659: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 before test
Feb 28 07:51:38.668: INFO: kube-proxy-q5pbs from kube-system started at 2019-02-28 07:18:44 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.668: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 28 07:51:38.668: INFO: calico-node-6z2pm from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.668: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:51:38.668: INFO: node-exporter-ffrzv from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.668: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:51:38.668: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw before test
Feb 28 07:51:38.724: INFO: blackbox-exporter-86f6cf4cb7-9tckp from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 07:51:38.724: INFO: kube-proxy-t74jn from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:51:38.724: INFO: node-exporter-4fg6q from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:51:38.724: INFO: calico-node-55zq6 from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:51:38.724: INFO: addons-nginx-ingress-controller-7455744d9b-vs4bz from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 07:51:38.724: INFO: addons-kubernetes-dashboard-6579b646c5-hzhgc from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 07:51:38.724: INFO: vpn-shoot-9899b5654-5z86s from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 07:51:38.724: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-c79ck from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 07:51:38.724: INFO: addons-kube-lego-69bbdc96b6-mrdv6 from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 07:51:38.724: INFO: metrics-server-8d5b849b7-zdqrh from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 07:51:38.724: INFO: coredns-67df79bbdd-7xb2x from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 07:51:38.724: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-af06e025-3b2d-11e9-a616-82a6c0035a5d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-af06e025-3b2d-11e9-a616-82a6c0035a5d off the node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
STEP: verifying the node doesn't have the label kubernetes.io/e2e-af06e025-3b2d-11e9-a616-82a6c0035a5d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-wlnz4" for this suite.
Feb 28 07:51:50.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:51.020: INFO: namespace: e2e-tests-sched-pred-wlnz4, resource: bindings, ignored listing per whitelist
Feb 28 07:51:51.053: INFO: namespace e2e-tests-sched-pred-wlnz4 deletion completed in 8.248229269s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.638 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:51:51.053: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-lvn8c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:51:55.351: INFO: Waiting up to 5m0s for pod "client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-pods-lvn8c" to be "success or failure"
Feb 28 07:51:55.358: INFO: Pod "client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98711ms
Feb 28 07:51:57.373: INFO: Pod "client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021376857s
STEP: Saw pod success
Feb 28 07:51:57.373: INFO: Pod "client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:51:57.377: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d container env3cont: <nil>
STEP: delete the pod
Feb 28 07:51:57.404: INFO: Waiting for pod client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:51:57.408: INFO: Pod client-envvars-b7b8f44a-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:51:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lvn8c" for this suite.
Feb 28 07:52:37.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:37.526: INFO: namespace: e2e-tests-pods-lvn8c, resource: bindings, ignored listing per whitelist
Feb 28 07:52:37.626: INFO: namespace e2e-tests-pods-lvn8c deletion completed in 40.212163317s

• [SLOW TEST:46.572 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:52:37.626: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8g8mk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 07:52:37.874: INFO: Waiting up to 5m0s for pod "pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-8g8mk" to be "success or failure"
Feb 28 07:52:37.879: INFO: Pod "pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146143ms
Feb 28 07:52:39.886: INFO: Pod "pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011403015s
STEP: Saw pod success
Feb 28 07:52:39.886: INFO: Pod "pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:52:39.891: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:52:39.916: INFO: Waiting for pod pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:52:39.920: INFO: Pod pod-d11143bd-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:52:39.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8g8mk" for this suite.
Feb 28 07:52:45.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:46.169: INFO: namespace: e2e-tests-emptydir-8g8mk, resource: bindings, ignored listing per whitelist
Feb 28 07:52:46.196: INFO: namespace e2e-tests-emptydir-8g8mk deletion completed in 6.271052758s

• [SLOW TEST:8.570 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:52:46.196: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-r562h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 07:52:46.442: INFO: Waiting up to 5m0s for pod "downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-r562h" to be "success or failure"
Feb 28 07:52:46.449: INFO: Pod "downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.868281ms
Feb 28 07:52:48.454: INFO: Pod "downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011944193s
STEP: Saw pod success
Feb 28 07:52:48.454: INFO: Pod "downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:52:48.461: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:52:48.486: INFO: Waiting for pod downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:52:48.491: INFO: Pod downward-api-d62cd38a-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:52:48.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r562h" for this suite.
Feb 28 07:52:54.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:54.695: INFO: namespace: e2e-tests-downward-api-r562h, resource: bindings, ignored listing per whitelist
Feb 28 07:52:54.728: INFO: namespace e2e-tests-downward-api-r562h deletion completed in 6.232773025s

• [SLOW TEST:8.532 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:52:54.728: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ld9f8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-ld9f8/configmap-test-db432f5a-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:52:54.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-ld9f8" to be "success or failure"
Feb 28 07:52:54.996: INFO: Pod "pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282251ms
Feb 28 07:52:57.004: INFO: Pod "pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012326822s
STEP: Saw pod success
Feb 28 07:52:57.004: INFO: Pod "pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:52:57.012: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d container env-test: <nil>
STEP: delete the pod
Feb 28 07:52:57.034: INFO: Waiting for pod pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:52:57.038: INFO: Pod pod-configmaps-db443797-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:52:57.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ld9f8" for this suite.
Feb 28 07:53:03.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:03.130: INFO: namespace: e2e-tests-configmap-ld9f8, resource: bindings, ignored listing per whitelist
Feb 28 07:53:03.241: INFO: namespace e2e-tests-configmap-ld9f8 deletion completed in 6.1977167s

• [SLOW TEST:8.512 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:03.241: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mspf5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-e0554b2e-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:53:03.492: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-mspf5" to be "success or failure"
Feb 28 07:53:03.499: INFO: Pod "pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.775543ms
Feb 28 07:53:05.504: INFO: Pod "pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012417007s
STEP: Saw pod success
Feb 28 07:53:05.504: INFO: Pod "pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:53:05.508: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:53:05.539: INFO: Waiting for pod pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:53:05.543: INFO: Pod pod-projected-secrets-e0566957-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:05.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mspf5" for this suite.
Feb 28 07:53:11.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:11.694: INFO: namespace: e2e-tests-projected-mspf5, resource: bindings, ignored listing per whitelist
Feb 28 07:53:11.803: INFO: namespace e2e-tests-projected-mspf5 deletion completed in 6.254494262s

• [SLOW TEST:8.562 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:11.804: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-xx2f4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-xx2f4/secret-test-e572d82f-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:53:12.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-xx2f4" to be "success or failure"
Feb 28 07:53:12.074: INFO: Pod "pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806144ms
Feb 28 07:53:14.080: INFO: Pod "pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010012629s
STEP: Saw pod success
Feb 28 07:53:14.080: INFO: Pod "pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:53:14.084: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d container env-test: <nil>
STEP: delete the pod
Feb 28 07:53:14.109: INFO: Waiting for pod pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:53:14.113: INFO: Pod pod-configmaps-e5737e41-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:14.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xx2f4" for this suite.
Feb 28 07:53:20.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:20.294: INFO: namespace: e2e-tests-secrets-xx2f4, resource: bindings, ignored listing per whitelist
Feb 28 07:53:20.371: INFO: namespace e2e-tests-secrets-xx2f4 deletion completed in 6.24657829s

• [SLOW TEST:8.567 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:20.371: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-thgpv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 28 07:53:20.615: INFO: Waiting up to 5m0s for pod "var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-var-expansion-thgpv" to be "success or failure"
Feb 28 07:53:20.619: INFO: Pod "var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.632468ms
Feb 28 07:53:22.624: INFO: Pod "var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008945454s
Feb 28 07:53:24.629: INFO: Pod "var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013964344s
STEP: Saw pod success
Feb 28 07:53:24.629: INFO: Pod "var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:53:24.634: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:53:24.657: INFO: Waiting for pod var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:53:24.661: INFO: Pod var-expansion-ea8b488a-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:24.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-thgpv" for this suite.
Feb 28 07:53:30.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:30.874: INFO: namespace: e2e-tests-var-expansion-thgpv, resource: bindings, ignored listing per whitelist
Feb 28 07:53:30.961: INFO: namespace e2e-tests-var-expansion-thgpv deletion completed in 6.295803401s

• [SLOW TEST:10.590 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:30.961: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-5plzt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5plzt.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5plzt.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5plzt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5plzt.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5plzt.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5plzt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 07:53:44.980: INFO: DNS probes using e2e-tests-dns-5plzt/dns-test-f0f53032-3b2d-11e9-a616-82a6c0035a5d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-5plzt" for this suite.
Feb 28 07:53:51.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:51.317: INFO: namespace: e2e-tests-dns-5plzt, resource: bindings, ignored listing per whitelist
Feb 28 07:53:51.348: INFO: namespace e2e-tests-dns-5plzt deletion completed in 6.350674548s

• [SLOW TEST:20.387 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:51.348: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gvv7n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-gvv7n/configmap-test-fd0407ba-3b2d-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:53:51.618: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-gvv7n" to be "success or failure"
Feb 28 07:53:51.623: INFO: Pod "pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.8323ms
Feb 28 07:53:53.629: INFO: Pod "pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010549966s
STEP: Saw pod success
Feb 28 07:53:53.629: INFO: Pod "pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:53:53.632: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d container env-test: <nil>
STEP: delete the pod
Feb 28 07:53:53.654: INFO: Waiting for pod pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:53:53.657: INFO: Pod pod-configmaps-fd04c4a0-3b2d-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:53:53.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gvv7n" for this suite.
Feb 28 07:53:59.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:59.741: INFO: namespace: e2e-tests-configmap-gvv7n, resource: bindings, ignored listing per whitelist
Feb 28 07:53:59.922: INFO: namespace e2e-tests-configmap-gvv7n deletion completed in 6.259110534s

• [SLOW TEST:8.574 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:53:59.923: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-b984s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:54:00.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-b984s" to be "success or failure"
Feb 28 07:54:00.189: INFO: Pod "downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.794759ms
Feb 28 07:54:02.195: INFO: Pod "downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00936039s
STEP: Saw pod success
Feb 28 07:54:02.195: INFO: Pod "downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:54:02.199: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:54:02.227: INFO: Waiting for pod downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:54:02.230: INFO: Pod downwardapi-volume-02210e4d-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:54:02.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b984s" for this suite.
Feb 28 07:54:08.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:08.395: INFO: namespace: e2e-tests-downward-api-b984s, resource: bindings, ignored listing per whitelist
Feb 28 07:54:08.609: INFO: namespace e2e-tests-downward-api-b984s deletion completed in 6.374546917s

• [SLOW TEST:8.686 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:54:08.609: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-4dzl8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:54:08.895: INFO: PodSpec: initContainers in spec.initContainers
Feb 28 07:54:51.110: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-075367f8-3b2e-11e9-a616-82a6c0035a5d", GenerateName:"", Namespace:"e2e-tests-init-container-4dzl8", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-4dzl8/pods/pod-init-075367f8-3b2e-11e9-a616-82a6c0035a5d", UID:"075446f5-3b2e-11e9-b268-0a027e1dd732", ResourceVersion:"6521", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686937248, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"895184806"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.0.66/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wldjg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000f74700), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wldjg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wldjg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wldjg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001811b18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b08de0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001811c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001811c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001811c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001811c9c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937248, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937248, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937248, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937248, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.8", PodIP:"100.96.0.66", StartTime:(*v1.Time)(0xc001994ee0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000139a40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000139ab0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d50426eeece830f287096ea90453d1b41dda091ec57764b552f05b847c1c4b79"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001994f20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001994f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:54:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4dzl8" for this suite.
Feb 28 07:55:13.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:13.311: INFO: namespace: e2e-tests-init-container-4dzl8, resource: bindings, ignored listing per whitelist
Feb 28 07:55:13.383: INFO: namespace e2e-tests-init-container-4dzl8 deletion completed in 22.268296438s

• [SLOW TEST:64.774 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:13.384: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cp29s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 07:55:16.219: INFO: Successfully updated pod "labelsupdate2dea8341-3b2e-11e9-a616-82a6c0035a5d"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cp29s" for this suite.
Feb 28 07:55:40.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:40.454: INFO: namespace: e2e-tests-downward-api-cp29s, resource: bindings, ignored listing per whitelist
Feb 28 07:55:40.532: INFO: namespace e2e-tests-downward-api-cp29s deletion completed in 22.25804551s

• [SLOW TEST:27.148 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:40.532: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vwqql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-3e1da338-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:55:40.829: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-vwqql" to be "success or failure"
Feb 28 07:55:40.833: INFO: Pod "pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392356ms
Feb 28 07:55:42.838: INFO: Pod "pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009624409s
STEP: Saw pod success
Feb 28 07:55:42.838: INFO: Pod "pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:55:42.842: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:55:42.865: INFO: Waiting for pod pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:55:42.869: INFO: Pod pod-projected-configmaps-3e1e5c21-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:42.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vwqql" for this suite.
Feb 28 07:55:48.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:49.093: INFO: namespace: e2e-tests-projected-vwqql, resource: bindings, ignored listing per whitelist
Feb 28 07:55:49.104: INFO: namespace e2e-tests-projected-vwqql deletion completed in 6.22963515s

• [SLOW TEST:8.572 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:49.104: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-7kdft
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:55:49.568: INFO: Creating deployment "test-recreate-deployment"
Feb 28 07:55:49.573: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 28 07:55:49.579: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 28 07:55:51.595: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 28 07:55:51.603: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 28 07:55:51.620: INFO: Updating deployment test-recreate-deployment
Feb 28 07:55:51.620: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:55:51.662: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-7kdft,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7kdft/deployments/test-recreate-deployment,UID:435554fd-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:6725,Generation:2,CreationTimestamp:2019-02-28 07:55:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-28 07:55:51 +0000 UTC 2019-02-28 07:55:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 07:55:51 +0000 UTC 2019-02-28 07:55:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 07:55:51.671: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-7kdft,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7kdft/replicasets/test-recreate-deployment-697fbf54bf,UID:44907c46-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:6724,Generation:1,CreationTimestamp:2019-02-28 07:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 435554fd-3b2e-11e9-b268-0a027e1dd732 0xc002083907 0xc002083908}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:55:51.671: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 28 07:55:51.671: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-7kdft,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-7kdft/replicasets/test-recreate-deployment-5dfdcc846d,UID:43566cbf-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:6717,Generation:2,CreationTimestamp:2019-02-28 07:55:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 435554fd-3b2e-11e9-b268-0a027e1dd732 0xc0020836e7 0xc0020836e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:55:51.679: INFO: Pod "test-recreate-deployment-697fbf54bf-zppvk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-zppvk,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-7kdft,SelfLink:/api/v1/namespaces/e2e-tests-deployment-7kdft/pods/test-recreate-deployment-697fbf54bf-zppvk,UID:4490fc83-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:6726,Generation:0,CreationTimestamp:2019-02-28 07:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 44907c46-3b2e-11e9-b268-0a027e1dd732 0xc001796137 0xc001796138}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-flnhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-flnhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-flnhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017961a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017961c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:55:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:55:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:55:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:55:51 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 07:55:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:55:51.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-7kdft" for this suite.
Feb 28 07:55:57.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:57.890: INFO: namespace: e2e-tests-deployment-7kdft, resource: bindings, ignored listing per whitelist
Feb 28 07:55:57.912: INFO: namespace e2e-tests-deployment-7kdft deletion completed in 6.224421678s

• [SLOW TEST:8.808 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:55:57.912: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wvvgb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:55:58.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-wvvgb" to be "success or failure"
Feb 28 07:55:58.167: INFO: Pod "downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.599118ms
Feb 28 07:56:00.172: INFO: Pod "downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0127575s
STEP: Saw pod success
Feb 28 07:56:00.172: INFO: Pod "downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:56:00.176: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 07:56:00.209: INFO: Waiting for pod downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:56:00.212: INFO: Pod downwardapi-volume-4872afb6-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:00.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wvvgb" for this suite.
Feb 28 07:56:06.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:06.387: INFO: namespace: e2e-tests-projected-wvvgb, resource: bindings, ignored listing per whitelist
Feb 28 07:56:06.519: INFO: namespace e2e-tests-projected-wvvgb deletion completed in 6.302309219s

• [SLOW TEST:8.607 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:06.520: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-4jfst
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-4d96c734-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:56:06.790: INFO: Waiting up to 5m0s for pod "pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-4jfst" to be "success or failure"
Feb 28 07:56:06.794: INFO: Pod "pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.680308ms
Feb 28 07:56:08.799: INFO: Pod "pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008622787s
STEP: Saw pod success
Feb 28 07:56:08.799: INFO: Pod "pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:56:08.802: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:56:08.824: INFO: Waiting for pod pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:56:08.828: INFO: Pod pod-configmaps-4d97a5a0-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4jfst" for this suite.
Feb 28 07:56:14.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:14.912: INFO: namespace: e2e-tests-configmap-4jfst, resource: bindings, ignored listing per whitelist
Feb 28 07:56:15.036: INFO: namespace e2e-tests-configmap-4jfst deletion completed in 6.201393795s

• [SLOW TEST:8.516 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:15.036: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-t98zq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 07:56:15.312: INFO: Waiting up to 5m0s for pod "pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-t98zq" to be "success or failure"
Feb 28 07:56:15.319: INFO: Pod "pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.727698ms
Feb 28 07:56:17.324: INFO: Pod "pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011979516s
STEP: Saw pod success
Feb 28 07:56:17.324: INFO: Pod "pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:56:17.329: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:56:17.354: INFO: Waiting for pod pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:56:17.357: INFO: Pod pod-52ac0660-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:17.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t98zq" for this suite.
Feb 28 07:56:23.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:23.557: INFO: namespace: e2e-tests-emptydir-t98zq, resource: bindings, ignored listing per whitelist
Feb 28 07:56:23.581: INFO: namespace e2e-tests-emptydir-t98zq deletion completed in 6.216947289s

• [SLOW TEST:8.545 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:23.581: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lw2tx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 28 07:56:23.827: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml cluster-info'
Feb 28 07:56:24.673: INFO: stderr: ""
Feb 28 07:56:24.673: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:24.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lw2tx" for this suite.
Feb 28 07:56:30.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:30.729: INFO: namespace: e2e-tests-kubectl-lw2tx, resource: bindings, ignored listing per whitelist
Feb 28 07:56:30.848: INFO: namespace e2e-tests-kubectl-lw2tx deletion completed in 6.169284925s

• [SLOW TEST:7.267 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:30.848: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-nm4b9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-nm4b9
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-nm4b9
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-nm4b9
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-nm4b9
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-nm4b9
Feb 28 07:56:33.130: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-nm4b9, name: ss-0, uid: 5c270c27-3b2e-11e9-b268-0a027e1dd732, status phase: Pending. Waiting for statefulset controller to delete.
Feb 28 07:56:35.603: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-nm4b9, name: ss-0, uid: 5c270c27-3b2e-11e9-b268-0a027e1dd732, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:56:35.610: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-nm4b9, name: ss-0, uid: 5c270c27-3b2e-11e9-b268-0a027e1dd732, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:56:35.612: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-nm4b9
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-nm4b9
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-nm4b9 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:56:37.631: INFO: Deleting all statefulset in ns e2e-tests-statefulset-nm4b9
Feb 28 07:56:37.635: INFO: Scaling statefulset ss to 0
Feb 28 07:56:47.658: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:56:47.666: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:56:47.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-nm4b9" for this suite.
Feb 28 07:56:53.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:53.797: INFO: namespace: e2e-tests-statefulset-nm4b9, resource: bindings, ignored listing per whitelist
Feb 28 07:56:53.875: INFO: namespace e2e-tests-statefulset-nm4b9 deletion completed in 6.172191768s

• [SLOW TEST:23.026 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:56:53.875: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-h6mt8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0228 07:57:34.379994   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:57:34.380: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:34.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-h6mt8" for this suite.
Feb 28 07:57:40.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:40.418: INFO: namespace: e2e-tests-gc-h6mt8, resource: bindings, ignored listing per whitelist
Feb 28 07:57:40.544: INFO: namespace e2e-tests-gc-h6mt8 deletion completed in 6.160069492s

• [SLOW TEST:46.670 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:40.545: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-jl9qt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:40.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-jl9qt" for this suite.
Feb 28 07:57:46.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:46.904: INFO: namespace: e2e-tests-services-jl9qt, resource: bindings, ignored listing per whitelist
Feb 28 07:57:47.056: INFO: namespace e2e-tests-services-jl9qt deletion completed in 6.226850043s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.511 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:47.056: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-tffh8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0228 07:57:48.340020   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:57:48.340: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:57:48.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tffh8" for this suite.
Feb 28 07:57:54.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:54.594: INFO: namespace: e2e-tests-gc-tffh8, resource: bindings, ignored listing per whitelist
Feb 28 07:57:54.604: INFO: namespace e2e-tests-gc-tffh8 deletion completed in 6.259297906s

• [SLOW TEST:7.548 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:57:54.604: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ch5rn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:57:54.885: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:57:55.053: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 07:57:55.053: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 28 07:57:55.061: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 28 07:57:55.072: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:57:55.072: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:58:10.908: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 07:58:10.909: INFO: stdout: "Created e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414\nScaling up e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 28 07:58:10.909: INFO: stdout: "Created e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414\nScaling up e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 28 07:58:10.909: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:58:11.078: INFO: stderr: ""
Feb 28 07:58:11.078: INFO: stdout: "e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414-m9tpt "
Feb 28 07:58:11.078: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414-m9tpt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:58:11.184: INFO: stderr: ""
Feb 28 07:58:11.184: INFO: stdout: "true"
Feb 28 07:58:11.184: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414-m9tpt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:58:11.272: INFO: stderr: ""
Feb 28 07:58:11.272: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 28 07:58:11.272: INFO: e2e-test-nginx-rc-15803cfcb26d5dab5b85aef6851cf414-m9tpt is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 28 07:58:11.272: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-ch5rn'
Feb 28 07:58:11.376: INFO: stderr: ""
Feb 28 07:58:11.376: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:58:11.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ch5rn" for this suite.
Feb 28 07:58:17.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:17.409: INFO: namespace: e2e-tests-kubectl-ch5rn, resource: bindings, ignored listing per whitelist
Feb 28 07:58:17.599: INFO: namespace e2e-tests-kubectl-ch5rn deletion completed in 6.217597614s

• [SLOW TEST:22.995 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:58:17.599: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-9dqsf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:58:19.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-9dqsf" for this suite.
Feb 28 07:59:11.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:12.082: INFO: namespace: e2e-tests-kubelet-test-9dqsf, resource: bindings, ignored listing per whitelist
Feb 28 07:59:12.153: INFO: namespace e2e-tests-kubelet-test-9dqsf deletion completed in 52.264202156s

• [SLOW TEST:54.554 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:12.154: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2mhz7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-bc4cdd7b-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 07:59:12.533: INFO: Waiting up to 5m0s for pod "pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-2mhz7" to be "success or failure"
Feb 28 07:59:12.536: INFO: Pod "pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.49799ms
Feb 28 07:59:14.542: INFO: Pod "pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009054067s
STEP: Saw pod success
Feb 28 07:59:14.542: INFO: Pod "pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:59:14.546: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:59:14.573: INFO: Waiting for pod pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:59:14.576: INFO: Pod pod-secrets-bc4dac06-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2mhz7" for this suite.
Feb 28 07:59:20.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:20.709: INFO: namespace: e2e-tests-secrets-2mhz7, resource: bindings, ignored listing per whitelist
Feb 28 07:59:20.771: INFO: namespace e2e-tests-secrets-2mhz7 deletion completed in 6.189540827s

• [SLOW TEST:8.617 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:20.771: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-tcngb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c1621e14-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 07:59:21.061: INFO: Waiting up to 5m0s for pod "pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-tcngb" to be "success or failure"
Feb 28 07:59:21.065: INFO: Pod "pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028257ms
Feb 28 07:59:23.071: INFO: Pod "pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009685781s
STEP: Saw pod success
Feb 28 07:59:23.071: INFO: Pod "pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:59:23.075: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:59:23.100: INFO: Waiting for pod pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:59:23.104: INFO: Pod pod-configmaps-c162da4d-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:23.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tcngb" for this suite.
Feb 28 07:59:29.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:29.192: INFO: namespace: e2e-tests-configmap-tcngb, resource: bindings, ignored listing per whitelist
Feb 28 07:59:29.295: INFO: namespace e2e-tests-configmap-tcngb deletion completed in 6.187132637s

• [SLOW TEST:8.524 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:29.296: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rtpr4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-c678fc5b-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating secret with name secret-projected-all-test-volume-c678fc45-3b2e-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 28 07:59:29.618: INFO: Waiting up to 5m0s for pod "projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-rtpr4" to be "success or failure"
Feb 28 07:59:29.622: INFO: Pod "projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.653394ms
Feb 28 07:59:31.629: INFO: Pod "projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011055897s
STEP: Saw pod success
Feb 28 07:59:31.629: INFO: Pod "projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:59:31.634: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 28 07:59:31.659: INFO: Waiting for pod projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:59:31.663: INFO: Pod projected-volume-c678fc10-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:31.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rtpr4" for this suite.
Feb 28 07:59:37.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:37.697: INFO: namespace: e2e-tests-projected-rtpr4, resource: bindings, ignored listing per whitelist
Feb 28 07:59:37.874: INFO: namespace e2e-tests-projected-rtpr4 deletion completed in 6.204316033s

• [SLOW TEST:8.579 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:37.874: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kvfrr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 28 07:59:38.170: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kvfrr,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvfrr/configmaps/e2e-watch-test-watch-closed,UID:cb94fbe4-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:7618,Generation:0,CreationTimestamp:2019-02-28 07:59:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 07:59:38.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kvfrr,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvfrr/configmaps/e2e-watch-test-watch-closed,UID:cb94fbe4-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:7619,Generation:0,CreationTimestamp:2019-02-28 07:59:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 28 07:59:38.191: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kvfrr,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvfrr/configmaps/e2e-watch-test-watch-closed,UID:cb94fbe4-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:7620,Generation:0,CreationTimestamp:2019-02-28 07:59:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 07:59:38.191: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kvfrr,SelfLink:/api/v1/namespaces/e2e-tests-watch-kvfrr/configmaps/e2e-watch-test-watch-closed,UID:cb94fbe4-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:7621,Generation:0,CreationTimestamp:2019-02-28 07:59:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:38.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kvfrr" for this suite.
Feb 28 07:59:44.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:44.452: INFO: namespace: e2e-tests-watch-kvfrr, resource: bindings, ignored listing per whitelist
Feb 28 07:59:44.477: INFO: namespace e2e-tests-watch-kvfrr deletion completed in 6.27947435s

• [SLOW TEST:6.603 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:44.477: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8vc6h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 07:59:44.822: INFO: Waiting up to 5m0s for pod "pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-8vc6h" to be "success or failure"
Feb 28 07:59:44.831: INFO: Pod "pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.835918ms
Feb 28 07:59:46.837: INFO: Pod "pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014227407s
STEP: Saw pod success
Feb 28 07:59:46.837: INFO: Pod "pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 07:59:46.842: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 07:59:46.867: INFO: Waiting for pod pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d to disappear
Feb 28 07:59:46.871: INFO: Pod pod-cf8b981a-3b2e-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:46.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8vc6h" for this suite.
Feb 28 07:59:52.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:53.055: INFO: namespace: e2e-tests-emptydir-8vc6h, resource: bindings, ignored listing per whitelist
Feb 28 07:59:53.077: INFO: namespace e2e-tests-emptydir-8vc6h deletion completed in 6.200895744s

• [SLOW TEST:8.600 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 07:59:53.077: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-j2zzj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:59:53.305: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 07:59:55.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-j2zzj" for this suite.
Feb 28 08:00:37.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:00:37.522: INFO: namespace: e2e-tests-pods-j2zzj, resource: bindings, ignored listing per whitelist
Feb 28 08:00:37.806: INFO: namespace e2e-tests-pods-j2zzj deletion completed in 42.37080847s

• [SLOW TEST:44.729 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:00:37.806: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-8z2vz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 28 08:00:40.295: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ef673071-3b2e-11e9-a616-82a6c0035a5d,GenerateName:,Namespace:e2e-tests-events-8z2vz,SelfLink:/api/v1/namespaces/e2e-tests-events-8z2vz/pods/send-events-ef673071-3b2e-11e9-a616-82a6c0035a5d,UID:ef67fed9-3b2e-11e9-b268-0a027e1dd732,ResourceVersion:7798,Generation:0,CreationTimestamp:2019-02-28 08:00:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 256246787,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.90/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gxglf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gxglf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-gxglf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016491d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016491f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:00:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:00:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:00:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:00:38 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.90,StartTime:2019-02-28 08:00:38 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-28 08:00:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://847e7d9ed1cee48dd00b6482e2b5025470a447eb1893f94b8e4ca1fc5b9f77fe}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 28 08:00:42.303: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 28 08:00:44.308: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:00:44.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-8z2vz" for this suite.
Feb 28 08:01:28.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:01:28.368: INFO: namespace: e2e-tests-events-8z2vz, resource: bindings, ignored listing per whitelist
Feb 28 08:01:28.563: INFO: namespace e2e-tests-events-8z2vz deletion completed in 44.235714866s

• [SLOW TEST:50.757 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:01:28.563: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cjgnk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-0da24b5c-3b2f-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:01:28.988: INFO: Waiting up to 5m0s for pod "pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-cjgnk" to be "success or failure"
Feb 28 08:01:28.993: INFO: Pod "pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799261ms
Feb 28 08:01:31.001: INFO: Pod "pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012126665s
STEP: Saw pod success
Feb 28 08:01:31.001: INFO: Pod "pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:01:31.221: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:01:31.249: INFO: Waiting for pod pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:01:31.255: INFO: Pod pod-configmaps-0da31541-3b2f-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:01:31.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cjgnk" for this suite.
Feb 28 08:01:37.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:01:37.471: INFO: namespace: e2e-tests-configmap-cjgnk, resource: bindings, ignored listing per whitelist
Feb 28 08:01:37.540: INFO: namespace e2e-tests-configmap-cjgnk deletion completed in 6.280341117s

• [SLOW TEST:8.977 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:01:37.541: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-w2q6r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:01:37.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-w2q6r" to be "success or failure"
Feb 28 08:01:37.802: INFO: Pod "downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946261ms
Feb 28 08:01:39.808: INFO: Pod "downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00935176s
STEP: Saw pod success
Feb 28 08:01:39.808: INFO: Pod "downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:01:39.812: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:01:39.837: INFO: Waiting for pod downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:01:39.840: INFO: Pod downwardapi-volume-12e2c07a-3b2f-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:01:39.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w2q6r" for this suite.
Feb 28 08:01:45.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:01:46.010: INFO: namespace: e2e-tests-downward-api-w2q6r, resource: bindings, ignored listing per whitelist
Feb 28 08:01:46.062: INFO: namespace e2e-tests-downward-api-w2q6r deletion completed in 6.21647716s

• [SLOW TEST:8.522 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:01:46.063: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-md9vh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:01:46.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-md9vh" to be "success or failure"
Feb 28 08:01:46.374: INFO: Pod "downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69655ms
Feb 28 08:01:48.380: INFO: Pod "downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010497383s
STEP: Saw pod success
Feb 28 08:01:48.380: INFO: Pod "downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:01:48.391: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:01:48.419: INFO: Waiting for pod downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:01:48.422: INFO: Pod downwardapi-volume-17fe54d5-3b2f-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:01:48.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-md9vh" for this suite.
Feb 28 08:01:54.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:01:54.478: INFO: namespace: e2e-tests-downward-api-md9vh, resource: bindings, ignored listing per whitelist
Feb 28 08:01:54.679: INFO: namespace e2e-tests-downward-api-md9vh deletion completed in 6.251234453s

• [SLOW TEST:8.616 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:01:54.679: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qnnqr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qnnqr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:01:54.930: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:02:17.027: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.95:8080/dial?request=hostName&protocol=udp&host=100.96.1.19&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qnnqr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:02:17.027: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:02:17.552: INFO: Waiting for endpoints: map[]
Feb 28 08:02:17.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.95:8080/dial?request=hostName&protocol=udp&host=100.96.0.94&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-qnnqr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:02:17.561: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:02:18.071: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:02:18.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qnnqr" for this suite.
Feb 28 08:02:40.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:40.290: INFO: namespace: e2e-tests-pod-network-test-qnnqr, resource: bindings, ignored listing per whitelist
Feb 28 08:02:40.465: INFO: namespace e2e-tests-pod-network-test-qnnqr deletion completed in 22.387271778s

• [SLOW TEST:45.786 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:02:40.465: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l8cds
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 28 08:02:40.730: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 28 08:02:40.730: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:40.996: INFO: stderr: ""
Feb 28 08:02:40.996: INFO: stdout: "service/redis-slave created\n"
Feb 28 08:02:40.997: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 28 08:02:40.997: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:41.210: INFO: stderr: ""
Feb 28 08:02:41.210: INFO: stdout: "service/redis-master created\n"
Feb 28 08:02:41.210: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 28 08:02:41.210: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:41.434: INFO: stderr: ""
Feb 28 08:02:41.434: INFO: stdout: "service/frontend created\n"
Feb 28 08:02:41.434: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 28 08:02:41.434: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:41.656: INFO: stderr: ""
Feb 28 08:02:41.657: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 28 08:02:41.657: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 28 08:02:41.657: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:42.021: INFO: stderr: ""
Feb 28 08:02:42.021: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 28 08:02:42.021: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 28 08:02:42.021: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:02:42.237: INFO: stderr: ""
Feb 28 08:02:42.237: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 28 08:02:42.237: INFO: Waiting for all frontend pods to be Running.
Feb 28 08:02:57.288: INFO: Waiting for frontend to serve content.
Feb 28 08:03:02.408: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 28 08:03:07.503: INFO: Trying to add a new entry to the guestbook.
Feb 28 08:03:07.551: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 28 08:03:07.572: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:07.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:07.731: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:03:07.731: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:07.833: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:07.833: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:03:07.833: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:07.942: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:07.942: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:03:07.942: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:08.052: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:08.052: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:03:08.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:08.174: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:08.174: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:03:08.174: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-l8cds'
Feb 28 08:03:08.289: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:03:08.290: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:03:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l8cds" for this suite.
Feb 28 08:03:48.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:48.440: INFO: namespace: e2e-tests-kubectl-l8cds, resource: bindings, ignored listing per whitelist
Feb 28 08:03:48.638: INFO: namespace e2e-tests-kubectl-l8cds deletion completed in 40.341803406s

• [SLOW TEST:68.174 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:03:48.639: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vpxj9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-610847c4-3b2f-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:03:48.908: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-vpxj9" to be "success or failure"
Feb 28 08:03:48.914: INFO: Pod "pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487545ms
Feb 28 08:03:50.923: INFO: Pod "pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014727418s
STEP: Saw pod success
Feb 28 08:03:50.923: INFO: Pod "pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:03:50.930: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:03:50.958: INFO: Waiting for pod pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:03:50.962: INFO: Pod pod-projected-configmaps-610944b5-3b2f-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:03:50.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vpxj9" for this suite.
Feb 28 08:03:56.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:57.683: INFO: namespace: e2e-tests-projected-vpxj9, resource: bindings, ignored listing per whitelist
Feb 28 08:03:57.733: INFO: namespace e2e-tests-projected-vpxj9 deletion completed in 6.764172554s

• [SLOW TEST:9.094 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:03:57.733: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-4rz4k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4rz4k
Feb 28 08:04:00.249: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4rz4k
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:04:00.259: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:01.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4rz4k" for this suite.
Feb 28 08:08:07.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:07.517: INFO: namespace: e2e-tests-container-probe-4rz4k, resource: bindings, ignored listing per whitelist
Feb 28 08:08:07.615: INFO: namespace e2e-tests-container-probe-4rz4k deletion completed in 6.212216525s

• [SLOW TEST:249.882 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:08:07.615: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-t52kk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-t52kk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:08:07.856: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:08:27.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.1.22 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-t52kk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:08:27.956: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:08:29.432: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:08:29.437: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.0.102 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-t52kk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:08:29.437: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:08:30.911: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-t52kk" for this suite.
Feb 28 08:08:52.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:52.968: INFO: namespace: e2e-tests-pod-network-test-t52kk, resource: bindings, ignored listing per whitelist
Feb 28 08:08:53.143: INFO: namespace e2e-tests-pod-network-test-t52kk deletion completed in 22.22643529s

• [SLOW TEST:45.528 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:08:53.144: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-h9h84
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:08:53.451: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:08:55.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-h9h84" for this suite.
Feb 28 08:09:37.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:38.288: INFO: namespace: e2e-tests-pods-h9h84, resource: bindings, ignored listing per whitelist
Feb 28 08:09:38.315: INFO: namespace e2e-tests-pods-h9h84 deletion completed in 42.448809156s

• [SLOW TEST:45.171 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:09:38.315: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-24clv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:09:38.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-24clv" to be "success or failure"
Feb 28 08:09:38.589: INFO: Pod "downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575332ms
Feb 28 08:09:40.596: INFO: Pod "downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011502753s
STEP: Saw pod success
Feb 28 08:09:40.596: INFO: Pod "downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:09:40.601: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:09:40.627: INFO: Waiting for pod downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:09:40.632: INFO: Pod downwardapi-volume-31757afe-3b30-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:09:40.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-24clv" for this suite.
Feb 28 08:09:46.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:47.098: INFO: namespace: e2e-tests-downward-api-24clv, resource: bindings, ignored listing per whitelist
Feb 28 08:09:47.124: INFO: namespace e2e-tests-downward-api-24clv deletion completed in 6.483287076s

• [SLOW TEST:8.809 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:09:47.124: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rkl98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 08:09:47.486: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rkl98'
Feb 28 08:09:48.741: INFO: stderr: ""
Feb 28 08:09:48.741: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 08:09:49.748: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:09:49.749: INFO: Found 0 / 1
Feb 28 08:09:50.749: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:09:50.749: INFO: Found 1 / 1
Feb 28 08:09:50.749: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 28 08:09:50.757: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:09:50.757: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 08:09:50.757: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml patch pod redis-master-7n8rd --namespace=e2e-tests-kubectl-rkl98 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 28 08:09:50.853: INFO: stderr: ""
Feb 28 08:09:50.853: INFO: stdout: "pod/redis-master-7n8rd patched\n"
STEP: checking annotations
Feb 28 08:09:50.858: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:09:50.858: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:09:50.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rkl98" for this suite.
Feb 28 08:10:12.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:14.514: INFO: namespace: e2e-tests-kubectl-rkl98, resource: bindings, ignored listing per whitelist
Feb 28 08:10:14.529: INFO: namespace e2e-tests-kubectl-rkl98 deletion completed in 23.665195728s

• [SLOW TEST:27.405 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:10:14.529: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-98zsx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:10:14.855: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:10:17.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-98zsx" for this suite.
Feb 28 08:10:23.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:23.249: INFO: namespace: e2e-tests-init-container-98zsx, resource: bindings, ignored listing per whitelist
Feb 28 08:10:23.325: INFO: namespace e2e-tests-init-container-98zsx deletion completed in 6.192846636s

• [SLOW TEST:8.796 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:10:23.326: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-2c5sb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-flrj
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:10:23.585: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-flrj" in namespace "e2e-tests-subpath-2c5sb" to be "success or failure"
Feb 28 08:10:23.589: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.936825ms
Feb 28 08:10:25.594: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009297835s
Feb 28 08:10:27.600: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 4.014766362s
Feb 28 08:10:29.607: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 6.021938494s
Feb 28 08:10:31.613: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 8.027908977s
Feb 28 08:10:33.618: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 10.033432523s
Feb 28 08:10:35.630: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 12.045434069s
Feb 28 08:10:37.635: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 14.049807724s
Feb 28 08:10:39.641: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 16.056481191s
Feb 28 08:10:41.648: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 18.063314873s
Feb 28 08:10:43.655: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 20.069924952s
Feb 28 08:10:45.661: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Running", Reason="", readiness=false. Elapsed: 22.076410338s
Feb 28 08:10:47.668: INFO: Pod "pod-subpath-test-configmap-flrj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.083188912s
STEP: Saw pod success
Feb 28 08:10:47.668: INFO: Pod "pod-subpath-test-configmap-flrj" satisfied condition "success or failure"
Feb 28 08:10:47.675: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-subpath-test-configmap-flrj container test-container-subpath-configmap-flrj: <nil>
STEP: delete the pod
Feb 28 08:10:47.701: INFO: Waiting for pod pod-subpath-test-configmap-flrj to disappear
Feb 28 08:10:47.705: INFO: Pod pod-subpath-test-configmap-flrj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-flrj
Feb 28 08:10:47.705: INFO: Deleting pod "pod-subpath-test-configmap-flrj" in namespace "e2e-tests-subpath-2c5sb"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:10:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-2c5sb" for this suite.
Feb 28 08:10:53.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:54.113: INFO: namespace: e2e-tests-subpath-2c5sb, resource: bindings, ignored listing per whitelist
Feb 28 08:10:54.135: INFO: namespace e2e-tests-subpath-2c5sb deletion completed in 6.421887287s

• [SLOW TEST:30.809 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:10:54.135: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-t2pdl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 08:10:54.450: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:10:54.743: INFO: stderr: ""
Feb 28 08:10:54.743: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:10:54.743: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:10:54.831: INFO: stderr: ""
Feb 28 08:10:54.831: INFO: stdout: "update-demo-nautilus-6z6qz update-demo-nautilus-lhgbj "
Feb 28 08:10:54.831: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-6z6qz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:10:54.925: INFO: stderr: ""
Feb 28 08:10:54.925: INFO: stdout: ""
Feb 28 08:10:54.925: INFO: update-demo-nautilus-6z6qz is created but not running
Feb 28 08:10:59.925: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.042: INFO: stderr: ""
Feb 28 08:11:00.042: INFO: stdout: "update-demo-nautilus-6z6qz update-demo-nautilus-lhgbj "
Feb 28 08:11:00.042: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-6z6qz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.134: INFO: stderr: ""
Feb 28 08:11:00.134: INFO: stdout: "true"
Feb 28 08:11:00.134: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-6z6qz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.223: INFO: stderr: ""
Feb 28 08:11:00.223: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:11:00.223: INFO: validating pod update-demo-nautilus-6z6qz
Feb 28 08:11:00.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:11:00.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:11:00.311: INFO: update-demo-nautilus-6z6qz is verified up and running
Feb 28 08:11:00.311: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-lhgbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.398: INFO: stderr: ""
Feb 28 08:11:00.398: INFO: stdout: "true"
Feb 28 08:11:00.398: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-lhgbj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.487: INFO: stderr: ""
Feb 28 08:11:00.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:11:00.487: INFO: validating pod update-demo-nautilus-lhgbj
Feb 28 08:11:00.575: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:11:00.575: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:11:00.575: INFO: update-demo-nautilus-lhgbj is verified up and running
STEP: using delete to clean up resources
Feb 28 08:11:00.575: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.667: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:11:00.667: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 08:11:00.667: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-t2pdl'
Feb 28 08:11:00.768: INFO: stderr: "No resources found.\n"
Feb 28 08:11:00.768: INFO: stdout: ""
Feb 28 08:11:00.768: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-t2pdl -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:11:00.869: INFO: stderr: ""
Feb 28 08:11:00.869: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:11:00.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t2pdl" for this suite.
Feb 28 08:11:22.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:23.245: INFO: namespace: e2e-tests-kubectl-t2pdl, resource: bindings, ignored listing per whitelist
Feb 28 08:11:23.330: INFO: namespace e2e-tests-kubectl-t2pdl deletion completed in 22.455243386s

• [SLOW TEST:29.195 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:11:23.330: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-ngtgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 28 08:11:23.579: INFO: Waiting up to 5m0s for pod "client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-containers-ngtgt" to be "success or failure"
Feb 28 08:11:23.585: INFO: Pod "client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626069ms
Feb 28 08:11:25.591: INFO: Pod "client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011485244s
Feb 28 08:11:27.602: INFO: Pod "client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022683006s
STEP: Saw pod success
Feb 28 08:11:27.602: INFO: Pod "client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:11:27.606: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:11:27.638: INFO: Waiting for pod client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:11:27.643: INFO: Pod client-containers-700a760e-3b30-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:11:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ngtgt" for this suite.
Feb 28 08:11:33.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:33.696: INFO: namespace: e2e-tests-containers-ngtgt, resource: bindings, ignored listing per whitelist
Feb 28 08:11:33.813: INFO: namespace e2e-tests-containers-ngtgt deletion completed in 6.163692308s

• [SLOW TEST:10.483 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:11:33.813: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-wptzr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-wptzr
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-wptzr
STEP: Deleting pre-stop pod
Feb 28 08:11:45.227: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:11:45.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-wptzr" for this suite.
Feb 28 08:12:23.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:12:23.412: INFO: namespace: e2e-tests-prestop-wptzr, resource: bindings, ignored listing per whitelist
Feb 28 08:12:23.469: INFO: namespace e2e-tests-prestop-wptzr deletion completed in 38.22532429s

• [SLOW TEST:49.656 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:12:23.469: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2vrf4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:12:26.254: INFO: Successfully updated pod "annotationupdate93e23d14-3b30-11e9-a616-82a6c0035a5d"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:12:28.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2vrf4" for this suite.
Feb 28 08:12:46.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:12:46.466: INFO: namespace: e2e-tests-downward-api-2vrf4, resource: bindings, ignored listing per whitelist
Feb 28 08:12:46.482: INFO: namespace e2e-tests-downward-api-2vrf4 deletion completed in 18.194231972s

• [SLOW TEST:23.012 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:12:46.482: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-pq8hf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:12:46.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-pq8hf" to be "success or failure"
Feb 28 08:12:46.785: INFO: Pod "downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.490834ms
Feb 28 08:12:48.790: INFO: Pod "downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009999148s
STEP: Saw pod success
Feb 28 08:12:48.791: INFO: Pod "downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:12:48.801: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:12:48.821: INFO: Waiting for pod downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:12:48.825: INFO: Pod downwardapi-volume-a1a1e1ab-3b30-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:12:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pq8hf" for this suite.
Feb 28 08:12:54.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:12:54.999: INFO: namespace: e2e-tests-downward-api-pq8hf, resource: bindings, ignored listing per whitelist
Feb 28 08:12:55.129: INFO: namespace e2e-tests-downward-api-pq8hf deletion completed in 6.298869478s

• [SLOW TEST:8.648 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:12:55.129: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sscp7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:12:55.394: INFO: Creating deployment "nginx-deployment"
Feb 28 08:12:55.399: INFO: Waiting for observed generation 1
Feb 28 08:12:57.409: INFO: Waiting for all required pods to come up
Feb 28 08:12:57.415: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 28 08:12:59.427: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 28 08:12:59.435: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 28 08:12:59.443: INFO: Updating deployment nginx-deployment
Feb 28 08:12:59.443: INFO: Waiting for observed generation 2
Feb 28 08:13:01.454: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 28 08:13:01.458: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 28 08:13:01.462: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:13:01.476: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 28 08:13:01.476: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 28 08:13:01.479: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:13:01.487: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 28 08:13:01.487: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 28 08:13:01.495: INFO: Updating deployment nginx-deployment
Feb 28 08:13:01.495: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:13:01.504: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 28 08:13:03.515: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:13:03.524: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-sscp7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sscp7/deployments/nginx-deployment,UID:a6c5ea94-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9969,Generation:3,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-28 08:13:01 +0000 UTC 2019-02-28 08:13:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 08:13:01 +0000 UTC 2019-02-28 08:12:55 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 28 08:13:03.529: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-sscp7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sscp7/replicasets/nginx-deployment-65bbdb5f8,UID:a92f9eb9-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9966,Generation:3,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a6c5ea94-3b30-11e9-b268-0a027e1dd732 0xc002108217 0xc002108218}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:13:03.529: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 28 08:13:03.529: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-sscp7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sscp7/replicasets/nginx-deployment-555b55d965,UID:a6c75e32-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9965,Generation:3,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a6c5ea94-3b30-11e9-b268-0a027e1dd732 0xc002108107 0xc002108108}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-2c4bx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2c4bx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-2c4bx,UID:a6ca1fdc-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9833,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0012a9dc0 0xc0012a9dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012a9e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012a9e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.1.26,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://58e37b2d3acbea7d38e09596ce5c58dfbfe860f53b9f7be3357a8fef1b340c52}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-2x6kf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2x6kf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-2x6kf,UID:aa6d2246-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9978,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d02a0 0xc0021d02a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d03b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d03d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-58gsr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-58gsr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-58gsr,UID:aa6dbcb7-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9975,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d04c0 0xc0021d04c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d0760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d0780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-86kq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-86kq8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-86kq8,UID:aa6a231c-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9989,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.125/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d0840 0xc0021d0841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d0a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d0aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-9kl78" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9kl78,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-9kl78,UID:aa6d9681-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9982,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d0b60 0xc0021d0b61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d0d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d0d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.539: INFO: Pod "nginx-deployment-555b55d965-9vbkw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9vbkw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-9vbkw,UID:a6ca2fd3-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9851,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.118/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d0df0 0xc0021d0df1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d0e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d0f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.118,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c3bbc64230424b405aca9d1db85210ca54efd2775faa8ebae2b550e857d2e1c5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-ccr9t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ccr9t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-ccr9t,UID:a6cb3a94-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9854,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.115/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d1330 0xc0021d1331}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d1390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d1490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.115,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://11edfebf5bdd8d0fb1683cf1a896b42df4a1f28437d3abfe7385c7bce1c54740}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-dkh7j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dkh7j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-dkh7j,UID:aa6d550c-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9973,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d1620 0xc0021d1621}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d1680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d16a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-dlltw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dlltw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-dlltw,UID:aa6a76c8-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9990,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d19a0 0xc0021d19a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d1a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d1a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-gx8l4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gx8l4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-gx8l4,UID:a6cb56f4-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9863,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d1be0 0xc0021d1be1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d1c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d1c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.1.27,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://d1666b274310bafac48dd05afade8a1d7ef1141b87d6bb99c81917cb86d63e92}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-h2lfl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-h2lfl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-h2lfl,UID:a6ccf01f-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9860,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.119/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d1de0 0xc0021d1de1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021d1eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021d1ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.119,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://a521be1b51e3a048eb3cb415970ee585213039d4d6b910583ebfb3fc3505c367}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-npq7h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-npq7h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-npq7h,UID:aa695ce8-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9986,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.124/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0021d1fa0 0xc0021d1fa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3e000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3e020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.540: INFO: Pod "nginx-deployment-555b55d965-p5gk4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-p5gk4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-p5gk4,UID:a6ccb492-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9836,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3e1a0 0xc001a3e1a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3e200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3e290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.1.25,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f5ee1075e6769f5cea3e4f3adab8cb4c09e15073b72c3429f69d960a50559c02}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-rzl65" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rzl65,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-rzl65,UID:aa6c047a-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9988,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3e500 0xc001a3e501}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3e570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3e590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-sjjwj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sjjwj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-sjjwj,UID:a6cb658c-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9857,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.116/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3e7d0 0xc001a3e7d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3f220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3f240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.116,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:57 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2a844ee92f921ad31c4143ac9651db05b0287237b0fed76efcaba62351432f6f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-skn9j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-skn9j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-skn9j,UID:aa6d76a1-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9974,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3f330 0xc001a3f331}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3f630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3f650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-sr2q7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sr2q7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-sr2q7,UID:aa6b569a-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9994,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.127/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3f7c0 0xc001a3f7c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3f890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3fa00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-tqm95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tqm95,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-tqm95,UID:aa6bd45f-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9971,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3fab0 0xc001a3fab1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3fbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3fbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.541: INFO: Pod "nginx-deployment-555b55d965-x48m2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x48m2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-x48m2,UID:a6cb3df8-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9839,Generation:0,CreationTimestamp:2019-02-28 08:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.24/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc001a3fd30 0xc001a3fd31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a3fea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a3fec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.1.24,StartTime:2019-02-28 08:12:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:12:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://232edc653e1947fd06b1cf018d4846e7fd2d1e1c9b7a1b6a6d283b97f7fc6db0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-555b55d965-z48mn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z48mn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-555b55d965-z48mn,UID:aa6b9e9a-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9991,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.126/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 a6c75e32-3b30-11e9-b268-0a027e1dd732 0xc0019a25f0 0xc0019a25f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a27c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a27e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-82bqf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-82bqf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-82bqf,UID:a93634ff-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9905,Generation:0,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a2950 0xc0019a2951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a29c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a2b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:12:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-9tqm9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-9tqm9,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-9tqm9,UID:aa6cbc54-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9976,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a2c00 0xc0019a2c01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a2c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a30a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-d76pl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-d76pl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-d76pl,UID:aa7688c7-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9984,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3170 0xc0019a3171}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a31e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-dk7xp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dk7xp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-dk7xp,UID:aa6e0119-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9977,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a32c0 0xc0019a32c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-f6mdl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-f6mdl,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-f6mdl,UID:a930d284-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9908,Generation:0,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.123/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3430 0xc0019a3431}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a34a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a34c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:12:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-g5xk8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-g5xk8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-g5xk8,UID:aa71b133-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9980,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3580 0xc0019a3581}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a35f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-hzczg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hzczg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-hzczg,UID:aa6a9f61-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9992,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a36e0 0xc0019a36e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.542: INFO: Pod "nginx-deployment-65bbdb5f8-mgjsw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mgjsw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-mgjsw,UID:a9371e09-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9907,Generation:0,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.122/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3840 0xc0019a3841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a38b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a38d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:12:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.543: INFO: Pod "nginx-deployment-65bbdb5f8-nwwd5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nwwd5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-nwwd5,UID:aa6ec79c-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9979,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3990 0xc0019a3991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.543: INFO: Pod "nginx-deployment-65bbdb5f8-pzbfw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pzbfw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-pzbfw,UID:a93021e2-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9985,Generation:0,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.121/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3af0 0xc0019a3af1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.121,StartTime:2019-02-28 08:12:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.543: INFO: Pod "nginx-deployment-65bbdb5f8-r8xs8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-r8xs8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-r8xs8,UID:aa6c82fb-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9972,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3c60 0xc0019a3c61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.543: INFO: Pod "nginx-deployment-65bbdb5f8-wdzn6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wdzn6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-wdzn6,UID:aa71d018-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9995,Generation:0,CreationTimestamp:2019-02-28 08:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3dc0 0xc0019a3dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:13:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:13:03.543: INFO: Pod "nginx-deployment-65bbdb5f8-z2lb6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-z2lb6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-sscp7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sscp7/pods/nginx-deployment-65bbdb5f8-z2lb6,UID:a930ae45-3b30-11e9-b268-0a027e1dd732,ResourceVersion:9904,Generation:0,CreationTimestamp:2019-02-28 08:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.28/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 a92f9eb9-3b30-11e9-b268-0a027e1dd732 0xc0019a3f20 0xc0019a3f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xxxmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxxmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xxxmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a3f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a3fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:12:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:12:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:13:03.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sscp7" for this suite.
Feb 28 08:13:11.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:13:11.771: INFO: namespace: e2e-tests-deployment-sscp7, resource: bindings, ignored listing per whitelist
Feb 28 08:13:11.774: INFO: namespace e2e-tests-deployment-sscp7 deletion completed in 8.226061327s

• [SLOW TEST:16.645 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:13:11.774: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-jl9wv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:13:12.014: INFO: Creating ReplicaSet my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d
Feb 28 08:13:12.024: INFO: Pod name my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d: Found 0 pods out of 1
Feb 28 08:13:17.032: INFO: Pod name my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d: Found 1 pods out of 1
Feb 28 08:13:17.032: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d" is running
Feb 28 08:13:17.037: INFO: Pod "my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d-2x2c9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:13:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:13:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:13:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:13:12 +0000 UTC Reason: Message:}])
Feb 28 08:13:17.037: INFO: Trying to dial the pod
Feb 28 08:13:22.137: INFO: Controller my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d: Got expected result from replica 1 [my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d-2x2c9]: "my-hostname-basic-b0ada0ac-3b30-11e9-a616-82a6c0035a5d-2x2c9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:13:22.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-jl9wv" for this suite.
Feb 28 08:13:28.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:13:28.510: INFO: namespace: e2e-tests-replicaset-jl9wv, resource: bindings, ignored listing per whitelist
Feb 28 08:13:28.535: INFO: namespace e2e-tests-replicaset-jl9wv deletion completed in 6.389922729s

• [SLOW TEST:16.760 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:13:28.535: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2zz5r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-babf8d1f-3b30-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:13:28.923: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-2zz5r" to be "success or failure"
Feb 28 08:13:28.928: INFO: Pod "pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.304781ms
Feb 28 08:13:30.933: INFO: Pod "pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009539973s
Feb 28 08:13:32.943: INFO: Pod "pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019548393s
STEP: Saw pod success
Feb 28 08:13:32.943: INFO: Pod "pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:13:32.948: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:13:32.970: INFO: Waiting for pod pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:13:32.973: INFO: Pod pod-projected-configmaps-bac055a0-3b30-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:13:32.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2zz5r" for this suite.
Feb 28 08:13:38.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:13:39.252: INFO: namespace: e2e-tests-projected-2zz5r, resource: bindings, ignored listing per whitelist
Feb 28 08:13:39.352: INFO: namespace e2e-tests-projected-2zz5r deletion completed in 6.375153761s

• [SLOW TEST:10.817 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:13:39.353: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mmg65
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-c120873d-3b30-11e9-a616-82a6c0035a5d
STEP: Creating secret with name s-test-opt-upd-c1208781-3b30-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c120873d-3b30-11e9-a616-82a6c0035a5d
STEP: Updating secret s-test-opt-upd-c1208781-3b30-11e9-a616-82a6c0035a5d
STEP: Creating secret with name s-test-opt-create-c120879c-3b30-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:13:44.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mmg65" for this suite.
Feb 28 08:14:06.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:14:06.344: INFO: namespace: e2e-tests-secrets-mmg65, resource: bindings, ignored listing per whitelist
Feb 28 08:14:06.562: INFO: namespace e2e-tests-secrets-mmg65 deletion completed in 22.463952618s

• [SLOW TEST:27.209 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:14:06.562: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-r9r9d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:14:07.057: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:14:10.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-r9r9d" for this suite.
Feb 28 08:14:32.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:14:32.944: INFO: namespace: e2e-tests-init-container-r9r9d, resource: bindings, ignored listing per whitelist
Feb 28 08:14:33.139: INFO: namespace e2e-tests-init-container-r9r9d deletion completed in 22.288576924s

• [SLOW TEST:26.576 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:14:33.139: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-hpvht
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-hpvht;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpvht.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-hpvht.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.193.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.193.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.193.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.193.95_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-hpvht;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-hpvht;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpvht.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpvht.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpvht.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpvht.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-hpvht.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.193.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.193.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.193.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.193.95_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 08:14:35.621: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.662: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.676: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.689: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.703: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.715: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.726: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:35.738: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.232: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.239: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.247: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.253: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.259: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.266: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.272: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:36.736: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:14:41.744: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.757: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.764: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.772: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.780: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.787: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:41.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.264: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.278: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.285: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.293: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.300: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.308: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.358: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.412: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:42.910: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:14:46.747: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.784: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.791: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.798: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.804: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.809: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.815: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:46.821: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.312: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.319: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.326: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.331: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.337: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.342: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.348: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.353: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:47.815: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:14:51.745: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.752: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.759: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.766: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.772: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.778: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.785: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:51.792: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.296: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.304: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.312: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.322: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.333: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.340: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.552: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:52.560: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:53.054: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:14:56.748: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.790: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.798: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.808: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.817: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.826: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.833: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:56.839: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.332: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.339: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.345: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.351: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.357: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.364: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.371: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.377: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:14:57.822: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:15:01.748: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.754: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.762: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.769: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.780: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.787: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.795: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:01.803: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.288: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.295: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.302: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.309: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.316: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.329: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.347: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc from pod e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d: the server could not find the requested resource (get pods dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d)
Feb 28 08:15:02.804: INFO: Lookups using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpvht wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht wheezy_udp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpvht.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpvht jessie_tcp@dns-test-service.e2e-tests-dns-hpvht jessie_udp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpvht.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpvht.svc]

Feb 28 08:15:09.208: INFO: DNS probes using e2e-tests-dns-hpvht/dns-test-e136202f-3b30-11e9-a616-82a6c0035a5d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:09.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-hpvht" for this suite.
Feb 28 08:15:15.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:15.403: INFO: namespace: e2e-tests-dns-hpvht, resource: bindings, ignored listing per whitelist
Feb 28 08:15:15.479: INFO: namespace e2e-tests-dns-hpvht deletion completed in 6.212641168s

• [SLOW TEST:42.341 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:15:15.480: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-cbx5j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 28 08:15:15.740: INFO: Waiting up to 5m0s for pod "var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-var-expansion-cbx5j" to be "success or failure"
Feb 28 08:15:15.746: INFO: Pod "var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.47683ms
Feb 28 08:15:17.753: INFO: Pod "var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011994133s
STEP: Saw pod success
Feb 28 08:15:17.753: INFO: Pod "var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:15:17.756: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:15:17.779: INFO: Waiting for pod var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:15:17.783: INFO: Pod var-expansion-fa6b60c1-3b30-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:17.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-cbx5j" for this suite.
Feb 28 08:15:23.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:23.960: INFO: namespace: e2e-tests-var-expansion-cbx5j, resource: bindings, ignored listing per whitelist
Feb 28 08:15:24.003: INFO: namespace e2e-tests-var-expansion-cbx5j deletion completed in 6.214788892s

• [SLOW TEST:8.523 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:15:24.003: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-dmnmm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 28 08:15:24.975: INFO: Waiting up to 5m0s for pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756" in namespace "e2e-tests-svcaccounts-dmnmm" to be "success or failure"
Feb 28 08:15:24.978: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949768ms
Feb 28 08:15:26.987: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011633282s
STEP: Saw pod success
Feb 28 08:15:26.987: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756" satisfied condition "success or failure"
Feb 28 08:15:26.994: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756 container token-test: <nil>
STEP: delete the pod
Feb 28 08:15:27.020: INFO: Waiting for pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756 to disappear
Feb 28 08:15:27.024: INFO: Pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-l5756 no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 28 08:15:27.030: INFO: Waiting up to 5m0s for pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf" in namespace "e2e-tests-svcaccounts-dmnmm" to be "success or failure"
Feb 28 08:15:27.034: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130189ms
Feb 28 08:15:29.043: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012799461s
STEP: Saw pod success
Feb 28 08:15:29.043: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf" satisfied condition "success or failure"
Feb 28 08:15:29.048: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf container root-ca-test: <nil>
STEP: delete the pod
Feb 28 08:15:29.073: INFO: Waiting for pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf to disappear
Feb 28 08:15:29.077: INFO: Pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-tnmtf no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 28 08:15:29.084: INFO: Waiting up to 5m0s for pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7" in namespace "e2e-tests-svcaccounts-dmnmm" to be "success or failure"
Feb 28 08:15:29.095: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.153423ms
Feb 28 08:15:31.101: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016836364s
STEP: Saw pod success
Feb 28 08:15:31.101: INFO: Pod "pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7" satisfied condition "success or failure"
Feb 28 08:15:31.113: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7 container namespace-test: <nil>
STEP: delete the pod
Feb 28 08:15:31.141: INFO: Waiting for pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7 to disappear
Feb 28 08:15:31.145: INFO: Pod pod-service-account-ffecbc71-3b30-11e9-a616-82a6c0035a5d-nnzq7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:31.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-dmnmm" for this suite.
Feb 28 08:15:37.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:37.252: INFO: namespace: e2e-tests-svcaccounts-dmnmm, resource: bindings, ignored listing per whitelist
Feb 28 08:15:37.369: INFO: namespace e2e-tests-svcaccounts-dmnmm deletion completed in 6.217901662s

• [SLOW TEST:13.366 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:15:37.369: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-sxk42
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-sxk42
Feb 28 08:15:41.635: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-sxk42
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:15:41.640: INFO: Initial restart count of pod liveness-http is 0
Feb 28 08:15:59.755: INFO: Restart count of pod e2e-tests-container-probe-sxk42/liveness-http is now 1 (18.11466214s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:15:59.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-sxk42" for this suite.
Feb 28 08:16:05.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:06.247: INFO: namespace: e2e-tests-container-probe-sxk42, resource: bindings, ignored listing per whitelist
Feb 28 08:16:06.264: INFO: namespace e2e-tests-container-probe-sxk42 deletion completed in 6.448522809s

• [SLOW TEST:28.895 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:16:06.265: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vbb2w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 28 08:16:08.798: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-18d60e2c-3b31-11e9-a616-82a6c0035a5d", GenerateName:"", Namespace:"e2e-tests-pods-vbb2w", SelfLink:"/api/v1/namespaces/e2e-tests-pods-vbb2w/pods/pod-submit-remove-18d60e2c-3b31-11e9-a616-82a6c0035a5d", UID:"18d7d877-3b31-11e9-b268-0a027e1dd732", ResourceVersion:"10726", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686938566, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"762863738"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.0.145/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sl2d2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001a4d900), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sl2d2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0016c1b28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001aaeb40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016c1b60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016c1b80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0016c1b88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0016c1b8c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938566, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938568, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938568, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938566, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.8", PodIP:"100.96.0.145", StartTime:(*v1.Time)(0xc0016e5120), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0016e5140), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://9982a15e7f33636fdba47c9fabe6f4f8ca869002a8f6fb3ff93c42d5a9c5e68e"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 28 08:16:13.820: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:16:13.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vbb2w" for this suite.
Feb 28 08:16:19.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:20.288: INFO: namespace: e2e-tests-pods-vbb2w, resource: bindings, ignored listing per whitelist
Feb 28 08:16:20.295: INFO: namespace e2e-tests-pods-vbb2w deletion completed in 6.460929547s

• [SLOW TEST:14.031 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:16:21.931: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8mjw8
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-221325c5-3b31-11e9-a616-82a6c0035a5d
STEP: Creating configMap with name cm-test-opt-upd-2213261e-3b31-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-221325c5-3b31-11e9-a616-82a6c0035a5d
STEP: Updating configmap cm-test-opt-upd-2213261e-3b31-11e9-a616-82a6c0035a5d
STEP: Creating configMap with name cm-test-opt-create-22132666-3b31-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:17:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8mjw8" for this suite.
Feb 28 08:18:01.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:01.590: INFO: namespace: e2e-tests-projected-8mjw8, resource: bindings, ignored listing per whitelist
Feb 28 08:18:01.594: INFO: namespace e2e-tests-projected-8mjw8 deletion completed in 22.241282862s

• [SLOW TEST:99.663 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:18:01.594: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-srp2g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:18:11.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-srp2g" for this suite.
Feb 28 08:18:33.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:33.563: INFO: namespace: e2e-tests-replication-controller-srp2g, resource: bindings, ignored listing per whitelist
Feb 28 08:18:33.627: INFO: namespace e2e-tests-replication-controller-srp2g deletion completed in 22.212091702s

• [SLOW TEST:32.033 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:18:33.627: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-qdmcd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 28 08:18:33.948: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:18:34.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-qdmcd" for this suite.
Feb 28 08:18:40.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:41.161: INFO: namespace: e2e-tests-replication-controller-qdmcd, resource: bindings, ignored listing per whitelist
Feb 28 08:18:41.413: INFO: namespace e2e-tests-replication-controller-qdmcd deletion completed in 6.437974939s

• [SLOW TEST:7.786 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:18:41.413: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5pjsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5pjsr
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 28 08:18:41.756: INFO: Found 0 stateful pods, waiting for 3
Feb 28 08:18:51.796: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:18:51.796: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:18:51.796: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:18:51.811: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5pjsr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:18:52.725: INFO: stderr: ""
Feb 28 08:18:52.725: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:18:52.725: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 08:19:02.784: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 28 08:19:12.819: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5pjsr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:19:13.374: INFO: stderr: ""
Feb 28 08:19:13.374: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:19:13.374: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:19:33.451: INFO: Waiting for StatefulSet e2e-tests-statefulset-5pjsr/ss2 to complete update
Feb 28 08:19:33.451: INFO: Waiting for Pod e2e-tests-statefulset-5pjsr/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 28 08:19:43.463: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5pjsr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:19:44.167: INFO: stderr: ""
Feb 28 08:19:44.167: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:19:44.167: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:19:54.212: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 28 08:20:04.456: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-5pjsr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:20:05.025: INFO: stderr: ""
Feb 28 08:20:05.025: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:20:05.025: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:20:15.079: INFO: Waiting for StatefulSet e2e-tests-statefulset-5pjsr/ss2 to complete update
Feb 28 08:20:15.079: INFO: Waiting for Pod e2e-tests-statefulset-5pjsr/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:20:15.079: INFO: Waiting for Pod e2e-tests-statefulset-5pjsr/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:20:15.079: INFO: Waiting for Pod e2e-tests-statefulset-5pjsr/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:20:25.092: INFO: Waiting for StatefulSet e2e-tests-statefulset-5pjsr/ss2 to complete update
Feb 28 08:20:25.092: INFO: Waiting for Pod e2e-tests-statefulset-5pjsr/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:20:35.095: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5pjsr
Feb 28 08:20:35.110: INFO: Scaling statefulset ss2 to 0
Feb 28 08:21:15.135: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:21:15.140: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:21:15.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5pjsr" for this suite.
Feb 28 08:21:21.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:21.295: INFO: namespace: e2e-tests-statefulset-5pjsr, resource: bindings, ignored listing per whitelist
Feb 28 08:21:21.750: INFO: namespace e2e-tests-statefulset-5pjsr deletion completed in 6.565406062s

• [SLOW TEST:160.337 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:21:21.750: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-nlbjc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-nlbjc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:21:22.737: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:21:44.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.42:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-nlbjc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:21:44.846: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:21:45.939: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:21:45.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.156:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-nlbjc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:21:45.949: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:21:46.384: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:21:46.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-nlbjc" for this suite.
Feb 28 08:22:08.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:08.482: INFO: namespace: e2e-tests-pod-network-test-nlbjc, resource: bindings, ignored listing per whitelist
Feb 28 08:22:08.794: INFO: namespace e2e-tests-pod-network-test-nlbjc deletion completed in 22.398039094s

• [SLOW TEST:47.044 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:22:08.795: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-lk4c5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 28 08:22:09.169: INFO: Waiting up to 5m0s for pod "var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-var-expansion-lk4c5" to be "success or failure"
Feb 28 08:22:09.174: INFO: Pod "var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160824ms
Feb 28 08:22:11.181: INFO: Pod "var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011835388s
STEP: Saw pod success
Feb 28 08:22:11.181: INFO: Pod "var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:22:11.185: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:22:11.212: INFO: Waiting for pod var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:22:11.217: INFO: Pod var-expansion-f0d79e4b-3b31-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:22:11.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-lk4c5" for this suite.
Feb 28 08:22:17.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:17.489: INFO: namespace: e2e-tests-var-expansion-lk4c5, resource: bindings, ignored listing per whitelist
Feb 28 08:22:17.495: INFO: namespace e2e-tests-var-expansion-lk4c5 deletion completed in 6.274187392s

• [SLOW TEST:8.701 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:22:17.496: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-7pgrq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:22:17.811: INFO: Number of nodes with available pods: 0
Feb 28 08:22:17.811: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:22:18.875: INFO: Number of nodes with available pods: 0
Feb 28 08:22:18.876: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:22:19.823: INFO: Number of nodes with available pods: 1
Feb 28 08:22:19.823: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw is running more than one daemon pod
Feb 28 08:22:20.823: INFO: Number of nodes with available pods: 1
Feb 28 08:22:20.823: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw is running more than one daemon pod
Feb 28 08:22:21.822: INFO: Number of nodes with available pods: 2
Feb 28 08:22:21.822: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 28 08:22:21.848: INFO: Number of nodes with available pods: 1
Feb 28 08:22:21.848: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:22:22.862: INFO: Number of nodes with available pods: 1
Feb 28 08:22:22.862: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:22:23.859: INFO: Number of nodes with available pods: 2
Feb 28 08:22:23.859: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-7pgrq, will wait for the garbage collector to delete the pods
Feb 28 08:22:23.944: INFO: Deleting DaemonSet.extensions daemon-set took: 10.939367ms
Feb 28 08:22:24.044: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.304086ms
Feb 28 08:23:05.652: INFO: Number of nodes with available pods: 0
Feb 28 08:23:05.652: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:23:05.659: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-7pgrq/daemonsets","resourceVersion":"11933"},"items":null}

Feb 28 08:23:05.663: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-7pgrq/pods","resourceVersion":"11933"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:23:05.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-7pgrq" for this suite.
Feb 28 08:23:11.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:12.066: INFO: namespace: e2e-tests-daemonsets-7pgrq, resource: bindings, ignored listing per whitelist
Feb 28 08:23:12.189: INFO: namespace e2e-tests-daemonsets-7pgrq deletion completed in 6.505914838s

• [SLOW TEST:54.693 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:23:12.190: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-pvk99
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:23:12.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-pvk99" for this suite.
Feb 28 08:23:18.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:18.738: INFO: namespace: e2e-tests-kubelet-test-pvk99, resource: bindings, ignored listing per whitelist
Feb 28 08:23:18.805: INFO: namespace e2e-tests-kubelet-test-pvk99 deletion completed in 6.225864892s

• [SLOW TEST:6.614 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:23:18.805: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-5r8qr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5r8qr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:23:19.051: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:23:39.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.162:8080/dial?request=hostName&protocol=http&host=100.96.0.161&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-5r8qr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:23:39.152: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:23:39.627: INFO: Waiting for endpoints: map[]
Feb 28 08:23:39.632: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.162:8080/dial?request=hostName&protocol=http&host=100.96.1.44&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-5r8qr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:23:39.632: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:23:40.073: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:23:40.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5r8qr" for this suite.
Feb 28 08:24:02.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:02.139: INFO: namespace: e2e-tests-pod-network-test-5r8qr, resource: bindings, ignored listing per whitelist
Feb 28 08:24:02.295: INFO: namespace e2e-tests-pod-network-test-5r8qr deletion completed in 22.216408091s

• [SLOW TEST:43.491 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:24:02.296: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-2wlpb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:24:05.128: INFO: Successfully updated pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d"
Feb 28 08:24:05.128: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-pods-2wlpb" to be "terminated due to deadline exceeded"
Feb 28 08:24:05.133: INFO: Pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d": Phase="Running", Reason="", readiness=true. Elapsed: 4.976731ms
Feb 28 08:24:07.138: INFO: Pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010520091s
Feb 28 08:24:09.145: INFO: Pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.016739697s
Feb 28 08:24:09.145: INFO: Pod "pod-update-activedeadlineseconds-347201e2-3b32-11e9-a616-82a6c0035a5d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:24:09.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2wlpb" for this suite.
Feb 28 08:24:15.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:15.570: INFO: namespace: e2e-tests-pods-2wlpb, resource: bindings, ignored listing per whitelist
Feb 28 08:24:15.605: INFO: namespace e2e-tests-pods-2wlpb deletion completed in 6.451577368s

• [SLOW TEST:13.310 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:24:15.605: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2zhwd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-3c5cb251-3b32-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:24:15.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-2zhwd" to be "success or failure"
Feb 28 08:24:15.890: INFO: Pod "pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903509ms
Feb 28 08:24:17.895: INFO: Pod "pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009907561s
STEP: Saw pod success
Feb 28 08:24:17.895: INFO: Pod "pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:24:17.899: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:24:17.929: INFO: Waiting for pod pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:24:17.933: INFO: Pod pod-projected-configmaps-3c5eb52c-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:24:17.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2zhwd" for this suite.
Feb 28 08:24:23.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:24.094: INFO: namespace: e2e-tests-projected-2zhwd, resource: bindings, ignored listing per whitelist
Feb 28 08:24:24.118: INFO: namespace e2e-tests-projected-2zhwd deletion completed in 6.180971969s

• [SLOW TEST:8.513 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:24:24.119: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-p7w48
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:24:24.359: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 28 08:24:29.369: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 08:24:29.370: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 28 08:24:31.376: INFO: Creating deployment "test-rollover-deployment"
Feb 28 08:24:31.386: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 28 08:24:33.396: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 28 08:24:33.409: INFO: Ensure that both replica sets have 1 created replica
Feb 28 08:24:33.420: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 28 08:24:33.436: INFO: Updating deployment test-rollover-deployment
Feb 28 08:24:33.436: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 28 08:24:35.447: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 28 08:24:35.456: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 28 08:24:35.466: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:24:35.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939075, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:24:37.478: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:24:37.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939075, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:24:39.478: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:24:39.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939075, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:24:41.478: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:24:41.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939075, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:24:43.478: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:24:43.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939075, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939071, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:24:45.477: INFO: 
Feb 28 08:24:45.477: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:24:45.502: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-p7w48,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-p7w48/deployments/test-rollover-deployment,UID:459c6dbc-3b32-11e9-b268-0a027e1dd732,ResourceVersion:12290,Generation:2,CreationTimestamp:2019-02-28 08:24:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 08:24:31 +0000 UTC 2019-02-28 08:24:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 08:24:45 +0000 UTC 2019-02-28 08:24:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 08:24:45.507: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-p7w48,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-p7w48/replicasets/test-rollover-deployment-6b7f9d6597,UID:46d685d2-3b32-11e9-b268-0a027e1dd732,ResourceVersion:12283,Generation:2,CreationTimestamp:2019-02-28 08:24:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 459c6dbc-3b32-11e9-b268-0a027e1dd732 0xc001cda2b7 0xc001cda2b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 08:24:45.507: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 28 08:24:45.508: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-p7w48,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-p7w48/replicasets/test-rollover-controller,UID:416c50b5-3b32-11e9-b268-0a027e1dd732,ResourceVersion:12289,Generation:2,CreationTimestamp:2019-02-28 08:24:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 459c6dbc-3b32-11e9-b268-0a027e1dd732 0xc002083fe7 0xc002083fe8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:24:45.508: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-p7w48,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-p7w48/replicasets/test-rollover-deployment-6586df867b,UID:459ee5a3-3b32-11e9-b268-0a027e1dd732,ResourceVersion:12251,Generation:2,CreationTimestamp:2019-02-28 08:24:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 459c6dbc-3b32-11e9-b268-0a027e1dd732 0xc001cda0a7 0xc001cda0a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:24:45.514: INFO: Pod "test-rollover-deployment-6b7f9d6597-p78nw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-p78nw,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-p7w48,SelfLink:/api/v1/namespaces/e2e-tests-deployment-p7w48/pods/test-rollover-deployment-6b7f9d6597-p78nw,UID:46d93e7c-3b32-11e9-b268-0a027e1dd732,ResourceVersion:12260,Generation:0,CreationTimestamp:2019-02-28 08:24:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.167/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 46d685d2-3b32-11e9-b268-0a027e1dd732 0xc001cdbf27 0xc001cdbf28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7jk7c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7jk7c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7jk7c true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cdbf90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cdbfb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:24:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:24:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:24:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:24:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.167,StartTime:2019-02-28 08:24:33 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 08:24:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9f12f23681cf5b200f4e00b3a74b5331856ffd07c850376477b3263764a8186d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:24:45.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-p7w48" for this suite.
Feb 28 08:24:51.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:51.729: INFO: namespace: e2e-tests-deployment-p7w48, resource: bindings, ignored listing per whitelist
Feb 28 08:24:51.729: INFO: namespace e2e-tests-deployment-p7w48 deletion completed in 6.20994048s

• [SLOW TEST:27.610 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:24:51.730: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-f2vrb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 08:24:52.056: INFO: Waiting up to 5m0s for pod "pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-f2vrb" to be "success or failure"
Feb 28 08:24:52.061: INFO: Pod "pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.192282ms
Feb 28 08:24:54.067: INFO: Pod "pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011127536s
STEP: Saw pod success
Feb 28 08:24:54.067: INFO: Pod "pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:24:54.071: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:24:54.094: INFO: Waiting for pod pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:24:54.099: INFO: Pod pod-51ee30f9-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:24:54.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-f2vrb" for this suite.
Feb 28 08:25:00.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:00.243: INFO: namespace: e2e-tests-emptydir-f2vrb, resource: bindings, ignored listing per whitelist
Feb 28 08:25:00.328: INFO: namespace e2e-tests-emptydir-f2vrb deletion completed in 6.223355434s

• [SLOW TEST:8.598 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:25:00.328: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8hjf8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:25:00.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-8hjf8" to be "success or failure"
Feb 28 08:25:00.604: INFO: Pod "downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.878912ms
Feb 28 08:25:02.610: INFO: Pod "downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011843758s
STEP: Saw pod success
Feb 28 08:25:02.610: INFO: Pod "downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:25:02.614: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:25:02.637: INFO: Waiting for pod downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:25:02.644: INFO: Pod downwardapi-volume-57054335-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:25:02.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8hjf8" for this suite.
Feb 28 08:25:08.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:08.802: INFO: namespace: e2e-tests-downward-api-8hjf8, resource: bindings, ignored listing per whitelist
Feb 28 08:25:08.823: INFO: namespace e2e-tests-downward-api-8hjf8 deletion completed in 6.173409608s

• [SLOW TEST:8.494 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:25:08.823: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6m9jt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:25:09.073: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version --client'
Feb 28 08:25:09.128: INFO: stderr: ""
Feb 28 08:25:09.128: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:30:11Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 28 08:25:09.133: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-6m9jt'
Feb 28 08:25:10.447: INFO: stderr: ""
Feb 28 08:25:10.447: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 28 08:25:10.447: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-6m9jt'
Feb 28 08:25:10.639: INFO: stderr: ""
Feb 28 08:25:10.639: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 08:25:11.646: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:25:11.646: INFO: Found 0 / 1
Feb 28 08:25:12.646: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:25:12.646: INFO: Found 1 / 1
Feb 28 08:25:12.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 08:25:12.652: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:25:12.652: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 08:25:12.652: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe pod redis-master-4sg7x --namespace=e2e-tests-kubectl-6m9jt'
Feb 28 08:25:12.758: INFO: stderr: ""
Feb 28 08:25:12.758: INFO: stdout: "Name:               redis-master-4sg7x\nNamespace:          e2e-tests-kubectl-6m9jt\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252/10.250.0.8\nStart Time:         Thu, 28 Feb 2019 08:25:10 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.0.170/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.0.170\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://53524ab58627cf6dee2008137bb707f463e236985b4001e5345b682909bc4497\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 28 Feb 2019 08:25:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-smkbw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-smkbw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-smkbw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  2s    default-scheduler                                                Successfully assigned e2e-tests-kubectl-6m9jt/redis-master-4sg7x to shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252\n  Normal  Pulled     1s    kubelet, shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Created container\n  Normal  Started    1s    kubelet, shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252  Started container\n"
Feb 28 08:25:12.758: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-6m9jt'
Feb 28 08:25:12.884: INFO: stderr: ""
Feb 28 08:25:12.884: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-6m9jt\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-4sg7x\n"
Feb 28 08:25:12.884: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-6m9jt'
Feb 28 08:25:12.996: INFO: stderr: ""
Feb 28 08:25:12.996: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-6m9jt\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.69.114.190\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.0.170:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 28 08:25:13.003: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252'
Feb 28 08:25:13.127: INFO: stderr: ""
Feb 28 08:25:13.127: INFO: stdout: "Name:               shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/hostname=shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.8/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 28 Feb 2019 07:18:43 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 28 Feb 2019 08:25:11 +0000   Thu, 28 Feb 2019 07:18:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 28 Feb 2019 08:25:11 +0000   Thu, 28 Feb 2019 07:18:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 28 Feb 2019 08:25:11 +0000   Thu, 28 Feb 2019 07:18:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 28 Feb 2019 08:25:11 +0000   Thu, 28 Feb 2019 07:19:26 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.8\nCapacity:\n cpu:                2\n ephemeral-storage:  38216108Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4042360Ki\n pods:               110\nAllocatable:\n cpu:                1920m\n ephemeral-storage:  37176629834\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2858665981\n pods:               110\nSystem Info:\n Machine ID:                 fad1f43c030b4db9914bfe745e82b6c3\n System UUID:                FAD1F43C-030B-4DB9-914B-FE745E82B6C3\n Boot ID:                    adea3318-6a87-4e33-9a99-d41a68f9ad3d\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     100.96.0.0/24\nProviderID:                  openstack:///fad1f43c-030b-4db9-914b-fe745e82b6c3\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                   ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-6m9jt    redis-master-4sg7x     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  kube-system                calico-node-6z2pm      100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)    66m\n  kube-system                kube-proxy-q5pbs       20m (1%)      900m (46%)  64Mi (2%)        200Mi (7%)     66m\n  kube-system                node-exporter-ffrzv    5m (0%)       15m (0%)    10Mi (0%)        50Mi (1%)      66m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                125m (6%)   1415m (73%)\n  memory             174Mi (6%)  950Mi (34%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Feb 28 08:25:13.127: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe namespace e2e-tests-kubectl-6m9jt'
Feb 28 08:25:13.227: INFO: stderr: ""
Feb 28 08:25:13.227: INFO: stdout: "Name:         e2e-tests-kubectl-6m9jt\nLabels:       e2e-framework=kubectl\n              e2e-run=f03bec75-3b2a-11e9-a616-82a6c0035a5d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:25:13.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6m9jt" for this suite.
Feb 28 08:25:35.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:35.430: INFO: namespace: e2e-tests-kubectl-6m9jt, resource: bindings, ignored listing per whitelist
Feb 28 08:25:35.489: INFO: namespace e2e-tests-kubectl-6m9jt deletion completed in 22.256723981s

• [SLOW TEST:26.666 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:25:35.490: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5468l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-6bf8d7e4-3b32-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 08:25:35.753: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-5468l" to be "success or failure"
Feb 28 08:25:35.759: INFO: Pod "pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898142ms
Feb 28 08:25:37.764: INFO: Pod "pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010400768s
STEP: Saw pod success
Feb 28 08:25:37.764: INFO: Pod "pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:25:37.768: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:25:37.790: INFO: Waiting for pod pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:25:37.793: INFO: Pod pod-projected-secrets-6bf9a70a-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:25:37.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5468l" for this suite.
Feb 28 08:25:43.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:43.844: INFO: namespace: e2e-tests-projected-5468l, resource: bindings, ignored listing per whitelist
Feb 28 08:25:44.054: INFO: namespace e2e-tests-projected-5468l deletion completed in 6.257061955s

• [SLOW TEST:8.565 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:25:44.054: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-v2qgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 28 08:25:48.486: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:48.487: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:48.958: INFO: Exec stderr: ""
Feb 28 08:25:48.959: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:48.959: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:49.371: INFO: Exec stderr: ""
Feb 28 08:25:49.371: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:49.371: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:49.915: INFO: Exec stderr: ""
Feb 28 08:25:49.915: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:49.915: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:50.357: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 28 08:25:50.357: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:50.357: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:50.783: INFO: Exec stderr: ""
Feb 28 08:25:50.783: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:50.783: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:51.255: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 28 08:25:51.255: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:51.255: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:51.645: INFO: Exec stderr: ""
Feb 28 08:25:51.645: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:51.645: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:51.969: INFO: Exec stderr: ""
Feb 28 08:25:51.970: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:51.970: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:52.517: INFO: Exec stderr: ""
Feb 28 08:25:52.517: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-v2qgt PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:25:52.517: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:25:53.103: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:25:53.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-v2qgt" for this suite.
Feb 28 08:26:35.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:26:35.487: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-v2qgt, resource: bindings, ignored listing per whitelist
Feb 28 08:26:35.588: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-v2qgt deletion completed in 42.478862718s

• [SLOW TEST:51.534 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:26:35.589: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cx6p9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 08:26:35.872: INFO: namespace e2e-tests-kubectl-cx6p9
Feb 28 08:26:35.872: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-cx6p9'
Feb 28 08:26:36.267: INFO: stderr: ""
Feb 28 08:26:36.267: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 08:26:37.272: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:26:37.272: INFO: Found 0 / 1
Feb 28 08:26:38.275: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:26:38.275: INFO: Found 1 / 1
Feb 28 08:26:38.275: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 08:26:38.282: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:26:38.282: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 08:26:38.282: INFO: wait on redis-master startup in e2e-tests-kubectl-cx6p9 
Feb 28 08:26:38.282: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-qwv8v redis-master --namespace=e2e-tests-kubectl-cx6p9'
Feb 28 08:26:38.399: INFO: stderr: ""
Feb 28 08:26:38.399: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 08:26:37.143 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 08:26:37.143 # Server started, Redis version 3.2.12\n1:M 28 Feb 08:26:37.143 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 08:26:37.143 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 28 08:26:38.399: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-cx6p9'
Feb 28 08:26:38.518: INFO: stderr: ""
Feb 28 08:26:38.518: INFO: stdout: "service/rm2 exposed\n"
Feb 28 08:26:38.525: INFO: Service rm2 in namespace e2e-tests-kubectl-cx6p9 found.
STEP: exposing service
Feb 28 08:26:40.538: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-cx6p9'
Feb 28 08:26:40.666: INFO: stderr: ""
Feb 28 08:26:40.666: INFO: stdout: "service/rm3 exposed\n"
Feb 28 08:26:40.672: INFO: Service rm3 in namespace e2e-tests-kubectl-cx6p9 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:26:42.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cx6p9" for this suite.
Feb 28 08:27:04.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:04.855: INFO: namespace: e2e-tests-kubectl-cx6p9, resource: bindings, ignored listing per whitelist
Feb 28 08:27:04.870: INFO: namespace e2e-tests-kubectl-cx6p9 deletion completed in 22.184225061s

• [SLOW TEST:29.282 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:27:04.871: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sx744
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:27:05.166: INFO: Waiting up to 5m0s for pod "downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-sx744" to be "success or failure"
Feb 28 08:27:05.173: INFO: Pod "downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190778ms
Feb 28 08:27:07.179: INFO: Pod "downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012020807s
STEP: Saw pod success
Feb 28 08:27:07.179: INFO: Pod "downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:27:07.182: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:27:07.204: INFO: Waiting for pod downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:27:07.207: INFO: Pod downward-api-a1454b1f-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:27:07.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sx744" for this suite.
Feb 28 08:27:13.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:13.336: INFO: namespace: e2e-tests-downward-api-sx744, resource: bindings, ignored listing per whitelist
Feb 28 08:27:13.389: INFO: namespace e2e-tests-downward-api-sx744 deletion completed in 6.176778168s

• [SLOW TEST:8.519 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:27:13.390: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7dxvc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a65b9ecb-3b32-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 08:27:13.709: INFO: Waiting up to 5m0s for pod "pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-7dxvc" to be "success or failure"
Feb 28 08:27:13.714: INFO: Pod "pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220435ms
Feb 28 08:27:15.719: INFO: Pod "pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009601525s
STEP: Saw pod success
Feb 28 08:27:15.719: INFO: Pod "pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:27:15.725: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:27:15.752: INFO: Waiting for pod pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:27:15.755: INFO: Pod pod-secrets-a65ccd4a-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:27:15.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7dxvc" for this suite.
Feb 28 08:27:21.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:21.885: INFO: namespace: e2e-tests-secrets-7dxvc, resource: bindings, ignored listing per whitelist
Feb 28 08:27:21.979: INFO: namespace e2e-tests-secrets-7dxvc deletion completed in 6.217336099s

• [SLOW TEST:8.589 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:27:21.979: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-lz796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d
Feb 28 08:27:22.278: INFO: Pod name my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d: Found 0 pods out of 1
Feb 28 08:27:27.284: INFO: Pod name my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d: Found 1 pods out of 1
Feb 28 08:27:27.284: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d" are running
Feb 28 08:27:27.289: INFO: Pod "my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d-h9jjk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:27:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:27:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:27:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:27:22 +0000 UTC Reason: Message:}])
Feb 28 08:27:27.289: INFO: Trying to dial the pod
Feb 28 08:27:32.391: INFO: Controller my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d: Got expected result from replica 1 [my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d-h9jjk]: "my-hostname-basic-ab77fbe0-3b32-11e9-a616-82a6c0035a5d-h9jjk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:27:32.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lz796" for this suite.
Feb 28 08:27:38.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:38.567: INFO: namespace: e2e-tests-replication-controller-lz796, resource: bindings, ignored listing per whitelist
Feb 28 08:27:38.629: INFO: namespace e2e-tests-replication-controller-lz796 deletion completed in 6.232978559s

• [SLOW TEST:16.650 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:27:38.630: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d5hz5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:27:38.923: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-d5hz5'
Feb 28 08:27:39.207: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 08:27:39.207: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 28 08:27:39.216: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-h2h5b]
Feb 28 08:27:39.216: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-h2h5b" in namespace "e2e-tests-kubectl-d5hz5" to be "running and ready"
Feb 28 08:27:39.220: INFO: Pod "e2e-test-nginx-rc-h2h5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780623ms
Feb 28 08:27:41.226: INFO: Pod "e2e-test-nginx-rc-h2h5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00998843s
Feb 28 08:27:41.226: INFO: Pod "e2e-test-nginx-rc-h2h5b" satisfied condition "running and ready"
Feb 28 08:27:41.226: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-h2h5b]
Feb 28 08:27:41.226: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-d5hz5'
Feb 28 08:27:41.335: INFO: stderr: ""
Feb 28 08:27:41.335: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 28 08:27:41.335: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-d5hz5'
Feb 28 08:27:41.427: INFO: stderr: ""
Feb 28 08:27:41.427: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:27:41.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d5hz5" for this suite.
Feb 28 08:28:03.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:03.481: INFO: namespace: e2e-tests-kubectl-d5hz5, resource: bindings, ignored listing per whitelist
Feb 28 08:28:03.592: INFO: namespace e2e-tests-kubectl-d5hz5 deletion completed in 22.159499548s

• [SLOW TEST:24.962 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:28:03.592: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8sx6q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:28:03.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-8sx6q" to be "success or failure"
Feb 28 08:28:03.843: INFO: Pod "downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642501ms
Feb 28 08:28:05.850: INFO: Pod "downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011859347s
STEP: Saw pod success
Feb 28 08:28:05.850: INFO: Pod "downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:28:05.854: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:28:05.876: INFO: Waiting for pod downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:28:05.880: INFO: Pod downwardapi-volume-c43d8aea-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:28:05.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8sx6q" for this suite.
Feb 28 08:28:11.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:12.066: INFO: namespace: e2e-tests-downward-api-8sx6q, resource: bindings, ignored listing per whitelist
Feb 28 08:28:12.105: INFO: namespace e2e-tests-downward-api-8sx6q deletion completed in 6.219576806s

• [SLOW TEST:8.513 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:28:12.106: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cw4w4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:12.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cw4w4" for this suite.
Feb 28 08:29:34.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:34.421: INFO: namespace: e2e-tests-container-probe-cw4w4, resource: bindings, ignored listing per whitelist
Feb 28 08:29:34.557: INFO: namespace e2e-tests-container-probe-cw4w4 deletion completed in 22.207579514s

• [SLOW TEST:82.451 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:34.557: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bqnzx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:29:34.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-bqnzx" to be "success or failure"
Feb 28 08:29:34.838: INFO: Pod "downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304313ms
Feb 28 08:29:36.844: INFO: Pod "downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011756199s
STEP: Saw pod success
Feb 28 08:29:36.844: INFO: Pod "downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:29:36.849: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:29:36.870: INFO: Waiting for pod downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:29:36.873: INFO: Pod downwardapi-volume-fa7a6cb4-3b32-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:36.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bqnzx" for this suite.
Feb 28 08:29:42.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:42.957: INFO: namespace: e2e-tests-projected-bqnzx, resource: bindings, ignored listing per whitelist
Feb 28 08:29:43.086: INFO: namespace e2e-tests-projected-bqnzx deletion completed in 6.208768689s

• [SLOW TEST:8.529 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:29:43.086: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lgp65
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:29:43.345: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-lgp65'
Feb 28 08:29:43.524: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 08:29:43.524: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 28 08:29:45.535: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-lgp65'
Feb 28 08:29:45.639: INFO: stderr: ""
Feb 28 08:29:45.639: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:29:45.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lgp65" for this suite.
Feb 28 08:30:07.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:07.706: INFO: namespace: e2e-tests-kubectl-lgp65, resource: bindings, ignored listing per whitelist
Feb 28 08:30:08.114: INFO: namespace e2e-tests-kubectl-lgp65 deletion completed in 22.462210563s

• [SLOW TEST:25.028 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:08.114: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-z6wpc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 28 08:30:13.406: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:30:13.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-z6wpc" for this suite.
Feb 28 08:30:35.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:35.750: INFO: namespace: e2e-tests-replicaset-z6wpc, resource: bindings, ignored listing per whitelist
Feb 28 08:30:35.856: INFO: namespace e2e-tests-replicaset-z6wpc deletion completed in 22.197094073s

• [SLOW TEST:27.741 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:35.857: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-cv526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-8qgdz
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-fmdgz
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:30:42.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-cv526" for this suite.
Feb 28 08:30:48.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:48.885: INFO: namespace: e2e-tests-namespaces-cv526, resource: bindings, ignored listing per whitelist
Feb 28 08:30:49.032: INFO: namespace e2e-tests-namespaces-cv526 deletion completed in 6.207038655s
STEP: Destroying namespace "e2e-tests-nsdeletetest-8qgdz" for this suite.
Feb 28 08:30:49.036: INFO: Namespace e2e-tests-nsdeletetest-8qgdz was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-fmdgz" for this suite.
Feb 28 08:30:55.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:55.109: INFO: namespace: e2e-tests-nsdeletetest-fmdgz, resource: bindings, ignored listing per whitelist
Feb 28 08:30:55.270: INFO: namespace e2e-tests-nsdeletetest-fmdgz deletion completed in 6.233686515s

• [SLOW TEST:19.413 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:30:55.270: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-wksf9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-wksf9
Feb 28 08:30:57.553: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-wksf9
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:30:57.557: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:34:58.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wksf9" for this suite.
Feb 28 08:35:04.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:04.853: INFO: namespace: e2e-tests-container-probe-wksf9, resource: bindings, ignored listing per whitelist
Feb 28 08:35:04.882: INFO: namespace e2e-tests-container-probe-wksf9 deletion completed in 6.292931531s

• [SLOW TEST:249.612 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:35:04.882: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-pnmqz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:35:05.190: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 28 08:35:05.201: INFO: Number of nodes with available pods: 0
Feb 28 08:35:05.201: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 28 08:35:05.430: INFO: Number of nodes with available pods: 0
Feb 28 08:35:05.430: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:06.438: INFO: Number of nodes with available pods: 0
Feb 28 08:35:06.438: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:07.436: INFO: Number of nodes with available pods: 1
Feb 28 08:35:07.437: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 28 08:35:07.460: INFO: Number of nodes with available pods: 0
Feb 28 08:35:07.460: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 28 08:35:07.470: INFO: Number of nodes with available pods: 0
Feb 28 08:35:07.470: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:08.484: INFO: Number of nodes with available pods: 0
Feb 28 08:35:08.484: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:09.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:09.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:10.477: INFO: Number of nodes with available pods: 0
Feb 28 08:35:10.477: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:11.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:11.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:12.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:12.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:13.478: INFO: Number of nodes with available pods: 0
Feb 28 08:35:13.478: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:14.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:14.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:15.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:15.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:16.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:16.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:17.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:17.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:18.477: INFO: Number of nodes with available pods: 0
Feb 28 08:35:18.477: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:19.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:19.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:20.477: INFO: Number of nodes with available pods: 0
Feb 28 08:35:20.477: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:21.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:21.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:22.475: INFO: Number of nodes with available pods: 0
Feb 28 08:35:22.475: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:23.478: INFO: Number of nodes with available pods: 0
Feb 28 08:35:23.478: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:24.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:24.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:25.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:25.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:26.476: INFO: Number of nodes with available pods: 0
Feb 28 08:35:26.476: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:27.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:27.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:28.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:28.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:29.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:29.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:30.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:30.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:31.478: INFO: Number of nodes with available pods: 0
Feb 28 08:35:31.478: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:32.481: INFO: Number of nodes with available pods: 0
Feb 28 08:35:32.481: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:33.479: INFO: Number of nodes with available pods: 0
Feb 28 08:35:33.479: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:34.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:34.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:35.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:35.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:36.481: INFO: Number of nodes with available pods: 0
Feb 28 08:35:36.481: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:37.478: INFO: Number of nodes with available pods: 0
Feb 28 08:35:37.478: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:38.482: INFO: Number of nodes with available pods: 0
Feb 28 08:35:38.482: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:39.481: INFO: Number of nodes with available pods: 0
Feb 28 08:35:39.481: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:40.482: INFO: Number of nodes with available pods: 0
Feb 28 08:35:40.482: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:41.478: INFO: Number of nodes with available pods: 0
Feb 28 08:35:41.478: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:42.477: INFO: Number of nodes with available pods: 0
Feb 28 08:35:42.477: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:43.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:43.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:44.482: INFO: Number of nodes with available pods: 0
Feb 28 08:35:44.482: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:45.480: INFO: Number of nodes with available pods: 0
Feb 28 08:35:45.480: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:46.482: INFO: Number of nodes with available pods: 0
Feb 28 08:35:46.482: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:35:47.483: INFO: Number of nodes with available pods: 1
Feb 28 08:35:47.483: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-pnmqz, will wait for the garbage collector to delete the pods
Feb 28 08:35:47.579: INFO: Deleting DaemonSet.extensions daemon-set took: 15.873103ms
Feb 28 08:35:47.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.312554ms
Feb 28 08:36:25.690: INFO: Number of nodes with available pods: 0
Feb 28 08:36:25.690: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:36:25.696: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-pnmqz/daemonsets","resourceVersion":"14144"},"items":null}

Feb 28 08:36:25.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-pnmqz/pods","resourceVersion":"14144"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:36:25.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-pnmqz" for this suite.
Feb 28 08:36:31.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:31.844: INFO: namespace: e2e-tests-daemonsets-pnmqz, resource: bindings, ignored listing per whitelist
Feb 28 08:36:32.280: INFO: namespace e2e-tests-daemonsets-pnmqz deletion completed in 6.533115578s

• [SLOW TEST:87.398 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:36:32.281: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pjdgv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:36:32.557: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-pjdgv'
Feb 28 08:36:33.302: INFO: stderr: ""
Feb 28 08:36:33.302: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 28 08:36:33.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-pjdgv'
Feb 28 08:36:35.613: INFO: stderr: ""
Feb 28 08:36:35.613: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:36:35.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pjdgv" for this suite.
Feb 28 08:36:41.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:41.664: INFO: namespace: e2e-tests-kubectl-pjdgv, resource: bindings, ignored listing per whitelist
Feb 28 08:36:41.849: INFO: namespace e2e-tests-kubectl-pjdgv deletion completed in 6.224983374s

• [SLOW TEST:9.568 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:36:41.849: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-dqrmz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:36:44.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-dqrmz" for this suite.
Feb 28 08:37:28.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:28.519: INFO: namespace: e2e-tests-kubelet-test-dqrmz, resource: bindings, ignored listing per whitelist
Feb 28 08:37:28.527: INFO: namespace e2e-tests-kubelet-test-dqrmz deletion completed in 44.385506204s

• [SLOW TEST:46.678 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:28.527: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-nwd6d
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-1522147e-3b34-11e9-a616-82a6c0035a5d
STEP: Creating configMap with name cm-test-opt-upd-152214b7-3b34-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1522147e-3b34-11e9-a616-82a6c0035a5d
STEP: Updating configmap cm-test-opt-upd-152214b7-3b34-11e9-a616-82a6c0035a5d
STEP: Creating configMap with name cm-test-opt-create-152214cf-3b34-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:33.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nwd6d" for this suite.
Feb 28 08:37:47.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:47.677: INFO: namespace: e2e-tests-configmap-nwd6d, resource: bindings, ignored listing per whitelist
Feb 28 08:37:47.791: INFO: namespace e2e-tests-configmap-nwd6d deletion completed in 14.260926089s

• [SLOW TEST:19.264 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:47.792: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-s9fxp
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 28 08:37:48.062: INFO: Waiting up to 5m0s for pod "pod-20773b01-3b34-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-s9fxp" to be "success or failure"
Feb 28 08:37:48.067: INFO: Pod "pod-20773b01-3b34-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.531211ms
Feb 28 08:37:50.074: INFO: Pod "pod-20773b01-3b34-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012237206s
STEP: Saw pod success
Feb 28 08:37:50.075: INFO: Pod "pod-20773b01-3b34-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:37:50.081: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-20773b01-3b34-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:37:50.141: INFO: Waiting for pod pod-20773b01-3b34-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:37:50.147: INFO: Pod pod-20773b01-3b34-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:50.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s9fxp" for this suite.
Feb 28 08:37:56.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:56.228: INFO: namespace: e2e-tests-emptydir-s9fxp, resource: bindings, ignored listing per whitelist
Feb 28 08:37:56.410: INFO: namespace e2e-tests-emptydir-s9fxp deletion completed in 6.256615293s

• [SLOW TEST:8.618 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:37:56.410: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-sl46d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 28 08:37:56.679: INFO: Waiting up to 5m0s for pod "client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-containers-sl46d" to be "success or failure"
Feb 28 08:37:56.685: INFO: Pod "client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.259859ms
Feb 28 08:37:58.692: INFO: Pod "client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012523196s
STEP: Saw pod success
Feb 28 08:37:58.692: INFO: Pod "client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:37:58.698: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:37:58.718: INFO: Waiting for pod client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:37:58.722: INFO: Pod client-containers-2599f506-3b34-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:37:58.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-sl46d" for this suite.
Feb 28 08:38:04.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:04.828: INFO: namespace: e2e-tests-containers-sl46d, resource: bindings, ignored listing per whitelist
Feb 28 08:38:04.945: INFO: namespace e2e-tests-containers-sl46d deletion completed in 6.217794827s

• [SLOW TEST:8.535 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:04.946: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-c5jxg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 28 08:38:05.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-c5jxg,SelfLink:/api/v1/namespaces/e2e-tests-watch-c5jxg/configmaps/e2e-watch-test-resource-version,UID:2acf5219-3b34-11e9-b268-0a027e1dd732,ResourceVersion:14457,Generation:0,CreationTimestamp:2019-02-28 08:38:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:38:05.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-c5jxg,SelfLink:/api/v1/namespaces/e2e-tests-watch-c5jxg/configmaps/e2e-watch-test-resource-version,UID:2acf5219-3b34-11e9-b268-0a027e1dd732,ResourceVersion:14458,Generation:0,CreationTimestamp:2019-02-28 08:38:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:05.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-c5jxg" for this suite.
Feb 28 08:38:11.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:11.477: INFO: namespace: e2e-tests-watch-c5jxg, resource: bindings, ignored listing per whitelist
Feb 28 08:38:11.657: INFO: namespace e2e-tests-watch-c5jxg deletion completed in 6.211737127s

• [SLOW TEST:6.712 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:11.657: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vfggx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-2eadb187-3b34-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:38:11.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-vfggx" to be "success or failure"
Feb 28 08:38:11.920: INFO: Pod "pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136262ms
Feb 28 08:38:13.926: INFO: Pod "pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0118757s
STEP: Saw pod success
Feb 28 08:38:13.926: INFO: Pod "pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:38:13.933: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:38:13.970: INFO: Waiting for pod pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:38:13.975: INFO: Pod pod-configmaps-2eae7bda-3b34-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:13.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vfggx" for this suite.
Feb 28 08:38:19.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:20.179: INFO: namespace: e2e-tests-configmap-vfggx, resource: bindings, ignored listing per whitelist
Feb 28 08:38:20.212: INFO: namespace e2e-tests-configmap-vfggx deletion completed in 6.231623787s

• [SLOW TEST:8.555 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:20.212: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-k7qgf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:38:20.482: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"33ca7a9a-3b34-11e9-b268-0a027e1dd732", Controller:(*bool)(0xc001c222ce), BlockOwnerDeletion:(*bool)(0xc001c222cf)}}
Feb 28 08:38:20.488: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"33c8c8ed-3b34-11e9-b268-0a027e1dd732", Controller:(*bool)(0xc001d025ce), BlockOwnerDeletion:(*bool)(0xc001d025cf)}}
Feb 28 08:38:20.494: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"33c99bb1-3b34-11e9-b268-0a027e1dd732", Controller:(*bool)(0xc001c2248e), BlockOwnerDeletion:(*bool)(0xc001c2248f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:25.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-k7qgf" for this suite.
Feb 28 08:38:31.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:31.744: INFO: namespace: e2e-tests-gc-k7qgf, resource: bindings, ignored listing per whitelist
Feb 28 08:38:31.771: INFO: namespace e2e-tests-gc-k7qgf deletion completed in 6.25491896s

• [SLOW TEST:11.559 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:31.771: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-9qlll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:34.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-9qlll" for this suite.
Feb 28 08:38:40.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:40.221: INFO: namespace: e2e-tests-emptydir-wrapper-9qlll, resource: bindings, ignored listing per whitelist
Feb 28 08:38:40.343: INFO: namespace e2e-tests-emptydir-wrapper-9qlll deletion completed in 6.24419184s

• [SLOW TEST:8.572 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:40.344: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-whssn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-whssn
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-whssn to expose endpoints map[]
Feb 28 08:38:40.624: INFO: Get endpoints failed (3.800774ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 28 08:38:41.629: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-whssn exposes endpoints map[] (1.009171499s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-whssn
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-whssn to expose endpoints map[pod1:[80]]
Feb 28 08:38:43.666: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-whssn exposes endpoints map[pod1:[80]] (2.029066674s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-whssn
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-whssn to expose endpoints map[pod1:[80] pod2:[80]]
Feb 28 08:38:45.743: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-whssn exposes endpoints map[pod2:[80] pod1:[80]] (2.070953068s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-whssn
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-whssn to expose endpoints map[pod2:[80]]
Feb 28 08:38:46.769: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-whssn exposes endpoints map[pod2:[80]] (1.01904491s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-whssn
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-whssn to expose endpoints map[]
Feb 28 08:38:46.785: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-whssn exposes endpoints map[] (6.080022ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:38:46.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-whssn" for this suite.
Feb 28 08:38:52.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:52.945: INFO: namespace: e2e-tests-services-whssn, resource: bindings, ignored listing per whitelist
Feb 28 08:38:53.021: INFO: namespace e2e-tests-services-whssn deletion completed in 6.210705362s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:12.678 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:38:53.021: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-kqllk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 08:38:57.586: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:38:57.590: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:38:59.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:38:59.596: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:39:01.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:39:01.596: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:39:03.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:39:03.596: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:39:05.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:39:05.597: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:39:07.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:39:07.606: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:39:07.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-kqllk" for this suite.
Feb 28 08:39:29.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:29.752: INFO: namespace: e2e-tests-container-lifecycle-hook-kqllk, resource: bindings, ignored listing per whitelist
Feb 28 08:39:30.084: INFO: namespace e2e-tests-container-lifecycle-hook-kqllk deletion completed in 22.450470907s

• [SLOW TEST:37.063 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:39:30.084: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mtr8c
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-5d763312-3b34-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5d763312-3b34-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:39:34.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mtr8c" for this suite.
Feb 28 08:39:56.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:56.723: INFO: namespace: e2e-tests-configmap-mtr8c, resource: bindings, ignored listing per whitelist
Feb 28 08:39:56.846: INFO: namespace e2e-tests-configmap-mtr8c deletion completed in 22.308067227s

• [SLOW TEST:26.762 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:39:56.846: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-z89w4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:39:57.364: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version'
Feb 28 08:39:57.580: INFO: stderr: ""
Feb 28 08:39:57.580: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:30:11Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:39:57.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z89w4" for this suite.
Feb 28 08:40:03.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:03.693: INFO: namespace: e2e-tests-kubectl-z89w4, resource: bindings, ignored listing per whitelist
Feb 28 08:40:03.848: INFO: namespace e2e-tests-kubectl-z89w4 deletion completed in 6.26032973s

• [SLOW TEST:7.002 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:40:03.848: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2blrh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 28 08:40:04.105: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:04.319: INFO: stderr: ""
Feb 28 08:40:04.319: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:40:04.319: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:04.415: INFO: stderr: ""
Feb 28 08:40:04.415: INFO: stdout: "update-demo-nautilus-gkqgv update-demo-nautilus-w2gxg "
Feb 28 08:40:04.415: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-gkqgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:04.511: INFO: stderr: ""
Feb 28 08:40:04.511: INFO: stdout: ""
Feb 28 08:40:04.511: INFO: update-demo-nautilus-gkqgv is created but not running
Feb 28 08:40:09.511: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:09.604: INFO: stderr: ""
Feb 28 08:40:09.604: INFO: stdout: "update-demo-nautilus-gkqgv update-demo-nautilus-w2gxg "
Feb 28 08:40:09.604: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-gkqgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:09.721: INFO: stderr: ""
Feb 28 08:40:09.721: INFO: stdout: "true"
Feb 28 08:40:09.721: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-gkqgv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:09.829: INFO: stderr: ""
Feb 28 08:40:09.829: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:40:09.829: INFO: validating pod update-demo-nautilus-gkqgv
Feb 28 08:40:09.918: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:40:09.918: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:40:09.918: INFO: update-demo-nautilus-gkqgv is verified up and running
Feb 28 08:40:09.918: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-w2gxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:10.221: INFO: stderr: ""
Feb 28 08:40:10.221: INFO: stdout: "true"
Feb 28 08:40:10.221: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-w2gxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:10.330: INFO: stderr: ""
Feb 28 08:40:10.330: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:40:10.330: INFO: validating pod update-demo-nautilus-w2gxg
Feb 28 08:40:10.383: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:40:10.383: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:40:10.383: INFO: update-demo-nautilus-w2gxg is verified up and running
STEP: rolling-update to new replication controller
Feb 28 08:40:10.392: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:40:10.392: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:32.892: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 08:40:32.892: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:40:32.892: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:32.995: INFO: stderr: ""
Feb 28 08:40:32.995: INFO: stdout: "update-demo-kitten-2rr9x update-demo-kitten-vqkch "
Feb 28 08:40:32.995: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-2rr9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:33.108: INFO: stderr: ""
Feb 28 08:40:33.108: INFO: stdout: "true"
Feb 28 08:40:33.108: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-2rr9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:33.198: INFO: stderr: ""
Feb 28 08:40:33.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:40:33.198: INFO: validating pod update-demo-kitten-2rr9x
Feb 28 08:40:33.284: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:40:33.284: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:40:33.284: INFO: update-demo-kitten-2rr9x is verified up and running
Feb 28 08:40:33.284: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-vqkch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:33.379: INFO: stderr: ""
Feb 28 08:40:33.379: INFO: stdout: "true"
Feb 28 08:40:33.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-vqkch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2blrh'
Feb 28 08:40:33.479: INFO: stderr: ""
Feb 28 08:40:33.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:40:33.479: INFO: validating pod update-demo-kitten-vqkch
Feb 28 08:40:33.576: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:40:33.576: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:40:33.576: INFO: update-demo-kitten-vqkch is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:40:33.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2blrh" for this suite.
Feb 28 08:40:55.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:55.727: INFO: namespace: e2e-tests-kubectl-2blrh, resource: bindings, ignored listing per whitelist
Feb 28 08:40:56.061: INFO: namespace e2e-tests-kubectl-2blrh deletion completed in 22.47812478s

• [SLOW TEST:52.212 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:40:56.061: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-68fd5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:40:56.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-68fd5" to be "success or failure"
Feb 28 08:40:56.387: INFO: Pod "downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.570319ms
Feb 28 08:40:58.394: INFO: Pod "downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024830249s
STEP: Saw pod success
Feb 28 08:40:58.394: INFO: Pod "downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:40:58.400: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:40:58.427: INFO: Waiting for pod downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:40:58.432: INFO: Pod downwardapi-volume-90b3e3d1-3b34-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:40:58.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-68fd5" for this suite.
Feb 28 08:41:04.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:41:04.677: INFO: namespace: e2e-tests-projected-68fd5, resource: bindings, ignored listing per whitelist
Feb 28 08:41:04.901: INFO: namespace e2e-tests-projected-68fd5 deletion completed in 6.453038421s

• [SLOW TEST:8.840 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:41:04.901: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wpkfb
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 28 08:41:05.415: INFO: Waiting up to 5m0s for pod "pod-9618ff15-3b34-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-wpkfb" to be "success or failure"
Feb 28 08:41:05.423: INFO: Pod "pod-9618ff15-3b34-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385444ms
Feb 28 08:41:07.435: INFO: Pod "pod-9618ff15-3b34-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019284443s
STEP: Saw pod success
Feb 28 08:41:07.435: INFO: Pod "pod-9618ff15-3b34-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:41:07.445: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-9618ff15-3b34-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:41:07.479: INFO: Waiting for pod pod-9618ff15-3b34-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:41:07.490: INFO: Pod pod-9618ff15-3b34-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:41:07.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wpkfb" for this suite.
Feb 28 08:41:13.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:41:13.646: INFO: namespace: e2e-tests-emptydir-wpkfb, resource: bindings, ignored listing per whitelist
Feb 28 08:41:13.815: INFO: namespace e2e-tests-emptydir-wpkfb deletion completed in 6.305036693s

• [SLOW TEST:8.915 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:41:13.816: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-th6rk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:41:14.289: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:41:14.313: INFO: Number of nodes with available pods: 0
Feb 28 08:41:14.313: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:41:15.338: INFO: Number of nodes with available pods: 0
Feb 28 08:41:15.338: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:41:16.333: INFO: Number of nodes with available pods: 2
Feb 28 08:41:16.333: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 28 08:41:16.391: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:16.392: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:17.411: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:17.411: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:18.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:18.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:19.411: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:19.411: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:20.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:20.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:21.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:21.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:22.413: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:22.413: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:23.407: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:23.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:24.412: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:24.412: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:25.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:25.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:26.409: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:26.409: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:27.411: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:27.411: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:28.408: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:28.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:29.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:29.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:30.408: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:30.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:31.413: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:31.413: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:32.408: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:32.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:33.407: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:33.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:34.407: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:34.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:35.405: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:35.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:36.408: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:36.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:37.406: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:37.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:38.405: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:38.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:39.406: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:39.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:40.405: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:40.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:41.405: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:41.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:42.411: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:42.411: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:43.409: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:43.409: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:44.424: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:44.424: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:45.406: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:45.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:46.409: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:46.409: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:47.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:47.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:48.406: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:48.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:49.408: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:49.409: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:49.409: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:50.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:50.410: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:50.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:51.415: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:51.415: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:51.415: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:52.410: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:52.413: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:52.413: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:53.414: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:53.414: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:53.414: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:54.407: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:54.407: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:54.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:55.407: INFO: Wrong image for pod: daemon-set-982lw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:55.408: INFO: Pod daemon-set-982lw is not available
Feb 28 08:41:55.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:56.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:56.408: INFO: Pod daemon-set-wzdth is not available
Feb 28 08:41:57.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:58.410: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:41:59.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:00.413: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:01.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:02.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:03.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:04.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:05.408: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:06.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:07.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:08.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:09.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:10.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:11.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:12.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:13.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:14.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:15.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:16.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:17.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:18.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:19.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:20.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:21.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:22.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:23.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:24.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:25.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:26.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:27.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:28.405: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:28.405: INFO: Pod daemon-set-mfjxz is not available
Feb 28 08:42:29.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:29.406: INFO: Pod daemon-set-mfjxz is not available
Feb 28 08:42:30.407: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:30.407: INFO: Pod daemon-set-mfjxz is not available
Feb 28 08:42:31.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:31.406: INFO: Pod daemon-set-mfjxz is not available
Feb 28 08:42:32.406: INFO: Wrong image for pod: daemon-set-mfjxz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:42:32.406: INFO: Pod daemon-set-mfjxz is not available
Feb 28 08:42:33.404: INFO: Pod daemon-set-mnvnt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 28 08:42:33.419: INFO: Number of nodes with available pods: 1
Feb 28 08:42:33.419: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw is running more than one daemon pod
Feb 28 08:42:34.431: INFO: Number of nodes with available pods: 1
Feb 28 08:42:34.431: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw is running more than one daemon pod
Feb 28 08:42:35.430: INFO: Number of nodes with available pods: 1
Feb 28 08:42:35.430: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw is running more than one daemon pod
Feb 28 08:42:36.431: INFO: Number of nodes with available pods: 2
Feb 28 08:42:36.431: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-th6rk, will wait for the garbage collector to delete the pods
Feb 28 08:42:36.515: INFO: Deleting DaemonSet.extensions daemon-set took: 8.076009ms
Feb 28 08:42:36.615: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.255571ms
Feb 28 08:42:45.620: INFO: Number of nodes with available pods: 0
Feb 28 08:42:45.620: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:42:45.624: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-th6rk/daemonsets","resourceVersion":"15370"},"items":null}

Feb 28 08:42:45.628: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-th6rk/pods","resourceVersion":"15370"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:42:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-th6rk" for this suite.
Feb 28 08:42:51.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:42:52.028: INFO: namespace: e2e-tests-daemonsets-th6rk, resource: bindings, ignored listing per whitelist
Feb 28 08:42:52.044: INFO: namespace e2e-tests-daemonsets-th6rk deletion completed in 6.398654991s

• [SLOW TEST:98.229 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:42:52.045: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4s8mg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:42:58.781238   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:42:58.781: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:42:58.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4s8mg" for this suite.
Feb 28 08:43:04.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:05.029: INFO: namespace: e2e-tests-gc-4s8mg, resource: bindings, ignored listing per whitelist
Feb 28 08:43:05.044: INFO: namespace e2e-tests-gc-4s8mg deletion completed in 6.254901798s

• [SLOW TEST:13.000 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:43:05.044: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-5sst7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-5sst7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-5sst7 to expose endpoints map[]
Feb 28 08:43:05.357: INFO: Get endpoints failed (5.735688ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 28 08:43:06.370: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-5sst7 exposes endpoints map[] (1.01818116s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-5sst7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-5sst7 to expose endpoints map[pod1:[100]]
Feb 28 08:43:08.414: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-5sst7 exposes endpoints map[pod1:[100]] (2.032517815s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-5sst7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-5sst7 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 28 08:43:10.461: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-5sst7 exposes endpoints map[pod2:[101] pod1:[100]] (2.037961217s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-5sst7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-5sst7 to expose endpoints map[pod2:[101]]
Feb 28 08:43:10.480: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-5sst7 exposes endpoints map[pod2:[101]] (11.737149ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-5sst7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-5sst7 to expose endpoints map[]
Feb 28 08:43:10.492: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-5sst7 exposes endpoints map[] (5.76503ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:43:10.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-5sst7" for this suite.
Feb 28 08:43:32.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:32.713: INFO: namespace: e2e-tests-services-5sst7, resource: bindings, ignored listing per whitelist
Feb 28 08:43:32.750: INFO: namespace e2e-tests-services-5sst7 deletion completed in 22.23134473s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:27.705 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:43:32.750: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wndzs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:43:35.782: INFO: Successfully updated pod "annotationupdateee3431ff-3b34-11e9-a616-82a6c0035a5d"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:43:37.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wndzs" for this suite.
Feb 28 08:43:59.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:59.960: INFO: namespace: e2e-tests-projected-wndzs, resource: bindings, ignored listing per whitelist
Feb 28 08:44:00.061: INFO: namespace e2e-tests-projected-wndzs deletion completed in 22.242647979s

• [SLOW TEST:27.311 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:00.061: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-lpcj8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:44:22.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-lpcj8" for this suite.
Feb 28 08:44:28.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:44:28.634: INFO: namespace: e2e-tests-container-runtime-lpcj8, resource: bindings, ignored listing per whitelist
Feb 28 08:44:28.808: INFO: namespace e2e-tests-container-runtime-lpcj8 deletion completed in 6.222188141s

• [SLOW TEST:28.746 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:28.808: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6kcpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0f79e2c6-3b35-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 08:44:29.058: INFO: Waiting up to 5m0s for pod "pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-6kcpt" to be "success or failure"
Feb 28 08:44:29.064: INFO: Pod "pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229066ms
Feb 28 08:44:31.070: INFO: Pod "pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011682716s
STEP: Saw pod success
Feb 28 08:44:31.070: INFO: Pod "pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:44:31.075: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:44:31.100: INFO: Waiting for pod pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:44:31.104: INFO: Pod pod-secrets-0f7a9f60-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:44:31.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6kcpt" for this suite.
Feb 28 08:44:37.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:44:37.262: INFO: namespace: e2e-tests-secrets-6kcpt, resource: bindings, ignored listing per whitelist
Feb 28 08:44:37.346: INFO: namespace e2e-tests-secrets-6kcpt deletion completed in 6.235505193s

• [SLOW TEST:8.538 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:44:37.346: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bpl4j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-bpl4j
Feb 28 08:44:39.606: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-bpl4j
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:44:39.611: INFO: Initial restart count of pod liveness-exec is 0
Feb 28 08:45:29.763: INFO: Restart count of pod e2e-tests-container-probe-bpl4j/liveness-exec is now 1 (50.151982332s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:45:29.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bpl4j" for this suite.
Feb 28 08:45:35.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:45:35.951: INFO: namespace: e2e-tests-container-probe-bpl4j, resource: bindings, ignored listing per whitelist
Feb 28 08:45:36.001: INFO: namespace e2e-tests-container-probe-bpl4j deletion completed in 6.219789706s

• [SLOW TEST:58.655 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:45:36.001: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bcd47
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 28 08:45:36.483: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix228792640/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:45:36.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bcd47" for this suite.
Feb 28 08:45:42.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:45:42.698: INFO: namespace: e2e-tests-kubectl-bcd47, resource: bindings, ignored listing per whitelist
Feb 28 08:45:42.741: INFO: namespace e2e-tests-kubectl-bcd47 deletion completed in 6.188133826s

• [SLOW TEST:6.739 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:45:42.741: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l8zgb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 28 08:45:42.994: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:45:43.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l8zgb" for this suite.
Feb 28 08:45:49.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:45:49.311: INFO: namespace: e2e-tests-kubectl-l8zgb, resource: bindings, ignored listing per whitelist
Feb 28 08:45:49.391: INFO: namespace e2e-tests-kubectl-l8zgb deletion completed in 6.221121548s

• [SLOW TEST:6.650 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:45:49.391: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jfzsx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:45:49.693: INFO: Waiting up to 5m0s for pod "pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-jfzsx" to be "success or failure"
Feb 28 08:45:49.699: INFO: Pod "pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710701ms
Feb 28 08:45:51.704: INFO: Pod "pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010938919s
STEP: Saw pod success
Feb 28 08:45:51.704: INFO: Pod "pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:45:51.707: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:45:51.730: INFO: Waiting for pod pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:45:51.733: INFO: Pod pod-3f8abe28-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:45:51.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jfzsx" for this suite.
Feb 28 08:45:57.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:45:57.820: INFO: namespace: e2e-tests-emptydir-jfzsx, resource: bindings, ignored listing per whitelist
Feb 28 08:45:57.962: INFO: namespace e2e-tests-emptydir-jfzsx deletion completed in 6.224375787s

• [SLOW TEST:8.571 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:45:57.962: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-s5dzg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:45:58.223: INFO: Waiting up to 5m0s for pod "pod-44a02342-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-s5dzg" to be "success or failure"
Feb 28 08:45:58.228: INFO: Pod "pod-44a02342-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.504017ms
Feb 28 08:46:00.233: INFO: Pod "pod-44a02342-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010456334s
STEP: Saw pod success
Feb 28 08:46:00.233: INFO: Pod "pod-44a02342-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:46:00.238: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-44a02342-3b35-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:46:00.259: INFO: Waiting for pod pod-44a02342-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:46:00.264: INFO: Pod pod-44a02342-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:00.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s5dzg" for this suite.
Feb 28 08:46:06.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:06.514: INFO: namespace: e2e-tests-emptydir-s5dzg, resource: bindings, ignored listing per whitelist
Feb 28 08:46:06.521: INFO: namespace e2e-tests-emptydir-s5dzg deletion completed in 6.251393189s

• [SLOW TEST:8.559 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:46:06.521: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-m62m6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:46:06.774: INFO: Waiting up to 5m0s for pod "downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-m62m6" to be "success or failure"
Feb 28 08:46:06.778: INFO: Pod "downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139962ms
Feb 28 08:46:08.787: INFO: Pod "downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01307877s
STEP: Saw pod success
Feb 28 08:46:08.787: INFO: Pod "downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:46:08.797: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:46:08.823: INFO: Waiting for pod downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:46:08.833: INFO: Pod downward-api-49b8bdf8-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:08.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-m62m6" for this suite.
Feb 28 08:46:14.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:14.976: INFO: namespace: e2e-tests-downward-api-m62m6, resource: bindings, ignored listing per whitelist
Feb 28 08:46:15.085: INFO: namespace e2e-tests-downward-api-m62m6 deletion completed in 6.241769972s

• [SLOW TEST:8.564 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:46:15.085: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xs8j5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:46:15.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-xs8j5" to be "success or failure"
Feb 28 08:46:15.351: INFO: Pod "downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.389051ms
Feb 28 08:46:17.356: INFO: Pod "downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015031034s
STEP: Saw pod success
Feb 28 08:46:17.357: INFO: Pod "downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:46:17.361: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:46:17.384: INFO: Waiting for pod downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:46:17.388: INFO: Pod downwardapi-volume-4ed3af76-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:17.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xs8j5" for this suite.
Feb 28 08:46:23.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:23.571: INFO: namespace: e2e-tests-downward-api-xs8j5, resource: bindings, ignored listing per whitelist
Feb 28 08:46:23.609: INFO: namespace e2e-tests-downward-api-xs8j5 deletion completed in 6.215967714s

• [SLOW TEST:8.523 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:46:23.609: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-7fwv4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kcwjw in namespace e2e-tests-proxy-7fwv4
I0228 08:46:23.949289   31734 runners.go:184] Created replication controller with name: proxy-service-kcwjw, namespace: e2e-tests-proxy-7fwv4, replica count: 1
I0228 08:46:24.999797   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:46:26.001582   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:46:27.001901   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:46:28.002180   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:46:29.002452   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:46:30.003615   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:46:31.003926   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:46:32.004184   31734 runners.go:184] proxy-service-kcwjw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 08:46:32.009: INFO: setup took 8.081459297s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 28 08:46:32.032: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 14.020529ms)
Feb 28 08:46:32.033: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 14.831334ms)
Feb 28 08:46:32.033: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 14.79681ms)
Feb 28 08:46:32.035: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 16.563433ms)
Feb 28 08:46:32.035: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 16.468612ms)
Feb 28 08:46:32.035: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 16.464075ms)
Feb 28 08:46:32.036: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 17.540266ms)
Feb 28 08:46:32.037: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 18.230594ms)
Feb 28 08:46:32.038: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 19.96783ms)
Feb 28 08:46:32.043: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 24.836907ms)
Feb 28 08:46:32.043: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 25.052898ms)
Feb 28 08:46:32.049: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 30.80538ms)
Feb 28 08:46:32.050: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 32.08456ms)
Feb 28 08:46:32.050: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 32.068737ms)
Feb 28 08:46:32.053: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 34.462886ms)
Feb 28 08:46:32.055: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 36.340298ms)
Feb 28 08:46:32.065: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.79287ms)
Feb 28 08:46:32.067: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 11.993369ms)
Feb 28 08:46:32.067: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 11.936078ms)
Feb 28 08:46:32.067: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 12.421154ms)
Feb 28 08:46:32.068: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 12.801418ms)
Feb 28 08:46:32.068: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 12.707597ms)
Feb 28 08:46:32.068: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 12.87087ms)
Feb 28 08:46:32.069: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 13.906135ms)
Feb 28 08:46:32.069: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 14.279967ms)
Feb 28 08:46:32.069: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 14.400196ms)
Feb 28 08:46:32.069: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 14.468894ms)
Feb 28 08:46:32.070: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 15.012317ms)
Feb 28 08:46:32.070: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 15.439516ms)
Feb 28 08:46:32.070: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 15.369493ms)
Feb 28 08:46:32.070: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 15.445763ms)
Feb 28 08:46:32.071: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 15.631329ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 12.888795ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 13.000964ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 12.867624ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 12.943798ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 13.032902ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 13.023945ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 12.944117ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 12.978458ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 13.004637ms)
Feb 28 08:46:32.084: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 13.710014ms)
Feb 28 08:46:32.085: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 14.422917ms)
Feb 28 08:46:32.085: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 14.559724ms)
Feb 28 08:46:32.086: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 15.164225ms)
Feb 28 08:46:32.086: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 15.287637ms)
Feb 28 08:46:32.086: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 15.331941ms)
Feb 28 08:46:32.088: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 17.560108ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 9.399377ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.627858ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 9.321391ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.978084ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.053019ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 9.546432ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.328779ms)
Feb 28 08:46:32.098: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 9.530309ms)
Feb 28 08:46:32.100: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 10.820472ms)
Feb 28 08:46:32.100: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 10.935881ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 13.052178ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 12.796912ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 13.289565ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 12.76839ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 13.155564ms)
Feb 28 08:46:32.102: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 13.037551ms)
Feb 28 08:46:32.110: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 7.704335ms)
Feb 28 08:46:32.110: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 7.80913ms)
Feb 28 08:46:32.110: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 7.740167ms)
Feb 28 08:46:32.110: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 7.907932ms)
Feb 28 08:46:32.110: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 7.94858ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 8.852282ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.864873ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 9.002419ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 8.930608ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 8.941604ms)
Feb 28 08:46:32.111: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 9.20573ms)
Feb 28 08:46:32.112: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 9.635871ms)
Feb 28 08:46:32.112: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 10.418151ms)
Feb 28 08:46:32.112: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 10.36675ms)
Feb 28 08:46:32.112: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 10.42496ms)
Feb 28 08:46:32.113: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 10.896457ms)
Feb 28 08:46:32.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 9.1911ms)
Feb 28 08:46:32.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 9.414929ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.567635ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.59279ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.612839ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.570944ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 9.898236ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.852104ms)
Feb 28 08:46:32.123: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.965506ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 10.934586ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 10.801915ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 11.029005ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 11.043812ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 11.212749ms)
Feb 28 08:46:32.124: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 11.42748ms)
Feb 28 08:46:32.125: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 12.147207ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 10.976249ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 10.926708ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 11.002069ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 10.970661ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 10.971275ms)
Feb 28 08:46:32.136: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 10.960043ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 12.044912ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 12.086032ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 12.253417ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 12.103393ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 12.067106ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 12.110728ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 12.217789ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 12.173059ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 12.505031ms)
Feb 28 08:46:32.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 12.570138ms)
Feb 28 08:46:32.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 24.564491ms)
Feb 28 08:46:32.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 24.482764ms)
Feb 28 08:46:32.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 24.776821ms)
Feb 28 08:46:32.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 24.959278ms)
Feb 28 08:46:32.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 25.037533ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 26.208438ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 25.391988ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 25.524026ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 25.683003ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 25.554997ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 25.420583ms)
Feb 28 08:46:32.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 25.874543ms)
Feb 28 08:46:32.165: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 26.010312ms)
Feb 28 08:46:32.165: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 26.585128ms)
Feb 28 08:46:32.165: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 26.705912ms)
Feb 28 08:46:32.166: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 26.940987ms)
Feb 28 08:46:32.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 7.871686ms)
Feb 28 08:46:32.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.300778ms)
Feb 28 08:46:32.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 7.70121ms)
Feb 28 08:46:32.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 7.857061ms)
Feb 28 08:46:32.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.258711ms)
Feb 28 08:46:32.175: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.553223ms)
Feb 28 08:46:32.175: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 9.154202ms)
Feb 28 08:46:32.175: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 8.469813ms)
Feb 28 08:46:32.175: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 8.96369ms)
Feb 28 08:46:32.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 8.667499ms)
Feb 28 08:46:32.177: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 11.214295ms)
Feb 28 08:46:32.177: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 11.177936ms)
Feb 28 08:46:32.177: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 10.344489ms)
Feb 28 08:46:32.177: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 10.514472ms)
Feb 28 08:46:32.178: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 11.450089ms)
Feb 28 08:46:32.178: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 11.855731ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.143641ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 9.304423ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.380237ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.161105ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 9.448835ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 9.208248ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 9.41643ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.378575ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.242346ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.316399ms)
Feb 28 08:46:32.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 10.034041ms)
Feb 28 08:46:32.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 9.999972ms)
Feb 28 08:46:32.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 11.419771ms)
Feb 28 08:46:32.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 11.227793ms)
Feb 28 08:46:32.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 11.051613ms)
Feb 28 08:46:32.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 11.098363ms)
Feb 28 08:46:32.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 10.316847ms)
Feb 28 08:46:32.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 10.262122ms)
Feb 28 08:46:32.201: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 11.390581ms)
Feb 28 08:46:32.201: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 11.319061ms)
Feb 28 08:46:32.201: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 11.341888ms)
Feb 28 08:46:32.201: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 11.394099ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 11.65747ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 11.524778ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 11.58548ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 11.679429ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 11.597376ms)
Feb 28 08:46:32.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 11.682733ms)
Feb 28 08:46:32.203: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 12.89823ms)
Feb 28 08:46:32.203: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 13.024036ms)
Feb 28 08:46:32.203: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 13.012053ms)
Feb 28 08:46:32.203: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 12.916779ms)
Feb 28 08:46:32.211: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 7.673896ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 8.226235ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 8.269309ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 8.376379ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.446605ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.871766ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 8.715217ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 8.710289ms)
Feb 28 08:46:32.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 8.756571ms)
Feb 28 08:46:32.213: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 9.931433ms)
Feb 28 08:46:32.213: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 9.993248ms)
Feb 28 08:46:32.213: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 10.096281ms)
Feb 28 08:46:32.214: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 10.679291ms)
Feb 28 08:46:32.214: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 10.749716ms)
Feb 28 08:46:32.215: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 11.89585ms)
Feb 28 08:46:32.215: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 11.855059ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 8.954533ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.236462ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.348179ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 9.316009ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.242448ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.569734ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 9.63597ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 9.342864ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 9.078068ms)
Feb 28 08:46:32.225: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 9.536946ms)
Feb 28 08:46:32.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 9.712991ms)
Feb 28 08:46:32.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.608532ms)
Feb 28 08:46:32.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 9.762808ms)
Feb 28 08:46:32.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 10.09351ms)
Feb 28 08:46:32.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 10.774641ms)
Feb 28 08:46:32.227: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 11.118361ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 34.277336ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 34.400606ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 34.474424ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 34.304604ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 34.235848ms)
Feb 28 08:46:32.262: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 34.298716ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 36.217654ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 35.775721ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 35.856383ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 36.408365ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 35.73516ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 36.103556ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 36.74885ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 36.253729ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 36.211772ms)
Feb 28 08:46:32.264: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 36.8702ms)
Feb 28 08:46:32.276: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 11.976824ms)
Feb 28 08:46:32.276: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 12.028197ms)
Feb 28 08:46:32.276: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 11.883681ms)
Feb 28 08:46:32.276: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 12.184692ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 15.081805ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 15.174445ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 14.993942ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 15.157133ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 15.078389ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 15.344334ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 15.257014ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 15.066366ms)
Feb 28 08:46:32.279: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 15.198246ms)
Feb 28 08:46:32.280: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 15.242238ms)
Feb 28 08:46:32.318: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 54.108042ms)
Feb 28 08:46:32.318: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 53.882601ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.305904ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.459144ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 9.647144ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 8.959661ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 9.953574ms)
Feb 28 08:46:32.328: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 9.009237ms)
Feb 28 08:46:32.329: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 10.476392ms)
Feb 28 08:46:32.329: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 10.176848ms)
Feb 28 08:46:32.329: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 10.776723ms)
Feb 28 08:46:32.329: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.991251ms)
Feb 28 08:46:32.332: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 12.247211ms)
Feb 28 08:46:32.332: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 12.903283ms)
Feb 28 08:46:32.332: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 14.024345ms)
Feb 28 08:46:32.376: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 56.922271ms)
Feb 28 08:46:32.376: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 57.055607ms)
Feb 28 08:46:32.376: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 56.823787ms)
Feb 28 08:46:32.386: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.507904ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 10.736628ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 10.895933ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 10.850808ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 10.75097ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 10.847132ms)
Feb 28 08:46:32.387: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 10.885227ms)
Feb 28 08:46:32.389: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 13.070905ms)
Feb 28 08:46:32.389: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 13.052208ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 13.044402ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 13.074858ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 13.358541ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 13.14071ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 13.149122ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 13.345688ms)
Feb 28 08:46:32.390: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 13.448796ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 16.075615ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 16.010354ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 16.16775ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 16.196338ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 16.318924ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 16.308657ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 16.266391ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 16.354254ms)
Feb 28 08:46:32.406: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 16.459944ms)
Feb 28 08:46:32.407: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 16.922289ms)
Feb 28 08:46:32.408: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 17.680799ms)
Feb 28 08:46:32.408: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 17.622109ms)
Feb 28 08:46:32.409: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 18.864481ms)
Feb 28 08:46:32.409: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 19.000041ms)
Feb 28 08:46:32.409: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 19.350773ms)
Feb 28 08:46:32.409: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 19.37952ms)
Feb 28 08:46:32.424: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 14.216779ms)
Feb 28 08:46:32.424: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 14.471956ms)
Feb 28 08:46:32.424: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 14.62067ms)
Feb 28 08:46:32.424: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 14.956695ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 16.273996ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 16.189069ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 16.397641ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 16.328264ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 16.282913ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 16.256567ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 16.448816ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 16.199564ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 16.400759ms)
Feb 28 08:46:32.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 16.582393ms)
Feb 28 08:46:32.427: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 17.022424ms)
Feb 28 08:46:32.427: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 17.056707ms)
Feb 28 08:46:32.436: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:462/proxy/: tls qux (200; 9.46649ms)
Feb 28 08:46:32.436: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:443/proxy/... (200; 9.309087ms)
Feb 28 08:46:32.436: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:1080/proxy/... (200; 9.407484ms)
Feb 28 08:46:32.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 9.985514ms)
Feb 28 08:46:32.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp/proxy/rewriteme"... (200; 10.062334ms)
Feb 28 08:46:32.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:160/proxy/: foo (200; 10.038048ms)
Feb 28 08:46:32.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/https:proxy-service-kcwjw-gzqhp:460/proxy/: tls baz (200; 10.115629ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:1080/proxy/rewri... (200; 12.368909ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname1/proxy/: tls baz (200; 12.451739ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname1/proxy/: foo (200; 12.424826ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname1/proxy/: foo (200; 12.453142ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/https:proxy-service-kcwjw:tlsportname2/proxy/: tls qux (200; 12.448891ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/proxy-service-kcwjw:portname2/proxy/: bar (200; 12.477562ms)
Feb 28 08:46:32.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/http:proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 12.508455ms)
Feb 28 08:46:32.481: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/services/http:proxy-service-kcwjw:portname2/proxy/: bar (200; 54.598798ms)
Feb 28 08:46:32.481: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7fwv4/pods/proxy-service-kcwjw-gzqhp:162/proxy/: bar (200; 54.512694ms)
STEP: deleting ReplicationController proxy-service-kcwjw in namespace e2e-tests-proxy-7fwv4, will wait for the garbage collector to delete the pods
Feb 28 08:46:32.544: INFO: Deleting ReplicationController proxy-service-kcwjw took: 7.807111ms
Feb 28 08:46:32.644: INFO: Terminating ReplicationController proxy-service-kcwjw pods took: 100.321707ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-7fwv4" for this suite.
Feb 28 08:46:51.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:51.689: INFO: namespace: e2e-tests-proxy-7fwv4, resource: bindings, ignored listing per whitelist
Feb 28 08:46:51.842: INFO: namespace e2e-tests-proxy-7fwv4 deletion completed in 6.191747976s

• [SLOW TEST:28.234 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:46:51.843: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-m4r58
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 28 08:46:52.354: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml api-versions'
Feb 28 08:46:52.541: INFO: stderr: ""
Feb 28 08:46:52.541: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:46:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m4r58" for this suite.
Feb 28 08:46:58.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:58.662: INFO: namespace: e2e-tests-kubectl-m4r58, resource: bindings, ignored listing per whitelist
Feb 28 08:46:58.968: INFO: namespace e2e-tests-kubectl-m4r58 deletion completed in 6.419451368s

• [SLOW TEST:7.125 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:46:58.968: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kstfx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-690113c6-3b35-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 08:46:59.267: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-kstfx" to be "success or failure"
Feb 28 08:46:59.271: INFO: Pod "pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.523044ms
Feb 28 08:47:01.277: INFO: Pod "pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010029006s
STEP: Saw pod success
Feb 28 08:47:01.277: INFO: Pod "pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:01.282: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:47:01.304: INFO: Waiting for pod pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:01.308: INFO: Pod pod-projected-secrets-6901c3c9-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kstfx" for this suite.
Feb 28 08:47:07.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:07.427: INFO: namespace: e2e-tests-projected-kstfx, resource: bindings, ignored listing per whitelist
Feb 28 08:47:07.540: INFO: namespace e2e-tests-projected-kstfx deletion completed in 6.224563885s

• [SLOW TEST:8.572 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:07.540: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vpfjw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 28 08:47:07.800: INFO: Waiting up to 5m0s for pod "client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-containers-vpfjw" to be "success or failure"
Feb 28 08:47:07.804: INFO: Pod "client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980001ms
Feb 28 08:47:09.809: INFO: Pod "client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009278785s
STEP: Saw pod success
Feb 28 08:47:09.810: INFO: Pod "client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:09.816: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:47:09.842: INFO: Waiting for pod client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:09.846: INFO: Pod client-containers-6e188d8f-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:09.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vpfjw" for this suite.
Feb 28 08:47:15.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:15.903: INFO: namespace: e2e-tests-containers-vpfjw, resource: bindings, ignored listing per whitelist
Feb 28 08:47:16.099: INFO: namespace e2e-tests-containers-vpfjw deletion completed in 6.246039039s

• [SLOW TEST:8.559 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:16.100: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-f897k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-733d8cbb-3b35-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:47:16.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-configmap-f897k" to be "success or failure"
Feb 28 08:47:16.439: INFO: Pod "pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377434ms
Feb 28 08:47:18.445: INFO: Pod "pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010442501s
STEP: Saw pod success
Feb 28 08:47:18.445: INFO: Pod "pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:18.449: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:47:18.475: INFO: Waiting for pod pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:18.481: INFO: Pod pod-configmaps-733e2fc5-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:18.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-f897k" for this suite.
Feb 28 08:47:24.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:24.608: INFO: namespace: e2e-tests-configmap-f897k, resource: bindings, ignored listing per whitelist
Feb 28 08:47:24.742: INFO: namespace e2e-tests-configmap-f897k deletion completed in 6.255187284s

• [SLOW TEST:8.642 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:24.742: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-vrzcb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-78596103-3b35-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 08:47:25.008: INFO: Waiting up to 5m0s for pod "pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-vrzcb" to be "success or failure"
Feb 28 08:47:25.012: INFO: Pod "pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.494589ms
Feb 28 08:47:27.019: INFO: Pod "pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0114775s
STEP: Saw pod success
Feb 28 08:47:27.019: INFO: Pod "pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:27.031: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:47:27.053: INFO: Waiting for pod pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:27.058: INFO: Pod pod-secrets-785a21e3-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:27.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vrzcb" for this suite.
Feb 28 08:47:33.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:33.104: INFO: namespace: e2e-tests-secrets-vrzcb, resource: bindings, ignored listing per whitelist
Feb 28 08:47:33.243: INFO: namespace e2e-tests-secrets-vrzcb deletion completed in 6.180759008s

• [SLOW TEST:8.501 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:33.243: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9c8sb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7d679f69-3b35-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 08:47:33.487: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-9c8sb" to be "success or failure"
Feb 28 08:47:33.492: INFO: Pod "pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.765651ms
Feb 28 08:47:35.499: INFO: Pod "pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011342564s
STEP: Saw pod success
Feb 28 08:47:35.499: INFO: Pod "pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:35.503: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:47:35.531: INFO: Waiting for pod pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:35.534: INFO: Pod pod-projected-configmaps-7d685a13-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:35.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9c8sb" for this suite.
Feb 28 08:47:41.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:41.641: INFO: namespace: e2e-tests-projected-9c8sb, resource: bindings, ignored listing per whitelist
Feb 28 08:47:41.749: INFO: namespace e2e-tests-projected-9c8sb deletion completed in 6.209214408s

• [SLOW TEST:8.506 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:41.749: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-vlfjl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:46.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-vlfjl" for this suite.
Feb 28 08:47:52.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:52.207: INFO: namespace: e2e-tests-kubelet-test-vlfjl, resource: bindings, ignored listing per whitelist
Feb 28 08:47:52.293: INFO: namespace e2e-tests-kubelet-test-vlfjl deletion completed in 6.272029146s

• [SLOW TEST:10.544 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:47:52.294: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vgtc7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:47:52.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-vgtc7" to be "success or failure"
Feb 28 08:47:52.553: INFO: Pod "downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.865049ms
Feb 28 08:47:54.562: INFO: Pod "downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012252272s
STEP: Saw pod success
Feb 28 08:47:54.562: INFO: Pod "downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:47:54.567: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 08:47:54.594: INFO: Waiting for pod downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:47:54.599: INFO: Pod downwardapi-volume-88c4a343-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:47:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vgtc7" for this suite.
Feb 28 08:48:00.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:00.841: INFO: namespace: e2e-tests-downward-api-vgtc7, resource: bindings, ignored listing per whitelist
Feb 28 08:48:00.859: INFO: namespace e2e-tests-downward-api-vgtc7 deletion completed in 6.254536878s

• [SLOW TEST:8.565 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:00.859: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xdl2t
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8ddfaa9c-3b35-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8ddfaa9c-3b35-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:05.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xdl2t" for this suite.
Feb 28 08:48:27.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:27.450: INFO: namespace: e2e-tests-projected-xdl2t, resource: bindings, ignored listing per whitelist
Feb 28 08:48:27.481: INFO: namespace e2e-tests-projected-xdl2t deletion completed in 22.231969994s

• [SLOW TEST:26.622 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:48:27.482: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-qcvd9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:48:29.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-qcvd9" for this suite.
Feb 28 08:49:19.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:19.865: INFO: namespace: e2e-tests-kubelet-test-qcvd9, resource: bindings, ignored listing per whitelist
Feb 28 08:49:20.189: INFO: namespace e2e-tests-kubelet-test-qcvd9 deletion completed in 50.381375287s

• [SLOW TEST:52.708 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:20.190: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5mqpz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 08:49:20.479: INFO: Waiting up to 5m0s for pod "pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-5mqpz" to be "success or failure"
Feb 28 08:49:20.485: INFO: Pod "pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.550936ms
Feb 28 08:49:22.491: INFO: Pod "pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012884413s
STEP: Saw pod success
Feb 28 08:49:22.492: INFO: Pod "pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:49:22.496: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 08:49:22.520: INFO: Waiting for pod pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:49:22.524: INFO: Pod pod-bd2db2fb-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:49:22.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5mqpz" for this suite.
Feb 28 08:49:28.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:28.640: INFO: namespace: e2e-tests-emptydir-5mqpz, resource: bindings, ignored listing per whitelist
Feb 28 08:49:28.750: INFO: namespace e2e-tests-emptydir-5mqpz deletion completed in 6.22091032s

• [SLOW TEST:8.560 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:28.750: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-854f4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:49:29.017: INFO: Waiting up to 5m0s for pod "downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-854f4" to be "success or failure"
Feb 28 08:49:29.020: INFO: Pod "downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.254761ms
Feb 28 08:49:31.026: INFO: Pod "downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009299131s
STEP: Saw pod success
Feb 28 08:49:31.026: INFO: Pod "downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 08:49:31.030: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:49:31.051: INFO: Waiting for pod downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d to disappear
Feb 28 08:49:31.055: INFO: Pod downward-api-c24493ea-3b35-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:49:31.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-854f4" for this suite.
Feb 28 08:49:37.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:37.124: INFO: namespace: e2e-tests-downward-api-854f4, resource: bindings, ignored listing per whitelist
Feb 28 08:49:37.261: INFO: namespace e2e-tests-downward-api-854f4 deletion completed in 6.2008288s

• [SLOW TEST:8.511 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:37.261: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-zflz4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:49:47.660873   31734 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:49:47.660: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:49:47.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zflz4" for this suite.
Feb 28 08:49:53.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:53.793: INFO: namespace: e2e-tests-gc-zflz4, resource: bindings, ignored listing per whitelist
Feb 28 08:49:53.900: INFO: namespace e2e-tests-gc-zflz4 deletion completed in 6.233923804s

• [SLOW TEST:16.639 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:49:53.900: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-cmgvs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-cv6vx
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 28 08:50:05.616: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-bjjx8
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:50:22.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-cmgvs" for this suite.
Feb 28 08:50:28.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:28.952: INFO: namespace: e2e-tests-namespaces-cmgvs, resource: bindings, ignored listing per whitelist
Feb 28 08:50:29.029: INFO: namespace e2e-tests-namespaces-cmgvs deletion completed in 6.286601143s
STEP: Destroying namespace "e2e-tests-nsdeletetest-cv6vx" for this suite.
Feb 28 08:50:29.033: INFO: Namespace e2e-tests-nsdeletetest-cv6vx was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-bjjx8" for this suite.
Feb 28 08:50:35.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:35.122: INFO: namespace: e2e-tests-nsdeletetest-bjjx8, resource: bindings, ignored listing per whitelist
Feb 28 08:50:35.256: INFO: namespace e2e-tests-nsdeletetest-bjjx8 deletion completed in 6.222452802s

• [SLOW TEST:41.355 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:50:35.256: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-p2gtb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 08:50:39.582: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:50:39.587: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:50:41.587: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:50:41.594: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:50:43.587: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:50:43.658: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:50:45.587: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:50:45.593: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:50:47.587: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:50:47.593: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:50:47.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-p2gtb" for this suite.
Feb 28 08:51:09.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:09.838: INFO: namespace: e2e-tests-container-lifecycle-hook-p2gtb, resource: bindings, ignored listing per whitelist
Feb 28 08:51:09.887: INFO: namespace e2e-tests-container-lifecycle-hook-p2gtb deletion completed in 22.287186187s

• [SLOW TEST:34.631 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:51:09.887: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-rfpw5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 28 08:51:10.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17154,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:51:10.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17154,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 28 08:51:20.168: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17174,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:51:20.168: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17174,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 28 08:51:30.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17194,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:51:30.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17194,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 28 08:51:40.192: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17215,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:51:40.192: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-a,UID:fe8e4879-3b35-11e9-b268-0a027e1dd732,ResourceVersion:17215,Generation:0,CreationTimestamp:2019-02-28 08:51:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 28 08:51:50.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-b,UID:166becad-3b36-11e9-b268-0a027e1dd732,ResourceVersion:17235,Generation:0,CreationTimestamp:2019-02-28 08:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:51:50.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-b,UID:166becad-3b36-11e9-b268-0a027e1dd732,ResourceVersion:17235,Generation:0,CreationTimestamp:2019-02-28 08:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 28 08:52:00.207: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-b,UID:166becad-3b36-11e9-b268-0a027e1dd732,ResourceVersion:17255,Generation:0,CreationTimestamp:2019-02-28 08:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:52:00.207: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-rfpw5,SelfLink:/api/v1/namespaces/e2e-tests-watch-rfpw5/configmaps/e2e-watch-test-configmap-b,UID:166becad-3b36-11e9-b268-0a027e1dd732,ResourceVersion:17255,Generation:0,CreationTimestamp:2019-02-28 08:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:52:10.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-rfpw5" for this suite.
Feb 28 08:52:16.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:16.284: INFO: namespace: e2e-tests-watch-rfpw5, resource: bindings, ignored listing per whitelist
Feb 28 08:52:16.441: INFO: namespace e2e-tests-watch-rfpw5 deletion completed in 6.227568139s

• [SLOW TEST:66.554 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:52:16.441: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-svx9r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:52:17.044: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-svx9r'
Feb 28 08:52:17.883: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 08:52:17.883: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 28 08:52:17.895: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-svx9r'
Feb 28 08:52:17.999: INFO: stderr: ""
Feb 28 08:52:17.999: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:52:17.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-svx9r" for this suite.
Feb 28 08:52:40.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:40.289: INFO: namespace: e2e-tests-kubectl-svx9r, resource: bindings, ignored listing per whitelist
Feb 28 08:52:40.392: INFO: namespace e2e-tests-kubectl-svx9r deletion completed in 22.381314482s

• [SLOW TEST:23.951 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:52:40.392: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-28q2l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-28q2l
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-28q2l
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-28q2l
Feb 28 08:52:40.658: INFO: Found 0 stateful pods, waiting for 1
Feb 28 08:52:50.664: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 28 08:52:50.668: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:52:51.176: INFO: stderr: ""
Feb 28 08:52:51.176: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:52:51.176: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:52:51.183: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 08:53:01.189: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:53:01.189: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:53:01.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999663s
Feb 28 08:53:02.217: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994548667s
Feb 28 08:53:03.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98915326s
Feb 28 08:53:04.231: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982495766s
Feb 28 08:53:05.238: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974312904s
Feb 28 08:53:06.250: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967690442s
Feb 28 08:53:07.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.955597449s
Feb 28 08:53:08.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.94182292s
Feb 28 08:53:09.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.877060287s
Feb 28 08:53:10.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 868.13978ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-28q2l
Feb 28 08:53:11.358: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:53:12.074: INFO: stderr: ""
Feb 28 08:53:12.074: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:53:12.074: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:53:12.081: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:53:22.089: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:53:22.089: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:53:22.089: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 28 08:53:22.099: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:53:22.626: INFO: stderr: ""
Feb 28 08:53:22.626: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:53:22.626: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:53:22.626: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:53:23.109: INFO: stderr: ""
Feb 28 08:53:23.109: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:53:23.109: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:53:23.109: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:53:23.579: INFO: stderr: ""
Feb 28 08:53:23.580: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:53:23.580: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:53:23.580: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:53:23.585: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 08:53:33.600: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:53:33.600: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:53:33.600: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:53:33.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999738s
Feb 28 08:53:34.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989284374s
Feb 28 08:53:35.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974253067s
Feb 28 08:53:36.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966450177s
Feb 28 08:53:37.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956971148s
Feb 28 08:53:38.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.945240686s
Feb 28 08:53:39.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.933331661s
Feb 28 08:53:40.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922588365s
Feb 28 08:53:41.720: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.913304187s
Feb 28 08:53:42.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 906.863704ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-28q2l
Feb 28 08:53:43.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:53:44.264: INFO: stderr: ""
Feb 28 08:53:44.264: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:53:44.264: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:53:44.264: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:53:44.798: INFO: stderr: ""
Feb 28 08:53:44.798: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:53:44.798: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:53:44.798: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:53:45.284: INFO: rc: 1
Feb 28 08:53:45.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (b36e54e3ee94305a586502c3c81c5ae163f24616dbca75151fb4f4285803b119)
 [] <nil> 0xc0018b5710 exit status 1 <nil> <nil> true [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a4b8 0xc00038a5e8] [0x933040 0x933040] 0xc001a321e0 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (b36e54e3ee94305a586502c3c81c5ae163f24616dbca75151fb4f4285803b119)

error:
exit status 1

Feb 28 08:53:55.284: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:53:55.484: INFO: rc: 1
Feb 28 08:53:55.484: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001e309f0 exit status 1 <nil> <nil> true [0xc0007120c0 0xc0007120e8 0xc000712168] [0xc0007120c0 0xc0007120e8 0xc000712168] [0xc0007120d0 0xc000712148] [0x933040 0x933040] 0xc001fc0660 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 08:54:05.485: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:05.573: INFO: rc: 1
Feb 28 08:54:05.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168ed80 exit status 1 <nil> <nil> true [0xc000160d28 0xc000160e08 0xc000160e58] [0xc000160d28 0xc000160e08 0xc000160e58] [0xc000160de8 0xc000160e30] [0x933040 0x933040] 0xc0022605a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:54:15.573: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:15.659: INFO: rc: 1
Feb 28 08:54:15.659: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b5a40 exit status 1 <nil> <nil> true [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a6c0 0xc00038a798] [0x933040 0x933040] 0xc001a324e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:54:25.660: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:25.840: INFO: rc: 1
Feb 28 08:54:25.840: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001e30de0 exit status 1 <nil> <nil> true [0xc000712170 0xc0007121e0 0xc000712258] [0xc000712170 0xc0007121e0 0xc000712258] [0xc0007121c0 0xc000712250] [0x933040 0x933040] 0xc001fc1200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:54:35.841: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:35.920: INFO: rc: 1
Feb 28 08:54:35.920: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168f050 exit status 1 <nil> <nil> true [0xc000160e60 0xc000160f08 0xc000160f80] [0xc000160e60 0xc000160f08 0xc000160f80] [0xc000160e88 0xc000160f78] [0x933040 0x933040] 0xc002261140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:54:45.920: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:45.996: INFO: rc: 1
Feb 28 08:54:45.997: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168f500 exit status 1 <nil> <nil> true [0xc000160f90 0xc000161070 0xc0001610e0] [0xc000160f90 0xc000161070 0xc0001610e0] [0xc000161058 0xc000161088] [0x933040 0x933040] 0xc002261440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:54:55.997: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:54:56.082: INFO: rc: 1
Feb 28 08:54:56.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b5a990 exit status 1 <nil> <nil> true [0xc000850008 0xc000850020 0xc000850048] [0xc000850008 0xc000850020 0xc000850048] [0xc000850018 0xc000850030] [0x933040 0x933040] 0xc001bfdb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:06.083: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:06.159: INFO: rc: 1
Feb 28 08:55:06.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b5ce0 exit status 1 <nil> <nil> true [0xc00038a8b8 0xc00038a908 0xc00038a9c8] [0xc00038a8b8 0xc00038a908 0xc00038a9c8] [0xc00038a8e0 0xc00038a970] [0x933040 0x933040] 0xc001a32840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:16.159: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:16.326: INFO: rc: 1
Feb 28 08:55:16.326: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b5fb0 exit status 1 <nil> <nil> true [0xc00038aa78 0xc00038abb0 0xc00038ac40] [0xc00038aa78 0xc00038abb0 0xc00038ac40] [0xc00038aba8 0xc00038ac10] [0x933040 0x933040] 0xc001a333e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:26.326: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:26.483: INFO: rc: 1
Feb 28 08:55:26.483: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2300 exit status 1 <nil> <nil> true [0xc00038ac58 0xc00038ad78 0xc00038ae30] [0xc00038ac58 0xc00038ad78 0xc00038ae30] [0xc00038ac90 0xc00038adf0] [0x933040 0x933040] 0xc001a33ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:36.483: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:36.577: INFO: rc: 1
Feb 28 08:55:36.577: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2600 exit status 1 <nil> <nil> true [0xc00038ae78 0xc00038af48 0xc00038af70] [0xc00038ae78 0xc00038af48 0xc00038af70] [0xc00038aee0 0xc00038af58] [0x933040 0x933040] 0xc0013ef4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:46.578: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:46.658: INFO: rc: 1
Feb 28 08:55:46.658: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b4420 exit status 1 <nil> <nil> true [0xc000160138 0xc0001602d8 0xc0001604f0] [0xc000160138 0xc0001602d8 0xc0001604f0] [0xc000160238 0xc000160468] [0x933040 0x933040] 0xc001a32240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:55:56.659: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:55:56.754: INFO: rc: 1
Feb 28 08:55:56.754: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b4810 exit status 1 <nil> <nil> true [0xc000160500 0xc000160668 0xc0001608c8] [0xc000160500 0xc000160668 0xc0001608c8] [0xc000160598 0xc000160868] [0x933040 0x933040] 0xc001a32540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:06.754: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:06.830: INFO: rc: 1
Feb 28 08:56:06.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168e450 exit status 1 <nil> <nil> true [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a248 0xc00038a2f8] [0x933040 0x933040] 0xc002233c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:16.831: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:16.905: INFO: rc: 1
Feb 28 08:56:16.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168e720 exit status 1 <nil> <nil> true [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a370 0xc00038a580 0xc00038a620] [0xc00038a4b8 0xc00038a5e8] [0x933040 0x933040] 0xc002260000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:26.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:26.984: INFO: rc: 1
Feb 28 08:56:26.984: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168e9c0 exit status 1 <nil> <nil> true [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a630 0xc00038a738 0xc00038a858] [0xc00038a6c0 0xc00038a798] [0x933040 0x933040] 0xc002260360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:36.984: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:37.066: INFO: rc: 1
Feb 28 08:56:37.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2330 exit status 1 <nil> <nil> true [0xc000712018 0xc000712030 0xc000712088] [0xc000712018 0xc000712030 0xc000712088] [0xc000712028 0xc000712068] [0x933040 0x933040] 0xc0013ef320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:47.066: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:47.149: INFO: rc: 1
Feb 28 08:56:47.149: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168ec60 exit status 1 <nil> <nil> true [0xc00038a8b8 0xc00038a908 0xc00038a9c8] [0xc00038a8b8 0xc00038a908 0xc00038a9c8] [0xc00038a8e0 0xc00038a970] [0x933040 0x933040] 0xc002260f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:56:57.150: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:56:57.278: INFO: rc: 1
Feb 28 08:56:57.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001e30570 exit status 1 <nil> <nil> true [0xc000850008 0xc000850020 0xc000850048] [0xc000850008 0xc000850020 0xc000850048] [0xc000850018 0xc000850030] [0x933040 0x933040] 0xc001fc0360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:07.278: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:07.363: INFO: rc: 1
Feb 28 08:57:07.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2660 exit status 1 <nil> <nil> true [0xc0007120b8 0xc0007120d0 0xc000712148] [0xc0007120b8 0xc0007120d0 0xc000712148] [0xc0007120c8 0xc000712128] [0x933040 0x933040] 0xc0013ef8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:17.363: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:17.565: INFO: rc: 1
Feb 28 08:57:17.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168efc0 exit status 1 <nil> <nil> true [0xc00038aa78 0xc00038abb0 0xc00038ac40] [0xc00038aa78 0xc00038abb0 0xc00038ac40] [0xc00038aba8 0xc00038ac10] [0x933040 0x933040] 0xc002261200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:27.565: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:27.674: INFO: rc: 1
Feb 28 08:57:27.674: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168f4a0 exit status 1 <nil> <nil> true [0xc00038ac58 0xc00038ad78 0xc00038ae30] [0xc00038ac58 0xc00038ad78 0xc00038ae30] [0xc00038ac90 0xc00038adf0] [0x933040 0x933040] 0xc002261500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:37.675: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:37.763: INFO: rc: 1
Feb 28 08:57:37.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168f7d0 exit status 1 <nil> <nil> true [0xc00038ae78 0xc00038af48 0xc00038af70] [0xc00038ae78 0xc00038af48 0xc00038af70] [0xc00038aee0 0xc00038af58] [0x933040 0x933040] 0xc002261e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:47.764: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:48.103: INFO: rc: 1
Feb 28 08:57:48.103: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2a50 exit status 1 <nil> <nil> true [0xc000712170 0xc0007121e0 0xc000712258] [0xc000712170 0xc0007121e0 0xc000712258] [0xc0007121c0 0xc000712250] [0x933040 0x933040] 0xc0013efbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:57:58.103: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:57:58.185: INFO: rc: 1
Feb 28 08:57:58.185: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00168e480 exit status 1 <nil> <nil> true [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a158 0xc00038a260 0xc00038a330] [0xc00038a248 0xc00038a2f8] [0x933040 0x933040] 0xc002233c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:58:08.185: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:58:08.278: INFO: rc: 1
Feb 28 08:58:08.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d22d0 exit status 1 <nil> <nil> true [0xc000160118 0xc000160238 0xc000160468] [0xc000160118 0xc000160238 0xc000160468] [0xc000160140 0xc0001603e8] [0x933040 0x933040] 0xc0022602a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:58:18.279: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:58:18.363: INFO: rc: 1
Feb 28 08:58:18.364: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2600 exit status 1 <nil> <nil> true [0xc0001604f0 0xc000160598 0xc000160868] [0xc0001604f0 0xc000160598 0xc000160868] [0xc000160528 0xc0001606e8] [0x933040 0x933040] 0xc0022605a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:58:28.364: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:58:28.470: INFO: rc: 1
Feb 28 08:58:28.470: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018b4480 exit status 1 <nil> <nil> true [0xc000712018 0xc000712030 0xc000712088] [0xc000712018 0xc000712030 0xc000712088] [0xc000712028 0xc000712068] [0x933040 0x933040] 0xc001a32240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:58:38.470: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:58:38.551: INFO: rc: 1
Feb 28 08:58:38.551: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0013d2d20 exit status 1 <nil> <nil> true [0xc0001608c8 0xc000160c20 0xc000160d28] [0xc0001608c8 0xc000160c20 0xc000160d28] [0xc0001609c0 0xc000160c70] [0x933040 0x933040] 0xc002261140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:58:48.551: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-28q2l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:58:48.666: INFO: rc: 1
Feb 28 08:58:48.666: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 28 08:58:48.666: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:58:48.698: INFO: Deleting all statefulset in ns e2e-tests-statefulset-28q2l
Feb 28 08:58:48.701: INFO: Scaling statefulset ss to 0
Feb 28 08:58:48.715: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:58:48.718: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 08:58:48.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-28q2l" for this suite.
Feb 28 08:58:54.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:54.809: INFO: namespace: e2e-tests-statefulset-28q2l, resource: bindings, ignored listing per whitelist
Feb 28 08:58:54.949: INFO: namespace e2e-tests-statefulset-28q2l deletion completed in 6.211387142s

• [SLOW TEST:374.557 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 08:58:54.950: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-rsws9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:58:55.251: INFO: Number of nodes with available pods: 0
Feb 28 08:58:55.251: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:58:56.264: INFO: Number of nodes with available pods: 0
Feb 28 08:58:56.264: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:58:57.262: INFO: Number of nodes with available pods: 2
Feb 28 08:58:57.262: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 28 08:58:57.282: INFO: Number of nodes with available pods: 1
Feb 28 08:58:57.282: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:58:58.292: INFO: Number of nodes with available pods: 1
Feb 28 08:58:58.292: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:58:59.294: INFO: Number of nodes with available pods: 1
Feb 28 08:58:59.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:00.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:00.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:01.292: INFO: Number of nodes with available pods: 1
Feb 28 08:59:01.292: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:02.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:02.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:03.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:03.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:04.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:04.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:05.337: INFO: Number of nodes with available pods: 1
Feb 28 08:59:05.337: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:06.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:06.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:07.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:07.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:08.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:08.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:09.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:09.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:10.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:10.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:11.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:11.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:12.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:12.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:13.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:13.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:14.297: INFO: Number of nodes with available pods: 1
Feb 28 08:59:14.298: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:15.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:15.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:16.381: INFO: Number of nodes with available pods: 1
Feb 28 08:59:16.381: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:17.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:17.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:18.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:18.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:19.308: INFO: Number of nodes with available pods: 1
Feb 28 08:59:19.308: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:20.301: INFO: Number of nodes with available pods: 1
Feb 28 08:59:20.301: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:21.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:21.293: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:22.295: INFO: Number of nodes with available pods: 1
Feb 28 08:59:22.295: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:23.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:23.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:24.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:24.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:25.295: INFO: Number of nodes with available pods: 1
Feb 28 08:59:25.295: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:26.298: INFO: Number of nodes with available pods: 1
Feb 28 08:59:26.298: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:27.293: INFO: Number of nodes with available pods: 1
Feb 28 08:59:27.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:28.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:28.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:29.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:29.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:30.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:30.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:31.292: INFO: Number of nodes with available pods: 1
Feb 28 08:59:31.292: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:32.294: INFO: Number of nodes with available pods: 1
Feb 28 08:59:32.294: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:33.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:33.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:34.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:34.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:35.296: INFO: Number of nodes with available pods: 1
Feb 28 08:59:35.296: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:36.291: INFO: Number of nodes with available pods: 1
Feb 28 08:59:36.291: INFO: Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 is running more than one daemon pod
Feb 28 08:59:37.294: INFO: Number of nodes with available pods: 2
Feb 28 08:59:37.294: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-rsws9, will wait for the garbage collector to delete the pods
Feb 28 08:59:37.362: INFO: Deleting DaemonSet.extensions daemon-set took: 7.604564ms
Feb 28 08:59:37.462: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.606819ms
Feb 28 09:00:15.669: INFO: Number of nodes with available pods: 0
Feb 28 09:00:15.669: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 09:00:15.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-rsws9/daemonsets","resourceVersion":"18379"},"items":null}

Feb 28 09:00:15.681: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-rsws9/pods","resourceVersion":"18379"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:15.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-rsws9" for this suite.
Feb 28 09:00:21.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:21.833: INFO: namespace: e2e-tests-daemonsets-rsws9, resource: bindings, ignored listing per whitelist
Feb 28 09:00:22.001: INFO: namespace e2e-tests-daemonsets-rsws9 deletion completed in 6.296069312s

• [SLOW TEST:87.051 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:22.001: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-n4x4n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-47a87734-3b37-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 09:00:22.313: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-n4x4n" to be "success or failure"
Feb 28 09:00:22.317: INFO: Pod "pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.305113ms
Feb 28 09:00:24.414: INFO: Pod "pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.101348298s
STEP: Saw pod success
Feb 28 09:00:24.414: INFO: Pod "pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:00:24.420: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:00:24.448: INFO: Waiting for pod pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:00:24.452: INFO: Pod pod-projected-secrets-47a9578c-3b37-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:24.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n4x4n" for this suite.
Feb 28 09:00:30.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:30.700: INFO: namespace: e2e-tests-projected-n4x4n, resource: bindings, ignored listing per whitelist
Feb 28 09:00:30.708: INFO: namespace e2e-tests-projected-n4x4n deletion completed in 6.250363371s

• [SLOW TEST:8.707 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:30.708: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-q626s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 28 09:00:31.039: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18441,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 09:00:31.039: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18442,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 09:00:31.039: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18443,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 28 09:00:41.080: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18466,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 09:00:41.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18467,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 28 09:00:41.080: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q626s,SelfLink:/api/v1/namespaces/e2e-tests-watch-q626s/configmaps/e2e-watch-test-label-changed,UID:4cd986fd-3b37-11e9-b268-0a027e1dd732,ResourceVersion:18468,Generation:0,CreationTimestamp:2019-02-28 09:00:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:41.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-q626s" for this suite.
Feb 28 09:00:47.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:47.293: INFO: namespace: e2e-tests-watch-q626s, resource: bindings, ignored listing per whitelist
Feb 28 09:00:47.311: INFO: namespace e2e-tests-watch-q626s deletion completed in 6.225072221s

• [SLOW TEST:16.604 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:47.312: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hbzt7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 09:00:47.670: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-hbzt7'
Feb 28 09:00:47.920: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 09:00:47.921: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 28 09:00:51.964: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-hbzt7'
Feb 28 09:00:52.092: INFO: stderr: ""
Feb 28 09:00:52.092: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:00:52.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hbzt7" for this suite.
Feb 28 09:00:58.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:58.329: INFO: namespace: e2e-tests-kubectl-hbzt7, resource: bindings, ignored listing per whitelist
Feb 28 09:00:58.368: INFO: namespace e2e-tests-kubectl-hbzt7 deletion completed in 6.271017465s

• [SLOW TEST:11.057 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:00:58.369: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sj2v5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-5d4d6288-3b37-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 09:00:58.622: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-sj2v5" to be "success or failure"
Feb 28 09:00:58.639: INFO: Pod "pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.654046ms
Feb 28 09:01:00.644: INFO: Pod "pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022183354s
STEP: Saw pod success
Feb 28 09:01:00.644: INFO: Pod "pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:01:00.648: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:01:00.670: INFO: Waiting for pod pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:01:00.674: INFO: Pod pod-projected-secrets-5d4e0d64-3b37-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sj2v5" for this suite.
Feb 28 09:01:06.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:06.776: INFO: namespace: e2e-tests-projected-sj2v5, resource: bindings, ignored listing per whitelist
Feb 28 09:01:06.899: INFO: namespace e2e-tests-projected-sj2v5 deletion completed in 6.211179249s

• [SLOW TEST:8.531 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:01:06.899: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bvp8z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 09:01:07.672: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:08.123: INFO: stderr: ""
Feb 28 09:01:08.123: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 09:01:08.124: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:08.238: INFO: stderr: ""
Feb 28 09:01:08.238: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-c9s7n "
Feb 28 09:01:08.239: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:08.327: INFO: stderr: ""
Feb 28 09:01:08.327: INFO: stdout: ""
Feb 28 09:01:08.327: INFO: update-demo-nautilus-bscf6 is created but not running
Feb 28 09:01:13.328: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:13.462: INFO: stderr: ""
Feb 28 09:01:13.462: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-c9s7n "
Feb 28 09:01:13.462: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:13.558: INFO: stderr: ""
Feb 28 09:01:13.558: INFO: stdout: "true"
Feb 28 09:01:13.558: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:13.681: INFO: stderr: ""
Feb 28 09:01:13.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:13.681: INFO: validating pod update-demo-nautilus-bscf6
Feb 28 09:01:13.767: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:13.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:13.767: INFO: update-demo-nautilus-bscf6 is verified up and running
Feb 28 09:01:13.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-c9s7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:13.874: INFO: stderr: ""
Feb 28 09:01:13.874: INFO: stdout: "true"
Feb 28 09:01:13.874: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-c9s7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:13.989: INFO: stderr: ""
Feb 28 09:01:13.989: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:13.989: INFO: validating pod update-demo-nautilus-c9s7n
Feb 28 09:01:14.091: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:14.091: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:14.091: INFO: update-demo-nautilus-c9s7n is verified up and running
STEP: scaling down the replication controller
Feb 28 09:01:14.096: INFO: scanned /root for discovery docs: <nil>
Feb 28 09:01:14.096: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:15.265: INFO: stderr: ""
Feb 28 09:01:15.265: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 09:01:15.265: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:15.383: INFO: stderr: ""
Feb 28 09:01:15.383: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-c9s7n "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 09:01:20.386: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:20.550: INFO: stderr: ""
Feb 28 09:01:20.550: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-c9s7n "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 09:01:25.551: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:25.647: INFO: stderr: ""
Feb 28 09:01:25.647: INFO: stdout: "update-demo-nautilus-bscf6 "
Feb 28 09:01:25.647: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:25.739: INFO: stderr: ""
Feb 28 09:01:25.739: INFO: stdout: "true"
Feb 28 09:01:25.739: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:25.825: INFO: stderr: ""
Feb 28 09:01:25.825: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:25.825: INFO: validating pod update-demo-nautilus-bscf6
Feb 28 09:01:25.835: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:25.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:25.835: INFO: update-demo-nautilus-bscf6 is verified up and running
STEP: scaling up the replication controller
Feb 28 09:01:25.839: INFO: scanned /root for discovery docs: <nil>
Feb 28 09:01:25.839: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:27.000: INFO: stderr: ""
Feb 28 09:01:27.000: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 09:01:27.000: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:27.103: INFO: stderr: ""
Feb 28 09:01:27.103: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-zbf8n "
Feb 28 09:01:27.103: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:27.258: INFO: stderr: ""
Feb 28 09:01:27.258: INFO: stdout: "true"
Feb 28 09:01:27.258: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:27.373: INFO: stderr: ""
Feb 28 09:01:27.373: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:27.373: INFO: validating pod update-demo-nautilus-bscf6
Feb 28 09:01:27.383: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:27.383: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:27.383: INFO: update-demo-nautilus-bscf6 is verified up and running
Feb 28 09:01:27.383: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zbf8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:27.482: INFO: stderr: ""
Feb 28 09:01:27.483: INFO: stdout: ""
Feb 28 09:01:27.486: INFO: update-demo-nautilus-zbf8n is created but not running
Feb 28 09:01:32.486: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:32.629: INFO: stderr: ""
Feb 28 09:01:32.629: INFO: stdout: "update-demo-nautilus-bscf6 update-demo-nautilus-zbf8n "
Feb 28 09:01:32.629: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:32.733: INFO: stderr: ""
Feb 28 09:01:32.733: INFO: stdout: "true"
Feb 28 09:01:32.733: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bscf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:32.884: INFO: stderr: ""
Feb 28 09:01:32.884: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:32.884: INFO: validating pod update-demo-nautilus-bscf6
Feb 28 09:01:32.897: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:32.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:32.897: INFO: update-demo-nautilus-bscf6 is verified up and running
Feb 28 09:01:32.897: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zbf8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:33.042: INFO: stderr: ""
Feb 28 09:01:33.042: INFO: stdout: "true"
Feb 28 09:01:33.042: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zbf8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:33.205: INFO: stderr: ""
Feb 28 09:01:33.205: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:01:33.205: INFO: validating pod update-demo-nautilus-zbf8n
Feb 28 09:01:33.293: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:01:33.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:01:33.293: INFO: update-demo-nautilus-zbf8n is verified up and running
STEP: using delete to clean up resources
Feb 28 09:01:33.293: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:33.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 09:01:33.410: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 09:01:33.410: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-bvp8z'
Feb 28 09:01:33.546: INFO: stderr: "No resources found.\n"
Feb 28 09:01:33.546: INFO: stdout: ""
Feb 28 09:01:33.546: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-bvp8z -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 09:01:33.660: INFO: stderr: ""
Feb 28 09:01:33.660: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:33.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bvp8z" for this suite.
Feb 28 09:01:39.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:39.816: INFO: namespace: e2e-tests-kubectl-bvp8z, resource: bindings, ignored listing per whitelist
Feb 28 09:01:39.862: INFO: namespace e2e-tests-kubectl-bvp8z deletion completed in 6.193237371s

• [SLOW TEST:32.963 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:01:39.863: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-gj9kq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-gj9kq
I0228 09:01:40.208643   31734 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-gj9kq, replica count: 1
I0228 09:01:41.259062   31734 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 09:01:42.259300   31734 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 09:01:42.385: INFO: Created: latency-svc-w6s8h
Feb 28 09:01:42.391: INFO: Got endpoints: latency-svc-w6s8h [32.316272ms]
Feb 28 09:01:42.409: INFO: Created: latency-svc-pbvct
Feb 28 09:01:42.412: INFO: Got endpoints: latency-svc-pbvct [20.742669ms]
Feb 28 09:01:42.417: INFO: Created: latency-svc-2ktxd
Feb 28 09:01:42.421: INFO: Got endpoints: latency-svc-2ktxd [28.851553ms]
Feb 28 09:01:42.425: INFO: Created: latency-svc-fqqxd
Feb 28 09:01:42.426: INFO: Got endpoints: latency-svc-fqqxd [33.989368ms]
Feb 28 09:01:42.435: INFO: Created: latency-svc-ttgm4
Feb 28 09:01:42.440: INFO: Got endpoints: latency-svc-ttgm4 [48.459018ms]
Feb 28 09:01:42.442: INFO: Created: latency-svc-cbg94
Feb 28 09:01:42.449: INFO: Got endpoints: latency-svc-cbg94 [56.717874ms]
Feb 28 09:01:42.454: INFO: Created: latency-svc-57s55
Feb 28 09:01:42.459: INFO: Got endpoints: latency-svc-57s55 [66.546589ms]
Feb 28 09:01:42.460: INFO: Created: latency-svc-jb92n
Feb 28 09:01:42.461: INFO: Got endpoints: latency-svc-jb92n [69.045194ms]
Feb 28 09:01:42.467: INFO: Created: latency-svc-7s8kr
Feb 28 09:01:42.469: INFO: Got endpoints: latency-svc-7s8kr [77.114677ms]
Feb 28 09:01:42.477: INFO: Created: latency-svc-6ff6t
Feb 28 09:01:42.480: INFO: Got endpoints: latency-svc-6ff6t [88.265001ms]
Feb 28 09:01:42.496: INFO: Created: latency-svc-bdm46
Feb 28 09:01:42.502: INFO: Got endpoints: latency-svc-bdm46 [110.151878ms]
Feb 28 09:01:42.503: INFO: Created: latency-svc-l5hhd
Feb 28 09:01:42.508: INFO: Got endpoints: latency-svc-l5hhd [115.942693ms]
Feb 28 09:01:42.516: INFO: Created: latency-svc-s6c8m
Feb 28 09:01:42.518: INFO: Got endpoints: latency-svc-s6c8m [125.797759ms]
Feb 28 09:01:42.522: INFO: Created: latency-svc-58wjx
Feb 28 09:01:42.528: INFO: Got endpoints: latency-svc-58wjx [135.553642ms]
Feb 28 09:01:42.531: INFO: Created: latency-svc-dthbz
Feb 28 09:01:42.540: INFO: Created: latency-svc-84fd7
Feb 28 09:01:42.550: INFO: Created: latency-svc-ljzcb
Feb 28 09:01:42.557: INFO: Created: latency-svc-dqb7c
Feb 28 09:01:42.565: INFO: Created: latency-svc-hlz6r
Feb 28 09:01:42.574: INFO: Created: latency-svc-4dssm
Feb 28 09:01:42.584: INFO: Created: latency-svc-9x6wm
Feb 28 09:01:42.598: INFO: Got endpoints: latency-svc-dthbz [80.137953ms]
Feb 28 09:01:42.598: INFO: Got endpoints: latency-svc-ljzcb [177.55616ms]
Feb 28 09:01:42.599: INFO: Got endpoints: latency-svc-84fd7 [186.691764ms]
Feb 28 09:01:42.599: INFO: Got endpoints: latency-svc-dqb7c [173.061849ms]
Feb 28 09:01:42.599: INFO: Got endpoints: latency-svc-hlz6r [158.572925ms]
Feb 28 09:01:42.601: INFO: Created: latency-svc-w2dr8
Feb 28 09:01:42.611: INFO: Created: latency-svc-lgpmd
Feb 28 09:01:42.619: INFO: Created: latency-svc-8lbd4
Feb 28 09:01:42.635: INFO: Created: latency-svc-dfhdn
Feb 28 09:01:42.649: INFO: Created: latency-svc-blf48
Feb 28 09:01:42.658: INFO: Created: latency-svc-cz9vf
Feb 28 09:01:42.669: INFO: Created: latency-svc-f2vjw
Feb 28 09:01:42.675: INFO: Created: latency-svc-plgjl
Feb 28 09:01:42.682: INFO: Created: latency-svc-7vksg
Feb 28 09:01:42.690: INFO: Got endpoints: latency-svc-8lbd4 [220.806161ms]
Feb 28 09:01:42.690: INFO: Created: latency-svc-8rrkw
Feb 28 09:01:42.690: INFO: Got endpoints: latency-svc-lgpmd [228.851857ms]
Feb 28 09:01:42.696: INFO: Got endpoints: latency-svc-cz9vf [193.896802ms]
Feb 28 09:01:42.696: INFO: Got endpoints: latency-svc-9x6wm [247.716159ms]
Feb 28 09:01:42.697: INFO: Got endpoints: latency-svc-w2dr8 [238.004218ms]
Feb 28 09:01:42.697: INFO: Got endpoints: latency-svc-4dssm [304.547538ms]
Feb 28 09:01:42.697: INFO: Got endpoints: latency-svc-blf48 [304.562027ms]
Feb 28 09:01:42.697: INFO: Got endpoints: latency-svc-dfhdn [216.189913ms]
Feb 28 09:01:42.706: INFO: Got endpoints: latency-svc-plgjl [178.722818ms]
Feb 28 09:01:42.707: INFO: Got endpoints: latency-svc-8rrkw [108.251631ms]
Feb 28 09:01:42.707: INFO: Got endpoints: latency-svc-f2vjw [198.699747ms]
Feb 28 09:01:42.707: INFO: Got endpoints: latency-svc-7vksg [108.576597ms]
Feb 28 09:01:42.707: INFO: Created: latency-svc-t7xc6
Feb 28 09:01:42.712: INFO: Got endpoints: latency-svc-t7xc6 [112.539521ms]
Feb 28 09:01:42.716: INFO: Created: latency-svc-cmpdq
Feb 28 09:01:42.721: INFO: Got endpoints: latency-svc-cmpdq [121.884673ms]
Feb 28 09:01:42.726: INFO: Created: latency-svc-965mb
Feb 28 09:01:42.728: INFO: Got endpoints: latency-svc-965mb [128.387596ms]
Feb 28 09:01:42.740: INFO: Created: latency-svc-bkng6
Feb 28 09:01:42.740: INFO: Got endpoints: latency-svc-bkng6 [49.567788ms]
Feb 28 09:01:42.742: INFO: Created: latency-svc-j5d45
Feb 28 09:01:42.747: INFO: Got endpoints: latency-svc-j5d45 [57.244148ms]
Feb 28 09:01:42.752: INFO: Created: latency-svc-gmx45
Feb 28 09:01:42.762: INFO: Got endpoints: latency-svc-gmx45 [65.565119ms]
Feb 28 09:01:42.770: INFO: Created: latency-svc-kzqm4
Feb 28 09:01:42.779: INFO: Created: latency-svc-7sgtf
Feb 28 09:01:42.786: INFO: Created: latency-svc-glcfq
Feb 28 09:01:42.790: INFO: Got endpoints: latency-svc-kzqm4 [93.482941ms]
Feb 28 09:01:42.794: INFO: Created: latency-svc-ltsmj
Feb 28 09:01:42.804: INFO: Created: latency-svc-ftp5n
Feb 28 09:01:42.818: INFO: Created: latency-svc-qvhtn
Feb 28 09:01:42.826: INFO: Created: latency-svc-2rwhg
Feb 28 09:01:42.832: INFO: Created: latency-svc-rqwtn
Feb 28 09:01:42.839: INFO: Created: latency-svc-4h782
Feb 28 09:01:42.839: INFO: Got endpoints: latency-svc-7sgtf [142.601559ms]
Feb 28 09:01:42.848: INFO: Created: latency-svc-crf6z
Feb 28 09:01:42.854: INFO: Created: latency-svc-2snln
Feb 28 09:01:42.862: INFO: Created: latency-svc-vrjzk
Feb 28 09:01:42.872: INFO: Created: latency-svc-wdkb9
Feb 28 09:01:42.883: INFO: Created: latency-svc-8fcfc
Feb 28 09:01:42.886: INFO: Got endpoints: latency-svc-glcfq [189.687382ms]
Feb 28 09:01:42.888: INFO: Created: latency-svc-p2hl4
Feb 28 09:01:42.903: INFO: Created: latency-svc-q5sf2
Feb 28 09:01:42.914: INFO: Created: latency-svc-bvsgz
Feb 28 09:01:42.924: INFO: Created: latency-svc-tmm97
Feb 28 09:01:42.938: INFO: Got endpoints: latency-svc-ltsmj [240.875794ms]
Feb 28 09:01:42.950: INFO: Created: latency-svc-7kbsl
Feb 28 09:01:42.987: INFO: Got endpoints: latency-svc-ftp5n [290.56059ms]
Feb 28 09:01:43.006: INFO: Created: latency-svc-c6wjs
Feb 28 09:01:43.037: INFO: Got endpoints: latency-svc-qvhtn [330.797776ms]
Feb 28 09:01:43.052: INFO: Created: latency-svc-hsfm7
Feb 28 09:01:43.087: INFO: Got endpoints: latency-svc-2rwhg [380.284906ms]
Feb 28 09:01:43.101: INFO: Created: latency-svc-tsbq5
Feb 28 09:01:43.138: INFO: Got endpoints: latency-svc-rqwtn [431.185335ms]
Feb 28 09:01:43.152: INFO: Created: latency-svc-qvjsq
Feb 28 09:01:43.188: INFO: Got endpoints: latency-svc-4h782 [481.202845ms]
Feb 28 09:01:43.204: INFO: Created: latency-svc-tpt4s
Feb 28 09:01:43.238: INFO: Got endpoints: latency-svc-crf6z [526.142227ms]
Feb 28 09:01:43.252: INFO: Created: latency-svc-stkxd
Feb 28 09:01:43.287: INFO: Got endpoints: latency-svc-2snln [565.808647ms]
Feb 28 09:01:43.306: INFO: Created: latency-svc-8bq99
Feb 28 09:01:43.340: INFO: Got endpoints: latency-svc-vrjzk [611.969332ms]
Feb 28 09:01:43.354: INFO: Created: latency-svc-hsbnv
Feb 28 09:01:43.387: INFO: Got endpoints: latency-svc-wdkb9 [646.915569ms]
Feb 28 09:01:43.405: INFO: Created: latency-svc-bvz6z
Feb 28 09:01:43.437: INFO: Got endpoints: latency-svc-8fcfc [689.504865ms]
Feb 28 09:01:43.452: INFO: Created: latency-svc-lhscf
Feb 28 09:01:43.487: INFO: Got endpoints: latency-svc-p2hl4 [724.468067ms]
Feb 28 09:01:43.499: INFO: Created: latency-svc-jt2xc
Feb 28 09:01:43.538: INFO: Got endpoints: latency-svc-q5sf2 [748.155058ms]
Feb 28 09:01:43.558: INFO: Created: latency-svc-br2cc
Feb 28 09:01:43.587: INFO: Got endpoints: latency-svc-bvsgz [747.725415ms]
Feb 28 09:01:43.602: INFO: Created: latency-svc-sjmlp
Feb 28 09:01:43.637: INFO: Got endpoints: latency-svc-tmm97 [750.294818ms]
Feb 28 09:01:43.657: INFO: Created: latency-svc-r77pg
Feb 28 09:01:43.688: INFO: Got endpoints: latency-svc-7kbsl [750.063178ms]
Feb 28 09:01:43.703: INFO: Created: latency-svc-fvbtc
Feb 28 09:01:43.737: INFO: Got endpoints: latency-svc-c6wjs [749.886827ms]
Feb 28 09:01:43.751: INFO: Created: latency-svc-q2sjv
Feb 28 09:01:43.789: INFO: Got endpoints: latency-svc-hsfm7 [751.375468ms]
Feb 28 09:01:43.803: INFO: Created: latency-svc-xmjkc
Feb 28 09:01:43.838: INFO: Got endpoints: latency-svc-tsbq5 [750.964563ms]
Feb 28 09:01:43.852: INFO: Created: latency-svc-lplg4
Feb 28 09:01:43.889: INFO: Got endpoints: latency-svc-qvjsq [750.731125ms]
Feb 28 09:01:43.905: INFO: Created: latency-svc-n26v7
Feb 28 09:01:43.939: INFO: Got endpoints: latency-svc-tpt4s [751.217139ms]
Feb 28 09:01:43.954: INFO: Created: latency-svc-8f4jw
Feb 28 09:01:43.988: INFO: Got endpoints: latency-svc-stkxd [750.51263ms]
Feb 28 09:01:44.040: INFO: Got endpoints: latency-svc-8bq99 [753.193389ms]
Feb 28 09:01:44.040: INFO: Created: latency-svc-86zgn
Feb 28 09:01:44.057: INFO: Created: latency-svc-9r92h
Feb 28 09:01:44.089: INFO: Got endpoints: latency-svc-hsbnv [748.945324ms]
Feb 28 09:01:44.101: INFO: Created: latency-svc-v8gw5
Feb 28 09:01:44.137: INFO: Got endpoints: latency-svc-bvz6z [750.314513ms]
Feb 28 09:01:44.149: INFO: Created: latency-svc-s78h8
Feb 28 09:01:44.187: INFO: Got endpoints: latency-svc-lhscf [750.11424ms]
Feb 28 09:01:44.206: INFO: Created: latency-svc-hdjxk
Feb 28 09:01:44.239: INFO: Got endpoints: latency-svc-jt2xc [751.997239ms]
Feb 28 09:01:44.252: INFO: Created: latency-svc-l7hlk
Feb 28 09:01:44.288: INFO: Got endpoints: latency-svc-br2cc [749.197913ms]
Feb 28 09:01:44.316: INFO: Created: latency-svc-79rmq
Feb 28 09:01:44.337: INFO: Got endpoints: latency-svc-sjmlp [749.989528ms]
Feb 28 09:01:44.356: INFO: Created: latency-svc-dbq5n
Feb 28 09:01:44.388: INFO: Got endpoints: latency-svc-r77pg [750.778741ms]
Feb 28 09:01:44.413: INFO: Created: latency-svc-kzn6l
Feb 28 09:01:44.458: INFO: Got endpoints: latency-svc-fvbtc [769.995102ms]
Feb 28 09:01:44.473: INFO: Created: latency-svc-r52z2
Feb 28 09:01:44.486: INFO: Got endpoints: latency-svc-q2sjv [749.23277ms]
Feb 28 09:01:44.565: INFO: Created: latency-svc-spgfv
Feb 28 09:01:44.566: INFO: Got endpoints: latency-svc-xmjkc [776.991169ms]
Feb 28 09:01:44.579: INFO: Created: latency-svc-65fzg
Feb 28 09:01:44.592: INFO: Got endpoints: latency-svc-lplg4 [753.59382ms]
Feb 28 09:01:44.658: INFO: Got endpoints: latency-svc-n26v7 [769.28318ms]
Feb 28 09:01:44.670: INFO: Created: latency-svc-pc4tx
Feb 28 09:01:44.677: INFO: Created: latency-svc-2mzvv
Feb 28 09:01:44.687: INFO: Got endpoints: latency-svc-8f4jw [747.237206ms]
Feb 28 09:01:44.705: INFO: Created: latency-svc-crtk5
Feb 28 09:01:44.737: INFO: Got endpoints: latency-svc-86zgn [749.016621ms]
Feb 28 09:01:44.754: INFO: Created: latency-svc-wmfb8
Feb 28 09:01:44.786: INFO: Got endpoints: latency-svc-9r92h [746.064377ms]
Feb 28 09:01:44.806: INFO: Created: latency-svc-zxgf4
Feb 28 09:01:44.838: INFO: Got endpoints: latency-svc-v8gw5 [748.8942ms]
Feb 28 09:01:44.850: INFO: Created: latency-svc-6tqqg
Feb 28 09:01:44.889: INFO: Got endpoints: latency-svc-s78h8 [751.737097ms]
Feb 28 09:01:44.907: INFO: Created: latency-svc-wgn5t
Feb 28 09:01:44.937: INFO: Got endpoints: latency-svc-hdjxk [750.090763ms]
Feb 28 09:01:44.951: INFO: Created: latency-svc-642jd
Feb 28 09:01:44.988: INFO: Got endpoints: latency-svc-l7hlk [749.673718ms]
Feb 28 09:01:45.005: INFO: Created: latency-svc-fjjqm
Feb 28 09:01:45.037: INFO: Got endpoints: latency-svc-79rmq [749.044876ms]
Feb 28 09:01:45.052: INFO: Created: latency-svc-m4cwd
Feb 28 09:01:45.087: INFO: Got endpoints: latency-svc-dbq5n [750.094413ms]
Feb 28 09:01:45.102: INFO: Created: latency-svc-7tlw4
Feb 28 09:01:45.136: INFO: Got endpoints: latency-svc-kzn6l [748.808857ms]
Feb 28 09:01:45.160: INFO: Created: latency-svc-9vjxh
Feb 28 09:01:45.187: INFO: Got endpoints: latency-svc-r52z2 [729.158437ms]
Feb 28 09:01:45.200: INFO: Created: latency-svc-4ndrq
Feb 28 09:01:45.238: INFO: Got endpoints: latency-svc-spgfv [751.849854ms]
Feb 28 09:01:45.257: INFO: Created: latency-svc-wlzt4
Feb 28 09:01:45.292: INFO: Got endpoints: latency-svc-65fzg [726.139075ms]
Feb 28 09:01:45.305: INFO: Created: latency-svc-r4kvn
Feb 28 09:01:45.337: INFO: Got endpoints: latency-svc-pc4tx [745.157707ms]
Feb 28 09:01:45.352: INFO: Created: latency-svc-h45mb
Feb 28 09:01:45.387: INFO: Got endpoints: latency-svc-2mzvv [728.57055ms]
Feb 28 09:01:45.400: INFO: Created: latency-svc-95gvb
Feb 28 09:01:45.475: INFO: Got endpoints: latency-svc-crtk5 [788.34112ms]
Feb 28 09:01:45.488: INFO: Got endpoints: latency-svc-wmfb8 [750.634713ms]
Feb 28 09:01:45.489: INFO: Created: latency-svc-4qfcj
Feb 28 09:01:45.504: INFO: Created: latency-svc-m64lv
Feb 28 09:01:45.592: INFO: Got endpoints: latency-svc-6tqqg [754.25415ms]
Feb 28 09:01:45.592: INFO: Got endpoints: latency-svc-zxgf4 [805.660082ms]
Feb 28 09:01:45.605: INFO: Created: latency-svc-jz96f
Feb 28 09:01:45.614: INFO: Created: latency-svc-tl8nv
Feb 28 09:01:45.645: INFO: Got endpoints: latency-svc-wgn5t [755.70663ms]
Feb 28 09:01:45.657: INFO: Created: latency-svc-5mcp5
Feb 28 09:01:45.687: INFO: Got endpoints: latency-svc-642jd [749.938312ms]
Feb 28 09:01:45.700: INFO: Created: latency-svc-xk2x2
Feb 28 09:01:45.741: INFO: Got endpoints: latency-svc-fjjqm [752.933279ms]
Feb 28 09:01:45.753: INFO: Created: latency-svc-7jrnw
Feb 28 09:01:45.788: INFO: Got endpoints: latency-svc-m4cwd [751.567074ms]
Feb 28 09:01:45.800: INFO: Created: latency-svc-hhj6g
Feb 28 09:01:45.837: INFO: Got endpoints: latency-svc-7tlw4 [750.218112ms]
Feb 28 09:01:45.854: INFO: Created: latency-svc-vp7sf
Feb 28 09:01:45.887: INFO: Got endpoints: latency-svc-9vjxh [750.544967ms]
Feb 28 09:01:45.900: INFO: Created: latency-svc-4vcd4
Feb 28 09:01:45.940: INFO: Got endpoints: latency-svc-4ndrq [752.706344ms]
Feb 28 09:01:45.965: INFO: Created: latency-svc-mgrw5
Feb 28 09:01:45.995: INFO: Got endpoints: latency-svc-wlzt4 [756.396784ms]
Feb 28 09:01:46.008: INFO: Created: latency-svc-gqq46
Feb 28 09:01:46.040: INFO: Got endpoints: latency-svc-r4kvn [747.790813ms]
Feb 28 09:01:46.054: INFO: Created: latency-svc-x6f6p
Feb 28 09:01:46.087: INFO: Got endpoints: latency-svc-h45mb [750.172232ms]
Feb 28 09:01:46.102: INFO: Created: latency-svc-ntlnr
Feb 28 09:01:46.138: INFO: Got endpoints: latency-svc-95gvb [750.945812ms]
Feb 28 09:01:46.154: INFO: Created: latency-svc-6h6rw
Feb 28 09:01:46.188: INFO: Got endpoints: latency-svc-4qfcj [712.408377ms]
Feb 28 09:01:46.202: INFO: Created: latency-svc-h828k
Feb 28 09:01:46.237: INFO: Got endpoints: latency-svc-m64lv [748.93904ms]
Feb 28 09:01:46.248: INFO: Created: latency-svc-8zpvr
Feb 28 09:01:46.289: INFO: Got endpoints: latency-svc-jz96f [696.613878ms]
Feb 28 09:01:46.304: INFO: Created: latency-svc-wdkrr
Feb 28 09:01:46.337: INFO: Got endpoints: latency-svc-tl8nv [745.136511ms]
Feb 28 09:01:46.352: INFO: Created: latency-svc-rht68
Feb 28 09:01:46.387: INFO: Got endpoints: latency-svc-5mcp5 [742.374116ms]
Feb 28 09:01:46.399: INFO: Created: latency-svc-bfz97
Feb 28 09:01:46.437: INFO: Got endpoints: latency-svc-xk2x2 [749.501929ms]
Feb 28 09:01:46.450: INFO: Created: latency-svc-2dct6
Feb 28 09:01:46.488: INFO: Got endpoints: latency-svc-7jrnw [746.427049ms]
Feb 28 09:01:46.501: INFO: Created: latency-svc-j87ct
Feb 28 09:01:46.537: INFO: Got endpoints: latency-svc-hhj6g [749.004688ms]
Feb 28 09:01:46.549: INFO: Created: latency-svc-rphtn
Feb 28 09:01:46.587: INFO: Got endpoints: latency-svc-vp7sf [749.817035ms]
Feb 28 09:01:46.600: INFO: Created: latency-svc-sv8cd
Feb 28 09:01:46.637: INFO: Got endpoints: latency-svc-4vcd4 [749.635109ms]
Feb 28 09:01:46.649: INFO: Created: latency-svc-58wct
Feb 28 09:01:46.687: INFO: Got endpoints: latency-svc-mgrw5 [747.394138ms]
Feb 28 09:01:46.704: INFO: Created: latency-svc-7vt94
Feb 28 09:01:46.737: INFO: Got endpoints: latency-svc-gqq46 [742.740859ms]
Feb 28 09:01:46.753: INFO: Created: latency-svc-cv226
Feb 28 09:01:46.787: INFO: Got endpoints: latency-svc-x6f6p [747.306534ms]
Feb 28 09:01:46.800: INFO: Created: latency-svc-jpbjr
Feb 28 09:01:46.839: INFO: Got endpoints: latency-svc-ntlnr [751.734352ms]
Feb 28 09:01:46.854: INFO: Created: latency-svc-ntjqf
Feb 28 09:01:46.888: INFO: Got endpoints: latency-svc-6h6rw [749.87414ms]
Feb 28 09:01:46.906: INFO: Created: latency-svc-74xxt
Feb 28 09:01:46.937: INFO: Got endpoints: latency-svc-h828k [749.770175ms]
Feb 28 09:01:46.952: INFO: Created: latency-svc-l7wbf
Feb 28 09:01:46.987: INFO: Got endpoints: latency-svc-8zpvr [749.505526ms]
Feb 28 09:01:47.008: INFO: Created: latency-svc-2jdpt
Feb 28 09:01:47.037: INFO: Got endpoints: latency-svc-wdkrr [748.56187ms]
Feb 28 09:01:47.051: INFO: Created: latency-svc-4mbks
Feb 28 09:01:47.087: INFO: Got endpoints: latency-svc-rht68 [749.830392ms]
Feb 28 09:01:47.115: INFO: Created: latency-svc-p5cz4
Feb 28 09:01:47.138: INFO: Got endpoints: latency-svc-bfz97 [750.690123ms]
Feb 28 09:01:47.151: INFO: Created: latency-svc-bhq2k
Feb 28 09:01:47.187: INFO: Got endpoints: latency-svc-2dct6 [750.302691ms]
Feb 28 09:01:47.202: INFO: Created: latency-svc-wcnzs
Feb 28 09:01:47.237: INFO: Got endpoints: latency-svc-j87ct [749.354771ms]
Feb 28 09:01:47.252: INFO: Created: latency-svc-q5v8b
Feb 28 09:01:47.287: INFO: Got endpoints: latency-svc-rphtn [749.802246ms]
Feb 28 09:01:47.300: INFO: Created: latency-svc-775xg
Feb 28 09:01:47.338: INFO: Got endpoints: latency-svc-sv8cd [750.341972ms]
Feb 28 09:01:47.360: INFO: Created: latency-svc-h44t5
Feb 28 09:01:47.388: INFO: Got endpoints: latency-svc-58wct [751.119893ms]
Feb 28 09:01:47.401: INFO: Created: latency-svc-pp28q
Feb 28 09:01:47.439: INFO: Got endpoints: latency-svc-7vt94 [751.923969ms]
Feb 28 09:01:47.455: INFO: Created: latency-svc-8tg89
Feb 28 09:01:47.493: INFO: Got endpoints: latency-svc-cv226 [755.320654ms]
Feb 28 09:01:47.507: INFO: Created: latency-svc-67t5x
Feb 28 09:01:47.539: INFO: Got endpoints: latency-svc-jpbjr [751.514008ms]
Feb 28 09:01:47.560: INFO: Created: latency-svc-ptbpz
Feb 28 09:01:47.588: INFO: Got endpoints: latency-svc-ntjqf [749.439242ms]
Feb 28 09:01:47.603: INFO: Created: latency-svc-mr49r
Feb 28 09:01:47.639: INFO: Got endpoints: latency-svc-74xxt [751.321671ms]
Feb 28 09:01:47.653: INFO: Created: latency-svc-ll8t6
Feb 28 09:01:47.687: INFO: Got endpoints: latency-svc-l7wbf [749.715475ms]
Feb 28 09:01:47.699: INFO: Created: latency-svc-9r5mp
Feb 28 09:01:47.737: INFO: Got endpoints: latency-svc-2jdpt [750.503852ms]
Feb 28 09:01:47.753: INFO: Created: latency-svc-w7mwq
Feb 28 09:01:47.792: INFO: Got endpoints: latency-svc-4mbks [754.660339ms]
Feb 28 09:01:47.803: INFO: Created: latency-svc-5bxq7
Feb 28 09:01:47.838: INFO: Got endpoints: latency-svc-p5cz4 [750.447348ms]
Feb 28 09:01:47.853: INFO: Created: latency-svc-v96nd
Feb 28 09:01:47.888: INFO: Got endpoints: latency-svc-bhq2k [749.761221ms]
Feb 28 09:01:47.903: INFO: Created: latency-svc-w45cv
Feb 28 09:01:47.942: INFO: Got endpoints: latency-svc-wcnzs [754.606391ms]
Feb 28 09:01:47.956: INFO: Created: latency-svc-bwt59
Feb 28 09:01:47.990: INFO: Got endpoints: latency-svc-q5v8b [752.257254ms]
Feb 28 09:01:48.010: INFO: Created: latency-svc-xc9n5
Feb 28 09:01:48.037: INFO: Got endpoints: latency-svc-775xg [749.934248ms]
Feb 28 09:01:48.053: INFO: Created: latency-svc-k8j96
Feb 28 09:01:48.087: INFO: Got endpoints: latency-svc-h44t5 [748.825222ms]
Feb 28 09:01:48.101: INFO: Created: latency-svc-ztqws
Feb 28 09:01:48.136: INFO: Got endpoints: latency-svc-pp28q [748.294678ms]
Feb 28 09:01:48.154: INFO: Created: latency-svc-grbr7
Feb 28 09:01:48.188: INFO: Got endpoints: latency-svc-8tg89 [748.782081ms]
Feb 28 09:01:48.204: INFO: Created: latency-svc-5jfhc
Feb 28 09:01:48.292: INFO: Got endpoints: latency-svc-ptbpz [753.030846ms]
Feb 28 09:01:48.292: INFO: Got endpoints: latency-svc-67t5x [798.986495ms]
Feb 28 09:01:48.308: INFO: Created: latency-svc-dl5v6
Feb 28 09:01:48.317: INFO: Created: latency-svc-sck6q
Feb 28 09:01:48.338: INFO: Got endpoints: latency-svc-mr49r [749.055982ms]
Feb 28 09:01:48.352: INFO: Created: latency-svc-57fgv
Feb 28 09:01:48.387: INFO: Got endpoints: latency-svc-ll8t6 [748.051564ms]
Feb 28 09:01:48.437: INFO: Created: latency-svc-8rpst
Feb 28 09:01:48.438: INFO: Got endpoints: latency-svc-9r5mp [750.284858ms]
Feb 28 09:01:48.450: INFO: Created: latency-svc-bvpph
Feb 28 09:01:48.487: INFO: Got endpoints: latency-svc-w7mwq [749.530318ms]
Feb 28 09:01:48.501: INFO: Created: latency-svc-9lvsx
Feb 28 09:01:48.537: INFO: Got endpoints: latency-svc-5bxq7 [745.144507ms]
Feb 28 09:01:48.550: INFO: Created: latency-svc-6ts7q
Feb 28 09:01:48.587: INFO: Got endpoints: latency-svc-v96nd [749.725071ms]
Feb 28 09:01:48.598: INFO: Created: latency-svc-qvmv2
Feb 28 09:01:48.637: INFO: Got endpoints: latency-svc-w45cv [748.897356ms]
Feb 28 09:01:48.649: INFO: Created: latency-svc-mv95j
Feb 28 09:01:48.687: INFO: Got endpoints: latency-svc-bwt59 [745.271477ms]
Feb 28 09:01:48.699: INFO: Created: latency-svc-nl8l6
Feb 28 09:01:48.821: INFO: Got endpoints: latency-svc-xc9n5 [830.844094ms]
Feb 28 09:01:48.821: INFO: Got endpoints: latency-svc-k8j96 [783.158328ms]
Feb 28 09:01:48.835: INFO: Created: latency-svc-q9txn
Feb 28 09:01:48.836: INFO: Got endpoints: latency-svc-ztqws [749.50459ms]
Feb 28 09:01:48.848: INFO: Created: latency-svc-s5rlw
Feb 28 09:01:48.856: INFO: Created: latency-svc-9hkrv
Feb 28 09:01:48.889: INFO: Got endpoints: latency-svc-grbr7 [752.425099ms]
Feb 28 09:01:48.902: INFO: Created: latency-svc-cq4lq
Feb 28 09:01:48.937: INFO: Got endpoints: latency-svc-5jfhc [748.548586ms]
Feb 28 09:01:48.950: INFO: Created: latency-svc-xv7x2
Feb 28 09:01:48.989: INFO: Got endpoints: latency-svc-dl5v6 [696.589211ms]
Feb 28 09:01:49.005: INFO: Created: latency-svc-bvsvg
Feb 28 09:01:49.037: INFO: Got endpoints: latency-svc-sck6q [744.800744ms]
Feb 28 09:01:49.049: INFO: Created: latency-svc-f9p5b
Feb 28 09:01:49.088: INFO: Got endpoints: latency-svc-57fgv [750.58303ms]
Feb 28 09:01:49.103: INFO: Created: latency-svc-slplx
Feb 28 09:01:49.137: INFO: Got endpoints: latency-svc-8rpst [749.766279ms]
Feb 28 09:01:49.151: INFO: Created: latency-svc-9nmx9
Feb 28 09:01:49.187: INFO: Got endpoints: latency-svc-bvpph [749.185885ms]
Feb 28 09:01:49.208: INFO: Created: latency-svc-45djh
Feb 28 09:01:49.238: INFO: Got endpoints: latency-svc-9lvsx [751.214565ms]
Feb 28 09:01:49.254: INFO: Created: latency-svc-7498m
Feb 28 09:01:49.289: INFO: Got endpoints: latency-svc-6ts7q [751.905161ms]
Feb 28 09:01:49.306: INFO: Created: latency-svc-q4q76
Feb 28 09:01:49.337: INFO: Got endpoints: latency-svc-qvmv2 [749.222556ms]
Feb 28 09:01:49.349: INFO: Created: latency-svc-zr2l5
Feb 28 09:01:49.388: INFO: Got endpoints: latency-svc-mv95j [751.041297ms]
Feb 28 09:01:49.402: INFO: Created: latency-svc-n6trj
Feb 28 09:01:49.438: INFO: Got endpoints: latency-svc-nl8l6 [750.971013ms]
Feb 28 09:01:49.452: INFO: Created: latency-svc-xxt22
Feb 28 09:01:49.488: INFO: Got endpoints: latency-svc-q9txn [667.14256ms]
Feb 28 09:01:49.503: INFO: Created: latency-svc-bxbzg
Feb 28 09:01:49.538: INFO: Got endpoints: latency-svc-s5rlw [717.061594ms]
Feb 28 09:01:49.552: INFO: Created: latency-svc-7r7j5
Feb 28 09:01:49.589: INFO: Got endpoints: latency-svc-9hkrv [752.91385ms]
Feb 28 09:01:49.604: INFO: Created: latency-svc-v6ch7
Feb 28 09:01:49.637: INFO: Got endpoints: latency-svc-cq4lq [748.514469ms]
Feb 28 09:01:49.651: INFO: Created: latency-svc-8ltm8
Feb 28 09:01:49.687: INFO: Got endpoints: latency-svc-xv7x2 [749.805659ms]
Feb 28 09:01:49.700: INFO: Created: latency-svc-xhc66
Feb 28 09:01:49.737: INFO: Got endpoints: latency-svc-bvsvg [748.765444ms]
Feb 28 09:01:49.750: INFO: Created: latency-svc-l9lg2
Feb 28 09:01:49.786: INFO: Got endpoints: latency-svc-f9p5b [749.252663ms]
Feb 28 09:01:49.800: INFO: Created: latency-svc-7jdxv
Feb 28 09:01:49.838: INFO: Got endpoints: latency-svc-slplx [749.39603ms]
Feb 28 09:01:49.876: INFO: Created: latency-svc-m6msl
Feb 28 09:01:49.888: INFO: Got endpoints: latency-svc-9nmx9 [750.706015ms]
Feb 28 09:01:49.901: INFO: Created: latency-svc-5br78
Feb 28 09:01:49.937: INFO: Got endpoints: latency-svc-45djh [749.893859ms]
Feb 28 09:01:49.951: INFO: Created: latency-svc-lpj6h
Feb 28 09:01:49.986: INFO: Got endpoints: latency-svc-7498m [748.103278ms]
Feb 28 09:01:49.999: INFO: Created: latency-svc-78lgg
Feb 28 09:01:50.037: INFO: Got endpoints: latency-svc-q4q76 [747.779494ms]
Feb 28 09:01:50.061: INFO: Created: latency-svc-nx65t
Feb 28 09:01:50.087: INFO: Got endpoints: latency-svc-zr2l5 [750.772902ms]
Feb 28 09:01:50.104: INFO: Created: latency-svc-b99zr
Feb 28 09:01:50.139: INFO: Got endpoints: latency-svc-n6trj [751.009646ms]
Feb 28 09:01:50.152: INFO: Created: latency-svc-6xrm9
Feb 28 09:01:50.189: INFO: Got endpoints: latency-svc-xxt22 [750.564443ms]
Feb 28 09:01:50.203: INFO: Created: latency-svc-5g72b
Feb 28 09:01:50.236: INFO: Got endpoints: latency-svc-bxbzg [747.841066ms]
Feb 28 09:01:50.287: INFO: Got endpoints: latency-svc-7r7j5 [749.358612ms]
Feb 28 09:01:50.337: INFO: Got endpoints: latency-svc-v6ch7 [748.188612ms]
Feb 28 09:01:50.386: INFO: Got endpoints: latency-svc-8ltm8 [749.068546ms]
Feb 28 09:01:50.467: INFO: Got endpoints: latency-svc-xhc66 [779.882582ms]
Feb 28 09:01:50.488: INFO: Got endpoints: latency-svc-l9lg2 [750.305221ms]
Feb 28 09:01:50.538: INFO: Got endpoints: latency-svc-7jdxv [751.530531ms]
Feb 28 09:01:50.587: INFO: Got endpoints: latency-svc-m6msl [749.330255ms]
Feb 28 09:01:50.637: INFO: Got endpoints: latency-svc-5br78 [749.304754ms]
Feb 28 09:01:50.688: INFO: Got endpoints: latency-svc-lpj6h [751.19033ms]
Feb 28 09:01:50.736: INFO: Got endpoints: latency-svc-78lgg [750.049171ms]
Feb 28 09:01:50.787: INFO: Got endpoints: latency-svc-nx65t [749.80954ms]
Feb 28 09:01:50.837: INFO: Got endpoints: latency-svc-b99zr [749.782511ms]
Feb 28 09:01:50.888: INFO: Got endpoints: latency-svc-6xrm9 [748.983134ms]
Feb 28 09:01:50.937: INFO: Got endpoints: latency-svc-5g72b [747.656724ms]
Feb 28 09:01:50.937: INFO: Latencies: [20.742669ms 28.851553ms 33.989368ms 48.459018ms 49.567788ms 56.717874ms 57.244148ms 65.565119ms 66.546589ms 69.045194ms 77.114677ms 80.137953ms 88.265001ms 93.482941ms 108.251631ms 108.576597ms 110.151878ms 112.539521ms 115.942693ms 121.884673ms 125.797759ms 128.387596ms 135.553642ms 142.601559ms 158.572925ms 173.061849ms 177.55616ms 178.722818ms 186.691764ms 189.687382ms 193.896802ms 198.699747ms 216.189913ms 220.806161ms 228.851857ms 238.004218ms 240.875794ms 247.716159ms 290.56059ms 304.547538ms 304.562027ms 330.797776ms 380.284906ms 431.185335ms 481.202845ms 526.142227ms 565.808647ms 611.969332ms 646.915569ms 667.14256ms 689.504865ms 696.589211ms 696.613878ms 712.408377ms 717.061594ms 724.468067ms 726.139075ms 728.57055ms 729.158437ms 742.374116ms 742.740859ms 744.800744ms 745.136511ms 745.144507ms 745.157707ms 745.271477ms 746.064377ms 746.427049ms 747.237206ms 747.306534ms 747.394138ms 747.656724ms 747.725415ms 747.779494ms 747.790813ms 747.841066ms 748.051564ms 748.103278ms 748.155058ms 748.188612ms 748.294678ms 748.514469ms 748.548586ms 748.56187ms 748.765444ms 748.782081ms 748.808857ms 748.825222ms 748.8942ms 748.897356ms 748.93904ms 748.945324ms 748.983134ms 749.004688ms 749.016621ms 749.044876ms 749.055982ms 749.068546ms 749.185885ms 749.197913ms 749.222556ms 749.23277ms 749.252663ms 749.304754ms 749.330255ms 749.354771ms 749.358612ms 749.39603ms 749.439242ms 749.501929ms 749.50459ms 749.505526ms 749.530318ms 749.635109ms 749.673718ms 749.715475ms 749.725071ms 749.761221ms 749.766279ms 749.770175ms 749.782511ms 749.802246ms 749.805659ms 749.80954ms 749.817035ms 749.830392ms 749.87414ms 749.886827ms 749.893859ms 749.934248ms 749.938312ms 749.989528ms 750.049171ms 750.063178ms 750.090763ms 750.094413ms 750.11424ms 750.172232ms 750.218112ms 750.284858ms 750.294818ms 750.302691ms 750.305221ms 750.314513ms 750.341972ms 750.447348ms 750.503852ms 750.51263ms 750.544967ms 750.564443ms 750.58303ms 750.634713ms 750.690123ms 750.706015ms 750.731125ms 750.772902ms 750.778741ms 750.945812ms 750.964563ms 750.971013ms 751.009646ms 751.041297ms 751.119893ms 751.19033ms 751.214565ms 751.217139ms 751.321671ms 751.375468ms 751.514008ms 751.530531ms 751.567074ms 751.734352ms 751.737097ms 751.849854ms 751.905161ms 751.923969ms 751.997239ms 752.257254ms 752.425099ms 752.706344ms 752.91385ms 752.933279ms 753.030846ms 753.193389ms 753.59382ms 754.25415ms 754.606391ms 754.660339ms 755.320654ms 755.70663ms 756.396784ms 769.28318ms 769.995102ms 776.991169ms 779.882582ms 783.158328ms 788.34112ms 798.986495ms 805.660082ms 830.844094ms]
Feb 28 09:01:50.937: INFO: 50 %ile: 749.222556ms
Feb 28 09:01:50.937: INFO: 90 %ile: 752.91385ms
Feb 28 09:01:50.937: INFO: 99 %ile: 805.660082ms
Feb 28 09:01:50.937: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:01:50.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-gj9kq" for this suite.
Feb 28 09:02:08.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:09.051: INFO: namespace: e2e-tests-svc-latency-gj9kq, resource: bindings, ignored listing per whitelist
Feb 28 09:02:09.132: INFO: namespace e2e-tests-svc-latency-gj9kq deletion completed in 18.184056145s

• [SLOW TEST:29.269 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:09.132: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-78w4s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 28 09:02:11.451: INFO: Pod pod-hostip-8782c513-3b37-11e9-a616-82a6c0035a5d has hostIP: 10.250.0.8
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:02:11.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-78w4s" for this suite.
Feb 28 09:02:33.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:33.518: INFO: namespace: e2e-tests-pods-78w4s, resource: bindings, ignored listing per whitelist
Feb 28 09:02:33.631: INFO: namespace e2e-tests-pods-78w4s deletion completed in 22.174135945s

• [SLOW TEST:24.499 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:33.631: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ktm8x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 28 09:02:33.890: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:35.201: INFO: stderr: ""
Feb 28 09:02:35.201: INFO: stdout: "pod/pause created\n"
Feb 28 09:02:35.201: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 28 09:02:35.201: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-ktm8x" to be "running and ready"
Feb 28 09:02:35.204: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.762569ms
Feb 28 09:02:37.210: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009551237s
Feb 28 09:02:37.210: INFO: Pod "pause" satisfied condition "running and ready"
Feb 28 09:02:37.210: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 28 09:02:37.210: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.307: INFO: stderr: ""
Feb 28 09:02:37.307: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 28 09:02:37.307: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.394: INFO: stderr: ""
Feb 28 09:02:37.394: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 28 09:02:37.394: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.488: INFO: stderr: ""
Feb 28 09:02:37.488: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 28 09:02:37.488: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.580: INFO: stderr: ""
Feb 28 09:02:37.580: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 28 09:02:37.580: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.673: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 09:02:37.673: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 28 09:02:37.673: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-ktm8x'
Feb 28 09:02:37.767: INFO: stderr: "No resources found.\n"
Feb 28 09:02:37.767: INFO: stdout: ""
Feb 28 09:02:37.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-ktm8x -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 09:02:37.868: INFO: stderr: ""
Feb 28 09:02:37.868: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:02:37.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ktm8x" for this suite.
Feb 28 09:02:43.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:43.953: INFO: namespace: e2e-tests-kubectl-ktm8x, resource: bindings, ignored listing per whitelist
Feb 28 09:02:44.268: INFO: namespace e2e-tests-kubectl-ktm8x deletion completed in 6.394924246s

• [SLOW TEST:10.637 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:44.269: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-b9rfb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:02:44.595: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 28 09:02:49.601: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 09:02:49.601: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 09:02:51.642: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-b9rfb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-b9rfb/deployments/test-cleanup-deployment,UID:9f76cbe7-3b37-11e9-b268-0a027e1dd732,ResourceVersion:20212,Generation:1,CreationTimestamp:2019-02-28 09:02:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 09:02:49 +0000 UTC 2019-02-28 09:02:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 09:02:51 +0000 UTC 2019-02-28 09:02:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-7dbbfcf846" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 09:02:51.646: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-b9rfb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-b9rfb/replicasets/test-cleanup-deployment-7dbbfcf846,UID:9f790f31-3b37-11e9-b268-0a027e1dd732,ResourceVersion:20205,Generation:1,CreationTimestamp:2019-02-28 09:02:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9f76cbe7-3b37-11e9-b268-0a027e1dd732 0xc00214a997 0xc00214a998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 09:02:51.653: INFO: Pod "test-cleanup-deployment-7dbbfcf846-6nb4d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-6nb4d,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-b9rfb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-b9rfb/pods/test-cleanup-deployment-7dbbfcf846-6nb4d,UID:9f7a08e6-3b37-11e9-b268-0a027e1dd732,ResourceVersion:20204,Generation:0,CreationTimestamp:2019-02-28 09:02:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.253/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 9f790f31-3b37-11e9-b268-0a027e1dd732 0xc00214b3a7 0xc00214b3a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vhb9l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vhb9l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vhb9l true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214b430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214b460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:02:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:02:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:02:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:02:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.253,StartTime:2019-02-28 09:02:49 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 09:02:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://09c08f9b7db6ddad206a94d3a55ccd4df7629672d4e43f48a327c22aa61f853d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:02:51.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-b9rfb" for this suite.
Feb 28 09:02:57.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:57.746: INFO: namespace: e2e-tests-deployment-b9rfb, resource: bindings, ignored listing per whitelist
Feb 28 09:02:57.883: INFO: namespace e2e-tests-deployment-b9rfb deletion completed in 6.222594317s

• [SLOW TEST:13.614 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:02:57.883: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-tjjq5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-tjjq5
Feb 28 09:03:00.220: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-tjjq5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 09:03:00.225: INFO: Initial restart count of pod liveness-http is 0
Feb 28 09:03:12.276: INFO: Restart count of pod e2e-tests-container-probe-tjjq5/liveness-http is now 1 (12.051356426s elapsed)
Feb 28 09:03:32.376: INFO: Restart count of pod e2e-tests-container-probe-tjjq5/liveness-http is now 2 (32.150788322s elapsed)
Feb 28 09:03:52.474: INFO: Restart count of pod e2e-tests-container-probe-tjjq5/liveness-http is now 3 (52.249176505s elapsed)
Feb 28 09:04:12.549: INFO: Restart count of pod e2e-tests-container-probe-tjjq5/liveness-http is now 4 (1m12.324600631s elapsed)
Feb 28 09:05:20.779: INFO: Restart count of pod e2e-tests-container-probe-tjjq5/liveness-http is now 5 (2m20.554362188s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:20.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-tjjq5" for this suite.
Feb 28 09:05:26.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:26.898: INFO: namespace: e2e-tests-container-probe-tjjq5, resource: bindings, ignored listing per whitelist
Feb 28 09:05:27.321: INFO: namespace e2e-tests-container-probe-tjjq5 deletion completed in 6.510055193s

• [SLOW TEST:149.438 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:27.321: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-dk87x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 28 09:05:27.617: INFO: Waiting up to 5m0s for pod "client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-containers-dk87x" to be "success or failure"
Feb 28 09:05:27.625: INFO: Pod "client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.780544ms
Feb 28 09:05:29.635: INFO: Pod "client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018087881s
STEP: Saw pod success
Feb 28 09:05:29.636: INFO: Pod "client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:05:29.644: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 09:05:29.680: INFO: Waiting for pod client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:05:29.686: INFO: Pod client-containers-fda35e2a-3b37-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:29.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-dk87x" for this suite.
Feb 28 09:05:35.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:35.899: INFO: namespace: e2e-tests-containers-dk87x, resource: bindings, ignored listing per whitelist
Feb 28 09:05:36.017: INFO: namespace e2e-tests-containers-dk87x deletion completed in 6.32328246s

• [SLOW TEST:8.696 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:36.017: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-tc5ms
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 28 09:05:36.274: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 09:05:36.288: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 09:05:36.496: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 before test
Feb 28 09:05:36.512: INFO: calico-node-6z2pm from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.512: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 09:05:36.512: INFO: kube-proxy-q5pbs from kube-system started at 2019-02-28 07:18:44 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.512: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 28 09:05:36.512: INFO: node-exporter-ffrzv from kube-system started at 2019-02-28 07:18:45 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.512: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 09:05:36.512: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw before test
Feb 28 09:05:36.535: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-c79ck from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 09:05:36.535: INFO: addons-kubernetes-dashboard-6579b646c5-hzhgc from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 09:05:36.535: INFO: kube-proxy-t74jn from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 09:05:36.535: INFO: node-exporter-4fg6q from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 09:05:36.535: INFO: addons-nginx-ingress-controller-7455744d9b-vs4bz from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 09:05:36.535: INFO: coredns-67df79bbdd-7xb2x from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container coredns ready: true, restart count 0
Feb 28 09:05:36.535: INFO: vpn-shoot-9899b5654-5z86s from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 09:05:36.535: INFO: addons-kube-lego-69bbdc96b6-mrdv6 from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 09:05:36.535: INFO: metrics-server-8d5b849b7-zdqrh from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 09:05:36.535: INFO: calico-node-55zq6 from kube-system started at 2019-02-28 07:18:54 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 09:05:36.535: INFO: blackbox-exporter-86f6cf4cb7-9tckp from kube-system started at 2019-02-28 07:19:13 +0000 UTC (1 container statuses recorded)
Feb 28 09:05:36.535: INFO: 	Container blackbox-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
STEP: verifying the node has the label node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod addons-kube-lego-69bbdc96b6-mrdv6 requesting resource cpu=20m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod addons-kubernetes-dashboard-6579b646c5-hzhgc requesting resource cpu=50m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod addons-nginx-ingress-controller-7455744d9b-vs4bz requesting resource cpu=100m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-c79ck requesting resource cpu=0m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod blackbox-exporter-86f6cf4cb7-9tckp requesting resource cpu=5m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod calico-node-55zq6 requesting resource cpu=100m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod calico-node-6z2pm requesting resource cpu=100m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
Feb 28 09:05:36.600: INFO: Pod coredns-67df79bbdd-7xb2x requesting resource cpu=50m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod kube-proxy-q5pbs requesting resource cpu=20m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
Feb 28 09:05:36.600: INFO: Pod kube-proxy-t74jn requesting resource cpu=20m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod metrics-server-8d5b849b7-zdqrh requesting resource cpu=20m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod node-exporter-4fg6q requesting resource cpu=5m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
Feb 28 09:05:36.600: INFO: Pod node-exporter-ffrzv requesting resource cpu=5m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
Feb 28 09:05:36.600: INFO: Pod vpn-shoot-9899b5654-5z86s requesting resource cpu=50m on Node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02ff4c5c-3b38-11e9-a616-82a6c0035a5d.15877bd18ddbca05], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-tc5ms/filler-pod-02ff4c5c-3b38-11e9-a616-82a6c0035a5d to shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02ff4c5c-3b38-11e9-a616-82a6c0035a5d.15877bd1b6ce4296], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02ff4c5c-3b38-11e9-a616-82a6c0035a5d.15877bd1b9ef3b58], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02ff4c5c-3b38-11e9-a616-82a6c0035a5d.15877bd1c09a628d], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0300c4ec-3b38-11e9-a616-82a6c0035a5d.15877bd18e42b2b0], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-tc5ms/filler-pod-0300c4ec-3b38-11e9-a616-82a6c0035a5d to shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0300c4ec-3b38-11e9-a616-82a6c0035a5d.15877bd1ba7a319b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0300c4ec-3b38-11e9-a616-82a6c0035a5d.15877bd1bdcbf0fa], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0300c4ec-3b38-11e9-a616-82a6c0035a5d.15877bd1c3e678fc], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15877bd207a2ca2b], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-r5zcw
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-tc5ms" for this suite.
Feb 28 09:05:45.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:45.922: INFO: namespace: e2e-tests-sched-pred-tc5ms, resource: bindings, ignored listing per whitelist
Feb 28 09:05:45.973: INFO: namespace e2e-tests-sched-pred-tc5ms deletion completed in 6.26584001s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.955 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:05:45.973: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xkh29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:05:46.328: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 28 09:05:46.337: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 28 09:05:51.343: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 09:05:51.343: INFO: Creating deployment "test-rolling-update-deployment"
Feb 28 09:05:51.350: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 28 09:05:51.357: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 28 09:05:53.368: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 28 09:05:53.372: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 09:05:53.386: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-xkh29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xkh29/deployments/test-rolling-update-deployment,UID:0bc957b6-3b38-11e9-b268-0a027e1dd732,ResourceVersion:20687,Generation:1,CreationTimestamp:2019-02-28 09:05:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 09:05:51 +0000 UTC 2019-02-28 09:05:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 09:05:52 +0000 UTC 2019-02-28 09:05:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 09:05:53.389: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-xkh29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xkh29/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:0bcbbf34-3b38-11e9-b268-0a027e1dd732,ResourceVersion:20680,Generation:1,CreationTimestamp:2019-02-28 09:05:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0bc957b6-3b38-11e9-b268-0a027e1dd732 0xc0021d93d7 0xc0021d93d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 09:05:53.390: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 28 09:05:53.390: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-xkh29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xkh29/replicasets/test-rolling-update-controller,UID:08cc0832-3b38-11e9-b268-0a027e1dd732,ResourceVersion:20686,Generation:2,CreationTimestamp:2019-02-28 09:05:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0bc957b6-3b38-11e9-b268-0a027e1dd732 0xc0021d9067 0xc0021d9068}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 09:05:53.629: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-4hhdv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-4hhdv,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-xkh29,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xkh29/pods/test-rolling-update-deployment-68b55d7bc6-4hhdv,UID:0bccae3a-3b38-11e9-b268-0a027e1dd732,ResourceVersion:20679,Generation:0,CreationTimestamp:2019-02-28 09:05:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.5/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 0bcbbf34-3b38-11e9-b268-0a027e1dd732 0xc001d33637 0xc001d33638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8tpcd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8tpcd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8tpcd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d33700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d337f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:05:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:05:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:05:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 09:05:51 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.0.5,StartTime:2019-02-28 09:05:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 09:05:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ec0ee74407aa8ee833cdbc1847d359fa9f062f2cf4914819baa8b338aaf29b3b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:05:53.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xkh29" for this suite.
Feb 28 09:05:59.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:59.934: INFO: namespace: e2e-tests-deployment-xkh29, resource: bindings, ignored listing per whitelist
Feb 28 09:06:00.021: INFO: namespace e2e-tests-deployment-xkh29 deletion completed in 6.379734127s

• [SLOW TEST:14.048 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:00.021: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-h4g2l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-bq7x
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 09:06:00.304: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bq7x" in namespace "e2e-tests-subpath-h4g2l" to be "success or failure"
Feb 28 09:06:00.309: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223363ms
Feb 28 09:06:02.315: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011511471s
Feb 28 09:06:04.325: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 4.02082411s
Feb 28 09:06:06.330: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 6.026369754s
Feb 28 09:06:08.336: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 8.032017549s
Feb 28 09:06:10.343: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 10.039016545s
Feb 28 09:06:12.350: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 12.046318035s
Feb 28 09:06:14.357: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 14.053413998s
Feb 28 09:06:16.382: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 16.078531274s
Feb 28 09:06:18.388: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 18.084540328s
Feb 28 09:06:20.394: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 20.090288762s
Feb 28 09:06:22.403: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Running", Reason="", readiness=false. Elapsed: 22.09939875s
Feb 28 09:06:24.409: INFO: Pod "pod-subpath-test-projected-bq7x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.105486147s
STEP: Saw pod success
Feb 28 09:06:24.409: INFO: Pod "pod-subpath-test-projected-bq7x" satisfied condition "success or failure"
Feb 28 09:06:24.414: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-subpath-test-projected-bq7x container test-container-subpath-projected-bq7x: <nil>
STEP: delete the pod
Feb 28 09:06:24.440: INFO: Waiting for pod pod-subpath-test-projected-bq7x to disappear
Feb 28 09:06:24.445: INFO: Pod pod-subpath-test-projected-bq7x no longer exists
STEP: Deleting pod pod-subpath-test-projected-bq7x
Feb 28 09:06:24.445: INFO: Deleting pod "pod-subpath-test-projected-bq7x" in namespace "e2e-tests-subpath-h4g2l"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:24.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-h4g2l" for this suite.
Feb 28 09:06:30.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:30.572: INFO: namespace: e2e-tests-subpath-h4g2l, resource: bindings, ignored listing per whitelist
Feb 28 09:06:30.646: INFO: namespace e2e-tests-subpath-h4g2l deletion completed in 6.191258187s

• [SLOW TEST:30.625 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:30.647: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mfjj9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-235c71aa-3b38-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 09:06:30.909: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-mfjj9" to be "success or failure"
Feb 28 09:06:30.913: INFO: Pod "pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.831957ms
Feb 28 09:06:32.918: INFO: Pod "pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009059247s
STEP: Saw pod success
Feb 28 09:06:32.918: INFO: Pod "pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:06:32.922: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:06:32.958: INFO: Waiting for pod pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:06:32.961: INFO: Pod pod-projected-configmaps-235d1504-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:32.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mfjj9" for this suite.
Feb 28 09:06:38.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:39.194: INFO: namespace: e2e-tests-projected-mfjj9, resource: bindings, ignored listing per whitelist
Feb 28 09:06:39.232: INFO: namespace e2e-tests-projected-mfjj9 deletion completed in 6.264232864s

• [SLOW TEST:8.586 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:06:39.232: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qwq9h
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-287ab733-3b38-11e9-a616-82a6c0035a5d
STEP: Creating secret with name s-test-opt-upd-287ab77d-3b38-11e9-a616-82a6c0035a5d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-287ab733-3b38-11e9-a616-82a6c0035a5d
STEP: Updating secret s-test-opt-upd-287ab77d-3b38-11e9-a616-82a6c0035a5d
STEP: Creating secret with name s-test-opt-create-287ab793-3b38-11e9-a616-82a6c0035a5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:06:43.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qwq9h" for this suite.
Feb 28 09:07:05.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:06.227: INFO: namespace: e2e-tests-projected-qwq9h, resource: bindings, ignored listing per whitelist
Feb 28 09:07:06.296: INFO: namespace e2e-tests-projected-qwq9h deletion completed in 22.438087706s

• [SLOW TEST:27.064 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:06.296: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-82x2r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-xbd6
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 09:07:07.060: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xbd6" in namespace "e2e-tests-subpath-82x2r" to be "success or failure"
Feb 28 09:07:07.069: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.452382ms
Feb 28 09:07:09.074: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014108273s
Feb 28 09:07:11.080: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 4.020197282s
Feb 28 09:07:13.087: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 6.027088176s
Feb 28 09:07:15.094: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 8.03417217s
Feb 28 09:07:17.100: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 10.040400058s
Feb 28 09:07:19.107: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 12.046713022s
Feb 28 09:07:21.112: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 14.052155175s
Feb 28 09:07:23.120: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 16.060346212s
Feb 28 09:07:25.126: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 18.066310454s
Feb 28 09:07:27.132: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 20.07215764s
Feb 28 09:07:29.139: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Running", Reason="", readiness=false. Elapsed: 22.07859896s
Feb 28 09:07:31.145: INFO: Pod "pod-subpath-test-secret-xbd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084612432s
STEP: Saw pod success
Feb 28 09:07:31.145: INFO: Pod "pod-subpath-test-secret-xbd6" satisfied condition "success or failure"
Feb 28 09:07:31.151: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-subpath-test-secret-xbd6 container test-container-subpath-secret-xbd6: <nil>
STEP: delete the pod
Feb 28 09:07:31.174: INFO: Waiting for pod pod-subpath-test-secret-xbd6 to disappear
Feb 28 09:07:31.178: INFO: Pod pod-subpath-test-secret-xbd6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-xbd6
Feb 28 09:07:31.178: INFO: Deleting pod "pod-subpath-test-secret-xbd6" in namespace "e2e-tests-subpath-82x2r"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:31.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-82x2r" for this suite.
Feb 28 09:07:37.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:37.440: INFO: namespace: e2e-tests-subpath-82x2r, resource: bindings, ignored listing per whitelist
Feb 28 09:07:37.540: INFO: namespace e2e-tests-subpath-82x2r deletion completed in 6.352846653s

• [SLOW TEST:31.244 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:37.540: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-c77pn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 09:07:38.096: INFO: Waiting up to 5m0s for pod "downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-downward-api-c77pn" to be "success or failure"
Feb 28 09:07:38.104: INFO: Pod "downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.396065ms
Feb 28 09:07:40.109: INFO: Pod "downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01283319s
STEP: Saw pod success
Feb 28 09:07:40.109: INFO: Pod "downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:07:40.113: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d container dapi-container: <nil>
STEP: delete the pod
Feb 28 09:07:40.135: INFO: Waiting for pod downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:07:40.142: INFO: Pod downward-api-4b68c34c-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:40.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-c77pn" for this suite.
Feb 28 09:07:46.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:46.205: INFO: namespace: e2e-tests-downward-api-c77pn, resource: bindings, ignored listing per whitelist
Feb 28 09:07:46.369: INFO: namespace e2e-tests-downward-api-c77pn deletion completed in 6.204355008s

• [SLOW TEST:8.828 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:46.369: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dkfxc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:07:46.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-dkfxc" to be "success or failure"
Feb 28 09:07:46.889: INFO: Pod "downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.518311ms
Feb 28 09:07:48.895: INFO: Pod "downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014486325s
STEP: Saw pod success
Feb 28 09:07:48.895: INFO: Pod "downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:07:48.900: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 09:07:48.928: INFO: Waiting for pod downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:07:48.933: INFO: Pod downwardapi-volume-50a4dcb2-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:48.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dkfxc" for this suite.
Feb 28 09:07:54.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:55.292: INFO: namespace: e2e-tests-projected-dkfxc, resource: bindings, ignored listing per whitelist
Feb 28 09:07:55.348: INFO: namespace e2e-tests-projected-dkfxc deletion completed in 6.404921325s

• [SLOW TEST:8.979 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:07:55.348: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-m78q6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 09:07:55.638: INFO: Waiting up to 5m0s for pod "pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-m78q6" to be "success or failure"
Feb 28 09:07:55.645: INFO: Pod "pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.213529ms
Feb 28 09:07:57.651: INFO: Pod "pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013194124s
STEP: Saw pod success
Feb 28 09:07:57.651: INFO: Pod "pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:07:57.656: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 09:07:57.679: INFO: Waiting for pod pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:07:57.684: INFO: Pod pod-55dda2d1-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:07:57.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m78q6" for this suite.
Feb 28 09:08:03.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:03.855: INFO: namespace: e2e-tests-emptydir-m78q6, resource: bindings, ignored listing per whitelist
Feb 28 09:08:03.910: INFO: namespace e2e-tests-emptydir-m78q6 deletion completed in 6.221580419s

• [SLOW TEST:8.562 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:08:03.911: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dthz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 09:08:04.262: INFO: Waiting up to 5m0s for pod "pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-emptydir-dthz9" to be "success or failure"
Feb 28 09:08:04.266: INFO: Pod "pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285407ms
Feb 28 09:08:06.272: INFO: Pod "pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010354554s
STEP: Saw pod success
Feb 28 09:08:06.272: INFO: Pod "pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:08:06.276: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d container test-container: <nil>
STEP: delete the pod
Feb 28 09:08:06.299: INFO: Waiting for pod pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:08:06.302: INFO: Pod pod-5b018e0e-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:08:06.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dthz9" for this suite.
Feb 28 09:08:12.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:12.609: INFO: namespace: e2e-tests-emptydir-dthz9, resource: bindings, ignored listing per whitelist
Feb 28 09:08:12.700: INFO: namespace e2e-tests-emptydir-dthz9 deletion completed in 6.391958301s

• [SLOW TEST:8.790 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:08:12.700: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-sm62v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 28 09:08:13.467: INFO: created pod pod-service-account-defaultsa
Feb 28 09:08:13.467: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 28 09:08:13.474: INFO: created pod pod-service-account-mountsa
Feb 28 09:08:13.474: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 28 09:08:13.480: INFO: created pod pod-service-account-nomountsa
Feb 28 09:08:13.480: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 28 09:08:13.487: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 28 09:08:13.487: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 28 09:08:13.493: INFO: created pod pod-service-account-mountsa-mountspec
Feb 28 09:08:13.493: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 28 09:08:13.499: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 28 09:08:13.499: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 28 09:08:13.503: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 28 09:08:13.503: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 28 09:08:13.509: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 28 09:08:13.509: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 28 09:08:13.515: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 28 09:08:13.515: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:08:13.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-sm62v" for this suite.
Feb 28 09:08:35.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:35.579: INFO: namespace: e2e-tests-svcaccounts-sm62v, resource: bindings, ignored listing per whitelist
Feb 28 09:08:35.700: INFO: namespace e2e-tests-svcaccounts-sm62v deletion completed in 22.17575485s

• [SLOW TEST:23.000 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:08:35.703: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kz8dp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-6deb345c-3b38-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 09:08:36.001: INFO: Waiting up to 5m0s for pod "pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-kz8dp" to be "success or failure"
Feb 28 09:08:36.012: INFO: Pod "pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.364347ms
Feb 28 09:08:38.017: INFO: Pod "pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015963354s
STEP: Saw pod success
Feb 28 09:08:38.018: INFO: Pod "pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:08:38.022: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:08:38.044: INFO: Waiting for pod pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:08:38.047: INFO: Pod pod-secrets-6dec03ba-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:08:38.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kz8dp" for this suite.
Feb 28 09:08:44.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:44.137: INFO: namespace: e2e-tests-secrets-kz8dp, resource: bindings, ignored listing per whitelist
Feb 28 09:08:44.472: INFO: namespace e2e-tests-secrets-kz8dp deletion completed in 6.418265548s

• [SLOW TEST:8.769 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:08:44.472: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-jb2qt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-731eba6a-3b38-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume secrets
Feb 28 09:08:44.724: INFO: Waiting up to 5m0s for pod "pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-secrets-jb2qt" to be "success or failure"
Feb 28 09:08:44.731: INFO: Pod "pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726614ms
Feb 28 09:08:46.735: INFO: Pod "pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011620048s
STEP: Saw pod success
Feb 28 09:08:46.735: INFO: Pod "pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:08:46.739: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:08:46.762: INFO: Waiting for pod pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:08:46.766: INFO: Pod pod-secrets-731f7d9b-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:08:46.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jb2qt" for this suite.
Feb 28 09:08:52.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:52.826: INFO: namespace: e2e-tests-secrets-jb2qt, resource: bindings, ignored listing per whitelist
Feb 28 09:08:53.025: INFO: namespace e2e-tests-secrets-jb2qt deletion completed in 6.251635116s

• [SLOW TEST:8.554 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:08:53.026: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d5f2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 09:08:53.307: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-d5f2j'
Feb 28 09:08:53.551: INFO: stderr: ""
Feb 28 09:08:53.551: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 28 09:08:58.602: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-d5f2j -o json'
Feb 28 09:08:58.716: INFO: stderr: ""
Feb 28 09:08:58.716: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.0.21/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-28T09:08:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-d5f2j\",\n        \"resourceVersion\": \"21373\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-d5f2j/pods/e2e-test-nginx-pod\",\n        \"uid\": \"7861db7f-3b38-11e9-b268-0a027e1dd732\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hg95z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hg95z\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hg95z\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:08:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:08:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:08:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T09:08:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://0f9d89e149c578ccb49dcdedab2f3e2904e2d299df245b6973b5740aa5eb535a\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-28T09:08:54Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.0.21\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-28T09:08:53Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 28 09:08:58.716: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-d5f2j'
Feb 28 09:08:58.965: INFO: stderr: ""
Feb 28 09:08:58.965: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 28 09:08:58.971: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-00btk.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-d5f2j'
Feb 28 09:09:05.611: INFO: stderr: ""
Feb 28 09:09:05.611: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:05.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d5f2j" for this suite.
Feb 28 09:09:11.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:11.693: INFO: namespace: e2e-tests-kubectl-d5f2j, resource: bindings, ignored listing per whitelist
Feb 28 09:09:11.815: INFO: namespace e2e-tests-kubectl-d5f2j deletion completed in 6.196314557s

• [SLOW TEST:18.789 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:11.815: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-w57gv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:09:12.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-w57gv" to be "success or failure"
Feb 28 09:09:12.194: INFO: Pod "downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837006ms
Feb 28 09:09:14.204: INFO: Pod "downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022053978s
STEP: Saw pod success
Feb 28 09:09:14.204: INFO: Pod "downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:09:14.211: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d container client-container: <nil>
STEP: delete the pod
Feb 28 09:09:14.237: INFO: Waiting for pod downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:09:14.241: INFO: Pod downwardapi-volume-837b2a9b-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:14.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w57gv" for this suite.
Feb 28 09:09:20.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:20.446: INFO: namespace: e2e-tests-projected-w57gv, resource: bindings, ignored listing per whitelist
Feb 28 09:09:20.480: INFO: namespace e2e-tests-projected-w57gv deletion completed in 6.234018516s

• [SLOW TEST:8.665 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:20.480: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-5mvsp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-c2hd
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 09:09:20.752: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c2hd" in namespace "e2e-tests-subpath-5mvsp" to be "success or failure"
Feb 28 09:09:20.756: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278766ms
Feb 28 09:09:22.761: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009363757s
Feb 28 09:09:24.771: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 4.019179276s
Feb 28 09:09:26.778: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 6.026127178s
Feb 28 09:09:28.784: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 8.032045417s
Feb 28 09:09:30.793: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 10.041583503s
Feb 28 09:09:32.801: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 12.049699582s
Feb 28 09:09:34.808: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 14.056238449s
Feb 28 09:09:36.813: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 16.061913501s
Feb 28 09:09:38.821: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 18.069894474s
Feb 28 09:09:40.827: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 20.075675954s
Feb 28 09:09:42.835: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Running", Reason="", readiness=false. Elapsed: 22.083053469s
Feb 28 09:09:44.840: INFO: Pod "pod-subpath-test-configmap-c2hd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088749579s
STEP: Saw pod success
Feb 28 09:09:44.840: INFO: Pod "pod-subpath-test-configmap-c2hd" satisfied condition "success or failure"
Feb 28 09:09:44.846: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-subpath-test-configmap-c2hd container test-container-subpath-configmap-c2hd: <nil>
STEP: delete the pod
Feb 28 09:09:44.874: INFO: Waiting for pod pod-subpath-test-configmap-c2hd to disappear
Feb 28 09:09:44.878: INFO: Pod pod-subpath-test-configmap-c2hd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c2hd
Feb 28 09:09:44.878: INFO: Deleting pod "pod-subpath-test-configmap-c2hd" in namespace "e2e-tests-subpath-5mvsp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-5mvsp" for this suite.
Feb 28 09:09:50.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:50.924: INFO: namespace: e2e-tests-subpath-5mvsp, resource: bindings, ignored listing per whitelist
Feb 28 09:09:51.128: INFO: namespace e2e-tests-subpath-5mvsp deletion completed in 6.239942595s

• [SLOW TEST:30.648 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:51.129: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-dj9ql
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:09:51.404: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 28 09:09:51.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-dj9ql/daemonsets","resourceVersion":"21541"},"items":null}

Feb 28 09:09:51.417: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-dj9ql/pods","resourceVersion":"21541"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:09:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-dj9ql" for this suite.
Feb 28 09:09:57.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:57.669: INFO: namespace: e2e-tests-daemonsets-dj9ql, resource: bindings, ignored listing per whitelist
Feb 28 09:09:57.774: INFO: namespace e2e-tests-daemonsets-dj9ql deletion completed in 6.336337262s

S [SKIPPING] [6.645 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 28 09:09:51.404: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 28 09:09:57.774: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bzkjl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9ed65f21-3b38-11e9-a616-82a6c0035a5d
STEP: Creating a pod to test consume configMaps
Feb 28 09:09:58.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d" in namespace "e2e-tests-projected-bzkjl" to be "success or failure"
Feb 28 09:09:58.074: INFO: Pod "pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459755ms
Feb 28 09:10:00.079: INFO: Pod "pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009595915s
Feb 28 09:10:02.085: INFO: Pod "pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01529326s
STEP: Saw pod success
Feb 28 09:10:02.085: INFO: Pod "pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d" satisfied condition "success or failure"
Feb 28 09:10:02.089: INFO: Trying to get logs from node shoot--it--pub-os-00btk-cpu-worker-z1-7bb4dd997d-hm252 pod pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 09:10:02.115: INFO: Waiting for pod pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d to disappear
Feb 28 09:10:02.118: INFO: Pod pod-projected-configmaps-9ed71716-3b38-11e9-a616-82a6c0035a5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 28 09:10:02.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bzkjl" for this suite.
Feb 28 09:10:08.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:10:08.341: INFO: namespace: e2e-tests-projected-bzkjl, resource: bindings, ignored listing per whitelist
Feb 28 09:10:08.349: INFO: namespace e2e-tests-projected-bzkjl deletion completed in 6.226571004s

• [SLOW TEST:10.575 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSFeb 28 09:10:08.349: INFO: Running AfterSuite actions on all nodes
Feb 28 09:10:08.349: INFO: Running AfterSuite actions on node 1
Feb 28 09:10:08.349: INFO: Skipping dumping logs from cluster

Ran 200 of 2161 Specs in 5885.938 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Flaked | 0 Pending | 1961 Skipped PASS

Ginkgo ran 1 suite in 1h38m6.79790922s
Test Suite Passed
