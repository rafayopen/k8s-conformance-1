I0513 08:34:40.060561      21 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-870666073
I0513 08:34:40.060728      21 e2e.go:224] Starting e2e run "f263d467-7559-11e9-bbcc-d288ccfb79a4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1557736478 - Will randomize all specs
Will run 201 of 1946 specs

May 13 08:34:40.269: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:34:40.272: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 13 08:34:40.289: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 13 08:34:40.384: INFO: The status of Pod iam-onboarding-bptkl is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod key-management-onboarding-x7xbz is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod logging-elk-elasticsearch-curator-1557617400-hlkf5 is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod logging-elk-elasticsearch-curator-1557703800-7c8c9 is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod logging-elk-elasticsearch-pki-init-96qch is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod logging-elk-elasticsearch-tls-init-dndvq is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod logging-elk-kibana-init-j9nlg is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod oidc-client-registration-htdq6 is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: The status of Pod security-onboarding-89nmn is Succeeded, skipping waiting
May 13 08:34:40.384: INFO: 76 / 85 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 13 08:34:40.384: INFO: expected 29 pod replicas in namespace 'kube-system', 29 are Running and Ready.
May 13 08:34:40.384: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'audit-logging-fluentd-ds' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-idp' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-pap' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-pdp' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'icp-management-ingress' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'image-manager-init-certs' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'k8s-proxy' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-dns' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'logging-elk-filebeat-ds' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'metering-reader' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'monitoring-prometheus-nodeexporter' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'nginx-ingress-controller' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'platform-header' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'platform-ui' (0 seconds elapsed)
May 13 08:34:40.413: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'service-catalog-apiserver' (0 seconds elapsed)
May 13 08:34:40.413: INFO: e2e test version: v1.13.0
May 13 08:34:40.418: INFO: kube-apiserver version: v1.13.5+icp-ee
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:34:40.418: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename svcaccounts
May 13 08:34:40.754: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May 13 08:34:40.769: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-t4nrt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
May 13 08:34:41.891: INFO: Waiting up to 5m0s for pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9" in namespace "e2e-tests-svcaccounts-t4nrt" to be "success or failure"
May 13 08:34:41.901: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.173528ms
May 13 08:34:43.906: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015470275s
May 13 08:34:45.910: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019518627s
STEP: Saw pod success
May 13 08:34:45.910: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9" satisfied condition "success or failure"
May 13 08:34:45.914: INFO: Trying to get logs from node 172.16.176.226 pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9 container token-test: <nil>
STEP: delete the pod
May 13 08:34:45.941: INFO: Waiting for pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9 to disappear
May 13 08:34:45.944: INFO: Pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-n7ls9 no longer exists
STEP: Creating a pod to test consume service account root CA
May 13 08:34:46.353: INFO: Waiting up to 5m0s for pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j" in namespace "e2e-tests-svcaccounts-t4nrt" to be "success or failure"
May 13 08:34:46.357: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23335ms
May 13 08:34:48.363: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009874228s
May 13 08:34:50.368: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014352622s
STEP: Saw pod success
May 13 08:34:50.368: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j" satisfied condition "success or failure"
May 13 08:34:50.373: INFO: Trying to get logs from node 172.16.176.226 pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j container root-ca-test: <nil>
STEP: delete the pod
May 13 08:34:50.406: INFO: Waiting for pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j to disappear
May 13 08:34:50.412: INFO: Pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-42h4j no longer exists
STEP: Creating a pod to test consume service account namespace
May 13 08:34:50.826: INFO: Waiting up to 5m0s for pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d" in namespace "e2e-tests-svcaccounts-t4nrt" to be "success or failure"
May 13 08:34:50.828: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.679262ms
May 13 08:34:52.833: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006975791s
May 13 08:34:54.837: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01155339s
STEP: Saw pod success
May 13 08:34:54.837: INFO: Pod "pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d" satisfied condition "success or failure"
May 13 08:34:54.840: INFO: Trying to get logs from node 172.16.176.226 pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d container namespace-test: <nil>
STEP: delete the pod
May 13 08:34:54.932: INFO: Waiting for pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d to disappear
May 13 08:34:54.935: INFO: Pod pod-service-account-f3c7ae5c-7559-11e9-bbcc-d288ccfb79a4-2cz7d no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:34:54.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-t4nrt" for this suite.
May 13 08:35:02.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:35:03.076: INFO: namespace: e2e-tests-svcaccounts-t4nrt, resource: bindings, ignored listing per whitelist
May 13 08:35:03.102: INFO: namespace e2e-tests-svcaccounts-t4nrt deletion completed in 8.158904859s

• [SLOW TEST:22.684 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:35:03.103: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-gbvg7
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
May 13 08:35:03.346: INFO: Waiting up to 5m0s for pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-gbvg7" to be "success or failure"
May 13 08:35:03.348: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622066ms
May 13 08:35:05.351: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005606941s
May 13 08:35:07.362: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015792073s
May 13 08:35:09.366: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4": Phase="Running", Reason="", readiness=true. Elapsed: 6.020138828s
May 13 08:35:11.370: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02430188s
STEP: Saw pod success
May 13 08:35:11.370: INFO: Pod "pod-00d92687-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:35:11.374: INFO: Trying to get logs from node 172.16.177.10 pod pod-00d92687-755a-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:35:11.424: INFO: Waiting for pod pod-00d92687-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:35:11.437: INFO: Pod pod-00d92687-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:35:11.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gbvg7" for this suite.
May 13 08:35:17.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:35:17.625: INFO: namespace: e2e-tests-emptydir-gbvg7, resource: bindings, ignored listing per whitelist
May 13 08:35:17.735: INFO: namespace e2e-tests-emptydir-gbvg7 deletion completed in 6.281212855s

• [SLOW TEST:14.632 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:35:17.735: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-bpktp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
May 13 08:35:18.094: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-bpktp" to be "success or failure"
May 13 08:35:18.097: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018688ms
May 13 08:35:20.107: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013085385s
May 13 08:35:22.114: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01975289s
STEP: Saw pod success
May 13 08:35:22.114: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 13 08:35:22.118: INFO: Trying to get logs from node 172.16.176.226 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 13 08:35:22.151: INFO: Waiting for pod pod-host-path-test to disappear
May 13 08:35:22.155: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:35:22.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-bpktp" for this suite.
May 13 08:35:28.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:35:28.372: INFO: namespace: e2e-tests-hostpath-bpktp, resource: bindings, ignored listing per whitelist
May 13 08:35:28.598: INFO: namespace e2e-tests-hostpath-bpktp deletion completed in 6.438594782s

• [SLOW TEST:10.863 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:35:28.599: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9dxjw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 13 08:35:28.984: INFO: Waiting up to 5m0s for pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-9dxjw" to be "success or failure"
May 13 08:35:28.989: INFO: Pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488484ms
May 13 08:35:30.994: INFO: Pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262206s
May 13 08:35:32.999: INFO: Pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013701944s
May 13 08:35:35.006: INFO: Pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021231777s
STEP: Saw pod success
May 13 08:35:35.006: INFO: Pod "pod-1018830f-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:35:35.008: INFO: Trying to get logs from node 172.16.177.10 pod pod-1018830f-755a-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:35:35.060: INFO: Waiting for pod pod-1018830f-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:35:35.063: INFO: Pod pod-1018830f-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:35:35.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9dxjw" for this suite.
May 13 08:35:41.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:35:41.237: INFO: namespace: e2e-tests-emptydir-9dxjw, resource: bindings, ignored listing per whitelist
May 13 08:35:41.351: INFO: namespace e2e-tests-emptydir-9dxjw deletion completed in 6.283871823s

• [SLOW TEST:12.753 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:35:41.352: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-nxfpf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0513 08:35:47.721604      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 08:35:47.721: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:35:47.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nxfpf" for this suite.
May 13 08:35:53.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:35:54.010: INFO: namespace: e2e-tests-gc-nxfpf, resource: bindings, ignored listing per whitelist
May 13 08:35:54.047: INFO: namespace e2e-tests-gc-nxfpf deletion completed in 6.31912776s

• [SLOW TEST:12.695 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:35:54.049: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vwf6m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
May 13 08:35:54.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 --namespace=e2e-tests-kubectl-vwf6m run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 13 08:35:57.544: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 13 08:35:57.544: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:35:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vwf6m" for this suite.
May 13 08:36:07.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:36:08.496: INFO: namespace: e2e-tests-kubectl-vwf6m, resource: bindings, ignored listing per whitelist
May 13 08:36:08.522: INFO: namespace e2e-tests-kubectl-vwf6m deletion completed in 8.96318805s

• [SLOW TEST:14.473 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:36:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-m2qkl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:36:09.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-m2qkl" for this suite.
May 13 08:36:15.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:36:15.298: INFO: namespace: e2e-tests-kubelet-test-m2qkl, resource: bindings, ignored listing per whitelist
May 13 08:36:15.415: INFO: namespace e2e-tests-kubelet-test-m2qkl deletion completed in 6.374850937s

• [SLOW TEST:6.892 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:36:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-x729h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 13 08:36:15.642: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:36:20.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-x729h" for this suite.
May 13 08:36:44.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:36:44.311: INFO: namespace: e2e-tests-init-container-x729h, resource: bindings, ignored listing per whitelist
May 13 08:36:44.418: INFO: namespace e2e-tests-init-container-x729h deletion completed in 24.225563088s

• [SLOW TEST:29.003 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:36:44.419: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fwbxh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 13 08:36:44.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:45.003: INFO: stderr: ""
May 13 08:36:45.003: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 08:36:45.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:45.129: INFO: stderr: ""
May 13 08:36:45.129: INFO: stdout: "update-demo-nautilus-lh5md update-demo-nautilus-vvgt6 "
May 13 08:36:45.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-lh5md -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:45.242: INFO: stderr: ""
May 13 08:36:45.242: INFO: stdout: ""
May 13 08:36:45.242: INFO: update-demo-nautilus-lh5md is created but not running
May 13 08:36:50.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:50.361: INFO: stderr: ""
May 13 08:36:50.361: INFO: stdout: "update-demo-nautilus-lh5md update-demo-nautilus-vvgt6 "
May 13 08:36:50.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-lh5md -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:50.539: INFO: stderr: ""
May 13 08:36:50.539: INFO: stdout: "true"
May 13 08:36:50.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-lh5md -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:50.730: INFO: stderr: ""
May 13 08:36:50.730: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 08:36:50.730: INFO: validating pod update-demo-nautilus-lh5md
May 13 08:36:50.737: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 08:36:50.738: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 08:36:50.738: INFO: update-demo-nautilus-lh5md is verified up and running
May 13 08:36:50.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-vvgt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:50.882: INFO: stderr: ""
May 13 08:36:50.882: INFO: stdout: ""
May 13 08:36:50.882: INFO: update-demo-nautilus-vvgt6 is created but not running
May 13 08:36:55.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.086: INFO: stderr: ""
May 13 08:36:56.086: INFO: stdout: "update-demo-nautilus-lh5md update-demo-nautilus-vvgt6 "
May 13 08:36:56.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-lh5md -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.226: INFO: stderr: ""
May 13 08:36:56.226: INFO: stdout: "true"
May 13 08:36:56.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-lh5md -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.378: INFO: stderr: ""
May 13 08:36:56.378: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 08:36:56.378: INFO: validating pod update-demo-nautilus-lh5md
May 13 08:36:56.383: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 08:36:56.383: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 08:36:56.383: INFO: update-demo-nautilus-lh5md is verified up and running
May 13 08:36:56.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-vvgt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.519: INFO: stderr: ""
May 13 08:36:56.519: INFO: stdout: "true"
May 13 08:36:56.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-vvgt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.645: INFO: stderr: ""
May 13 08:36:56.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 08:36:56.645: INFO: validating pod update-demo-nautilus-vvgt6
May 13 08:36:56.653: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 08:36:56.653: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 08:36:56.653: INFO: update-demo-nautilus-vvgt6 is verified up and running
STEP: using delete to clean up resources
May 13 08:36:56.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.764: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 08:36:56.764: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 13 08:36:56.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-fwbxh'
May 13 08:36:56.881: INFO: stderr: "No resources found.\n"
May 13 08:36:56.881: INFO: stdout: ""
May 13 08:36:56.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -l name=update-demo --namespace=e2e-tests-kubectl-fwbxh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 08:36:56.996: INFO: stderr: ""
May 13 08:36:56.996: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:36:56.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fwbxh" for this suite.
May 13 08:37:21.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:37:21.170: INFO: namespace: e2e-tests-kubectl-fwbxh, resource: bindings, ignored listing per whitelist
May 13 08:37:21.286: INFO: namespace e2e-tests-kubectl-fwbxh deletion completed in 24.282036901s

• [SLOW TEST:36.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:37:21.288: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jbt9n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 08:37:21.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 version --client'
May 13 08:37:21.644: INFO: stderr: ""
May 13 08:37:21.644: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 13 08:37:21.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-jbt9n'
May 13 08:37:22.024: INFO: stderr: ""
May 13 08:37:22.024: INFO: stdout: "replicationcontroller/redis-master created\n"
May 13 08:37:22.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-jbt9n'
May 13 08:37:22.311: INFO: stderr: ""
May 13 08:37:22.311: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 08:37:23.316: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:23.316: INFO: Found 0 / 1
May 13 08:37:24.316: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:24.316: INFO: Found 0 / 1
May 13 08:37:25.317: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:25.317: INFO: Found 0 / 1
May 13 08:37:26.316: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:26.316: INFO: Found 0 / 1
May 13 08:37:27.316: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:27.316: INFO: Found 0 / 1
May 13 08:37:28.320: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:28.321: INFO: Found 0 / 1
May 13 08:37:29.316: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:29.316: INFO: Found 1 / 1
May 13 08:37:29.316: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 08:37:29.321: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:37:29.321: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 08:37:29.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 describe pod redis-master-nbd6k --namespace=e2e-tests-kubectl-jbt9n'
May 13 08:37:29.514: INFO: stderr: ""
May 13 08:37:29.514: INFO: stdout: "Name:               redis-master-nbd6k\nNamespace:          e2e-tests-kubectl-jbt9n\nPriority:           0\nPriorityClassName:  <none>\nNode:               172.16.177.10/172.16.177.10\nStart Time:         Mon, 13 May 2019 08:36:27 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 10.1.22.248\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://042854a9c3de3c7290ba9aca81e496e07fee4372c7858dd8e0e623cee1c0d0bb\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 13 May 2019 08:36:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5rsf7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5rsf7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5rsf7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                    Message\n  ----    ------     ----  ----                    -------\n  Normal  Pulling    61s   kubelet, 172.16.177.10  pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     56s   kubelet, 172.16.177.10  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    56s   kubelet, 172.16.177.10  Created container\n  Normal  Started    55s   kubelet, 172.16.177.10  Started container\n  Normal  Scheduled  7s    default-scheduler       Successfully assigned e2e-tests-kubectl-jbt9n/redis-master-nbd6k to 172.16.177.10\n"
May 13 08:37:29.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 describe rc redis-master --namespace=e2e-tests-kubectl-jbt9n'
May 13 08:37:29.672: INFO: stderr: ""
May 13 08:37:29.672: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-jbt9n\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-nbd6k\n"
May 13 08:37:29.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 describe service redis-master --namespace=e2e-tests-kubectl-jbt9n'
May 13 08:37:29.883: INFO: stderr: ""
May 13 08:37:29.883: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-jbt9n\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.194.134\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.1.22.248:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 13 08:37:29.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 describe node 172.16.173.202'
May 13 08:37:30.127: INFO: stderr: ""
May 13 08:37:30.127: INFO: stdout: "Name:               172.16.173.202\nRoles:              etcd,master,proxy\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    etcd=true\n                    kubernetes.io/hostname=172.16.173.202\n                    master=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/proxy=true\n                    proxy=true\n                    role=master\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 08 May 2019 03:22:21 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 13 May 2019 08:37:29 +0000   Wed, 08 May 2019 03:22:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 13 May 2019 08:37:29 +0000   Wed, 08 May 2019 03:22:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 13 May 2019 08:37:29 +0000   Wed, 08 May 2019 03:22:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 13 May 2019 08:37:29 +0000   Sun, 12 May 2019 14:18:30 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.173.202\n  Hostname:    172.16.173.202\nCapacity:\n cpu:                8\n ephemeral-storage:  249436164Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16424812Ki\n pods:               80\nAllocatable:\n cpu:                7600m\n ephemeral-storage:  247236612Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             15273836Ki\n pods:               80\nSystem Info:\n Machine ID:                 428e44fb1ec74efba5d4e3ca11fa2ac9\n System UUID:                15CA1C89-A357-4416-9470-DED2A136E99D\n Boot ID:                    1f68050f-95ee-4655-a814-78fe8e5fb1e9\n Kernel Version:             4.15.0-39-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.2\n Kubelet Version:            v1.13.5+icp-ee\n Kube-Proxy Version:         v1.13.5+icp-ee\nNon-terminated Pods:         (35 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------   ---------------  -------------  ---\n  cert-manager               cert-manager-ibm-cert-manager-7775495cb4-n84nn             0 (0%)        0 (0%)       150Mi (1%)       300Mi (2%)     5d5h\n  heptio-sonobuoy            sonobuoy-e2e-job-9886345f531c45a5                          0 (0%)        0 (0%)       0 (0%)           0 (0%)         3m31s\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fv8lz    0 (0%)        0 (0%)       0 (0%)           0 (0%)         3m29s\n  kube-system                audit-logging-fluentd-ds-lr8k9                             300m (3%)     300m (3%)    512Mi (3%)       512Mi (3%)     5d5h\n  kube-system                auth-idp-cqjpn                                             300m (3%)     3200m (42%)  768Mi (5%)       3584Mi (24%)   5d5h\n  kube-system                auth-pap-9bjv2                                             150m (1%)     1200m (15%)  456Mi (3%)       1536Mi (10%)   5d5h\n  kube-system                auth-pdp-82gc5                                             600m (7%)     950m (12%)   768Mi (5%)       2012Mi (13%)   5d5h\n  kube-system                calico-kube-controllers-7f949c47f-sgggp                    250m (3%)     0 (0%)       100Mi (0%)       0 (0%)         5d5h\n  kube-system                calico-node-krfk6                                          250m (3%)     0 (0%)       100Mi (0%)       0 (0%)         5d5h\n  kube-system                default-http-backend-567686995f-psknt                      20m (0%)      0 (0%)       64Mi (0%)        0 (0%)         5d5h\n  kube-system                helm-api-67456c54d4-rf5rv                                  350m (4%)     550m (7%)    556Mi (3%)       656Mi (4%)     5d5h\n  kube-system                helm-repo-548dbc4c5c-lj4r2                                 150m (1%)     200m (2%)    640Mi (4%)       640Mi (4%)     5d5h\n  kube-system                ibmcloud-image-enforcement-7c7d9688-tqjm9                  128m (1%)     256m (3%)    128Mi (0%)       256Mi (1%)     5d5h\n  kube-system                icp-management-ingress-8jgdq                               200m (2%)     0 (0%)       256Mi (1%)       0 (0%)         5d5h\n  kube-system                icp-mongodb-0                                              0 (0%)        0 (0%)       4352Mi (29%)     8448Mi (56%)   5d5h\n  kube-system                image-manager-0                                            110m (1%)     0 (0%)       192Mi (1%)       0 (0%)         5d5h\n  kube-system                image-manager-init-certs-5cjmr                             10m (0%)      0 (0%)       64Mi (0%)        0 (0%)         5d5h\n  kube-system                k8s-etcd-172.16.173.202                                    0 (0%)        0 (0%)       0 (0%)           0 (0%)         5d5h\n  kube-system                k8s-kmsplugin-172.16.173.202                               5m (0%)       25m (0%)     10Mi (0%)        40Mi (0%)      5d5h\n  kube-system                k8s-master-172.16.173.202                                  0 (0%)        0 (0%)       0 (0%)           0 (0%)         5d5h\n  kube-system                k8s-proxy-kxd6g                                            0 (0%)        0 (0%)       0 (0%)           0 (0%)         5d5h\n  kube-system                kube-dns-28wg9                                             100m (1%)     0 (0%)       70Mi (0%)        170Mi (1%)     5d5h\n  kube-system                logging-elk-filebeat-ds-tlc56                              0 (0%)        0 (0%)       64Mi (0%)        256Mi (1%)     5d5h\n  kube-system                metering-reader-46vqp                                      250m (3%)     500m (6%)    512Mi (3%)       512Mi (3%)     5d5h\n  kube-system                mgmt-repo-79f95d66b8-lp7sf                                 150m (1%)     200m (2%)    640Mi (4%)       640Mi (4%)     5d5h\n  kube-system                monitoring-prometheus-nodeexporter-lwpx9                   0 (0%)        0 (0%)       128Mi (0%)       512Mi (3%)     5d5h\n  kube-system                nginx-ingress-controller-vf97z                             50m (0%)      0 (0%)       256Mi (1%)       0 (0%)         5d5h\n  kube-system                nvidia-device-plugin-vwsvk                                 150m (1%)     0 (0%)       0 (0%)           0 (0%)         5d5h\n  kube-system                platform-api-994c46799-5f4ml                               50m (0%)      500m (6%)    96Mi (0%)        512Mi (3%)     5d5h\n  kube-system                platform-header-5hnzj                                      300m (3%)     300m (3%)    256Mi (1%)       256Mi (1%)     5d5h\n  kube-system                platform-ui-sfdsp                                          300m (3%)     300m (3%)    256Mi (1%)       256Mi (1%)     5d5h\n  kube-system                secret-watcher-54fd9cfc6d-gw9pw                            0 (0%)        0 (0%)       0 (0%)           0 (0%)         5d5h\n  kube-system                service-catalog-apiserver-9dt69                            100m (1%)     100m (1%)    20Mi (0%)        200Mi (1%)     5d5h\n  kube-system                service-catalog-controller-manager-88d749c7d-6xqsk         100m (1%)     100m (1%)    20Mi (0%)        200Mi (1%)     5d5h\n  kube-system                tiller-deploy-57774fdbd5-t4zqz                             100m (1%)     0 (0%)       128Mi (0%)       0 (0%)         5d5h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                4473m (58%)    8681m (114%)\n  memory             11562Mi (77%)  21498Mi (144%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
May 13 08:37:30.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 describe namespace e2e-tests-kubectl-jbt9n'
May 13 08:37:30.301: INFO: stderr: ""
May 13 08:37:30.301: INFO: stdout: "Name:         e2e-tests-kubectl-jbt9n\nLabels:       e2e-framework=kubectl\n              e2e-run=f263d467-7559-11e9-bbcc-d288ccfb79a4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:37:30.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jbt9n" for this suite.
May 13 08:37:54.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:37:54.369: INFO: namespace: e2e-tests-kubectl-jbt9n, resource: bindings, ignored listing per whitelist
May 13 08:37:54.555: INFO: namespace e2e-tests-kubectl-jbt9n deletion completed in 24.245363957s

• [SLOW TEST:33.267 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:37:54.555: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-pb7hr
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-6714a59f-755a-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:37:58.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pb7hr" for this suite.
May 13 08:38:22.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:38:23.211: INFO: namespace: e2e-tests-configmap-pb7hr, resource: bindings, ignored listing per whitelist
May 13 08:38:23.257: INFO: namespace e2e-tests-configmap-pb7hr deletion completed in 24.308798139s

• [SLOW TEST:28.702 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:38:23.258: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-hzbnb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 13 08:38:23.523: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:38:31.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-hzbnb" for this suite.
May 13 08:38:37.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:38:37.869: INFO: namespace: e2e-tests-init-container-hzbnb, resource: bindings, ignored listing per whitelist
May 13 08:38:38.110: INFO: namespace e2e-tests-init-container-hzbnb deletion completed in 6.353503326s

• [SLOW TEST:14.852 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:38:38.110: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-dkw2r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-dkw2r
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-dkw2r
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-dkw2r
May 13 08:38:38.417: INFO: Found 0 stateful pods, waiting for 1
May 13 08:38:48.422: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 13 08:38:48.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 08:38:48.722: INFO: stderr: ""
May 13 08:38:48.723: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 08:38:48.723: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 08:38:48.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 13 08:38:58.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 08:38:58.735: INFO: Waiting for statefulset status.replicas updated to 0
May 13 08:38:58.799: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999263s
May 13 08:38:59.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995219204s
May 13 08:39:00.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985781728s
May 13 08:39:01.817: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981871322s
May 13 08:39:02.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977572569s
May 13 08:39:03.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.958305315s
May 13 08:39:04.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951611174s
May 13 08:39:05.851: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947327614s
May 13 08:39:06.857: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.944218045s
May 13 08:39:07.861: INFO: Verifying statefulset ss doesn't scale past 1 for another 938.494759ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-dkw2r
May 13 08:39:08.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 08:39:09.311: INFO: stderr: ""
May 13 08:39:09.311: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 08:39:09.313: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 08:39:09.318: INFO: Found 1 stateful pods, waiting for 3
May 13 08:39:19.324: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 08:39:19.324: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 08:39:19.324: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 13 08:39:19.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 08:39:19.762: INFO: stderr: ""
May 13 08:39:19.762: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 08:39:19.762: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 08:39:19.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 08:39:20.194: INFO: stderr: ""
May 13 08:39:20.194: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 08:39:20.194: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 08:39:20.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 08:39:20.717: INFO: stderr: ""
May 13 08:39:20.717: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 08:39:20.717: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 08:39:20.717: INFO: Waiting for statefulset status.replicas updated to 0
May 13 08:39:20.733: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 13 08:39:30.759: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 08:39:30.759: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 13 08:39:30.759: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 13 08:39:30.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999934s
May 13 08:39:31.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993685704s
May 13 08:39:32.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988593224s
May 13 08:39:33.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983061376s
May 13 08:39:34.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976568648s
May 13 08:39:35.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971271181s
May 13 08:39:36.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965604466s
May 13 08:39:37.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961504191s
May 13 08:39:38.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957768525s
May 13 08:39:39.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.001189ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-dkw2r
May 13 08:39:40.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 08:39:41.193: INFO: stderr: ""
May 13 08:39:41.193: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 08:39:41.193: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 08:39:41.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 08:39:41.488: INFO: stderr: ""
May 13 08:39:41.488: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 08:39:41.488: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 08:39:41.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-dkw2r ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 08:39:41.869: INFO: stderr: ""
May 13 08:39:41.869: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 08:39:41.869: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 08:39:41.869: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 08:40:11.896: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dkw2r
May 13 08:40:11.900: INFO: Scaling statefulset ss to 0
May 13 08:40:11.913: INFO: Waiting for statefulset status.replicas updated to 0
May 13 08:40:11.915: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:40:11.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dkw2r" for this suite.
May 13 08:40:17.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:40:18.110: INFO: namespace: e2e-tests-statefulset-dkw2r, resource: bindings, ignored listing per whitelist
May 13 08:40:18.164: INFO: namespace e2e-tests-statefulset-dkw2r deletion completed in 6.229070024s

• [SLOW TEST:100.054 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:40:18.170: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4ffdt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 08:40:18.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-4ffdt" to be "success or failure"
May 13 08:40:18.521: INFO: Pod "downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.968416ms
May 13 08:40:20.525: INFO: Pod "downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006900441s
May 13 08:40:22.532: INFO: Pod "downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013052802s
STEP: Saw pod success
May 13 08:40:22.532: INFO: Pod "downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:40:22.549: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 08:40:22.582: INFO: Waiting for pod downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:40:22.588: INFO: Pod downwardapi-volume-bca2ed43-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:40:22.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4ffdt" for this suite.
May 13 08:40:28.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:40:28.799: INFO: namespace: e2e-tests-downward-api-4ffdt, resource: bindings, ignored listing per whitelist
May 13 08:40:28.912: INFO: namespace e2e-tests-downward-api-4ffdt deletion completed in 6.315935948s

• [SLOW TEST:10.742 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:40:28.912: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7f7tm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 13 08:40:29.178: INFO: Waiting up to 5m0s for pod "pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-7f7tm" to be "success or failure"
May 13 08:40:29.181: INFO: Pod "pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367858ms
May 13 08:40:31.184: INFO: Pod "pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006763381s
STEP: Saw pod success
May 13 08:40:31.185: INFO: Pod "pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:40:31.187: INFO: Trying to get logs from node 172.16.176.226 pod pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:40:31.207: INFO: Waiting for pod pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:40:31.211: INFO: Pod pod-c309d9b5-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:40:31.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7f7tm" for this suite.
May 13 08:40:37.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:40:37.335: INFO: namespace: e2e-tests-emptydir-7f7tm, resource: bindings, ignored listing per whitelist
May 13 08:40:37.392: INFO: namespace e2e-tests-emptydir-7f7tm deletion completed in 6.173342871s

• [SLOW TEST:8.480 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:40:37.393: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-fxmqx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c817356d-755a-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 08:40:37.679: INFO: Waiting up to 5m0s for pod "pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-fxmqx" to be "success or failure"
May 13 08:40:37.682: INFO: Pod "pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796834ms
May 13 08:40:39.686: INFO: Pod "pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006341415s
May 13 08:40:41.689: INFO: Pod "pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009810267s
STEP: Saw pod success
May 13 08:40:41.689: INFO: Pod "pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:40:41.692: INFO: Trying to get logs from node 172.16.177.10 pod pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 08:40:41.722: INFO: Waiting for pod pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:40:41.725: INFO: Pod pod-secrets-c817ce89-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:40:41.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fxmqx" for this suite.
May 13 08:40:47.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:40:47.799: INFO: namespace: e2e-tests-secrets-fxmqx, resource: bindings, ignored listing per whitelist
May 13 08:40:47.922: INFO: namespace e2e-tests-secrets-fxmqx deletion completed in 6.19285756s

• [SLOW TEST:10.529 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:40:47.922: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-5x2lr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
May 13 08:40:52.218: INFO: Pod pod-hostip-ce5ca35e-755a-11e9-bbcc-d288ccfb79a4 has hostIP: 172.16.176.226
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:40:52.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5x2lr" for this suite.
May 13 08:41:16.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:41:16.690: INFO: namespace: e2e-tests-pods-5x2lr, resource: bindings, ignored listing per whitelist
May 13 08:41:16.803: INFO: namespace e2e-tests-pods-5x2lr deletion completed in 24.579676484s

• [SLOW TEST:28.881 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:41:16.805: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hhdjc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-df966c94-755a-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 08:41:17.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-hhdjc" to be "success or failure"
May 13 08:41:17.088: INFO: Pod "pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248356ms
May 13 08:41:19.092: INFO: Pod "pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007146825s
May 13 08:41:21.117: INFO: Pod "pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032246732s
STEP: Saw pod success
May 13 08:41:21.117: INFO: Pod "pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:41:21.123: INFO: Trying to get logs from node 172.16.177.10 pod pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 08:41:21.153: INFO: Waiting for pod pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:41:21.170: INFO: Pod pod-configmaps-df9711ba-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:41:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hhdjc" for this suite.
May 13 08:41:27.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:41:27.390: INFO: namespace: e2e-tests-configmap-hhdjc, resource: bindings, ignored listing per whitelist
May 13 08:41:27.423: INFO: namespace e2e-tests-configmap-hhdjc deletion completed in 6.245300501s

• [SLOW TEST:10.619 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:41:27.431: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-mj8ml
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0513 08:41:37.829377      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 08:41:37.829: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:41:37.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mj8ml" for this suite.
May 13 08:41:43.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:41:43.920: INFO: namespace: e2e-tests-gc-mj8ml, resource: bindings, ignored listing per whitelist
May 13 08:41:44.008: INFO: namespace e2e-tests-gc-mj8ml deletion completed in 6.173609488s

• [SLOW TEST:16.578 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:41:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ddxgq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-efd176a5-755a-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 08:41:44.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-ddxgq" to be "success or failure"
May 13 08:41:44.292: INFO: Pod "pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00895ms
May 13 08:41:46.309: INFO: Pod "pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019844186s
STEP: Saw pod success
May 13 08:41:46.309: INFO: Pod "pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:41:46.318: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 08:41:46.351: INFO: Waiting for pod pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:41:46.354: INFO: Pod pod-configmaps-efd33a91-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:41:46.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ddxgq" for this suite.
May 13 08:41:52.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:41:52.478: INFO: namespace: e2e-tests-configmap-ddxgq, resource: bindings, ignored listing per whitelist
May 13 08:41:52.540: INFO: namespace e2e-tests-configmap-ddxgq deletion completed in 6.178603011s

• [SLOW TEST:8.532 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:41:52.541: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-vf5pc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:41:56.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-vf5pc" for this suite.
May 13 08:42:02.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:42:03.027: INFO: namespace: e2e-tests-kubelet-test-vf5pc, resource: bindings, ignored listing per whitelist
May 13 08:42:03.081: INFO: namespace e2e-tests-kubelet-test-vf5pc deletion completed in 6.289432802s

• [SLOW TEST:10.541 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:42:03.082: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-j24cc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 08:42:03.395: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-j24cc" to be "success or failure"
May 13 08:42:03.398: INFO: Pod "downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.384475ms
May 13 08:42:05.402: INFO: Pod "downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007502588s
May 13 08:42:07.407: INFO: Pod "downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011702788s
STEP: Saw pod success
May 13 08:42:07.407: INFO: Pod "downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:42:07.411: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 08:42:07.445: INFO: Waiting for pod downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:42:07.449: INFO: Pod downwardapi-volume-fb2fc160-755a-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:42:07.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j24cc" for this suite.
May 13 08:42:13.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:42:13.793: INFO: namespace: e2e-tests-projected-j24cc, resource: bindings, ignored listing per whitelist
May 13 08:42:13.832: INFO: namespace e2e-tests-projected-j24cc deletion completed in 6.377134238s

• [SLOW TEST:10.750 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:42:13.834: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-ktwtw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ktwtw
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 08:42:14.065: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 08:42:56.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.255:8080/dial?request=hostName&protocol=udp&host=10.1.209.38&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktwtw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:42:56.234: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:42:56.426: INFO: Waiting for endpoints: map[]
May 13 08:42:56.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.255:8080/dial?request=hostName&protocol=udp&host=10.1.22.198&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktwtw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:42:56.433: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:42:56.633: INFO: Waiting for endpoints: map[]
May 13 08:42:56.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.255:8080/dial?request=hostName&protocol=udp&host=10.1.15.52&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktwtw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:42:56.636: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:42:56.793: INFO: Waiting for endpoints: map[]
May 13 08:42:56.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.255:8080/dial?request=hostName&protocol=udp&host=10.1.197.247&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktwtw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:42:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:42:56.956: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:42:56.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ktwtw" for this suite.
May 13 08:43:20.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:43:21.216: INFO: namespace: e2e-tests-pod-network-test-ktwtw, resource: bindings, ignored listing per whitelist
May 13 08:43:21.226: INFO: namespace e2e-tests-pod-network-test-ktwtw deletion completed in 24.263213069s

• [SLOW TEST:67.393 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:43:21.228: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-fqmll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 13 08:43:21.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844856,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 08:43:21.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844856,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 13 08:43:31.505: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844875,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 13 08:43:31.505: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844875,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 13 08:43:41.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844894,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 08:43:41.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844894,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 13 08:43:51.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844912,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 08:43:51.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-a,UID:29c6e8b9-755b-11e9-b9b7-00163e01adca,ResourceVersion:844912,Generation:0,CreationTimestamp:2019-05-13 08:43:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 13 08:44:01.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-b,UID:41a5e4df-755b-11e9-b9b7-00163e01adca,ResourceVersion:844929,Generation:0,CreationTimestamp:2019-05-13 08:44:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 08:44:01.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-b,UID:41a5e4df-755b-11e9-b9b7-00163e01adca,ResourceVersion:844929,Generation:0,CreationTimestamp:2019-05-13 08:44:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 13 08:44:11.554: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-b,UID:41a5e4df-755b-11e9-b9b7-00163e01adca,ResourceVersion:844948,Generation:0,CreationTimestamp:2019-05-13 08:44:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 08:44:11.556: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fqmll,SelfLink:/api/v1/namespaces/e2e-tests-watch-fqmll/configmaps/e2e-watch-test-configmap-b,UID:41a5e4df-755b-11e9-b9b7-00163e01adca,ResourceVersion:844948,Generation:0,CreationTimestamp:2019-05-13 08:44:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:44:21.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fqmll" for this suite.
May 13 08:44:27.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:44:27.755: INFO: namespace: e2e-tests-watch-fqmll, resource: bindings, ignored listing per whitelist
May 13 08:44:27.867: INFO: namespace e2e-tests-watch-fqmll deletion completed in 6.301224757s

• [SLOW TEST:66.639 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:44:27.868: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-96kv7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
May 13 08:44:28.193: INFO: Waiting up to 5m0s for pod "var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-var-expansion-96kv7" to be "success or failure"
May 13 08:44:28.204: INFO: Pod "var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.105102ms
May 13 08:44:30.209: INFO: Pod "var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015627101s
May 13 08:44:32.217: INFO: Pod "var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023635478s
STEP: Saw pod success
May 13 08:44:32.217: INFO: Pod "var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:44:32.220: INFO: Trying to get logs from node 172.16.177.10 pod var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 08:44:32.262: INFO: Waiting for pod var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:44:32.272: INFO: Pod var-expansion-517ff344-755b-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:44:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-96kv7" for this suite.
May 13 08:44:38.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:44:38.366: INFO: namespace: e2e-tests-var-expansion-96kv7, resource: bindings, ignored listing per whitelist
May 13 08:44:38.515: INFO: namespace e2e-tests-var-expansion-96kv7 deletion completed in 6.228059744s

• [SLOW TEST:10.647 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:44:38.515: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6z4k7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 13 08:44:38.781: INFO: Waiting up to 5m0s for pod "downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-6z4k7" to be "success or failure"
May 13 08:44:38.785: INFO: Pod "downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414969ms
May 13 08:44:40.790: INFO: Pod "downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008586922s
May 13 08:44:42.796: INFO: Pod "downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015255647s
STEP: Saw pod success
May 13 08:44:42.797: INFO: Pod "downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:44:42.800: INFO: Trying to get logs from node 172.16.177.10 pod downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 08:44:42.822: INFO: Waiting for pod downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:44:42.825: INFO: Pod downward-api-57d42fb8-755b-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:44:42.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6z4k7" for this suite.
May 13 08:44:48.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:44:48.980: INFO: namespace: e2e-tests-downward-api-6z4k7, resource: bindings, ignored listing per whitelist
May 13 08:44:49.221: INFO: namespace e2e-tests-downward-api-6z4k7 deletion completed in 6.389846553s

• [SLOW TEST:10.706 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:44:49.223: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-d69lc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-sshj
STEP: Creating a pod to test atomic-volume-subpath
May 13 08:44:49.623: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sshj" in namespace "e2e-tests-subpath-d69lc" to be "success or failure"
May 13 08:44:49.639: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Pending", Reason="", readiness=false. Elapsed: 14.940561ms
May 13 08:44:51.644: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020560569s
May 13 08:44:53.648: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024776086s
May 13 08:44:55.653: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 6.029222917s
May 13 08:44:57.661: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 8.03732876s
May 13 08:44:59.671: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 10.047326796s
May 13 08:45:01.699: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 12.075219153s
May 13 08:45:03.705: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 14.081374442s
May 13 08:45:05.716: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 16.091956749s
May 13 08:45:07.720: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 18.09684101s
May 13 08:45:09.729: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 20.105132511s
May 13 08:45:11.733: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 22.109301946s
May 13 08:45:13.738: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Running", Reason="", readiness=false. Elapsed: 24.114861831s
May 13 08:45:15.753: INFO: Pod "pod-subpath-test-configmap-sshj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.128950644s
STEP: Saw pod success
May 13 08:45:15.753: INFO: Pod "pod-subpath-test-configmap-sshj" satisfied condition "success or failure"
May 13 08:45:15.758: INFO: Trying to get logs from node 172.16.177.10 pod pod-subpath-test-configmap-sshj container test-container-subpath-configmap-sshj: <nil>
STEP: delete the pod
May 13 08:45:15.798: INFO: Waiting for pod pod-subpath-test-configmap-sshj to disappear
May 13 08:45:15.811: INFO: Pod pod-subpath-test-configmap-sshj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sshj
May 13 08:45:15.816: INFO: Deleting pod "pod-subpath-test-configmap-sshj" in namespace "e2e-tests-subpath-d69lc"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:45:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-d69lc" for this suite.
May 13 08:45:23.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:45:23.891: INFO: namespace: e2e-tests-subpath-d69lc, resource: bindings, ignored listing per whitelist
May 13 08:45:24.064: INFO: namespace e2e-tests-subpath-d69lc deletion completed in 8.233538284s

• [SLOW TEST:34.840 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:45:24.064: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bm779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 13 08:45:24.384: INFO: Waiting up to 5m0s for pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-bm779" to be "success or failure"
May 13 08:45:24.387: INFO: Pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.377739ms
May 13 08:45:26.393: INFO: Pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008880128s
May 13 08:45:28.397: INFO: Pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012658524s
May 13 08:45:30.401: INFO: Pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016551548s
STEP: Saw pod success
May 13 08:45:30.401: INFO: Pod "pod-73016041-755b-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:45:30.403: INFO: Trying to get logs from node 172.16.176.226 pod pod-73016041-755b-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:45:30.431: INFO: Waiting for pod pod-73016041-755b-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:45:30.436: INFO: Pod pod-73016041-755b-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:45:30.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bm779" for this suite.
May 13 08:45:36.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:45:36.621: INFO: namespace: e2e-tests-emptydir-bm779, resource: bindings, ignored listing per whitelist
May 13 08:45:36.720: INFO: namespace e2e-tests-emptydir-bm779 deletion completed in 6.278214369s

• [SLOW TEST:12.656 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:45:36.721: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-m4wd5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-p6jt
STEP: Creating a pod to test atomic-volume-subpath
May 13 08:45:36.995: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p6jt" in namespace "e2e-tests-subpath-m4wd5" to be "success or failure"
May 13 08:45:36.999: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103532ms
May 13 08:45:39.005: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00939388s
May 13 08:45:41.017: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021649866s
May 13 08:45:43.023: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 6.027226266s
May 13 08:45:45.027: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 8.031555127s
May 13 08:45:47.032: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 10.036383866s
May 13 08:45:49.036: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 12.041107973s
May 13 08:45:51.057: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 14.061182905s
May 13 08:45:53.070: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 16.075102921s
May 13 08:45:55.081: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 18.085720817s
May 13 08:45:57.087: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 20.091679636s
May 13 08:45:59.097: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 22.101320872s
May 13 08:46:01.117: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Running", Reason="", readiness=false. Elapsed: 24.121266017s
May 13 08:46:03.123: INFO: Pod "pod-subpath-test-downwardapi-p6jt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.127214436s
STEP: Saw pod success
May 13 08:46:03.123: INFO: Pod "pod-subpath-test-downwardapi-p6jt" satisfied condition "success or failure"
May 13 08:46:03.126: INFO: Trying to get logs from node 172.16.177.10 pod pod-subpath-test-downwardapi-p6jt container test-container-subpath-downwardapi-p6jt: <nil>
STEP: delete the pod
May 13 08:46:03.175: INFO: Waiting for pod pod-subpath-test-downwardapi-p6jt to disappear
May 13 08:46:03.179: INFO: Pod pod-subpath-test-downwardapi-p6jt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-p6jt
May 13 08:46:03.179: INFO: Deleting pod "pod-subpath-test-downwardapi-p6jt" in namespace "e2e-tests-subpath-m4wd5"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:46:03.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-m4wd5" for this suite.
May 13 08:46:09.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:46:09.291: INFO: namespace: e2e-tests-subpath-m4wd5, resource: bindings, ignored listing per whitelist
May 13 08:46:09.443: INFO: namespace e2e-tests-subpath-m4wd5 deletion completed in 6.249211599s

• [SLOW TEST:32.723 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:46:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-rqpbb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:46:09.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-rqpbb" for this suite.
May 13 08:46:15.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:46:15.891: INFO: namespace: e2e-tests-services-rqpbb, resource: bindings, ignored listing per whitelist
May 13 08:46:15.905: INFO: namespace e2e-tests-services-rqpbb deletion completed in 6.24311713s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.461 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:46:15.906: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-52btx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
May 13 08:46:16.180: INFO: Waiting up to 5m0s for pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-containers-52btx" to be "success or failure"
May 13 08:46:16.185: INFO: Pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672495ms
May 13 08:46:18.190: INFO: Pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009275521s
May 13 08:46:20.194: INFO: Pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014095398s
May 13 08:46:22.198: INFO: Pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017932462s
STEP: Saw pod success
May 13 08:46:22.198: INFO: Pod "client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:46:22.202: INFO: Trying to get logs from node 172.16.176.226 pod client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:46:22.270: INFO: Waiting for pod client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:46:22.286: INFO: Pod client-containers-91dda646-755b-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:46:22.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-52btx" for this suite.
May 13 08:46:28.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:46:28.339: INFO: namespace: e2e-tests-containers-52btx, resource: bindings, ignored listing per whitelist
May 13 08:46:28.480: INFO: namespace e2e-tests-containers-52btx deletion completed in 6.179210117s

• [SLOW TEST:12.574 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:46:28.480: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-9rt96
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-9rt96
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 08:46:28.708: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 08:46:56.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.202:8080/dial?request=hostName&protocol=http&host=10.1.209.41&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-9rt96 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:46:56.263: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:46:56.440: INFO: Waiting for endpoints: map[]
May 13 08:46:56.444: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.202:8080/dial?request=hostName&protocol=http&host=10.1.15.53&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-9rt96 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:46:56.444: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:46:56.592: INFO: Waiting for endpoints: map[]
May 13 08:46:56.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.202:8080/dial?request=hostName&protocol=http&host=10.1.197.248&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-9rt96 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:46:56.597: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:46:56.726: INFO: Waiting for endpoints: map[]
May 13 08:46:56.729: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.22.202:8080/dial?request=hostName&protocol=http&host=10.1.22.203&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-9rt96 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:46:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:46:56.868: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:46:56.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-9rt96" for this suite.
May 13 08:47:20.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:47:21.070: INFO: namespace: e2e-tests-pod-network-test-9rt96, resource: bindings, ignored listing per whitelist
May 13 08:47:21.206: INFO: namespace e2e-tests-pod-network-test-9rt96 deletion completed in 24.325532323s

• [SLOW TEST:52.726 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:47:21.207: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x7glj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
May 13 08:47:21.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 cluster-info'
May 13 08:47:22.044: INFO: stderr: ""
May 13 08:47:22.044: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mcatalog-ui\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/catalog-ui:catalog-ui/proxy\x1b[0m\n\x1b[0;32mimage-manager\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/image-manager:image-manager/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mplatform-header\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/platform-header:platform-header/proxy\x1b[0m\n\x1b[0;32mplatform-ui\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/platform-ui:platform-ui/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:47:22.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x7glj" for this suite.
May 13 08:47:28.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:47:28.410: INFO: namespace: e2e-tests-kubectl-x7glj, resource: bindings, ignored listing per whitelist
May 13 08:47:28.507: INFO: namespace e2e-tests-kubectl-x7glj deletion completed in 6.45542798s

• [SLOW TEST:7.300 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:47:28.508: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-4p999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 13 08:47:34.910: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:34.913: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:36.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:36.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:38.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:38.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:40.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:40.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:42.914: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:42.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:44.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:44.917: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:46.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:46.917: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:48.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:48.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:50.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:50.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:52.914: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:52.917: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:54.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:54.917: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:56.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:56.917: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:47:58.914: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:47:58.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:48:00.914: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:48:00.922: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:48:02.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:48:02.918: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 08:48:04.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 08:48:04.919: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:48:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-4p999" for this suite.
May 13 08:48:28.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:48:29.020: INFO: namespace: e2e-tests-container-lifecycle-hook-4p999, resource: bindings, ignored listing per whitelist
May 13 08:48:29.183: INFO: namespace e2e-tests-container-lifecycle-hook-4p999 deletion completed in 24.25796728s

• [SLOW TEST:60.675 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:48:29.184: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4bxqk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-hn86
STEP: Creating a pod to test atomic-volume-subpath
May 13 08:48:29.507: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hn86" in namespace "e2e-tests-subpath-4bxqk" to be "success or failure"
May 13 08:48:29.511: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173081ms
May 13 08:48:31.524: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016564507s
May 13 08:48:33.527: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=true. Elapsed: 4.020463869s
May 13 08:48:35.532: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 6.02501278s
May 13 08:48:37.536: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 8.028770549s
May 13 08:48:39.540: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 10.033153529s
May 13 08:48:41.552: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 12.044988051s
May 13 08:48:43.557: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 14.04971405s
May 13 08:48:45.561: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 16.053980213s
May 13 08:48:47.565: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 18.058289953s
May 13 08:48:49.569: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 20.062516981s
May 13 08:48:51.592: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Running", Reason="", readiness=false. Elapsed: 22.084822013s
May 13 08:48:53.596: INFO: Pod "pod-subpath-test-projected-hn86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088723232s
STEP: Saw pod success
May 13 08:48:53.596: INFO: Pod "pod-subpath-test-projected-hn86" satisfied condition "success or failure"
May 13 08:48:53.599: INFO: Trying to get logs from node 172.16.177.10 pod pod-subpath-test-projected-hn86 container test-container-subpath-projected-hn86: <nil>
STEP: delete the pod
May 13 08:48:53.627: INFO: Waiting for pod pod-subpath-test-projected-hn86 to disappear
May 13 08:48:53.629: INFO: Pod pod-subpath-test-projected-hn86 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hn86
May 13 08:48:53.630: INFO: Deleting pod "pod-subpath-test-projected-hn86" in namespace "e2e-tests-subpath-4bxqk"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:48:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4bxqk" for this suite.
May 13 08:48:59.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:48:59.678: INFO: namespace: e2e-tests-subpath-4bxqk, resource: bindings, ignored listing per whitelist
May 13 08:48:59.845: INFO: namespace e2e-tests-subpath-4bxqk deletion completed in 6.206526904s

• [SLOW TEST:30.661 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:48:59.846: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-g9j9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-gxnf
STEP: Creating a pod to test atomic-volume-subpath
May 13 08:49:00.196: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gxnf" in namespace "e2e-tests-subpath-g9j9t" to be "success or failure"
May 13 08:49:00.199: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315943ms
May 13 08:49:02.203: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007428648s
May 13 08:49:04.208: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 4.01220566s
May 13 08:49:06.214: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 6.017809989s
May 13 08:49:08.218: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 8.022090092s
May 13 08:49:10.222: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 10.026025644s
May 13 08:49:12.234: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 12.037660099s
May 13 08:49:14.239: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 14.042632772s
May 13 08:49:16.243: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 16.046708151s
May 13 08:49:18.248: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 18.051930127s
May 13 08:49:20.259: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 20.063090619s
May 13 08:49:22.263: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Running", Reason="", readiness=false. Elapsed: 22.066944627s
May 13 08:49:24.269: INFO: Pod "pod-subpath-test-configmap-gxnf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07263453s
STEP: Saw pod success
May 13 08:49:24.269: INFO: Pod "pod-subpath-test-configmap-gxnf" satisfied condition "success or failure"
May 13 08:49:24.272: INFO: Trying to get logs from node 172.16.176.226 pod pod-subpath-test-configmap-gxnf container test-container-subpath-configmap-gxnf: <nil>
STEP: delete the pod
May 13 08:49:24.329: INFO: Waiting for pod pod-subpath-test-configmap-gxnf to disappear
May 13 08:49:24.334: INFO: Pod pod-subpath-test-configmap-gxnf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gxnf
May 13 08:49:24.334: INFO: Deleting pod "pod-subpath-test-configmap-gxnf" in namespace "e2e-tests-subpath-g9j9t"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:49:24.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-g9j9t" for this suite.
May 13 08:49:30.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:49:30.665: INFO: namespace: e2e-tests-subpath-g9j9t, resource: bindings, ignored listing per whitelist
May 13 08:49:30.850: INFO: namespace e2e-tests-subpath-g9j9t deletion completed in 6.504754625s

• [SLOW TEST:31.005 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:49:30.851: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gp9js
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 08:49:31.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:31.453: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 08:49:31.453: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 13 08:49:31.464: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 13 08:49:31.472: INFO: scanned /root for discovery docs: <nil>
May 13 08:49:31.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:48.701: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 13 08:49:48.701: INFO: stdout: "Created e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601\nScaling up e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 13 08:49:48.701: INFO: stdout: "Created e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601\nScaling up e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 13 08:49:48.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:48.819: INFO: stderr: ""
May 13 08:49:48.819: INFO: stdout: "e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601-ft4vm "
May 13 08:49:48.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601-ft4vm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:48.946: INFO: stderr: ""
May 13 08:49:48.946: INFO: stdout: "true"
May 13 08:49:48.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601-ft4vm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:49.086: INFO: stderr: ""
May 13 08:49:49.086: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 13 08:49:49.086: INFO: e2e-test-nginx-rc-78f2581ac63a7a61efeda4edfdfb7601-ft4vm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
May 13 08:49:49.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gp9js'
May 13 08:49:49.250: INFO: stderr: ""
May 13 08:49:49.250: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:49:49.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gp9js" for this suite.
May 13 08:49:55.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:49:55.486: INFO: namespace: e2e-tests-kubectl-gp9js, resource: bindings, ignored listing per whitelist
May 13 08:49:55.523: INFO: namespace e2e-tests-kubectl-gp9js deletion completed in 6.260879812s

• [SLOW TEST:24.672 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:49:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-vwrrr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-14c58e84-755c-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 08:49:55.781: INFO: Waiting up to 5m0s for pod "pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-vwrrr" to be "success or failure"
May 13 08:49:55.785: INFO: Pod "pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.499547ms
May 13 08:49:57.788: INFO: Pod "pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006634308s
May 13 08:49:59.791: INFO: Pod "pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009839343s
STEP: Saw pod success
May 13 08:49:59.791: INFO: Pod "pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:49:59.793: INFO: Trying to get logs from node 172.16.177.10 pod pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4 container secret-env-test: <nil>
STEP: delete the pod
May 13 08:49:59.816: INFO: Waiting for pod pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:49:59.819: INFO: Pod pod-secrets-14c6166f-755c-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:49:59.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vwrrr" for this suite.
May 13 08:50:05.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:50:06.010: INFO: namespace: e2e-tests-secrets-vwrrr, resource: bindings, ignored listing per whitelist
May 13 08:50:06.034: INFO: namespace e2e-tests-secrets-vwrrr deletion completed in 6.210556087s

• [SLOW TEST:10.510 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:50:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6gh7j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-1b08347e-755c-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 08:50:06.286: INFO: Waiting up to 5m0s for pod "pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-6gh7j" to be "success or failure"
May 13 08:50:06.308: INFO: Pod "pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.466304ms
May 13 08:50:08.314: INFO: Pod "pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027525705s
STEP: Saw pod success
May 13 08:50:08.314: INFO: Pod "pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:50:08.319: INFO: Trying to get logs from node 172.16.176.226 pod pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 08:50:08.354: INFO: Waiting for pod pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:50:08.357: INFO: Pod pod-secrets-1b08d18c-755c-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:50:08.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6gh7j" for this suite.
May 13 08:50:14.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:50:14.433: INFO: namespace: e2e-tests-secrets-6gh7j, resource: bindings, ignored listing per whitelist
May 13 08:50:14.580: INFO: namespace e2e-tests-secrets-6gh7j deletion completed in 6.216895435s

• [SLOW TEST:8.545 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:50:14.581: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dtp47
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 08:50:14.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-dtp47'
May 13 08:50:15.107: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 08:50:15.107: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
May 13 08:50:17.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-dtp47'
May 13 08:50:17.233: INFO: stderr: ""
May 13 08:50:17.233: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:50:17.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dtp47" for this suite.
May 13 08:52:25.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:52:25.375: INFO: namespace: e2e-tests-kubectl-dtp47, resource: bindings, ignored listing per whitelist
May 13 08:52:25.471: INFO: namespace e2e-tests-kubectl-dtp47 deletion completed in 2m8.233806108s

• [SLOW TEST:130.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:52:25.472: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xs825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 08:52:26.151: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6e40679b-755c-11e9-b9b7-00163e01adca", Controller:(*bool)(0xc0019b0342), BlockOwnerDeletion:(*bool)(0xc0019b0343)}}
May 13 08:52:26.165: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6e2b189d-755c-11e9-b9b7-00163e01adca", Controller:(*bool)(0xc001544ae2), BlockOwnerDeletion:(*bool)(0xc001544ae3)}}
May 13 08:52:26.177: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6e33fa70-755c-11e9-b9b7-00163e01adca", Controller:(*bool)(0xc0019b063a), BlockOwnerDeletion:(*bool)(0xc0019b063b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:52:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xs825" for this suite.
May 13 08:52:37.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:52:37.337: INFO: namespace: e2e-tests-gc-xs825, resource: bindings, ignored listing per whitelist
May 13 08:52:37.408: INFO: namespace e2e-tests-gc-xs825 deletion completed in 6.205923959s

• [SLOW TEST:11.936 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:52:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nwh6n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 08:52:37.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-nwh6n" to be "success or failure"
May 13 08:52:37.686: INFO: Pod "downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108317ms
May 13 08:52:39.690: INFO: Pod "downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006667142s
May 13 08:52:41.695: INFO: Pod "downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01188719s
STEP: Saw pod success
May 13 08:52:41.695: INFO: Pod "downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:52:41.700: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 08:52:41.740: INFO: Waiting for pod downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:52:41.755: INFO: Pod downwardapi-volume-75412a86-755c-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:52:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nwh6n" for this suite.
May 13 08:52:47.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:52:47.928: INFO: namespace: e2e-tests-projected-nwh6n, resource: bindings, ignored listing per whitelist
May 13 08:52:47.987: INFO: namespace e2e-tests-projected-nwh6n deletion completed in 6.226110706s

• [SLOW TEST:10.579 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:52:47.989: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-pjc7b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
May 13 08:52:48.278: INFO: Waiting up to 5m0s for pod "client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-containers-pjc7b" to be "success or failure"
May 13 08:52:48.282: INFO: Pod "client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.98814ms
May 13 08:52:50.288: INFO: Pod "client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009581767s
STEP: Saw pod success
May 13 08:52:50.288: INFO: Pod "client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:52:50.291: INFO: Trying to get logs from node 172.16.176.226 pod client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 08:52:50.313: INFO: Waiting for pod client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:52:50.316: INFO: Pod client-containers-7b8fc4c7-755c-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:52:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-pjc7b" for this suite.
May 13 08:52:56.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:52:56.597: INFO: namespace: e2e-tests-containers-pjc7b, resource: bindings, ignored listing per whitelist
May 13 08:52:56.623: INFO: namespace e2e-tests-containers-pjc7b deletion completed in 6.301013245s

• [SLOW TEST:8.635 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:52:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-9qgm7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 13 08:53:04.889: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-80b74e27-755c-11e9-bbcc-d288ccfb79a4,GenerateName:,Namespace:e2e-tests-events-9qgm7,SelfLink:/api/v1/namespaces/e2e-tests-events-9qgm7/pods/send-events-80b74e27-755c-11e9-bbcc-d288ccfb79a4,UID:80b9bb13-755c-11e9-b9b7-00163e01adca,ResourceVersion:846974,Generation:0,CreationTimestamp:2019-05-13 08:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 844866293,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-t7ffq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t7ffq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-t7ffq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00227af40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00227af60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:52:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:52:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:52:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:10.1.22.212,StartTime:2019-05-13 08:52:01 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-13 08:52:08 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://d62edd7527f93b804513199e1b48d59ceb79caf850a8abefbae357ae2372fee6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 13 08:53:06.894: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 13 08:53:08.902: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:53:08.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-9qgm7" for this suite.
May 13 08:53:52.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:53:52.976: INFO: namespace: e2e-tests-events-9qgm7, resource: bindings, ignored listing per whitelist
May 13 08:53:53.101: INFO: namespace e2e-tests-events-9qgm7 deletion completed in 44.183990844s

• [SLOW TEST:56.477 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:53:53.103: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-xp5x2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-xp5x2
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 08:53:53.292: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 08:54:16.807: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.1.197.246 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xp5x2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:54:16.807: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:54:17.995: INFO: Found all expected endpoints: [netserver-0]
May 13 08:54:18.005: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.1.209.47 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xp5x2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:54:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:54:19.189: INFO: Found all expected endpoints: [netserver-1]
May 13 08:54:19.202: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.1.22.209 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xp5x2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:54:19.202: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:54:20.398: INFO: Found all expected endpoints: [netserver-2]
May 13 08:54:20.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.1.15.54 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xp5x2 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 08:54:20.405: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 08:54:21.568: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:54:21.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-xp5x2" for this suite.
May 13 08:54:45.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:54:45.816: INFO: namespace: e2e-tests-pod-network-test-xp5x2, resource: bindings, ignored listing per whitelist
May 13 08:54:46.046: INFO: namespace e2e-tests-pod-network-test-xp5x2 deletion completed in 24.453488095s

• [SLOW TEST:52.943 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:54:46.047: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-5br7r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0513 08:55:31.449719      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 08:55:31.449: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:55:31.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5br7r" for this suite.
May 13 08:55:39.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:55:39.710: INFO: namespace: e2e-tests-gc-5br7r, resource: bindings, ignored listing per whitelist
May 13 08:55:39.730: INFO: namespace e2e-tests-gc-5br7r deletion completed in 8.269285232s

• [SLOW TEST:53.683 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:55:39.731: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jddh6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e1eed976-755c-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 08:55:39.987: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-jddh6" to be "success or failure"
May 13 08:55:39.991: INFO: Pod "pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284622ms
May 13 08:55:41.995: INFO: Pod "pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007971132s
STEP: Saw pod success
May 13 08:55:41.996: INFO: Pod "pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:55:42.011: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 08:55:42.047: INFO: Waiting for pod pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:55:42.050: INFO: Pod pod-configmaps-e1ef9904-755c-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:55:42.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jddh6" for this suite.
May 13 08:55:48.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:55:48.176: INFO: namespace: e2e-tests-configmap-jddh6, resource: bindings, ignored listing per whitelist
May 13 08:55:48.367: INFO: namespace e2e-tests-configmap-jddh6 deletion completed in 6.307383475s

• [SLOW TEST:8.636 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:55:48.367: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-s62nm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-s62nm
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-s62nm
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-s62nm
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-s62nm
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-s62nm
May 13 08:55:54.731: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s62nm, name: ss-0, uid: e8dae47a-755c-11e9-b9b7-00163e01adca, status phase: Pending. Waiting for statefulset controller to delete.
May 13 08:55:57.114: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s62nm, name: ss-0, uid: e8dae47a-755c-11e9-b9b7-00163e01adca, status phase: Failed. Waiting for statefulset controller to delete.
May 13 08:55:57.135: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s62nm, name: ss-0, uid: e8dae47a-755c-11e9-b9b7-00163e01adca, status phase: Failed. Waiting for statefulset controller to delete.
May 13 08:55:57.136: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-s62nm
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-s62nm
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-s62nm and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 08:56:09.242: INFO: Deleting all statefulset in ns e2e-tests-statefulset-s62nm
May 13 08:56:09.246: INFO: Scaling statefulset ss to 0
May 13 08:56:19.272: INFO: Waiting for statefulset status.replicas updated to 0
May 13 08:56:19.276: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:56:19.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-s62nm" for this suite.
May 13 08:56:25.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:56:25.390: INFO: namespace: e2e-tests-statefulset-s62nm, resource: bindings, ignored listing per whitelist
May 13 08:56:25.593: INFO: namespace e2e-tests-statefulset-s62nm deletion completed in 6.296070833s

• [SLOW TEST:37.226 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:56:25.594: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-twxtn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 13 08:56:30.607: INFO: Successfully updated pod "labelsupdatefd4a2974-755c-11e9-bbcc-d288ccfb79a4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:56:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-twxtn" for this suite.
May 13 08:56:56.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:56:56.882: INFO: namespace: e2e-tests-projected-twxtn, resource: bindings, ignored listing per whitelist
May 13 08:56:56.954: INFO: namespace e2e-tests-projected-twxtn deletion completed in 24.276338219s

• [SLOW TEST:31.360 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:56:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xbssg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
May 13 08:56:57.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-xbssg'
May 13 08:56:57.503: INFO: stderr: ""
May 13 08:56:57.503: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
May 13 08:56:58.516: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:56:58.516: INFO: Found 0 / 1
May 13 08:56:59.511: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:56:59.511: INFO: Found 0 / 1
May 13 08:57:00.509: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:57:00.509: INFO: Found 1 / 1
May 13 08:57:00.509: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 08:57:00.519: INFO: Selector matched 1 pods for map[app:redis]
May 13 08:57:00.519: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 13 08:57:00.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 logs redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg'
May 13 08:57:00.685: INFO: stderr: ""
May 13 08:57:00.685: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 08:56:03.794 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 08:56:03.794 # Server started, Redis version 3.2.12\n1:M 13 May 08:56:03.795 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 08:56:03.795 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 13 08:57:00.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 log redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg --tail=1'
May 13 08:57:00.857: INFO: stderr: ""
May 13 08:57:00.857: INFO: stdout: "1:M 13 May 08:56:03.795 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 13 08:57:00.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 log redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg --limit-bytes=1'
May 13 08:57:01.131: INFO: stderr: ""
May 13 08:57:01.131: INFO: stdout: " "
STEP: exposing timestamps
May 13 08:57:01.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 log redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg --tail=1 --timestamps'
May 13 08:57:01.337: INFO: stderr: ""
May 13 08:57:01.337: INFO: stdout: "2019-05-13T08:56:03.795576084Z 1:M 13 May 08:56:03.795 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 13 08:57:03.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 log redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg --since=1s'
May 13 08:57:03.969: INFO: stderr: ""
May 13 08:57:03.969: INFO: stdout: ""
May 13 08:57:03.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 log redis-master-st9b8 redis-master --namespace=e2e-tests-kubectl-xbssg --since=24h'
May 13 08:57:04.144: INFO: stderr: ""
May 13 08:57:04.145: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 08:56:03.794 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 08:56:03.794 # Server started, Redis version 3.2.12\n1:M 13 May 08:56:03.795 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 08:56:03.795 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
May 13 08:57:04.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xbssg'
May 13 08:57:04.369: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 08:57:04.369: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 13 08:57:04.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-xbssg'
May 13 08:57:04.568: INFO: stderr: "No resources found.\n"
May 13 08:57:04.568: INFO: stdout: ""
May 13 08:57:04.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -l name=nginx --namespace=e2e-tests-kubectl-xbssg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 08:57:04.733: INFO: stderr: ""
May 13 08:57:04.733: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:57:04.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xbssg" for this suite.
May 13 08:57:10.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:57:10.889: INFO: namespace: e2e-tests-kubectl-xbssg, resource: bindings, ignored listing per whitelist
May 13 08:57:11.024: INFO: namespace e2e-tests-kubectl-xbssg deletion completed in 6.283134964s

• [SLOW TEST:14.071 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:57:11.027: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-bm9sp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:57:26.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-bm9sp" for this suite.
May 13 08:57:50.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:57:50.621: INFO: namespace: e2e-tests-replication-controller-bm9sp, resource: bindings, ignored listing per whitelist
May 13 08:57:50.766: INFO: namespace e2e-tests-replication-controller-bm9sp deletion completed in 24.23013936s

• [SLOW TEST:39.740 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:57:50.768: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-nhxqq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 08:57:55.180: INFO: Waiting up to 5m0s for pod "client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-pods-nhxqq" to be "success or failure"
May 13 08:57:55.186: INFO: Pod "client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201575ms
May 13 08:57:57.190: INFO: Pod "client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008290888s
May 13 08:57:59.197: INFO: Pod "client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015510499s
STEP: Saw pod success
May 13 08:57:59.197: INFO: Pod "client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:57:59.202: INFO: Trying to get logs from node 172.16.177.10 pod client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4 container env3cont: <nil>
STEP: delete the pod
May 13 08:57:59.270: INFO: Waiting for pod client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:57:59.289: INFO: Pod client-envvars-32823a89-755d-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:57:59.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nhxqq" for this suite.
May 13 08:58:39.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:58:39.572: INFO: namespace: e2e-tests-pods-nhxqq, resource: bindings, ignored listing per whitelist
May 13 08:58:39.632: INFO: namespace e2e-tests-pods-nhxqq deletion completed in 40.331923894s

• [SLOW TEST:48.864 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:58:39.633: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-gxwnp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 08:58:39.929: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 13 08:58:39.984: INFO: Pod name sample-pod: Found 0 pods out of 1
May 13 08:58:44.989: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 08:58:44.990: INFO: Creating deployment "test-rolling-update-deployment"
May 13 08:58:45.113: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 13 08:58:45.118: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 13 08:58:47.126: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 13 08:58:47.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693334725, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693334725, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693334725, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693334725, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 08:58:49.132: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 08:58:49.143: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-gxwnp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gxwnp/deployments/test-rolling-update-deployment,UID:5049303d-755d-11e9-b9b7-00163e01adca,ResourceVersion:848500,Generation:1,CreationTimestamp:2019-05-13 08:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-13 08:58:45 +0000 UTC 2019-05-13 08:58:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-13 08:58:49 +0000 UTC 2019-05-13 08:58:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 13 08:58:49.146: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-gxwnp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gxwnp/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:504f866f-755d-11e9-b9b7-00163e01adca,ResourceVersion:848490,Generation:1,CreationTimestamp:2019-05-13 08:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 5049303d-755d-11e9-b9b7-00163e01adca 0xc00229b127 0xc00229b128}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 13 08:58:49.146: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 13 08:58:49.147: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-gxwnp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gxwnp/replicasets/test-rolling-update-controller,UID:4d375909-755d-11e9-b9b7-00163e01adca,ResourceVersion:848499,Generation:2,CreationTimestamp:2019-05-13 08:58:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 5049303d-755d-11e9-b9b7-00163e01adca 0xc00229b067 0xc00229b068}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 08:58:49.150: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-tztp5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-tztp5,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-gxwnp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-gxwnp/pods/test-rolling-update-deployment-68b55d7bc6-tztp5,UID:5050c396-755d-11e9-b9b7-00163e01adca,ResourceVersion:848489,Generation:0,CreationTimestamp:2019-05-13 08:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 504f866f-755d-11e9-b9b7-00163e01adca 0xc00229b9e7 0xc00229b9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-lz48s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lz48s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lz48s true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00229ba60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00229ba80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:57:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:57:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:57:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 08:58:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:10.1.22.219,StartTime:2019-05-13 08:57:49 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-13 08:57:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://223d0deb9a7bbe8aa972b71f641147dfcef62491d5cffd2e50455c0b32866563}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:58:49.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-gxwnp" for this suite.
May 13 08:58:55.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:58:55.316: INFO: namespace: e2e-tests-deployment-gxwnp, resource: bindings, ignored listing per whitelist
May 13 08:58:55.380: INFO: namespace e2e-tests-deployment-gxwnp deletion completed in 6.223614375s

• [SLOW TEST:15.747 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:58:55.380: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-27h6s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 13 08:58:55.679: INFO: Waiting up to 5m0s for pod "downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-27h6s" to be "success or failure"
May 13 08:58:55.683: INFO: Pod "downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407934ms
May 13 08:58:57.687: INFO: Pod "downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00696101s
STEP: Saw pod success
May 13 08:58:57.687: INFO: Pod "downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 08:58:57.689: INFO: Trying to get logs from node 172.16.176.226 pod downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 08:58:57.726: INFO: Waiting for pod downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4 to disappear
May 13 08:58:57.730: INFO: Pod downward-api-569205a8-755d-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:58:57.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-27h6s" for this suite.
May 13 08:59:03.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 08:59:04.085: INFO: namespace: e2e-tests-downward-api-27h6s, resource: bindings, ignored listing per whitelist
May 13 08:59:04.127: INFO: namespace e2e-tests-downward-api-27h6s deletion completed in 6.39162919s

• [SLOW TEST:8.746 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 08:59:04.127: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-z6k6l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
May 13 08:59:04.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:05.309: INFO: stderr: ""
May 13 08:59:05.309: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 08:59:05.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:05.489: INFO: stderr: ""
May 13 08:59:05.489: INFO: stdout: "update-demo-nautilus-2mcjv update-demo-nautilus-l578q "
May 13 08:59:05.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-2mcjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:05.632: INFO: stderr: ""
May 13 08:59:05.632: INFO: stdout: ""
May 13 08:59:05.632: INFO: update-demo-nautilus-2mcjv is created but not running
May 13 08:59:10.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:10.752: INFO: stderr: ""
May 13 08:59:10.752: INFO: stdout: "update-demo-nautilus-2mcjv update-demo-nautilus-l578q "
May 13 08:59:10.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-2mcjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:10.885: INFO: stderr: ""
May 13 08:59:10.885: INFO: stdout: "true"
May 13 08:59:10.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-2mcjv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:11.003: INFO: stderr: ""
May 13 08:59:11.003: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 08:59:11.003: INFO: validating pod update-demo-nautilus-2mcjv
May 13 08:59:11.014: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 08:59:11.014: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 08:59:11.014: INFO: update-demo-nautilus-2mcjv is verified up and running
May 13 08:59:11.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-l578q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:11.162: INFO: stderr: ""
May 13 08:59:11.162: INFO: stdout: "true"
May 13 08:59:11.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-l578q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:11.328: INFO: stderr: ""
May 13 08:59:11.328: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 08:59:11.328: INFO: validating pod update-demo-nautilus-l578q
May 13 08:59:11.360: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 08:59:11.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 08:59:11.361: INFO: update-demo-nautilus-l578q is verified up and running
STEP: rolling-update to new replication controller
May 13 08:59:11.369: INFO: scanned /root for discovery docs: <nil>
May 13 08:59:11.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:37.502: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 13 08:59:37.502: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 08:59:37.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:37.606: INFO: stderr: ""
May 13 08:59:37.606: INFO: stdout: "update-demo-kitten-c8ghv update-demo-kitten-dm9j6 "
May 13 08:59:37.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-kitten-c8ghv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:37.695: INFO: stderr: ""
May 13 08:59:37.695: INFO: stdout: "true"
May 13 08:59:37.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-kitten-c8ghv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:37.814: INFO: stderr: ""
May 13 08:59:37.814: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 13 08:59:37.814: INFO: validating pod update-demo-kitten-c8ghv
May 13 08:59:37.825: INFO: got data: {
  "image": "kitten.jpg"
}

May 13 08:59:37.826: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 13 08:59:37.826: INFO: update-demo-kitten-c8ghv is verified up and running
May 13 08:59:37.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-kitten-dm9j6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:37.945: INFO: stderr: ""
May 13 08:59:37.945: INFO: stdout: "true"
May 13 08:59:37.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-kitten-dm9j6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z6k6l'
May 13 08:59:38.079: INFO: stderr: ""
May 13 08:59:38.079: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 13 08:59:38.079: INFO: validating pod update-demo-kitten-dm9j6
May 13 08:59:38.085: INFO: got data: {
  "image": "kitten.jpg"
}

May 13 08:59:38.086: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 13 08:59:38.086: INFO: update-demo-kitten-dm9j6 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 08:59:38.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z6k6l" for this suite.
May 13 09:00:02.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:00:02.290: INFO: namespace: e2e-tests-kubectl-z6k6l, resource: bindings, ignored listing per whitelist
May 13 09:00:02.400: INFO: namespace e2e-tests-kubectl-z6k6l deletion completed in 24.307877896s

• [SLOW TEST:58.273 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:00:02.409: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-fc9hq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 13 09:00:03.096: INFO: Pod name wrapped-volume-race-7eba145f-755d-11e9-bbcc-d288ccfb79a4: Found 0 pods out of 5
May 13 09:00:08.106: INFO: Pod name wrapped-volume-race-7eba145f-755d-11e9-bbcc-d288ccfb79a4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7eba145f-755d-11e9-bbcc-d288ccfb79a4 in namespace e2e-tests-emptydir-wrapper-fc9hq, will wait for the garbage collector to delete the pods
May 13 09:03:42.213: INFO: Deleting ReplicationController wrapped-volume-race-7eba145f-755d-11e9-bbcc-d288ccfb79a4 took: 11.709779ms
May 13 09:03:42.314: INFO: Terminating ReplicationController wrapped-volume-race-7eba145f-755d-11e9-bbcc-d288ccfb79a4 pods took: 100.467424ms
STEP: Creating RC which spawns configmap-volume pods
May 13 09:04:27.588: INFO: Pod name wrapped-volume-race-1c633448-755e-11e9-bbcc-d288ccfb79a4: Found 0 pods out of 5
May 13 09:04:32.605: INFO: Pod name wrapped-volume-race-1c633448-755e-11e9-bbcc-d288ccfb79a4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1c633448-755e-11e9-bbcc-d288ccfb79a4 in namespace e2e-tests-emptydir-wrapper-fc9hq, will wait for the garbage collector to delete the pods
May 13 09:06:44.705: INFO: Deleting ReplicationController wrapped-volume-race-1c633448-755e-11e9-bbcc-d288ccfb79a4 took: 9.55035ms
May 13 09:06:44.808: INFO: Terminating ReplicationController wrapped-volume-race-1c633448-755e-11e9-bbcc-d288ccfb79a4 pods took: 102.370626ms
STEP: Creating RC which spawns configmap-volume pods
May 13 09:07:26.735: INFO: Pod name wrapped-volume-race-872351cb-755e-11e9-bbcc-d288ccfb79a4: Found 0 pods out of 5
May 13 09:07:31.744: INFO: Pod name wrapped-volume-race-872351cb-755e-11e9-bbcc-d288ccfb79a4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-872351cb-755e-11e9-bbcc-d288ccfb79a4 in namespace e2e-tests-emptydir-wrapper-fc9hq, will wait for the garbage collector to delete the pods
May 13 09:10:53.843: INFO: Deleting ReplicationController wrapped-volume-race-872351cb-755e-11e9-bbcc-d288ccfb79a4 took: 9.216685ms
May 13 09:10:53.943: INFO: Terminating ReplicationController wrapped-volume-race-872351cb-755e-11e9-bbcc-d288ccfb79a4 pods took: 100.463651ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:11:37.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-fc9hq" for this suite.
May 13 09:11:45.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:11:45.444: INFO: namespace: e2e-tests-emptydir-wrapper-fc9hq, resource: bindings, ignored listing per whitelist
May 13 09:11:45.513: INFO: namespace e2e-tests-emptydir-wrapper-fc9hq deletion completed in 8.229980471s

• [SLOW TEST:703.104 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:11:45.514: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-t7ptm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 13 09:11:45.784: INFO: Waiting up to 5m0s for pod "downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-t7ptm" to be "success or failure"
May 13 09:11:45.786: INFO: Pod "downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.716214ms
May 13 09:11:47.791: INFO: Pod "downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00765204s
May 13 09:11:49.795: INFO: Pod "downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011776234s
STEP: Saw pod success
May 13 09:11:49.795: INFO: Pod "downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:11:49.805: INFO: Trying to get logs from node 172.16.176.226 pod downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 09:11:49.832: INFO: Waiting for pod downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:11:49.835: INFO: Pod downward-api-21958f9b-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:11:49.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-t7ptm" for this suite.
May 13 09:11:55.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:11:56.028: INFO: namespace: e2e-tests-downward-api-t7ptm, resource: bindings, ignored listing per whitelist
May 13 09:11:56.048: INFO: namespace e2e-tests-downward-api-t7ptm deletion completed in 6.206129198s

• [SLOW TEST:10.534 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:11:56.048: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t4hzc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-27dd504c-755f-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:11:56.383: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-t4hzc" to be "success or failure"
May 13 09:11:56.387: INFO: Pod "pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.897409ms
May 13 09:11:58.390: INFO: Pod "pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007665511s
May 13 09:12:00.395: INFO: Pod "pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011981499s
STEP: Saw pod success
May 13 09:12:00.395: INFO: Pod "pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:12:00.397: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 09:12:00.419: INFO: Waiting for pod pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:12:00.424: INFO: Pod pod-projected-secrets-27ddf92f-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:12:00.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t4hzc" for this suite.
May 13 09:12:06.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:12:06.982: INFO: namespace: e2e-tests-projected-t4hzc, resource: bindings, ignored listing per whitelist
May 13 09:12:07.208: INFO: namespace e2e-tests-projected-t4hzc deletion completed in 6.778685145s

• [SLOW TEST:11.160 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:12:07.211: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-64qfn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 13 09:12:13.746: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:13.750: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:15.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:15.754: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:17.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:17.754: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:19.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:19.755: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:21.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:21.767: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:23.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:23.756: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:25.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:25.756: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:27.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:27.755: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:29.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:29.756: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:31.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:31.755: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 09:12:33.751: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 09:12:33.763: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:12:33.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-64qfn" for this suite.
May 13 09:12:57.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:12:57.885: INFO: namespace: e2e-tests-container-lifecycle-hook-64qfn, resource: bindings, ignored listing per whitelist
May 13 09:12:58.002: INFO: namespace e2e-tests-container-lifecycle-hook-64qfn deletion completed in 24.222903732s

• [SLOW TEST:50.791 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:12:58.003: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cfh8m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-4cc8e998-755f-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:12:58.292: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-cfh8m" to be "success or failure"
May 13 09:12:58.296: INFO: Pod "pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.020705ms
May 13 09:13:00.299: INFO: Pod "pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006874449s
May 13 09:13:02.304: INFO: Pod "pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01161151s
STEP: Saw pod success
May 13 09:13:02.304: INFO: Pod "pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:13:02.307: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:13:02.326: INFO: Waiting for pod pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:13:02.330: INFO: Pod pod-configmaps-4cc98573-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:13:02.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cfh8m" for this suite.
May 13 09:13:08.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:13:08.445: INFO: namespace: e2e-tests-configmap-cfh8m, resource: bindings, ignored listing per whitelist
May 13 09:13:08.542: INFO: namespace e2e-tests-configmap-cfh8m deletion completed in 6.206829481s

• [SLOW TEST:10.539 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:13:08.542: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-llv9v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:13:08.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-llv9v" to be "success or failure"
May 13 09:13:08.790: INFO: Pod "downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524746ms
May 13 09:13:10.794: INFO: Pod "downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007529952s
STEP: Saw pod success
May 13 09:13:10.794: INFO: Pod "downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:13:10.801: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:13:10.839: INFO: Waiting for pod downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:13:10.843: INFO: Pod downwardapi-volume-5313288e-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:13:10.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-llv9v" for this suite.
May 13 09:13:16.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:13:17.091: INFO: namespace: e2e-tests-projected-llv9v, resource: bindings, ignored listing per whitelist
May 13 09:13:17.127: INFO: namespace e2e-tests-projected-llv9v deletion completed in 6.279116042s

• [SLOW TEST:8.585 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:13:17.128: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xlhzh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 09:13:17.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xlhzh'
May 13 09:13:17.894: INFO: stderr: ""
May 13 09:13:17.894: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 13 09:13:22.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xlhzh -o json'
May 13 09:13:23.056: INFO: stderr: ""
May 13 09:13:23.056: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-05-13T09:13:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-xlhzh\",\n        \"resourceVersion\": \"851128\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-xlhzh/pods/e2e-test-nginx-pod\",\n        \"uid\": \"58753a94-755f-11e9-b9b7-00163e01adca\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-24t6l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"172.16.177.10\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-24t6l\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-24t6l\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T09:12:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T09:12:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T09:12:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T09:13:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2de6ce46276e332fed53b7cee83c2b33d0cc8126fc6352c27785dbcfd5d8527b\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-13T09:12:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.177.10\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.22.223\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-13T09:12:22Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 13 09:13:23.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 replace -f - --namespace=e2e-tests-kubectl-xlhzh'
May 13 09:13:23.486: INFO: stderr: ""
May 13 09:13:23.486: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
May 13 09:13:23.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xlhzh'
May 13 09:13:25.438: INFO: stderr: ""
May 13 09:13:25.438: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:13:25.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xlhzh" for this suite.
May 13 09:13:31.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:13:31.646: INFO: namespace: e2e-tests-kubectl-xlhzh, resource: bindings, ignored listing per whitelist
May 13 09:13:31.702: INFO: namespace e2e-tests-kubectl-xlhzh deletion completed in 6.259299164s

• [SLOW TEST:14.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:13:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-kmtqx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:13:31.961: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:13:36.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kmtqx" for this suite.
May 13 09:14:24.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:14:24.156: INFO: namespace: e2e-tests-pods-kmtqx, resource: bindings, ignored listing per whitelist
May 13 09:14:24.324: INFO: namespace e2e-tests-pods-kmtqx deletion completed in 48.28288419s

• [SLOW TEST:52.622 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:14:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-nfrqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
May 13 09:14:24.681: INFO: Waiting up to 5m0s for pod "var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-var-expansion-nfrqg" to be "success or failure"
May 13 09:14:24.687: INFO: Pod "var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856603ms
May 13 09:14:26.692: INFO: Pod "var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010721163s
May 13 09:14:28.695: INFO: Pod "var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013973177s
STEP: Saw pod success
May 13 09:14:28.695: INFO: Pod "var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:14:28.698: INFO: Trying to get logs from node 172.16.177.10 pod var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 09:14:28.726: INFO: Waiting for pod var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:14:28.729: INFO: Pod var-expansion-80484602-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:14:28.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-nfrqg" for this suite.
May 13 09:14:34.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:14:34.842: INFO: namespace: e2e-tests-var-expansion-nfrqg, resource: bindings, ignored listing per whitelist
May 13 09:14:34.912: INFO: namespace e2e-tests-var-expansion-nfrqg deletion completed in 6.177358404s

• [SLOW TEST:10.587 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:14:34.913: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-bx6c8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-bx6c8/secret-test-86934730-755f-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:14:35.291: INFO: Waiting up to 5m0s for pod "pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-bx6c8" to be "success or failure"
May 13 09:14:35.297: INFO: Pod "pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992041ms
May 13 09:14:37.303: INFO: Pod "pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012043221s
May 13 09:14:39.306: INFO: Pod "pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015648073s
STEP: Saw pod success
May 13 09:14:39.306: INFO: Pod "pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:14:39.308: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4 container env-test: <nil>
STEP: delete the pod
May 13 09:14:39.331: INFO: Waiting for pod pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:14:39.333: INFO: Pod pod-configmaps-8693f7d9-755f-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:14:39.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bx6c8" for this suite.
May 13 09:14:45.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:14:45.411: INFO: namespace: e2e-tests-secrets-bx6c8, resource: bindings, ignored listing per whitelist
May 13 09:14:45.490: INFO: namespace e2e-tests-secrets-bx6c8 deletion completed in 6.152776632s

• [SLOW TEST:10.578 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:14:45.493: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zxzx8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 13 09:14:50.376: INFO: Successfully updated pod "annotationupdate8cdaf120-755f-11e9-bbcc-d288ccfb79a4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:14:52.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zxzx8" for this suite.
May 13 09:15:14.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:15:14.573: INFO: namespace: e2e-tests-projected-zxzx8, resource: bindings, ignored listing per whitelist
May 13 09:15:14.626: INFO: namespace e2e-tests-projected-zxzx8 deletion completed in 22.214030802s

• [SLOW TEST:29.134 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:15:14.627: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vd8wh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:15:14.903: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:15:17.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vd8wh" for this suite.
May 13 09:16:11.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:16:11.246: INFO: namespace: e2e-tests-pods-vd8wh, resource: bindings, ignored listing per whitelist
May 13 09:16:11.325: INFO: namespace e2e-tests-pods-vd8wh deletion completed in 54.157188447s

• [SLOW TEST:56.698 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:16:11.325: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-5zn6z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0513 09:16:42.137303      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 09:16:42.137: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:16:42.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5zn6z" for this suite.
May 13 09:16:48.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:16:48.477: INFO: namespace: e2e-tests-gc-5zn6z, resource: bindings, ignored listing per whitelist
May 13 09:16:48.486: INFO: namespace e2e-tests-gc-5zn6z deletion completed in 6.341419054s

• [SLOW TEST:37.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:16:48.488: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nzg7s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
May 13 09:16:48.720: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-870666073 proxy --unix-socket=/tmp/kubectl-proxy-unix608343780/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:16:48.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nzg7s" for this suite.
May 13 09:16:54.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:16:54.969: INFO: namespace: e2e-tests-kubectl-nzg7s, resource: bindings, ignored listing per whitelist
May 13 09:16:54.981: INFO: namespace e2e-tests-kubectl-nzg7s deletion completed in 6.166108424s

• [SLOW TEST:6.493 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:16:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7pxjq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 13 09:16:55.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-7pxjq'
May 13 09:16:55.584: INFO: stderr: ""
May 13 09:16:55.584: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 09:16:56.588: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:16:56.588: INFO: Found 0 / 1
May 13 09:16:57.595: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:16:57.595: INFO: Found 0 / 1
May 13 09:16:58.589: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:16:58.589: INFO: Found 1 / 1
May 13 09:16:58.589: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 13 09:16:58.593: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:16:58.593: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 09:16:58.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 patch pod redis-master-9q6vt --namespace=e2e-tests-kubectl-7pxjq -p {"metadata":{"annotations":{"x":"y"}}}'
May 13 09:16:58.776: INFO: stderr: ""
May 13 09:16:58.776: INFO: stdout: "pod/redis-master-9q6vt patched\n"
STEP: checking annotations
May 13 09:16:58.779: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:16:58.779: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:16:58.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7pxjq" for this suite.
May 13 09:17:22.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:17:23.007: INFO: namespace: e2e-tests-kubectl-7pxjq, resource: bindings, ignored listing per whitelist
May 13 09:17:23.024: INFO: namespace e2e-tests-kubectl-7pxjq deletion completed in 24.238124555s

• [SLOW TEST:28.041 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:17:23.027: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-tj2lm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:17:27.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-tj2lm" for this suite.
May 13 09:18:21.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:18:21.725: INFO: namespace: e2e-tests-kubelet-test-tj2lm, resource: bindings, ignored listing per whitelist
May 13 09:18:21.745: INFO: namespace e2e-tests-kubelet-test-tj2lm deletion completed in 54.318764614s

• [SLOW TEST:58.718 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:18:21.748: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-c7c8g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 13 09:18:22.089: INFO: Waiting up to 5m0s for pod "downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-c7c8g" to be "success or failure"
May 13 09:18:22.093: INFO: Pod "downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.163704ms
May 13 09:18:24.096: INFO: Pod "downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006439275s
May 13 09:18:26.109: INFO: Pod "downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019014205s
STEP: Saw pod success
May 13 09:18:26.109: INFO: Pod "downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:18:26.112: INFO: Trying to get logs from node 172.16.177.10 pod downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 09:18:26.144: INFO: Waiting for pod downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:18:26.147: INFO: Pod downward-api-0dcce2f2-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:18:26.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-c7c8g" for this suite.
May 13 09:18:32.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:18:32.283: INFO: namespace: e2e-tests-downward-api-c7c8g, resource: bindings, ignored listing per whitelist
May 13 09:18:32.422: INFO: namespace e2e-tests-downward-api-c7c8g deletion completed in 6.270167647s

• [SLOW TEST:10.674 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:18:32.422: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5vclq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 13 09:18:32.681: INFO: Waiting up to 5m0s for pod "pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-5vclq" to be "success or failure"
May 13 09:18:32.684: INFO: Pod "pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559308ms
May 13 09:18:34.687: INFO: Pod "pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005698346s
STEP: Saw pod success
May 13 09:18:34.687: INFO: Pod "pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:18:34.694: INFO: Trying to get logs from node 172.16.176.226 pod pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:18:34.719: INFO: Waiting for pod pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:18:34.724: INFO: Pod pod-141fea6b-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:18:34.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5vclq" for this suite.
May 13 09:18:40.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:18:40.834: INFO: namespace: e2e-tests-emptydir-5vclq, resource: bindings, ignored listing per whitelist
May 13 09:18:40.942: INFO: namespace e2e-tests-emptydir-5vclq deletion completed in 6.213141287s

• [SLOW TEST:8.521 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:18:40.943: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-htb4j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 13 09:18:41.207: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-htb4j,SelfLink:/api/v1/namespaces/e2e-tests-watch-htb4j/configmaps/e2e-watch-test-watch-closed,UID:1938b6c9-7560-11e9-b9b7-00163e01adca,ResourceVersion:852226,Generation:0,CreationTimestamp:2019-05-13 09:18:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 09:18:41.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-htb4j,SelfLink:/api/v1/namespaces/e2e-tests-watch-htb4j/configmaps/e2e-watch-test-watch-closed,UID:1938b6c9-7560-11e9-b9b7-00163e01adca,ResourceVersion:852227,Generation:0,CreationTimestamp:2019-05-13 09:18:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 13 09:18:41.224: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-htb4j,SelfLink:/api/v1/namespaces/e2e-tests-watch-htb4j/configmaps/e2e-watch-test-watch-closed,UID:1938b6c9-7560-11e9-b9b7-00163e01adca,ResourceVersion:852228,Generation:0,CreationTimestamp:2019-05-13 09:18:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 09:18:41.224: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-htb4j,SelfLink:/api/v1/namespaces/e2e-tests-watch-htb4j/configmaps/e2e-watch-test-watch-closed,UID:1938b6c9-7560-11e9-b9b7-00163e01adca,ResourceVersion:852229,Generation:0,CreationTimestamp:2019-05-13 09:18:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:18:41.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-htb4j" for this suite.
May 13 09:18:47.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:18:47.405: INFO: namespace: e2e-tests-watch-htb4j, resource: bindings, ignored listing per whitelist
May 13 09:18:47.412: INFO: namespace e2e-tests-watch-htb4j deletion completed in 6.181921179s

• [SLOW TEST:6.469 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:18:47.412: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-46c2m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0513 09:18:48.726938      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 09:18:48.727: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:18:48.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-46c2m" for this suite.
May 13 09:18:54.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:18:54.891: INFO: namespace: e2e-tests-gc-46c2m, resource: bindings, ignored listing per whitelist
May 13 09:18:54.927: INFO: namespace e2e-tests-gc-46c2m deletion completed in 6.195312261s

• [SLOW TEST:7.515 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:18:54.928: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-gmhch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 13 09:18:55.170: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-gmhch,SelfLink:/api/v1/namespaces/e2e-tests-watch-gmhch/configmaps/e2e-watch-test-resource-version,UID:218a70a8-7560-11e9-b9b7-00163e01adca,ResourceVersion:852322,Generation:0,CreationTimestamp:2019-05-13 09:18:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 09:18:55.170: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-gmhch,SelfLink:/api/v1/namespaces/e2e-tests-watch-gmhch/configmaps/e2e-watch-test-resource-version,UID:218a70a8-7560-11e9-b9b7-00163e01adca,ResourceVersion:852323,Generation:0,CreationTimestamp:2019-05-13 09:18:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:18:55.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-gmhch" for this suite.
May 13 09:19:01.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:19:01.807: INFO: namespace: e2e-tests-watch-gmhch, resource: bindings, ignored listing per whitelist
May 13 09:19:01.813: INFO: namespace e2e-tests-watch-gmhch deletion completed in 6.639009395s

• [SLOW TEST:6.884 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:19:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lmj2c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 13 09:19:02.098: INFO: Waiting up to 5m0s for pod "pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-lmj2c" to be "success or failure"
May 13 09:19:02.104: INFO: Pod "pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590565ms
May 13 09:19:04.110: INFO: Pod "pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011423994s
May 13 09:19:06.113: INFO: Pod "pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014905943s
STEP: Saw pod success
May 13 09:19:06.113: INFO: Pod "pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:19:06.117: INFO: Trying to get logs from node 172.16.177.10 pod pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:19:06.146: INFO: Waiting for pod pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:19:06.152: INFO: Pod pod-25a81ad0-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:19:06.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lmj2c" for this suite.
May 13 09:19:12.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:19:12.303: INFO: namespace: e2e-tests-emptydir-lmj2c, resource: bindings, ignored listing per whitelist
May 13 09:19:12.405: INFO: namespace e2e-tests-emptydir-lmj2c deletion completed in 6.248534923s

• [SLOW TEST:10.592 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:19:12.405: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-skz4t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-skz4t
May 13 09:19:14.686: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-skz4t
STEP: checking the pod's current state and verifying that restartCount is present
May 13 09:19:14.689: INFO: Initial restart count of pod liveness-exec is 0
May 13 09:20:04.817: INFO: Restart count of pod e2e-tests-container-probe-skz4t/liveness-exec is now 1 (50.127428282s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:20:04.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-skz4t" for this suite.
May 13 09:20:10.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:20:11.007: INFO: namespace: e2e-tests-container-probe-skz4t, resource: bindings, ignored listing per whitelist
May 13 09:20:11.026: INFO: namespace e2e-tests-container-probe-skz4t deletion completed in 6.192954795s

• [SLOW TEST:58.621 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:20:11.027: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xh6tm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:20:11.288: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 13 09:20:16.292: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 09:20:16.293: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 09:20:16.402: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-xh6tm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xh6tm/deployments/test-cleanup-deployment,UID:51f39038-7560-11e9-b9b7-00163e01adca,ResourceVersion:852586,Generation:1,CreationTimestamp:2019-05-13 09:20:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 13 09:20:16.410: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:20:16.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xh6tm" for this suite.
May 13 09:20:22.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:20:22.679: INFO: namespace: e2e-tests-deployment-xh6tm, resource: bindings, ignored listing per whitelist
May 13 09:20:22.692: INFO: namespace e2e-tests-deployment-xh6tm deletion completed in 6.218560674s

• [SLOW TEST:11.665 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:20:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pmr8c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-55d6e507-7560-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:20:22.987: INFO: Waiting up to 5m0s for pod "pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-pmr8c" to be "success or failure"
May 13 09:20:22.998: INFO: Pod "pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.427145ms
May 13 09:20:25.003: INFO: Pod "pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015911144s
May 13 09:20:27.014: INFO: Pod "pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026717702s
STEP: Saw pod success
May 13 09:20:27.014: INFO: Pod "pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:20:27.019: INFO: Trying to get logs from node 172.16.177.10 pod pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 09:20:27.047: INFO: Waiting for pod pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:20:27.051: INFO: Pod pod-secrets-55d7aad8-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:20:27.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pmr8c" for this suite.
May 13 09:20:33.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:20:33.196: INFO: namespace: e2e-tests-secrets-pmr8c, resource: bindings, ignored listing per whitelist
May 13 09:20:33.320: INFO: namespace e2e-tests-secrets-pmr8c deletion completed in 6.264907826s

• [SLOW TEST:10.627 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:20:33.321: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-d6tkv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-d6tkv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-d6tkv to expose endpoints map[]
May 13 09:20:33.619: INFO: Get endpoints failed (5.073331ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 13 09:20:34.624: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-d6tkv exposes endpoints map[] (1.010580083s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-d6tkv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-d6tkv to expose endpoints map[pod1:[100]]
May 13 09:20:36.739: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-d6tkv exposes endpoints map[pod1:[100]] (2.036503431s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-d6tkv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-d6tkv to expose endpoints map[pod1:[100] pod2:[101]]
May 13 09:20:39.797: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-d6tkv exposes endpoints map[pod1:[100] pod2:[101]] (3.038807899s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-d6tkv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-d6tkv to expose endpoints map[pod2:[101]]
May 13 09:20:40.817: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-d6tkv exposes endpoints map[pod2:[101]] (1.015721094s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-d6tkv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-d6tkv to expose endpoints map[]
May 13 09:20:41.832: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-d6tkv exposes endpoints map[] (1.00977337s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:20:41.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-d6tkv" for this suite.
May 13 09:21:05.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:21:05.981: INFO: namespace: e2e-tests-services-d6tkv, resource: bindings, ignored listing per whitelist
May 13 09:21:06.090: INFO: namespace e2e-tests-services-d6tkv deletion completed in 24.223333063s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:32.769 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:21:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-x4fl8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-x4fl8
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-x4fl8
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-x4fl8
May 13 09:21:06.397: INFO: Found 0 stateful pods, waiting for 1
May 13 09:21:16.409: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 13 09:21:16.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 09:21:16.754: INFO: stderr: ""
May 13 09:21:16.755: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 09:21:16.755: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 09:21:16.759: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 13 09:21:26.764: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 09:21:26.764: INFO: Waiting for statefulset status.replicas updated to 0
May 13 09:21:26.895: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:21:26.895: INFO: ss-0  172.16.176.226  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:06 +0000 UTC  }]
May 13 09:21:26.895: INFO: 
May 13 09:21:26.895: INFO: StatefulSet ss has not reached scale 3, at 1
May 13 09:21:27.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990595767s
May 13 09:21:28.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986183473s
May 13 09:21:29.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981653096s
May 13 09:21:30.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977041508s
May 13 09:21:31.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971722155s
May 13 09:21:32.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967732574s
May 13 09:21:33.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963136032s
May 13 09:21:34.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956781226s
May 13 09:21:35.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.683966ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-x4fl8
May 13 09:21:36.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 09:21:37.195: INFO: stderr: ""
May 13 09:21:37.195: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 09:21:37.195: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 09:21:37.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 09:21:37.499: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 13 09:21:37.500: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 09:21:37.500: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 09:21:37.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 09:21:37.774: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 13 09:21:37.774: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 09:21:37.774: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 09:21:37.778: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 13 09:21:47.784: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 09:21:47.784: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 09:21:47.784: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 13 09:21:47.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 09:21:48.058: INFO: stderr: ""
May 13 09:21:48.058: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 09:21:48.058: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 09:21:48.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 09:21:48.416: INFO: stderr: ""
May 13 09:21:48.416: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 09:21:48.416: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 09:21:48.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-x4fl8 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 09:21:48.745: INFO: stderr: ""
May 13 09:21:48.745: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 09:21:48.745: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 09:21:48.745: INFO: Waiting for statefulset status.replicas updated to 0
May 13 09:21:48.748: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 13 09:21:58.757: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 09:21:58.757: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 13 09:21:58.757: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 13 09:21:58.779: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:21:58.779: INFO: ss-0  172.16.176.226  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:06 +0000 UTC  }]
May 13 09:21:58.779: INFO: ss-1  172.16.177.10   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:21:58.779: INFO: ss-2  172.16.173.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:21:58.779: INFO: 
May 13 09:21:58.779: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 09:21:59.793: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:21:59.793: INFO: ss-0  172.16.176.226  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:06 +0000 UTC  }]
May 13 09:21:59.794: INFO: ss-1  172.16.177.10   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:21:59.794: INFO: ss-2  172.16.173.202  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:21:59.794: INFO: 
May 13 09:21:59.794: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 09:22:00.802: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:00.802: INFO: ss-0  172.16.176.226  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:06 +0000 UTC  }]
May 13 09:22:00.804: INFO: ss-1  172.16.177.10   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:20:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:00.804: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:00.804: INFO: 
May 13 09:22:00.804: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 09:22:01.813: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:01.813: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:01.813: INFO: 
May 13 09:22:01.813: INFO: StatefulSet ss has not reached scale 0, at 1
May 13 09:22:02.817: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:02.817: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:02.817: INFO: 
May 13 09:22:02.817: INFO: StatefulSet ss has not reached scale 0, at 1
May 13 09:22:03.821: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:03.822: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:03.822: INFO: 
May 13 09:22:03.822: INFO: StatefulSet ss has not reached scale 0, at 1
May 13 09:22:04.827: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:04.827: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:04.827: INFO: 
May 13 09:22:04.827: INFO: StatefulSet ss has not reached scale 0, at 1
May 13 09:22:05.832: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 13 09:22:05.832: INFO: ss-2  172.16.173.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:21:26 +0000 UTC  }]
May 13 09:22:05.832: INFO: 
May 13 09:22:05.832: INFO: StatefulSet ss has not reached scale 0, at 1
May 13 09:22:06.836: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.942300813s
May 13 09:22:07.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 938.256535ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-x4fl8
May 13 09:22:08.845: INFO: Scaling statefulset ss to 0
May 13 09:22:08.857: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 09:22:08.860: INFO: Deleting all statefulset in ns e2e-tests-statefulset-x4fl8
May 13 09:22:08.862: INFO: Scaling statefulset ss to 0
May 13 09:22:08.873: INFO: Waiting for statefulset status.replicas updated to 0
May 13 09:22:08.875: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:22:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-x4fl8" for this suite.
May 13 09:22:14.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:22:15.039: INFO: namespace: e2e-tests-statefulset-x4fl8, resource: bindings, ignored listing per whitelist
May 13 09:22:15.131: INFO: namespace e2e-tests-statefulset-x4fl8 deletion completed in 6.23605834s

• [SLOW TEST:69.040 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:22:15.131: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hv28x
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-98e1caaa-7560-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-98e1caaa-7560-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:23:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hv28x" for this suite.
May 13 09:24:00.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:24:00.294: INFO: namespace: e2e-tests-projected-hv28x, resource: bindings, ignored listing per whitelist
May 13 09:24:00.314: INFO: namespace e2e-tests-projected-hv28x deletion completed in 22.178448206s

• [SLOW TEST:105.182 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:24:00.315: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nxwtc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:24:00.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-nxwtc" to be "success or failure"
May 13 09:24:00.589: INFO: Pod "downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5111ms
May 13 09:24:02.593: INFO: Pod "downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009109997s
STEP: Saw pod success
May 13 09:24:02.593: INFO: Pod "downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:24:02.595: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:24:02.619: INFO: Waiting for pod downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:24:02.622: INFO: Pod downwardapi-volume-d78f27ef-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:24:02.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nxwtc" for this suite.
May 13 09:24:08.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:24:08.766: INFO: namespace: e2e-tests-projected-nxwtc, resource: bindings, ignored listing per whitelist
May 13 09:24:08.828: INFO: namespace e2e-tests-projected-nxwtc deletion completed in 6.201784021s

• [SLOW TEST:8.514 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:24:08.829: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qz68r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-dca50d31-7560-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:24:09.195: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-qz68r" to be "success or failure"
May 13 09:24:09.200: INFO: Pod "pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265334ms
May 13 09:24:11.205: INFO: Pod "pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009441305s
May 13 09:24:13.209: INFO: Pod "pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014048241s
STEP: Saw pod success
May 13 09:24:13.210: INFO: Pod "pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:24:13.212: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 09:24:13.242: INFO: Waiting for pod pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:24:13.248: INFO: Pod pod-projected-secrets-dca5dec7-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:24:13.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qz68r" for this suite.
May 13 09:24:19.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:24:19.476: INFO: namespace: e2e-tests-projected-qz68r, resource: bindings, ignored listing per whitelist
May 13 09:24:19.595: INFO: namespace e2e-tests-projected-qz68r deletion completed in 6.340500759s

• [SLOW TEST:10.766 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:24:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-z8bxp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e30dc6b2-7560-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:24:19.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-z8bxp" to be "success or failure"
May 13 09:24:19.881: INFO: Pod "pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825577ms
May 13 09:24:21.885: INFO: Pod "pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007005623s
May 13 09:24:23.891: INFO: Pod "pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012825085s
STEP: Saw pod success
May 13 09:24:23.891: INFO: Pod "pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:24:23.895: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:24:23.953: INFO: Waiting for pod pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:24:23.955: INFO: Pod pod-projected-configmaps-e30e4967-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:24:23.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z8bxp" for this suite.
May 13 09:24:29.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:24:30.100: INFO: namespace: e2e-tests-projected-z8bxp, resource: bindings, ignored listing per whitelist
May 13 09:24:30.203: INFO: namespace e2e-tests-projected-z8bxp deletion completed in 6.241655205s

• [SLOW TEST:10.608 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:24:30.205: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-lv584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-lv584
I0513 09:24:30.500436      21 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-lv584, replica count: 1
I0513 09:24:31.553255      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 09:24:32.553533      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 13 09:24:32.680: INFO: Created: latency-svc-m5b8q
May 13 09:24:32.694: INFO: Got endpoints: latency-svc-m5b8q [27.034584ms]
May 13 09:24:32.708: INFO: Created: latency-svc-9rbfs
May 13 09:24:32.713: INFO: Created: latency-svc-fxwvt
May 13 09:24:32.714: INFO: Got endpoints: latency-svc-9rbfs [18.967031ms]
May 13 09:24:32.721: INFO: Created: latency-svc-hfxpd
May 13 09:24:32.725: INFO: Got endpoints: latency-svc-fxwvt [30.648714ms]
May 13 09:24:32.731: INFO: Got endpoints: latency-svc-hfxpd [35.955187ms]
May 13 09:24:32.734: INFO: Created: latency-svc-plz2m
May 13 09:24:32.749: INFO: Created: latency-svc-b9cmz
May 13 09:24:32.751: INFO: Got endpoints: latency-svc-plz2m [55.786507ms]
May 13 09:24:32.756: INFO: Got endpoints: latency-svc-b9cmz [61.153943ms]
May 13 09:24:32.761: INFO: Created: latency-svc-4m5js
May 13 09:24:32.764: INFO: Got endpoints: latency-svc-4m5js [69.034524ms]
May 13 09:24:32.772: INFO: Created: latency-svc-9946l
May 13 09:24:32.781: INFO: Got endpoints: latency-svc-9946l [85.806209ms]
May 13 09:24:32.781: INFO: Created: latency-svc-mxctz
May 13 09:24:32.789: INFO: Got endpoints: latency-svc-mxctz [94.192529ms]
May 13 09:24:32.789: INFO: Created: latency-svc-ljvfv
May 13 09:24:32.797: INFO: Got endpoints: latency-svc-ljvfv [101.790444ms]
May 13 09:24:32.797: INFO: Created: latency-svc-cqwvc
May 13 09:24:32.803: INFO: Created: latency-svc-dvrq5
May 13 09:24:32.805: INFO: Got endpoints: latency-svc-cqwvc [109.615728ms]
May 13 09:24:32.809: INFO: Got endpoints: latency-svc-dvrq5 [114.062313ms]
May 13 09:24:32.816: INFO: Created: latency-svc-zltwn
May 13 09:24:32.822: INFO: Created: latency-svc-wslzh
May 13 09:24:32.822: INFO: Got endpoints: latency-svc-zltwn [126.736553ms]
May 13 09:24:32.825: INFO: Got endpoints: latency-svc-wslzh [129.999745ms]
May 13 09:24:32.833: INFO: Created: latency-svc-9sbvd
May 13 09:24:32.839: INFO: Created: latency-svc-jp5db
May 13 09:24:32.841: INFO: Got endpoints: latency-svc-9sbvd [146.582269ms]
May 13 09:24:32.850: INFO: Got endpoints: latency-svc-jp5db [155.096284ms]
May 13 09:24:32.851: INFO: Created: latency-svc-5pzsw
May 13 09:24:32.870: INFO: Got endpoints: latency-svc-5pzsw [156.759715ms]
May 13 09:24:32.871: INFO: Created: latency-svc-hkkq6
May 13 09:24:32.873: INFO: Created: latency-svc-k2grh
May 13 09:24:32.878: INFO: Got endpoints: latency-svc-hkkq6 [152.315116ms]
May 13 09:24:32.882: INFO: Created: latency-svc-k74wn
May 13 09:24:32.884: INFO: Got endpoints: latency-svc-k2grh [153.254831ms]
May 13 09:24:32.892: INFO: Got endpoints: latency-svc-k74wn [141.007236ms]
May 13 09:24:32.894: INFO: Created: latency-svc-mtzp9
May 13 09:24:32.896: INFO: Created: latency-svc-mjhxv
May 13 09:24:32.899: INFO: Got endpoints: latency-svc-mtzp9 [142.754404ms]
May 13 09:24:32.907: INFO: Got endpoints: latency-svc-mjhxv [142.515753ms]
May 13 09:24:32.907: INFO: Created: latency-svc-lxh5g
May 13 09:24:32.913: INFO: Got endpoints: latency-svc-lxh5g [132.066858ms]
May 13 09:24:32.914: INFO: Created: latency-svc-l2ff9
May 13 09:24:32.919: INFO: Created: latency-svc-t569v
May 13 09:24:32.925: INFO: Created: latency-svc-k9vb8
May 13 09:24:32.926: INFO: Got endpoints: latency-svc-t569v [129.562069ms]
May 13 09:24:32.926: INFO: Got endpoints: latency-svc-l2ff9 [137.233026ms]
May 13 09:24:32.935: INFO: Got endpoints: latency-svc-k9vb8 [130.24624ms]
May 13 09:24:32.937: INFO: Created: latency-svc-rhbmx
May 13 09:24:32.941: INFO: Got endpoints: latency-svc-rhbmx [132.118818ms]
May 13 09:24:32.942: INFO: Created: latency-svc-l8g9l
May 13 09:24:32.949: INFO: Got endpoints: latency-svc-l8g9l [127.049125ms]
May 13 09:24:32.952: INFO: Created: latency-svc-shzzb
May 13 09:24:32.955: INFO: Got endpoints: latency-svc-shzzb [130.419471ms]
May 13 09:24:32.956: INFO: Created: latency-svc-7zx44
May 13 09:24:32.962: INFO: Got endpoints: latency-svc-7zx44 [120.404715ms]
May 13 09:24:32.963: INFO: Created: latency-svc-tvk5n
May 13 09:24:32.989: INFO: Created: latency-svc-g9d8p
May 13 09:24:32.989: INFO: Got endpoints: latency-svc-tvk5n [138.65499ms]
May 13 09:24:32.990: INFO: Created: latency-svc-8465f
May 13 09:24:32.999: INFO: Created: latency-svc-zk7pk
May 13 09:24:32.999: INFO: Got endpoints: latency-svc-g9d8p [128.707865ms]
May 13 09:24:32.999: INFO: Got endpoints: latency-svc-8465f [121.534618ms]
May 13 09:24:33.011: INFO: Created: latency-svc-lbkrv
May 13 09:24:33.011: INFO: Got endpoints: latency-svc-zk7pk [126.715239ms]
May 13 09:24:33.016: INFO: Got endpoints: latency-svc-lbkrv [124.286076ms]
May 13 09:24:33.016: INFO: Created: latency-svc-7bw5f
May 13 09:24:33.022: INFO: Got endpoints: latency-svc-7bw5f [123.086979ms]
May 13 09:24:33.028: INFO: Created: latency-svc-xjhc6
May 13 09:24:33.030: INFO: Created: latency-svc-cwtbj
May 13 09:24:33.037: INFO: Got endpoints: latency-svc-xjhc6 [130.614087ms]
May 13 09:24:33.038: INFO: Created: latency-svc-9wrqz
May 13 09:24:33.043: INFO: Created: latency-svc-gd2tr
May 13 09:24:33.050: INFO: Created: latency-svc-5qss9
May 13 09:24:33.056: INFO: Created: latency-svc-g9x79
May 13 09:24:33.063: INFO: Created: latency-svc-h4fkk
May 13 09:24:33.069: INFO: Created: latency-svc-tjrwz
May 13 09:24:33.075: INFO: Created: latency-svc-bk4vt
May 13 09:24:33.083: INFO: Created: latency-svc-5r5j7
May 13 09:24:33.102: INFO: Got endpoints: latency-svc-cwtbj [189.227212ms]
May 13 09:24:33.106: INFO: Created: latency-svc-crnr7
May 13 09:24:33.118: INFO: Created: latency-svc-rhngm
May 13 09:24:33.127: INFO: Created: latency-svc-hsttb
May 13 09:24:33.143: INFO: Got endpoints: latency-svc-9wrqz [216.410821ms]
May 13 09:24:33.147: INFO: Created: latency-svc-slbhk
May 13 09:24:33.166: INFO: Created: latency-svc-qrcc2
May 13 09:24:33.166: INFO: Created: latency-svc-jqgrv
May 13 09:24:33.166: INFO: Created: latency-svc-pz9cb
May 13 09:24:33.178: INFO: Created: latency-svc-f2crs
May 13 09:24:33.190: INFO: Got endpoints: latency-svc-gd2tr [263.583777ms]
May 13 09:24:33.199: INFO: Created: latency-svc-wt27g
May 13 09:24:33.240: INFO: Got endpoints: latency-svc-5qss9 [304.773933ms]
May 13 09:24:33.252: INFO: Created: latency-svc-44kls
May 13 09:24:33.287: INFO: Got endpoints: latency-svc-g9x79 [345.717711ms]
May 13 09:24:33.303: INFO: Created: latency-svc-rsddh
May 13 09:24:33.347: INFO: Got endpoints: latency-svc-h4fkk [398.512088ms]
May 13 09:24:33.363: INFO: Created: latency-svc-tc96p
May 13 09:24:33.386: INFO: Got endpoints: latency-svc-tjrwz [430.98097ms]
May 13 09:24:33.402: INFO: Created: latency-svc-c76l4
May 13 09:24:33.440: INFO: Got endpoints: latency-svc-bk4vt [478.305567ms]
May 13 09:24:33.453: INFO: Created: latency-svc-hcl4n
May 13 09:24:33.489: INFO: Got endpoints: latency-svc-5r5j7 [500.183637ms]
May 13 09:24:33.503: INFO: Created: latency-svc-lc4f9
May 13 09:24:33.537: INFO: Got endpoints: latency-svc-crnr7 [537.48546ms]
May 13 09:24:33.548: INFO: Created: latency-svc-p2wjl
May 13 09:24:33.587: INFO: Got endpoints: latency-svc-rhngm [587.572155ms]
May 13 09:24:33.599: INFO: Created: latency-svc-t67kw
May 13 09:24:33.640: INFO: Got endpoints: latency-svc-hsttb [629.498062ms]
May 13 09:24:33.652: INFO: Created: latency-svc-kvpmq
May 13 09:24:33.688: INFO: Got endpoints: latency-svc-slbhk [672.101628ms]
May 13 09:24:33.710: INFO: Created: latency-svc-w6nvv
May 13 09:24:33.737: INFO: Got endpoints: latency-svc-jqgrv [715.227239ms]
May 13 09:24:33.753: INFO: Created: latency-svc-td4pg
May 13 09:24:33.786: INFO: Got endpoints: latency-svc-pz9cb [749.063417ms]
May 13 09:24:33.795: INFO: Created: latency-svc-dwzw9
May 13 09:24:33.837: INFO: Got endpoints: latency-svc-qrcc2 [735.05805ms]
May 13 09:24:33.847: INFO: Created: latency-svc-9tw58
May 13 09:24:33.887: INFO: Got endpoints: latency-svc-f2crs [744.601035ms]
May 13 09:24:33.898: INFO: Created: latency-svc-lf6xl
May 13 09:24:33.937: INFO: Got endpoints: latency-svc-wt27g [747.183528ms]
May 13 09:24:33.949: INFO: Created: latency-svc-l5fjh
May 13 09:24:33.986: INFO: Got endpoints: latency-svc-44kls [746.743398ms]
May 13 09:24:33.999: INFO: Created: latency-svc-tlf6x
May 13 09:24:34.036: INFO: Got endpoints: latency-svc-rsddh [749.031611ms]
May 13 09:24:34.047: INFO: Created: latency-svc-tlf7f
May 13 09:24:34.090: INFO: Got endpoints: latency-svc-tc96p [741.814208ms]
May 13 09:24:34.102: INFO: Created: latency-svc-hd2ch
May 13 09:24:34.138: INFO: Got endpoints: latency-svc-c76l4 [751.72754ms]
May 13 09:24:34.150: INFO: Created: latency-svc-qzb9x
May 13 09:24:34.187: INFO: Got endpoints: latency-svc-hcl4n [746.204065ms]
May 13 09:24:34.197: INFO: Created: latency-svc-9w2dk
May 13 09:24:34.237: INFO: Got endpoints: latency-svc-lc4f9 [747.883474ms]
May 13 09:24:34.245: INFO: Created: latency-svc-tdkjk
May 13 09:24:34.289: INFO: Got endpoints: latency-svc-p2wjl [751.912668ms]
May 13 09:24:34.300: INFO: Created: latency-svc-65kpw
May 13 09:24:34.337: INFO: Got endpoints: latency-svc-t67kw [749.89743ms]
May 13 09:24:34.352: INFO: Created: latency-svc-fmcft
May 13 09:24:34.391: INFO: Got endpoints: latency-svc-kvpmq [750.405484ms]
May 13 09:24:34.402: INFO: Created: latency-svc-95kdc
May 13 09:24:34.437: INFO: Got endpoints: latency-svc-w6nvv [749.244817ms]
May 13 09:24:34.450: INFO: Created: latency-svc-gqzzr
May 13 09:24:34.487: INFO: Got endpoints: latency-svc-td4pg [749.42659ms]
May 13 09:24:34.507: INFO: Created: latency-svc-s5fqt
May 13 09:24:34.539: INFO: Got endpoints: latency-svc-dwzw9 [752.34932ms]
May 13 09:24:34.556: INFO: Created: latency-svc-dqqg5
May 13 09:24:34.593: INFO: Got endpoints: latency-svc-9tw58 [756.063954ms]
May 13 09:24:34.612: INFO: Created: latency-svc-x2cr2
May 13 09:24:34.640: INFO: Got endpoints: latency-svc-lf6xl [752.995072ms]
May 13 09:24:34.655: INFO: Created: latency-svc-l64nh
May 13 09:24:34.688: INFO: Got endpoints: latency-svc-l5fjh [750.746651ms]
May 13 09:24:34.705: INFO: Created: latency-svc-vghbh
May 13 09:24:34.740: INFO: Got endpoints: latency-svc-tlf6x [753.072688ms]
May 13 09:24:34.757: INFO: Created: latency-svc-8qpv9
May 13 09:24:34.789: INFO: Got endpoints: latency-svc-tlf7f [752.965209ms]
May 13 09:24:34.807: INFO: Created: latency-svc-sd2vg
May 13 09:24:34.837: INFO: Got endpoints: latency-svc-hd2ch [746.981154ms]
May 13 09:24:34.850: INFO: Created: latency-svc-jq8gt
May 13 09:24:34.888: INFO: Got endpoints: latency-svc-qzb9x [749.810599ms]
May 13 09:24:34.902: INFO: Created: latency-svc-mr5hn
May 13 09:24:34.946: INFO: Got endpoints: latency-svc-9w2dk [759.375051ms]
May 13 09:24:34.958: INFO: Created: latency-svc-tm9m8
May 13 09:24:34.989: INFO: Got endpoints: latency-svc-tdkjk [752.143896ms]
May 13 09:24:35.007: INFO: Created: latency-svc-fxwxl
May 13 09:24:35.045: INFO: Got endpoints: latency-svc-65kpw [756.073031ms]
May 13 09:24:35.054: INFO: Created: latency-svc-mpdrq
May 13 09:24:35.087: INFO: Got endpoints: latency-svc-fmcft [749.51452ms]
May 13 09:24:35.100: INFO: Created: latency-svc-k5hz4
May 13 09:24:35.149: INFO: Got endpoints: latency-svc-95kdc [758.584054ms]
May 13 09:24:35.167: INFO: Created: latency-svc-gnzjn
May 13 09:24:35.196: INFO: Got endpoints: latency-svc-gqzzr [759.027789ms]
May 13 09:24:35.211: INFO: Created: latency-svc-5dqms
May 13 09:24:35.241: INFO: Got endpoints: latency-svc-s5fqt [754.425369ms]
May 13 09:24:35.278: INFO: Created: latency-svc-d7bd6
May 13 09:24:35.291: INFO: Got endpoints: latency-svc-dqqg5 [751.921281ms]
May 13 09:24:35.306: INFO: Created: latency-svc-87rzj
May 13 09:24:35.352: INFO: Got endpoints: latency-svc-x2cr2 [758.814015ms]
May 13 09:24:35.368: INFO: Created: latency-svc-tcrjh
May 13 09:24:35.400: INFO: Got endpoints: latency-svc-l64nh [759.961092ms]
May 13 09:24:35.414: INFO: Created: latency-svc-jl24k
May 13 09:24:35.438: INFO: Got endpoints: latency-svc-vghbh [749.578348ms]
May 13 09:24:35.460: INFO: Created: latency-svc-dnx4s
May 13 09:24:35.489: INFO: Got endpoints: latency-svc-8qpv9 [749.618839ms]
May 13 09:24:35.500: INFO: Created: latency-svc-kxbmf
May 13 09:24:35.538: INFO: Got endpoints: latency-svc-sd2vg [749.038014ms]
May 13 09:24:35.559: INFO: Created: latency-svc-nmjhn
May 13 09:24:35.596: INFO: Got endpoints: latency-svc-jq8gt [758.894496ms]
May 13 09:24:35.606: INFO: Created: latency-svc-k8nfk
May 13 09:24:35.641: INFO: Got endpoints: latency-svc-mr5hn [752.274045ms]
May 13 09:24:35.650: INFO: Created: latency-svc-pfnff
May 13 09:24:35.688: INFO: Got endpoints: latency-svc-tm9m8 [742.180004ms]
May 13 09:24:35.710: INFO: Created: latency-svc-68jxm
May 13 09:24:35.741: INFO: Got endpoints: latency-svc-fxwxl [751.769515ms]
May 13 09:24:35.763: INFO: Created: latency-svc-wb2xg
May 13 09:24:35.788: INFO: Got endpoints: latency-svc-mpdrq [742.897275ms]
May 13 09:24:35.803: INFO: Created: latency-svc-drdxb
May 13 09:24:35.839: INFO: Got endpoints: latency-svc-k5hz4 [751.812673ms]
May 13 09:24:35.853: INFO: Created: latency-svc-jb422
May 13 09:24:35.888: INFO: Got endpoints: latency-svc-gnzjn [737.844529ms]
May 13 09:24:35.904: INFO: Created: latency-svc-kmptq
May 13 09:24:35.937: INFO: Got endpoints: latency-svc-5dqms [740.273803ms]
May 13 09:24:35.949: INFO: Created: latency-svc-5tfhm
May 13 09:24:35.991: INFO: Got endpoints: latency-svc-d7bd6 [748.952185ms]
May 13 09:24:36.004: INFO: Created: latency-svc-n5pfx
May 13 09:24:36.037: INFO: Got endpoints: latency-svc-87rzj [746.334228ms]
May 13 09:24:36.047: INFO: Created: latency-svc-v6sc2
May 13 09:24:36.086: INFO: Got endpoints: latency-svc-tcrjh [733.707686ms]
May 13 09:24:36.099: INFO: Created: latency-svc-slg5f
May 13 09:24:36.138: INFO: Got endpoints: latency-svc-jl24k [737.405918ms]
May 13 09:24:36.148: INFO: Created: latency-svc-nlbj2
May 13 09:24:36.187: INFO: Got endpoints: latency-svc-dnx4s [749.109616ms]
May 13 09:24:36.196: INFO: Created: latency-svc-mxjrm
May 13 09:24:36.238: INFO: Got endpoints: latency-svc-kxbmf [748.314798ms]
May 13 09:24:36.253: INFO: Created: latency-svc-wvxgj
May 13 09:24:36.287: INFO: Got endpoints: latency-svc-nmjhn [748.949956ms]
May 13 09:24:36.301: INFO: Created: latency-svc-5wpx6
May 13 09:24:36.337: INFO: Got endpoints: latency-svc-k8nfk [741.168497ms]
May 13 09:24:36.353: INFO: Created: latency-svc-8zqz8
May 13 09:24:36.391: INFO: Got endpoints: latency-svc-pfnff [749.919877ms]
May 13 09:24:36.404: INFO: Created: latency-svc-vssvp
May 13 09:24:36.439: INFO: Got endpoints: latency-svc-68jxm [749.956519ms]
May 13 09:24:36.451: INFO: Created: latency-svc-srs89
May 13 09:24:36.488: INFO: Got endpoints: latency-svc-wb2xg [746.801945ms]
May 13 09:24:36.498: INFO: Created: latency-svc-qrlhz
May 13 09:24:36.540: INFO: Got endpoints: latency-svc-drdxb [751.672007ms]
May 13 09:24:36.549: INFO: Created: latency-svc-rg28g
May 13 09:24:36.589: INFO: Got endpoints: latency-svc-jb422 [750.653077ms]
May 13 09:24:36.600: INFO: Created: latency-svc-g2d2n
May 13 09:24:36.640: INFO: Got endpoints: latency-svc-kmptq [752.246398ms]
May 13 09:24:36.653: INFO: Created: latency-svc-8chlw
May 13 09:24:36.688: INFO: Got endpoints: latency-svc-5tfhm [750.507084ms]
May 13 09:24:36.697: INFO: Created: latency-svc-kqfcj
May 13 09:24:36.737: INFO: Got endpoints: latency-svc-n5pfx [746.649893ms]
May 13 09:24:36.749: INFO: Created: latency-svc-dkqq8
May 13 09:24:36.787: INFO: Got endpoints: latency-svc-v6sc2 [749.911666ms]
May 13 09:24:36.806: INFO: Created: latency-svc-gk7s8
May 13 09:24:36.838: INFO: Got endpoints: latency-svc-slg5f [751.770185ms]
May 13 09:24:36.849: INFO: Created: latency-svc-qjjhc
May 13 09:24:36.888: INFO: Got endpoints: latency-svc-nlbj2 [749.634208ms]
May 13 09:24:36.898: INFO: Created: latency-svc-dd6z9
May 13 09:24:36.945: INFO: Got endpoints: latency-svc-mxjrm [758.234635ms]
May 13 09:24:36.961: INFO: Created: latency-svc-9p566
May 13 09:24:36.997: INFO: Got endpoints: latency-svc-wvxgj [759.024605ms]
May 13 09:24:37.011: INFO: Created: latency-svc-jf45d
May 13 09:24:37.037: INFO: Got endpoints: latency-svc-5wpx6 [749.68078ms]
May 13 09:24:37.047: INFO: Created: latency-svc-xc7td
May 13 09:24:37.086: INFO: Got endpoints: latency-svc-8zqz8 [748.518174ms]
May 13 09:24:37.101: INFO: Created: latency-svc-fsxbd
May 13 09:24:37.138: INFO: Got endpoints: latency-svc-vssvp [746.958976ms]
May 13 09:24:37.150: INFO: Created: latency-svc-qnlf4
May 13 09:24:37.186: INFO: Got endpoints: latency-svc-srs89 [746.49929ms]
May 13 09:24:37.195: INFO: Created: latency-svc-vffqp
May 13 09:24:37.237: INFO: Got endpoints: latency-svc-qrlhz [748.956422ms]
May 13 09:24:37.254: INFO: Created: latency-svc-thdwh
May 13 09:24:37.287: INFO: Got endpoints: latency-svc-rg28g [747.009423ms]
May 13 09:24:37.297: INFO: Created: latency-svc-hmshn
May 13 09:24:37.336: INFO: Got endpoints: latency-svc-g2d2n [746.256233ms]
May 13 09:24:37.346: INFO: Created: latency-svc-d4gpq
May 13 09:24:37.386: INFO: Got endpoints: latency-svc-8chlw [746.242256ms]
May 13 09:24:37.398: INFO: Created: latency-svc-rn9sz
May 13 09:24:37.438: INFO: Got endpoints: latency-svc-kqfcj [750.340418ms]
May 13 09:24:37.455: INFO: Created: latency-svc-f7qvr
May 13 09:24:37.489: INFO: Got endpoints: latency-svc-dkqq8 [751.296429ms]
May 13 09:24:37.500: INFO: Created: latency-svc-wqhzh
May 13 09:24:37.539: INFO: Got endpoints: latency-svc-gk7s8 [751.842975ms]
May 13 09:24:37.558: INFO: Created: latency-svc-zgrmv
May 13 09:24:37.590: INFO: Got endpoints: latency-svc-qjjhc [750.905905ms]
May 13 09:24:37.604: INFO: Created: latency-svc-mvr8w
May 13 09:24:37.640: INFO: Got endpoints: latency-svc-dd6z9 [751.722084ms]
May 13 09:24:37.649: INFO: Created: latency-svc-b97xq
May 13 09:24:37.693: INFO: Got endpoints: latency-svc-9p566 [747.572338ms]
May 13 09:24:37.705: INFO: Created: latency-svc-fplnm
May 13 09:24:37.741: INFO: Got endpoints: latency-svc-jf45d [743.829847ms]
May 13 09:24:37.750: INFO: Created: latency-svc-ln8rc
May 13 09:24:37.787: INFO: Got endpoints: latency-svc-xc7td [749.672599ms]
May 13 09:24:37.797: INFO: Created: latency-svc-znqqp
May 13 09:24:37.838: INFO: Got endpoints: latency-svc-fsxbd [751.657449ms]
May 13 09:24:37.853: INFO: Created: latency-svc-c7r87
May 13 09:24:37.888: INFO: Got endpoints: latency-svc-qnlf4 [749.851099ms]
May 13 09:24:37.897: INFO: Created: latency-svc-nkpcl
May 13 09:24:37.936: INFO: Got endpoints: latency-svc-vffqp [750.736221ms]
May 13 09:24:37.946: INFO: Created: latency-svc-4x5rj
May 13 09:24:37.987: INFO: Got endpoints: latency-svc-thdwh [749.548471ms]
May 13 09:24:37.997: INFO: Created: latency-svc-wm2jp
May 13 09:24:38.037: INFO: Got endpoints: latency-svc-hmshn [749.945045ms]
May 13 09:24:38.045: INFO: Created: latency-svc-lwwqk
May 13 09:24:38.089: INFO: Got endpoints: latency-svc-d4gpq [753.007907ms]
May 13 09:24:38.098: INFO: Created: latency-svc-j4gs6
May 13 09:24:38.141: INFO: Got endpoints: latency-svc-rn9sz [754.158999ms]
May 13 09:24:38.155: INFO: Created: latency-svc-ck8gd
May 13 09:24:38.187: INFO: Got endpoints: latency-svc-f7qvr [747.621326ms]
May 13 09:24:38.200: INFO: Created: latency-svc-8p6l8
May 13 09:24:38.238: INFO: Got endpoints: latency-svc-wqhzh [749.089223ms]
May 13 09:24:38.247: INFO: Created: latency-svc-5nsfv
May 13 09:24:38.289: INFO: Got endpoints: latency-svc-zgrmv [750.012622ms]
May 13 09:24:38.301: INFO: Created: latency-svc-pqmv7
May 13 09:24:38.338: INFO: Got endpoints: latency-svc-mvr8w [748.041436ms]
May 13 09:24:38.348: INFO: Created: latency-svc-nxm7k
May 13 09:24:38.387: INFO: Got endpoints: latency-svc-b97xq [747.634298ms]
May 13 09:24:38.396: INFO: Created: latency-svc-9ds68
May 13 09:24:38.438: INFO: Got endpoints: latency-svc-fplnm [744.809102ms]
May 13 09:24:38.449: INFO: Created: latency-svc-7sk5x
May 13 09:24:38.486: INFO: Got endpoints: latency-svc-ln8rc [745.527242ms]
May 13 09:24:38.506: INFO: Created: latency-svc-vfwjq
May 13 09:24:38.546: INFO: Got endpoints: latency-svc-znqqp [759.010046ms]
May 13 09:24:38.607: INFO: Got endpoints: latency-svc-c7r87 [769.205347ms]
May 13 09:24:38.608: INFO: Created: latency-svc-nr5ln
May 13 09:24:38.617: INFO: Created: latency-svc-w9zjm
May 13 09:24:38.638: INFO: Got endpoints: latency-svc-nkpcl [749.746678ms]
May 13 09:24:38.646: INFO: Created: latency-svc-tblpl
May 13 09:24:38.686: INFO: Got endpoints: latency-svc-4x5rj [749.253717ms]
May 13 09:24:38.698: INFO: Created: latency-svc-w287h
May 13 09:24:38.736: INFO: Got endpoints: latency-svc-wm2jp [749.544296ms]
May 13 09:24:38.744: INFO: Created: latency-svc-dwv8c
May 13 09:24:38.787: INFO: Got endpoints: latency-svc-lwwqk [749.933233ms]
May 13 09:24:38.795: INFO: Created: latency-svc-76tkj
May 13 09:24:38.837: INFO: Got endpoints: latency-svc-j4gs6 [747.925298ms]
May 13 09:24:38.845: INFO: Created: latency-svc-zzhrl
May 13 09:24:38.886: INFO: Got endpoints: latency-svc-ck8gd [745.192392ms]
May 13 09:24:38.895: INFO: Created: latency-svc-7xwgm
May 13 09:24:38.937: INFO: Got endpoints: latency-svc-8p6l8 [749.57713ms]
May 13 09:24:38.948: INFO: Created: latency-svc-qqlkw
May 13 09:24:38.989: INFO: Got endpoints: latency-svc-5nsfv [750.605093ms]
May 13 09:24:39.000: INFO: Created: latency-svc-qqzdk
May 13 09:24:39.036: INFO: Got endpoints: latency-svc-pqmv7 [747.124893ms]
May 13 09:24:39.046: INFO: Created: latency-svc-2ltpm
May 13 09:24:39.086: INFO: Got endpoints: latency-svc-nxm7k [747.989223ms]
May 13 09:24:39.097: INFO: Created: latency-svc-vnl85
May 13 09:24:39.138: INFO: Got endpoints: latency-svc-9ds68 [750.309688ms]
May 13 09:24:39.156: INFO: Created: latency-svc-j26mb
May 13 09:24:39.187: INFO: Got endpoints: latency-svc-7sk5x [749.025087ms]
May 13 09:24:39.202: INFO: Created: latency-svc-xwvz5
May 13 09:24:39.237: INFO: Got endpoints: latency-svc-vfwjq [750.337614ms]
May 13 09:24:39.255: INFO: Created: latency-svc-2jdpz
May 13 09:24:39.288: INFO: Got endpoints: latency-svc-nr5ln [741.50471ms]
May 13 09:24:39.298: INFO: Created: latency-svc-wqfbb
May 13 09:24:39.336: INFO: Got endpoints: latency-svc-w9zjm [728.800202ms]
May 13 09:24:39.356: INFO: Created: latency-svc-gsdfx
May 13 09:24:39.388: INFO: Got endpoints: latency-svc-tblpl [750.127213ms]
May 13 09:24:39.396: INFO: Created: latency-svc-bphpv
May 13 09:24:39.437: INFO: Got endpoints: latency-svc-w287h [750.851584ms]
May 13 09:24:39.446: INFO: Created: latency-svc-rdvwm
May 13 09:24:39.487: INFO: Got endpoints: latency-svc-dwv8c [750.319847ms]
May 13 09:24:39.496: INFO: Created: latency-svc-n9mfc
May 13 09:24:39.537: INFO: Got endpoints: latency-svc-76tkj [750.408172ms]
May 13 09:24:39.549: INFO: Created: latency-svc-55wm4
May 13 09:24:39.605: INFO: Got endpoints: latency-svc-zzhrl [767.498956ms]
May 13 09:24:39.621: INFO: Created: latency-svc-64mhx
May 13 09:24:39.637: INFO: Got endpoints: latency-svc-7xwgm [750.903511ms]
May 13 09:24:39.650: INFO: Created: latency-svc-gj57t
May 13 09:24:39.690: INFO: Got endpoints: latency-svc-qqlkw [752.730868ms]
May 13 09:24:39.703: INFO: Created: latency-svc-kwg7p
May 13 09:24:39.794: INFO: Got endpoints: latency-svc-qqzdk [804.726393ms]
May 13 09:24:39.795: INFO: Got endpoints: latency-svc-2ltpm [758.901546ms]
May 13 09:24:39.807: INFO: Created: latency-svc-wdrxf
May 13 09:24:39.819: INFO: Created: latency-svc-dh4lt
May 13 09:24:39.840: INFO: Got endpoints: latency-svc-vnl85 [753.202994ms]
May 13 09:24:39.858: INFO: Created: latency-svc-z8w96
May 13 09:24:39.894: INFO: Got endpoints: latency-svc-j26mb [755.792186ms]
May 13 09:24:39.920: INFO: Created: latency-svc-kwnv7
May 13 09:24:39.942: INFO: Got endpoints: latency-svc-xwvz5 [755.087229ms]
May 13 09:24:39.957: INFO: Created: latency-svc-n8pvl
May 13 09:24:39.996: INFO: Got endpoints: latency-svc-2jdpz [758.764249ms]
May 13 09:24:40.005: INFO: Created: latency-svc-5ntkb
May 13 09:24:40.038: INFO: Got endpoints: latency-svc-wqfbb [749.808169ms]
May 13 09:24:40.046: INFO: Created: latency-svc-zc457
May 13 09:24:40.088: INFO: Got endpoints: latency-svc-gsdfx [751.173198ms]
May 13 09:24:40.103: INFO: Created: latency-svc-jg5xn
May 13 09:24:40.138: INFO: Got endpoints: latency-svc-bphpv [749.528387ms]
May 13 09:24:40.148: INFO: Created: latency-svc-58jrc
May 13 09:24:40.186: INFO: Got endpoints: latency-svc-rdvwm [749.210399ms]
May 13 09:24:40.195: INFO: Created: latency-svc-9xdzm
May 13 09:24:40.239: INFO: Got endpoints: latency-svc-n9mfc [751.662845ms]
May 13 09:24:40.251: INFO: Created: latency-svc-4qhvc
May 13 09:24:40.294: INFO: Got endpoints: latency-svc-55wm4 [756.858373ms]
May 13 09:24:40.303: INFO: Created: latency-svc-9rhzr
May 13 09:24:40.337: INFO: Got endpoints: latency-svc-64mhx [732.279208ms]
May 13 09:24:40.346: INFO: Created: latency-svc-fhk4h
May 13 09:24:40.387: INFO: Got endpoints: latency-svc-gj57t [749.856907ms]
May 13 09:24:40.396: INFO: Created: latency-svc-kfw49
May 13 09:24:40.438: INFO: Got endpoints: latency-svc-kwg7p [748.877931ms]
May 13 09:24:40.450: INFO: Created: latency-svc-ckrj2
May 13 09:24:40.487: INFO: Got endpoints: latency-svc-wdrxf [691.734094ms]
May 13 09:24:40.497: INFO: Created: latency-svc-jg2d2
May 13 09:24:40.542: INFO: Got endpoints: latency-svc-dh4lt [748.380528ms]
May 13 09:24:40.589: INFO: Got endpoints: latency-svc-z8w96 [748.826273ms]
May 13 09:24:40.642: INFO: Got endpoints: latency-svc-kwnv7 [748.379299ms]
May 13 09:24:40.688: INFO: Got endpoints: latency-svc-n8pvl [745.559701ms]
May 13 09:24:40.737: INFO: Got endpoints: latency-svc-5ntkb [741.231716ms]
May 13 09:24:40.787: INFO: Got endpoints: latency-svc-zc457 [748.575167ms]
May 13 09:24:40.840: INFO: Got endpoints: latency-svc-jg5xn [752.425701ms]
May 13 09:24:40.890: INFO: Got endpoints: latency-svc-58jrc [752.345918ms]
May 13 09:24:40.945: INFO: Got endpoints: latency-svc-9xdzm [758.465004ms]
May 13 09:24:41.009: INFO: Got endpoints: latency-svc-4qhvc [770.195692ms]
May 13 09:24:41.037: INFO: Got endpoints: latency-svc-9rhzr [742.087085ms]
May 13 09:24:41.088: INFO: Got endpoints: latency-svc-fhk4h [750.31676ms]
May 13 09:24:41.138: INFO: Got endpoints: latency-svc-kfw49 [750.687044ms]
May 13 09:24:41.189: INFO: Got endpoints: latency-svc-ckrj2 [750.218398ms]
May 13 09:24:41.237: INFO: Got endpoints: latency-svc-jg2d2 [750.090546ms]
May 13 09:24:41.237: INFO: Latencies: [18.967031ms 30.648714ms 35.955187ms 55.786507ms 61.153943ms 69.034524ms 85.806209ms 94.192529ms 101.790444ms 109.615728ms 114.062313ms 120.404715ms 121.534618ms 123.086979ms 124.286076ms 126.715239ms 126.736553ms 127.049125ms 128.707865ms 129.562069ms 129.999745ms 130.24624ms 130.419471ms 130.614087ms 132.066858ms 132.118818ms 137.233026ms 138.65499ms 141.007236ms 142.515753ms 142.754404ms 146.582269ms 152.315116ms 153.254831ms 155.096284ms 156.759715ms 189.227212ms 216.410821ms 263.583777ms 304.773933ms 345.717711ms 398.512088ms 430.98097ms 478.305567ms 500.183637ms 537.48546ms 587.572155ms 629.498062ms 672.101628ms 691.734094ms 715.227239ms 728.800202ms 732.279208ms 733.707686ms 735.05805ms 737.405918ms 737.844529ms 740.273803ms 741.168497ms 741.231716ms 741.50471ms 741.814208ms 742.087085ms 742.180004ms 742.897275ms 743.829847ms 744.601035ms 744.809102ms 745.192392ms 745.527242ms 745.559701ms 746.204065ms 746.242256ms 746.256233ms 746.334228ms 746.49929ms 746.649893ms 746.743398ms 746.801945ms 746.958976ms 746.981154ms 747.009423ms 747.124893ms 747.183528ms 747.572338ms 747.621326ms 747.634298ms 747.883474ms 747.925298ms 747.989223ms 748.041436ms 748.314798ms 748.379299ms 748.380528ms 748.518174ms 748.575167ms 748.826273ms 748.877931ms 748.949956ms 748.952185ms 748.956422ms 749.025087ms 749.031611ms 749.038014ms 749.063417ms 749.089223ms 749.109616ms 749.210399ms 749.244817ms 749.253717ms 749.42659ms 749.51452ms 749.528387ms 749.544296ms 749.548471ms 749.57713ms 749.578348ms 749.618839ms 749.634208ms 749.672599ms 749.68078ms 749.746678ms 749.808169ms 749.810599ms 749.851099ms 749.856907ms 749.89743ms 749.911666ms 749.919877ms 749.933233ms 749.945045ms 749.956519ms 750.012622ms 750.090546ms 750.127213ms 750.218398ms 750.309688ms 750.31676ms 750.319847ms 750.337614ms 750.340418ms 750.405484ms 750.408172ms 750.507084ms 750.605093ms 750.653077ms 750.687044ms 750.736221ms 750.746651ms 750.851584ms 750.903511ms 750.905905ms 751.173198ms 751.296429ms 751.657449ms 751.662845ms 751.672007ms 751.722084ms 751.72754ms 751.769515ms 751.770185ms 751.812673ms 751.842975ms 751.912668ms 751.921281ms 752.143896ms 752.246398ms 752.274045ms 752.345918ms 752.34932ms 752.425701ms 752.730868ms 752.965209ms 752.995072ms 753.007907ms 753.072688ms 753.202994ms 754.158999ms 754.425369ms 755.087229ms 755.792186ms 756.063954ms 756.073031ms 756.858373ms 758.234635ms 758.465004ms 758.584054ms 758.764249ms 758.814015ms 758.894496ms 758.901546ms 759.010046ms 759.024605ms 759.027789ms 759.375051ms 759.961092ms 767.498956ms 769.205347ms 770.195692ms 804.726393ms]
May 13 09:24:41.238: INFO: 50 %ile: 748.956422ms
May 13 09:24:41.238: INFO: 90 %ile: 755.792186ms
May 13 09:24:41.238: INFO: 99 %ile: 770.195692ms
May 13 09:24:41.238: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:24:41.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-lv584" for this suite.
May 13 09:24:57.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:24:57.335: INFO: namespace: e2e-tests-svc-latency-lv584, resource: bindings, ignored listing per whitelist
May 13 09:24:57.416: INFO: namespace e2e-tests-svc-latency-lv584 deletion completed in 16.170202525s

• [SLOW TEST:27.212 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:24:57.421: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-s5svz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-s5svz/configmap-test-f999940d-7560-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:24:57.678: INFO: Waiting up to 5m0s for pod "pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-s5svz" to be "success or failure"
May 13 09:24:57.682: INFO: Pod "pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946496ms
May 13 09:24:59.686: INFO: Pod "pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008167549s
May 13 09:25:01.690: INFO: Pod "pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012462147s
STEP: Saw pod success
May 13 09:25:01.690: INFO: Pod "pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:25:01.694: INFO: Trying to get logs from node 172.16.177.10 pod pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4 container env-test: <nil>
STEP: delete the pod
May 13 09:25:01.719: INFO: Waiting for pod pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:25:01.722: INFO: Pod pod-configmaps-f99a639e-7560-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:25:01.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-s5svz" for this suite.
May 13 09:25:07.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:25:07.857: INFO: namespace: e2e-tests-configmap-s5svz, resource: bindings, ignored listing per whitelist
May 13 09:25:07.977: INFO: namespace e2e-tests-configmap-s5svz deletion completed in 6.24713685s

• [SLOW TEST:10.556 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:25:07.977: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sgsr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 13 09:25:12.982: INFO: Successfully updated pod "labelsupdatefff0d4cb-7560-11e9-bbcc-d288ccfb79a4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:25:15.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sgsr5" for this suite.
May 13 09:25:39.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:25:39.288: INFO: namespace: e2e-tests-downward-api-sgsr5, resource: bindings, ignored listing per whitelist
May 13 09:25:39.299: INFO: namespace e2e-tests-downward-api-sgsr5 deletion completed in 24.249813308s

• [SLOW TEST:31.322 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:25:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-b58xb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 13 09:25:48.693: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:25:49.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-b58xb" for this suite.
May 13 09:26:13.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:26:13.841: INFO: namespace: e2e-tests-replicaset-b58xb, resource: bindings, ignored listing per whitelist
May 13 09:26:13.887: INFO: namespace e2e-tests-replicaset-b58xb deletion completed in 24.170834266s

• [SLOW TEST:34.587 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:26:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l6r5d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
May 13 09:26:14.089: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-870666073 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:26:14.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l6r5d" for this suite.
May 13 09:26:20.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:26:20.302: INFO: namespace: e2e-tests-kubectl-l6r5d, resource: bindings, ignored listing per whitelist
May 13 09:26:20.400: INFO: namespace e2e-tests-kubectl-l6r5d deletion completed in 6.174386957s

• [SLOW TEST:6.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:26:20.404: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5jbtl
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-2b165227-7561-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2b165227-7561-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:27:35.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5jbtl" for this suite.
May 13 09:27:59.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:27:59.441: INFO: namespace: e2e-tests-configmap-5jbtl, resource: bindings, ignored listing per whitelist
May 13 09:27:59.542: INFO: namespace e2e-tests-configmap-5jbtl deletion completed in 24.212531112s

• [SLOW TEST:99.138 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:27:59.545: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-t75zf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t75zf.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t75zf.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t75zf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t75zf.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t75zf.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t75zf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 13 09:28:36.301: INFO: DNS probes using e2e-tests-dns-t75zf/dns-test-66270a5e-7561-11e9-bbcc-d288ccfb79a4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:28:36.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-t75zf" for this suite.
May 13 09:28:42.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:28:42.445: INFO: namespace: e2e-tests-dns-t75zf, resource: bindings, ignored listing per whitelist
May 13 09:28:42.516: INFO: namespace e2e-tests-dns-t75zf deletion completed in 6.197812135s

• [SLOW TEST:42.971 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:28:42.517: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-ncjvv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
May 13 09:28:42.783: INFO: Waiting up to 5m0s for pod "var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-var-expansion-ncjvv" to be "success or failure"
May 13 09:28:42.786: INFO: Pod "var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.878079ms
May 13 09:28:44.790: INFO: Pod "var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006885141s
STEP: Saw pod success
May 13 09:28:44.790: INFO: Pod "var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:28:44.792: INFO: Trying to get logs from node 172.16.176.226 pod var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 09:28:44.813: INFO: Waiting for pod var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:28:44.818: INFO: Pod var-expansion-7fc5a4cf-7561-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:28:44.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-ncjvv" for this suite.
May 13 09:28:50.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:28:50.857: INFO: namespace: e2e-tests-var-expansion-ncjvv, resource: bindings, ignored listing per whitelist
May 13 09:28:51.020: INFO: namespace e2e-tests-var-expansion-ncjvv deletion completed in 6.196322686s

• [SLOW TEST:8.503 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:28:51.020: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-pmkjg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 09:28:51.234: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 09:28:51.246: INFO: Waiting for terminating namespaces to be deleted...
May 13 09:28:51.248: INFO: 
Logging pods the kubelet thinks is on node 172.16.173.202 before test
May 13 09:28:51.273: INFO: iam-onboarding-bptkl from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.273: INFO: 	Container iam-onboarding ready: false, restart count 0
May 13 09:28:51.273: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fv8lz from heptio-sonobuoy started at 2019-05-13 08:34:01 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:28:51.273: INFO: 	Container systemd-logs ready: true, restart count 0
May 13 09:28:51.273: INFO: image-manager-0 from kube-system started at 2019-05-08 03:24:10 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.273: INFO: 	Container icp-registry ready: true, restart count 1
May 13 09:28:51.273: INFO: 	Container image-manager ready: true, restart count 0
May 13 09:28:51.273: INFO: calico-node-krfk6 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.273: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:28:51.273: INFO: nvidia-device-plugin-vwsvk from kube-system started at 2019-05-08 03:27:07 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container nvidia-device-plugin ready: true, restart count 1
May 13 09:28:51.274: INFO: ibmcloud-image-enforcement-7c7d9688-tqjm9 from kube-system started at 2019-05-08 03:32:31 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
May 13 09:28:51.274: INFO: audit-logging-fluentd-ds-lr8k9 from kube-system started at 2019-05-08 03:34:29 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:28:51.274: INFO: k8s-master-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:28:51.274: INFO: service-catalog-controller-manager-88d749c7d-6xqsk from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container controller-manager ready: true, restart count 1
May 13 09:28:51.274: INFO: nginx-ingress-controller-vf97z from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container nginx-ingress ready: true, restart count 1
May 13 09:28:51.274: INFO: metering-reader-46vqp from kube-system started at 2019-05-08 03:32:37 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container metering-reader ready: true, restart count 3
May 13 09:28:51.274: INFO: oidc-client-registration-htdq6 from kube-system started at 2019-05-08 03:29:48 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container oidc-client-registration ready: false, restart count 0
May 13 09:28:51.274: INFO: secret-watcher-54fd9cfc6d-gw9pw from kube-system started at 2019-05-08 03:34:09 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container secret-watcher ready: true, restart count 1
May 13 09:28:51.274: INFO: k8s-kmsplugin-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:28:51.274: INFO: k8s-proxy-kxd6g from kube-system started at 2019-05-08 03:23:38 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container proxy ready: true, restart count 0
May 13 09:28:51.274: INFO: cert-manager-ibm-cert-manager-7775495cb4-n84nn from cert-manager started at 2019-05-08 03:23:52 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container ibm-cert-manager ready: true, restart count 8
May 13 09:28:51.274: INFO: kube-dns-28wg9 from kube-system started at 2019-05-08 03:26:55 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container kube-dns ready: true, restart count 1
May 13 09:28:51.274: INFO: helm-api-67456c54d4-rf5rv from kube-system started at 2019-05-08 03:33:58 +0000 UTC (3 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container helmapi ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container rudder ready: true, restart count 0
May 13 09:28:51.274: INFO: image-manager-init-certs-5cjmr from kube-system started at 2019-05-08 03:24:09 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container init-certs ready: true, restart count 1
May 13 09:28:51.274: INFO: auth-idp-cqjpn from kube-system started at 2019-05-08 03:29:48 +0000 UTC (4 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container platform-auth-service ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container platform-identity-manager ready: true, restart count 1
May 13 09:28:51.274: INFO: 	Container platform-identity-provider ready: true, restart count 1
May 13 09:28:51.274: INFO: platform-ui-sfdsp from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container platform-ui ready: true, restart count 0
May 13 09:28:51.274: INFO: platform-header-5hnzj from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container platform-header ready: true, restart count 1
May 13 09:28:51.274: INFO: calico-kube-controllers-7f949c47f-sgggp from kube-system started at 2019-05-08 03:26:42 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 13 09:28:51.274: INFO: default-http-backend-567686995f-psknt from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container default-http-backend ready: true, restart count 1
May 13 09:28:51.274: INFO: sonobuoy-e2e-job-9886345f531c45a5 from heptio-sonobuoy started at 2019-05-13 08:33:59 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container e2e ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:28:51.274: INFO: logging-elk-filebeat-ds-tlc56 from kube-system started at 2019-05-08 03:33:38 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container filebeat ready: true, restart count 1
May 13 09:28:51.274: INFO: k8s-etcd-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:28:51.274: INFO: tiller-deploy-57774fdbd5-t4zqz from kube-system started at 2019-05-08 03:23:25 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container tiller ready: true, restart count 1
May 13 09:28:51.274: INFO: auth-pdp-82gc5 from kube-system started at 2019-05-08 03:30:03 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container auth-pdp ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: icp-management-ingress-8jgdq from kube-system started at 2019-05-08 03:30:09 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container icp-management-ingress ready: true, restart count 0
May 13 09:28:51.274: INFO: icp-mongodb-0 from kube-system started at 2019-05-08 03:27:17 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container icp-mongodb ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container metrics ready: true, restart count 0
May 13 09:28:51.274: INFO: service-catalog-apiserver-9dt69 from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container apiserver ready: true, restart count 4
May 13 09:28:51.274: INFO: auth-pap-9bjv2 from kube-system started at 2019-05-08 03:29:58 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container auth-pap ready: true, restart count 2
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: helm-repo-548dbc4c5c-lj4r2 from kube-system started at 2019-05-08 03:32:19 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container helm-repo ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: platform-api-994c46799-5f4ml from kube-system started at 2019-05-08 03:27:31 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container audit-service ready: true, restart count 1
May 13 09:28:51.274: INFO: 	Container platform-api ready: true, restart count 10
May 13 09:28:51.274: INFO: monitoring-prometheus-nodeexporter-lwpx9 from kube-system started at 2019-05-08 03:33:46 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container nodeexporter ready: true, restart count 1
May 13 09:28:51.274: INFO: 	Container router ready: true, restart count 1
May 13 09:28:51.274: INFO: mgmt-repo-79f95d66b8-lp7sf from kube-system started at 2019-05-08 03:34:03 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.274: INFO: 	Container mgmt-repo ready: true, restart count 0
May 13 09:28:51.274: INFO: security-onboarding-89nmn from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.274: INFO: 	Container security-onboarding ready: false, restart count 0
May 13 09:28:51.274: INFO: 
Logging pods the kubelet thinks is on node 172.16.175.32 before test
May 13 09:28:51.303: INFO: logging-elk-elasticsearch-pki-init-96qch from kube-system started at 2019-05-08 03:32:13 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.303: INFO: 	Container kubectl ready: false, restart count 0
May 13 09:28:51.303: INFO: logging-elk-logstash-7f64bbdf8f-qbzn6 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.303: INFO: 	Container logstash ready: true, restart count 0
May 13 09:28:51.303: INFO: monitoring-prometheus-collectdexporter-86465d9df-xhcb9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.303: INFO: 	Container collectd-exporter ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: monitoring-prometheus-kubestatemetrics-5f8846cf48-2qt72 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container kubestatemetrics ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: key-management-pep-6cf498f8-7kfl9 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container key-management-pep ready: true, restart count 0
May 13 09:28:51.304: INFO: image-manager-init-certs-lc5z5 from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container init-certs ready: true, restart count 0
May 13 09:28:51.304: INFO: k8s-proxy-dbfrf from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container proxy ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-elasticsearch-tls-init-dndvq from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container searchguard-init ready: false, restart count 1
May 13 09:28:51.304: INFO: key-management-api-595d8867f9-v86wj from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container key-management-api ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-kibana-init-j9nlg from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container init ready: false, restart count 6
May 13 09:28:51.304: INFO: monitoring-prometheus-nodeexporter-f97h9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: monitoring-prometheus-elasticsearchexporter-799646cdd4-pstw6 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container elasticsearchexporter ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: monitoring-prometheus-78d4f54b74-nhxvb from kube-system started at 2019-05-08 03:33:10 +0000 UTC (4 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container alert-rule-controller ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container configmap-reload-prometheus ready: true, restart count 16
May 13 09:28:51.304: INFO: 	Container prometheus ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 14
May 13 09:28:51.304: INFO: key-management-onboarding-x7xbz from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container key-management-onboarding ready: false, restart count 0
May 13 09:28:51.304: INFO: logging-elk-elasticsearch-curator-1557703800-7c8c9 from kube-system started at 2019-05-12 23:30:51 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container curator ready: false, restart count 0
May 13 09:28:51.304: INFO: calico-node-nqqhg from kube-system started at 2019-05-08 03:25:57 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-data-0 from kube-system started at 2019-05-08 03:33:04 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container es-data ready: true, restart count 0
May 13 09:28:51.304: INFO: audit-logging-fluentd-ds-xmfv9 from kube-system started at 2019-05-08 03:33:53 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:28:51.304: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fskhj from heptio-sonobuoy started at 2019-05-13 08:34:47 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container systemd-logs ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-kibana-558c869fd5-n8hq4 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container kibana ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: monitoring-prometheus-alertmanager-7fdbfd5559-589pt from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container alertmanager ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container configmap-reload ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: nvidia-device-plugin-lk7j7 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:28:51.304: INFO: metering-dm-767bd75d59-tdvcl from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container metering-dm ready: true, restart count 2
May 13 09:28:51.304: INFO: metering-reader-md2rh from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container metering-reader ready: true, restart count 4
May 13 09:28:51.304: INFO: logging-elk-filebeat-ds-z4wqk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-master-bf9784b7c-g2l6c from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container es-master ready: true, restart count 0
May 13 09:28:51.304: INFO: key-management-persistence-5c68555d9d-jlzj6 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container key-management-persistence ready: true, restart count 0
May 13 09:28:51.304: INFO: metrics-server-5bf88fc7bc-m9fzp from kube-system started at 2019-05-08 03:27:01 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container metrics-server ready: true, restart count 25
May 13 09:28:51.304: INFO: key-management-crypto-79d7dc5c95-dxfsh from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container key-management-crypto ready: true, restart count 0
May 13 09:28:51.304: INFO: metering-ui-5856c4c88f-8c2sk from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container metering-ui ready: true, restart count 3
May 13 09:28:51.304: INFO: logging-elk-client-694fc7bd59-7s6rk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container es-client ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.304: INFO: monitoring-grafana-9d5f8498-j9hss from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container grafana ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container router ready: true, restart count 1
May 13 09:28:51.304: INFO: key-management-lifecycle-5d858b8c4f-5qd8h from kube-system started at 2019-05-08 03:33:47 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:28:51.304: INFO: 	Container key-management-lifecycle ready: true, restart count 0
May 13 09:28:51.304: INFO: logging-elk-elasticsearch-curator-1557617400-hlkf5 from kube-system started at 2019-05-11 23:29:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.304: INFO: 	Container curator ready: false, restart count 0
May 13 09:28:51.304: INFO: 
Logging pods the kubelet thinks is on node 172.16.176.226 before test
May 13 09:28:51.316: INFO: metering-reader-vh24d from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container metering-reader ready: true, restart count 2
May 13 09:28:51.316: INFO: image-manager-init-certs-tl8kt from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container init-certs ready: true, restart count 0
May 13 09:28:51.316: INFO: logging-elk-filebeat-ds-9nhbn from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:28:51.316: INFO: audit-logging-fluentd-ds-m65f6 from kube-system started at 2019-05-08 03:33:17 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:28:51.316: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-68z9l from heptio-sonobuoy started at 2019-05-13 08:33:06 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:28:51.316: INFO: 	Container systemd-logs ready: true, restart count 0
May 13 09:28:51.316: INFO: k8s-proxy-44zsd from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container proxy ready: true, restart count 0
May 13 09:28:51.316: INFO: nvidia-device-plugin-d945k from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:28:51.316: INFO: php-apache-59dfbd948-bcn7g from ivt started at 2019-05-08 06:16:30 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container php-apache ready: true, restart count 0
May 13 09:28:51.316: INFO: web-terminal-5666fd8f98-fmtnz from kube-system started at 2019-05-10 07:45:34 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container web-terminal ready: true, restart count 0
May 13 09:28:51.316: INFO: calico-node-vqnm2 from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:28:51.316: INFO: monitoring-prometheus-nodeexporter-997tc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.316: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:28:51.316: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.316: INFO: 
Logging pods the kubelet thinks is on node 172.16.177.10 before test
May 13 09:28:51.333: INFO: calico-node-vv57p from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.333: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:28:51.333: INFO: nvidia-device-plugin-5ppzs from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.333: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:28:51.333: INFO: logging-elk-filebeat-ds-kp99k from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.333: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:28:51.334: INFO: monitoring-prometheus-nodeexporter-bbgtc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:28:51.334: INFO: 	Container router ready: true, restart count 0
May 13 09:28:51.334: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-mk9hv from heptio-sonobuoy started at 2019-05-13 08:33:07 +0000 UTC (2 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:28:51.334: INFO: 	Container systemd-logs ready: true, restart count 0
May 13 09:28:51.334: INFO: image-manager-init-certs-rwqdh from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container init-certs ready: true, restart count 0
May 13 09:28:51.334: INFO: k8s-proxy-dd6wd from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container proxy ready: true, restart count 0
May 13 09:28:51.334: INFO: metering-reader-t7zsm from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container metering-reader ready: true, restart count 5
May 13 09:28:51.334: INFO: audit-logging-fluentd-ds-hcn7v from kube-system started at 2019-05-10 07:49:28 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.334: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:28:51.334: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 08:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:28:51.335: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159e340854f3e4ec], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:28:52.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-pmkjg" for this suite.
May 13 09:28:58.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:28:58.674: INFO: namespace: e2e-tests-sched-pred-pmkjg, resource: bindings, ignored listing per whitelist
May 13 09:28:58.732: INFO: namespace e2e-tests-sched-pred-pmkjg deletion completed in 6.215040571s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.712 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:28:58.733: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6kqvj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
May 13 09:28:58.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 api-versions'
May 13 09:28:59.064: INFO: stderr: ""
May 13 09:28:59.064: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nicp.ibm.com/v1\nmetrics.k8s.io/v1beta1\nmonitoringcontroller.cloud.ibm.com/v1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nsecurityenforcement.admission.cloud.ibm.com/v1beta1\nservicecatalog.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:28:59.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6kqvj" for this suite.
May 13 09:29:05.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:29:05.273: INFO: namespace: e2e-tests-kubectl-6kqvj, resource: bindings, ignored listing per whitelist
May 13 09:29:05.403: INFO: namespace e2e-tests-kubectl-6kqvj deletion completed in 6.33298115s

• [SLOW TEST:6.669 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:29:05.403: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-tg7x9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:30:05.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-tg7x9" for this suite.
May 13 09:30:27.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:30:27.815: INFO: namespace: e2e-tests-container-probe-tg7x9, resource: bindings, ignored listing per whitelist
May 13 09:30:27.901: INFO: namespace e2e-tests-container-probe-tg7x9 deletion completed in 22.206136289s

• [SLOW TEST:82.499 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:30:27.903: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-grhn9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-be949f6d-7561-11e9-bbcc-d288ccfb79a4
STEP: Creating configMap with name cm-test-opt-upd-be94a001-7561-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-be949f6d-7561-11e9-bbcc-d288ccfb79a4
STEP: Updating configmap cm-test-opt-upd-be94a001-7561-11e9-bbcc-d288ccfb79a4
STEP: Creating configMap with name cm-test-opt-create-be94a01a-7561-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:31:59.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-grhn9" for this suite.
May 13 09:32:23.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:32:23.388: INFO: namespace: e2e-tests-configmap-grhn9, resource: bindings, ignored listing per whitelist
May 13 09:32:23.586: INFO: namespace e2e-tests-configmap-grhn9 deletion completed in 24.311102362s

• [SLOW TEST:115.683 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:32:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jxl2s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-03877751-7562-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:32:23.886: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-jxl2s" to be "success or failure"
May 13 09:32:23.890: INFO: Pod "pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651544ms
May 13 09:32:25.894: INFO: Pod "pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007425807s
STEP: Saw pod success
May 13 09:32:25.894: INFO: Pod "pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:32:25.896: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 09:32:25.939: INFO: Waiting for pod pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:32:25.951: INFO: Pod pod-projected-secrets-03881f88-7562-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:32:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jxl2s" for this suite.
May 13 09:32:31.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:32:32.262: INFO: namespace: e2e-tests-projected-jxl2s, resource: bindings, ignored listing per whitelist
May 13 09:32:32.287: INFO: namespace e2e-tests-projected-jxl2s deletion completed in 6.330836169s

• [SLOW TEST:8.700 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:32:32.288: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xkh5j
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
May 13 09:32:32.586: INFO: Waiting up to 5m0s for pod "pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-xkh5j" to be "success or failure"
May 13 09:32:32.595: INFO: Pod "pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.511542ms
May 13 09:32:34.605: INFO: Pod "pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018367406s
STEP: Saw pod success
May 13 09:32:34.605: INFO: Pod "pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:32:34.615: INFO: Trying to get logs from node 172.16.176.226 pod pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:32:34.645: INFO: Waiting for pod pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:32:34.649: INFO: Pod pod-08c024d6-7562-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:32:34.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xkh5j" for this suite.
May 13 09:32:40.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:32:40.895: INFO: namespace: e2e-tests-emptydir-xkh5j, resource: bindings, ignored listing per whitelist
May 13 09:32:40.948: INFO: namespace e2e-tests-emptydir-xkh5j deletion completed in 6.290963009s

• [SLOW TEST:8.661 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:32:40.949: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8v22n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:32:41.284: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-8v22n" to be "success or failure"
May 13 09:32:41.292: INFO: Pod "downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.943676ms
May 13 09:32:43.297: INFO: Pod "downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012321701s
May 13 09:32:45.300: INFO: Pod "downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01541708s
STEP: Saw pod success
May 13 09:32:45.300: INFO: Pod "downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:32:45.303: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:32:45.323: INFO: Waiting for pod downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:32:45.325: INFO: Pod downwardapi-volume-0de1d387-7562-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:32:45.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8v22n" for this suite.
May 13 09:32:51.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:32:51.516: INFO: namespace: e2e-tests-downward-api-8v22n, resource: bindings, ignored listing per whitelist
May 13 09:32:51.563: INFO: namespace e2e-tests-downward-api-8v22n deletion completed in 6.23247257s

• [SLOW TEST:10.614 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:32:51.567: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-kpldb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:32:51.813: INFO: Creating deployment "test-recreate-deployment"
May 13 09:32:51.884: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 13 09:32:51.889: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
May 13 09:32:53.898: INFO: Waiting deployment "test-recreate-deployment" to complete
May 13 09:32:53.901: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 13 09:32:53.985: INFO: Updating deployment test-recreate-deployment
May 13 09:32:53.985: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 09:32:54.063: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-kpldb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kpldb/deployments/test-recreate-deployment,UID:14438ec1-7562-11e9-b9b7-00163e01adca,ResourceVersion:856453,Generation:2,CreationTimestamp:2019-05-13 09:32:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-13 09:32:54 +0000 UTC 2019-05-13 09:32:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-13 09:32:54 +0000 UTC 2019-05-13 09:32:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 13 09:32:54.068: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-kpldb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kpldb/replicasets/test-recreate-deployment-697fbf54bf,UID:158ba7d7-7562-11e9-b9b7-00163e01adca,ResourceVersion:856452,Generation:1,CreationTimestamp:2019-05-13 09:32:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 14438ec1-7562-11e9-b9b7-00163e01adca 0xc00297ae77 0xc00297ae78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 09:32:54.068: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 13 09:32:54.068: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-kpldb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kpldb/replicasets/test-recreate-deployment-5dfdcc846d,UID:144762b8-7562-11e9-b9b7-00163e01adca,ResourceVersion:856442,Generation:2,CreationTimestamp:2019-05-13 09:32:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 14438ec1-7562-11e9-b9b7-00163e01adca 0xc00297ada7 0xc00297ada8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 09:32:54.073: INFO: Pod "test-recreate-deployment-697fbf54bf-pdnft" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-pdnft,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-kpldb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kpldb/pods/test-recreate-deployment-697fbf54bf-pdnft,UID:158cd6e8-7562-11e9-b9b7-00163e01adca,ResourceVersion:856454,Generation:0,CreationTimestamp:2019-05-13 09:32:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 158ba7d7-7562-11e9-b9b7-00163e01adca 0xc002612da7 0xc002612da8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-clblw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-clblw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-clblw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002612e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002612f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:31:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:31:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 09:32:54 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:,StartTime:2019-05-13 09:31:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:32:54.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-kpldb" for this suite.
May 13 09:33:00.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:33:00.138: INFO: namespace: e2e-tests-deployment-kpldb, resource: bindings, ignored listing per whitelist
May 13 09:33:00.318: INFO: namespace e2e-tests-deployment-kpldb deletion completed in 6.238751651s

• [SLOW TEST:8.752 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:33:00.321: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-d8czd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 13 09:33:00.618: INFO: Number of nodes with available pods: 0
May 13 09:33:00.618: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:33:01.633: INFO: Number of nodes with available pods: 0
May 13 09:33:01.633: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:33:02.627: INFO: Number of nodes with available pods: 0
May 13 09:33:02.627: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:33:03.642: INFO: Number of nodes with available pods: 4
May 13 09:33:03.642: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 13 09:33:03.697: INFO: Number of nodes with available pods: 3
May 13 09:33:03.697: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 09:33:04.714: INFO: Number of nodes with available pods: 3
May 13 09:33:04.714: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 09:33:05.713: INFO: Number of nodes with available pods: 3
May 13 09:33:05.713: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 09:33:06.706: INFO: Number of nodes with available pods: 4
May 13 09:33:06.706: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-d8czd, will wait for the garbage collector to delete the pods
May 13 09:33:06.777: INFO: Deleting DaemonSet.extensions daemon-set took: 12.112337ms
May 13 09:33:06.877: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.438237ms
May 13 09:33:46.581: INFO: Number of nodes with available pods: 0
May 13 09:33:46.581: INFO: Number of running nodes: 0, number of available pods: 0
May 13 09:33:46.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-d8czd/daemonsets","resourceVersion":"856706"},"items":null}

May 13 09:33:46.593: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-d8czd/pods","resourceVersion":"856706"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:33:46.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-d8czd" for this suite.
May 13 09:33:52.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:33:52.816: INFO: namespace: e2e-tests-daemonsets-d8czd, resource: bindings, ignored listing per whitelist
May 13 09:33:52.816: INFO: namespace e2e-tests-daemonsets-d8czd deletion completed in 6.202000828s

• [SLOW TEST:52.495 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:33:52.817: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lgb45
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 13 09:33:53.186: INFO: Waiting up to 5m0s for pod "pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-lgb45" to be "success or failure"
May 13 09:33:53.193: INFO: Pod "pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523964ms
May 13 09:33:55.198: INFO: Pod "pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011006927s
May 13 09:33:57.202: INFO: Pod "pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015631907s
STEP: Saw pod success
May 13 09:33:57.202: INFO: Pod "pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:33:57.210: INFO: Trying to get logs from node 172.16.177.10 pod pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:33:57.236: INFO: Waiting for pod pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:33:57.242: INFO: Pod pod-38bcbfe0-7562-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:33:57.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lgb45" for this suite.
May 13 09:34:03.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:34:03.440: INFO: namespace: e2e-tests-emptydir-lgb45, resource: bindings, ignored listing per whitelist
May 13 09:34:03.528: INFO: namespace e2e-tests-emptydir-lgb45 deletion completed in 6.281291252s

• [SLOW TEST:10.711 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:34:03.529: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-zh6vn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-zh6vn
May 13 09:34:05.799: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-zh6vn
STEP: checking the pod's current state and verifying that restartCount is present
May 13 09:34:05.801: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:38:06.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zh6vn" for this suite.
May 13 09:38:12.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:38:12.692: INFO: namespace: e2e-tests-container-probe-zh6vn, resource: bindings, ignored listing per whitelist
May 13 09:38:12.721: INFO: namespace e2e-tests-container-probe-zh6vn deletion completed in 6.277516192s

• [SLOW TEST:249.192 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:38:12.722: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tdpmn
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-d3a2d881-7562-11e9-bbcc-d288ccfb79a4
STEP: Creating secret with name s-test-opt-upd-d3a2d9a2-7562-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d3a2d881-7562-11e9-bbcc-d288ccfb79a4
STEP: Updating secret s-test-opt-upd-d3a2d9a2-7562-11e9-bbcc-d288ccfb79a4
STEP: Creating secret with name s-test-opt-create-d3a2d9ce-7562-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:38:19.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tdpmn" for this suite.
May 13 09:38:43.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:38:43.772: INFO: namespace: e2e-tests-secrets-tdpmn, resource: bindings, ignored listing per whitelist
May 13 09:38:43.954: INFO: namespace e2e-tests-secrets-tdpmn deletion completed in 24.319407606s

• [SLOW TEST:31.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:38:43.954: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-29kxr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 13 09:38:50.400: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:38:50.403: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:38:52.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:38:52.407: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:38:54.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:38:54.408: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:38:56.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:38:56.411: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:38:58.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:38:58.409: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:39:00.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:39:00.423: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:39:02.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:39:02.409: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:39:04.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:39:04.408: INFO: Pod pod-with-prestop-http-hook still exists
May 13 09:39:06.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 09:39:06.408: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:39:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-29kxr" for this suite.
May 13 09:39:30.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:39:30.576: INFO: namespace: e2e-tests-container-lifecycle-hook-29kxr, resource: bindings, ignored listing per whitelist
May 13 09:39:30.678: INFO: namespace e2e-tests-container-lifecycle-hook-29kxr deletion completed in 24.250697685s

• [SLOW TEST:46.724 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:39:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7hwg2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-02196c1c-7563-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:39:30.991: INFO: Waiting up to 5m0s for pod "pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-7hwg2" to be "success or failure"
May 13 09:39:30.996: INFO: Pod "pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.688157ms
May 13 09:39:33.000: INFO: Pod "pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0091071s
May 13 09:39:35.009: INFO: Pod "pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017621991s
STEP: Saw pod success
May 13 09:39:35.009: INFO: Pod "pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:39:35.015: INFO: Trying to get logs from node 172.16.177.10 pod pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:39:35.047: INFO: Waiting for pod pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:39:35.050: INFO: Pod pod-configmaps-021a2f5a-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:39:35.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7hwg2" for this suite.
May 13 09:39:41.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:39:41.179: INFO: namespace: e2e-tests-configmap-7hwg2, resource: bindings, ignored listing per whitelist
May 13 09:39:41.297: INFO: namespace e2e-tests-configmap-7hwg2 deletion completed in 6.241330788s

• [SLOW TEST:10.619 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:39:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-ddsxp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-0870720c-7563-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:39:41.591: INFO: Waiting up to 5m0s for pod "pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-ddsxp" to be "success or failure"
May 13 09:39:41.597: INFO: Pod "pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990529ms
May 13 09:39:43.600: INFO: Pod "pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009382661s
May 13 09:39:45.603: INFO: Pod "pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012363983s
STEP: Saw pod success
May 13 09:39:45.603: INFO: Pod "pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:39:45.605: INFO: Trying to get logs from node 172.16.177.10 pod pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 09:39:45.628: INFO: Waiting for pod pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:39:45.634: INFO: Pod pod-secrets-08711ed9-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:39:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ddsxp" for this suite.
May 13 09:39:51.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:39:51.855: INFO: namespace: e2e-tests-secrets-ddsxp, resource: bindings, ignored listing per whitelist
May 13 09:39:51.906: INFO: namespace e2e-tests-secrets-ddsxp deletion completed in 6.26272316s

• [SLOW TEST:10.608 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:39:51.906: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-skmmx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-skmmx;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-skmmx.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-skmmx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 52.73.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.73.52_udp@PTR;check="$$(dig +tcp +noall +answer +search 52.73.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.73.52_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-skmmx;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-skmmx;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-skmmx.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-skmmx.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-skmmx.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-skmmx.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-skmmx.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 52.73.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.73.52_udp@PTR;check="$$(dig +tcp +noall +answer +search 52.73.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.73.52_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 13 09:39:56.626: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.629: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.638: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.645: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.648: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.654: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.684: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.688: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.693: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.696: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.707: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.712: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.717: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.721: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:39:56.743: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:01.756: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.761: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.778: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.789: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.799: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.828: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.833: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.839: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.843: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.848: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.852: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:01.887: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:06.749: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.753: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.763: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.774: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.778: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.827: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.831: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.835: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.840: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.848: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.855: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.866: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:06.909: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:11.760: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.764: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.772: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.782: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.786: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.790: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.815: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.819: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.823: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.829: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.833: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.837: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.842: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.845: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:11.870: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:16.750: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.754: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.763: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.776: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.782: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.787: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.817: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.821: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.824: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.827: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.831: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.835: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.839: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:16.865: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:21.749: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.752: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.761: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.769: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.777: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.812: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.816: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.821: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.826: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.841: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.844: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.847: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.851: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc from pod e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4: the server could not find the requested resource (get pods dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4)
May 13 09:40:21.875: INFO: Lookups using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx wheezy_tcp@dns-test-service.e2e-tests-dns-skmmx.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-skmmx jessie_tcp@dns-test-service.e2e-tests-dns-skmmx jessie_udp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@dns-test-service.e2e-tests-dns-skmmx.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-skmmx.svc]

May 13 09:40:26.870: INFO: DNS probes using e2e-tests-dns-skmmx/dns-test-0ec8861f-7563-11e9-bbcc-d288ccfb79a4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:40:26.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-skmmx" for this suite.
May 13 09:40:32.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:40:33.179: INFO: namespace: e2e-tests-dns-skmmx, resource: bindings, ignored listing per whitelist
May 13 09:40:33.192: INFO: namespace e2e-tests-dns-skmmx deletion completed in 6.255440531s

• [SLOW TEST:41.286 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:40:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sm5ls
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:40:33.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-sm5ls" to be "success or failure"
May 13 09:40:33.434: INFO: Pod "downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.177528ms
May 13 09:40:35.441: INFO: Pod "downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014409319s
STEP: Saw pod success
May 13 09:40:35.441: INFO: Pod "downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:40:35.444: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:40:35.461: INFO: Waiting for pod downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:40:35.464: INFO: Pod downwardapi-volume-275a4995-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:40:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sm5ls" for this suite.
May 13 09:40:41.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:40:41.498: INFO: namespace: e2e-tests-downward-api-sm5ls, resource: bindings, ignored listing per whitelist
May 13 09:40:41.744: INFO: namespace e2e-tests-downward-api-sm5ls deletion completed in 6.275778648s

• [SLOW TEST:8.552 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:40:41.746: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rlxnj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:40:42.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-rlxnj" to be "success or failure"
May 13 09:40:42.088: INFO: Pod "downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.983297ms
May 13 09:40:44.096: INFO: Pod "downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010039067s
May 13 09:40:46.100: INFO: Pod "downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015010181s
STEP: Saw pod success
May 13 09:40:46.101: INFO: Pod "downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:40:46.103: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:40:46.124: INFO: Waiting for pod downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:40:46.126: INFO: Pod downwardapi-volume-2c75f753-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:40:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rlxnj" for this suite.
May 13 09:40:52.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:40:52.258: INFO: namespace: e2e-tests-downward-api-rlxnj, resource: bindings, ignored listing per whitelist
May 13 09:40:52.374: INFO: namespace e2e-tests-downward-api-rlxnj deletion completed in 6.243172916s

• [SLOW TEST:10.629 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:40:52.376: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tfh5s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 13 09:40:52.665: INFO: namespace e2e-tests-kubectl-tfh5s
May 13 09:40:52.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-tfh5s'
May 13 09:40:53.592: INFO: stderr: ""
May 13 09:40:53.592: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 09:40:54.600: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:40:54.600: INFO: Found 0 / 1
May 13 09:40:55.596: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:40:55.596: INFO: Found 1 / 1
May 13 09:40:55.596: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 09:40:55.600: INFO: Selector matched 1 pods for map[app:redis]
May 13 09:40:55.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 09:40:55.600: INFO: wait on redis-master startup in e2e-tests-kubectl-tfh5s 
May 13 09:40:55.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 logs redis-master-25rwm redis-master --namespace=e2e-tests-kubectl-tfh5s'
May 13 09:40:55.784: INFO: stderr: ""
May 13 09:40:55.784: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 09:39:58.371 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 09:39:58.371 # Server started, Redis version 3.2.12\n1:M 13 May 09:39:58.371 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 09:39:58.371 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 13 09:40:55.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-tfh5s'
May 13 09:40:55.979: INFO: stderr: ""
May 13 09:40:55.979: INFO: stdout: "service/rm2 exposed\n"
May 13 09:40:55.990: INFO: Service rm2 in namespace e2e-tests-kubectl-tfh5s found.
STEP: exposing service
May 13 09:40:57.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-tfh5s'
May 13 09:40:58.202: INFO: stderr: ""
May 13 09:40:58.202: INFO: stdout: "service/rm3 exposed\n"
May 13 09:40:58.207: INFO: Service rm3 in namespace e2e-tests-kubectl-tfh5s found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:41:00.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tfh5s" for this suite.
May 13 09:41:24.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:41:24.329: INFO: namespace: e2e-tests-kubectl-tfh5s, resource: bindings, ignored listing per whitelist
May 13 09:41:24.408: INFO: namespace e2e-tests-kubectl-tfh5s deletion completed in 24.18864657s

• [SLOW TEST:32.032 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:41:24.408: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nrrww
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:41:46.695: INFO: Container started at 2019-05-13 09:40:29 +0000 UTC, pod became ready at 2019-05-13 09:40:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:41:46.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nrrww" for this suite.
May 13 09:42:10.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:42:11.129: INFO: namespace: e2e-tests-container-probe-nrrww, resource: bindings, ignored listing per whitelist
May 13 09:42:11.129: INFO: namespace e2e-tests-container-probe-nrrww deletion completed in 24.427866964s

• [SLOW TEST:46.721 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:42:11.138: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-cr5ws
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
May 13 09:42:11.508: INFO: Waiting up to 5m0s for pod "client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-containers-cr5ws" to be "success or failure"
May 13 09:42:11.515: INFO: Pod "client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373232ms
May 13 09:42:13.520: INFO: Pod "client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011217229s
STEP: Saw pod success
May 13 09:42:13.520: INFO: Pod "client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:42:13.525: INFO: Trying to get logs from node 172.16.176.226 pod client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:42:13.586: INFO: Waiting for pod client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:42:13.592: INFO: Pod client-containers-61c8ffcd-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:42:13.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-cr5ws" for this suite.
May 13 09:42:19.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:42:19.803: INFO: namespace: e2e-tests-containers-cr5ws, resource: bindings, ignored listing per whitelist
May 13 09:42:19.870: INFO: namespace e2e-tests-containers-cr5ws deletion completed in 6.270391139s

• [SLOW TEST:8.733 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:42:19.872: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-dnxkx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 09:42:20.140: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 09:42:20.151: INFO: Waiting for terminating namespaces to be deleted...
May 13 09:42:20.157: INFO: 
Logging pods the kubelet thinks is on node 172.16.173.202 before test
May 13 09:42:20.184: INFO: platform-api-994c46799-5f4ml from kube-system started at 2019-05-08 03:27:31 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.184: INFO: 	Container audit-service ready: true, restart count 1
May 13 09:42:20.184: INFO: 	Container platform-api ready: true, restart count 10
May 13 09:42:20.184: INFO: monitoring-prometheus-nodeexporter-lwpx9 from kube-system started at 2019-05-08 03:33:46 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container nodeexporter ready: true, restart count 1
May 13 09:42:20.185: INFO: 	Container router ready: true, restart count 1
May 13 09:42:20.185: INFO: mgmt-repo-79f95d66b8-lp7sf from kube-system started at 2019-05-08 03:34:03 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.185: INFO: 	Container mgmt-repo ready: true, restart count 0
May 13 09:42:20.185: INFO: security-onboarding-89nmn from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container security-onboarding ready: false, restart count 0
May 13 09:42:20.185: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fv8lz from heptio-sonobuoy started at 2019-05-13 08:34:01 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 09:42:20.185: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 09:42:20.185: INFO: image-manager-0 from kube-system started at 2019-05-08 03:24:10 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container icp-registry ready: true, restart count 1
May 13 09:42:20.185: INFO: 	Container image-manager ready: true, restart count 0
May 13 09:42:20.185: INFO: calico-node-krfk6 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.185: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:42:20.186: INFO: nvidia-device-plugin-vwsvk from kube-system started at 2019-05-08 03:27:07 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container nvidia-device-plugin ready: true, restart count 1
May 13 09:42:20.186: INFO: ibmcloud-image-enforcement-7c7d9688-tqjm9 from kube-system started at 2019-05-08 03:32:31 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
May 13 09:42:20.186: INFO: iam-onboarding-bptkl from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container iam-onboarding ready: false, restart count 0
May 13 09:42:20.186: INFO: audit-logging-fluentd-ds-lr8k9 from kube-system started at 2019-05-08 03:34:29 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:42:20.186: INFO: k8s-master-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:42:20.186: INFO: service-catalog-controller-manager-88d749c7d-6xqsk from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container controller-manager ready: true, restart count 1
May 13 09:42:20.186: INFO: nginx-ingress-controller-vf97z from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container nginx-ingress ready: true, restart count 1
May 13 09:42:20.186: INFO: metering-reader-46vqp from kube-system started at 2019-05-08 03:32:37 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.186: INFO: 	Container metering-reader ready: true, restart count 3
May 13 09:42:20.186: INFO: secret-watcher-54fd9cfc6d-gw9pw from kube-system started at 2019-05-08 03:34:09 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container secret-watcher ready: true, restart count 1
May 13 09:42:20.187: INFO: k8s-kmsplugin-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:42:20.187: INFO: k8s-proxy-kxd6g from kube-system started at 2019-05-08 03:23:38 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container proxy ready: true, restart count 0
May 13 09:42:20.187: INFO: cert-manager-ibm-cert-manager-7775495cb4-n84nn from cert-manager started at 2019-05-08 03:23:52 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container ibm-cert-manager ready: true, restart count 8
May 13 09:42:20.187: INFO: kube-dns-28wg9 from kube-system started at 2019-05-08 03:26:55 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container kube-dns ready: true, restart count 1
May 13 09:42:20.187: INFO: oidc-client-registration-htdq6 from kube-system started at 2019-05-08 03:29:48 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container oidc-client-registration ready: false, restart count 0
May 13 09:42:20.187: INFO: image-manager-init-certs-5cjmr from kube-system started at 2019-05-08 03:24:09 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container init-certs ready: true, restart count 1
May 13 09:42:20.187: INFO: auth-idp-cqjpn from kube-system started at 2019-05-08 03:29:48 +0000 UTC (4 container statuses recorded)
May 13 09:42:20.187: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.187: INFO: 	Container platform-auth-service ready: true, restart count 0
May 13 09:42:20.188: INFO: 	Container platform-identity-manager ready: true, restart count 1
May 13 09:42:20.188: INFO: 	Container platform-identity-provider ready: true, restart count 1
May 13 09:42:20.188: INFO: platform-ui-sfdsp from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container platform-ui ready: true, restart count 0
May 13 09:42:20.188: INFO: platform-header-5hnzj from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container platform-header ready: true, restart count 1
May 13 09:42:20.188: INFO: helm-api-67456c54d4-rf5rv from kube-system started at 2019-05-08 03:33:58 +0000 UTC (3 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container helmapi ready: true, restart count 0
May 13 09:42:20.188: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.188: INFO: 	Container rudder ready: true, restart count 0
May 13 09:42:20.188: INFO: calico-kube-controllers-7f949c47f-sgggp from kube-system started at 2019-05-08 03:26:42 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 13 09:42:20.188: INFO: default-http-backend-567686995f-psknt from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container default-http-backend ready: true, restart count 1
May 13 09:42:20.188: INFO: sonobuoy-e2e-job-9886345f531c45a5 from heptio-sonobuoy started at 2019-05-13 08:33:59 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.188: INFO: 	Container e2e ready: true, restart count 0
May 13 09:42:20.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 09:42:20.189: INFO: k8s-etcd-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 09:42:20.189: INFO: tiller-deploy-57774fdbd5-t4zqz from kube-system started at 2019-05-08 03:23:25 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.189: INFO: 	Container tiller ready: true, restart count 1
May 13 09:42:20.189: INFO: auth-pdp-82gc5 from kube-system started at 2019-05-08 03:30:03 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.189: INFO: 	Container auth-pdp ready: true, restart count 0
May 13 09:42:20.189: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.189: INFO: icp-management-ingress-8jgdq from kube-system started at 2019-05-08 03:30:09 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.189: INFO: 	Container icp-management-ingress ready: true, restart count 0
May 13 09:42:20.189: INFO: logging-elk-filebeat-ds-tlc56 from kube-system started at 2019-05-08 03:33:38 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.190: INFO: 	Container filebeat ready: true, restart count 1
May 13 09:42:20.190: INFO: icp-mongodb-0 from kube-system started at 2019-05-08 03:27:17 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.190: INFO: 	Container icp-mongodb ready: true, restart count 0
May 13 09:42:20.190: INFO: 	Container metrics ready: true, restart count 0
May 13 09:42:20.190: INFO: service-catalog-apiserver-9dt69 from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.190: INFO: 	Container apiserver ready: true, restart count 4
May 13 09:42:20.190: INFO: auth-pap-9bjv2 from kube-system started at 2019-05-08 03:29:58 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.190: INFO: 	Container auth-pap ready: true, restart count 2
May 13 09:42:20.190: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.190: INFO: helm-repo-548dbc4c5c-lj4r2 from kube-system started at 2019-05-08 03:32:19 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.190: INFO: 	Container helm-repo ready: true, restart count 0
May 13 09:42:20.190: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.191: INFO: 
Logging pods the kubelet thinks is on node 172.16.175.32 before test
May 13 09:42:20.238: INFO: logging-elk-elasticsearch-tls-init-dndvq from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.238: INFO: 	Container searchguard-init ready: false, restart count 1
May 13 09:42:20.239: INFO: key-management-api-595d8867f9-v86wj from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.239: INFO: 	Container key-management-api ready: true, restart count 0
May 13 09:42:20.239: INFO: image-manager-init-certs-lc5z5 from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.239: INFO: 	Container init-certs ready: true, restart count 0
May 13 09:42:20.239: INFO: k8s-proxy-dbfrf from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.239: INFO: 	Container proxy ready: true, restart count 0
May 13 09:42:20.239: INFO: monitoring-prometheus-elasticsearchexporter-799646cdd4-pstw6 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.239: INFO: 	Container elasticsearchexporter ready: true, restart count 0
May 13 09:42:20.239: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.240: INFO: monitoring-prometheus-78d4f54b74-nhxvb from kube-system started at 2019-05-08 03:33:10 +0000 UTC (4 container statuses recorded)
May 13 09:42:20.240: INFO: 	Container alert-rule-controller ready: true, restart count 0
May 13 09:42:20.240: INFO: 	Container configmap-reload-prometheus ready: true, restart count 16
May 13 09:42:20.240: INFO: 	Container prometheus ready: true, restart count 0
May 13 09:42:20.240: INFO: 	Container router ready: true, restart count 14
May 13 09:42:20.240: INFO: key-management-onboarding-x7xbz from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.240: INFO: 	Container key-management-onboarding ready: false, restart count 0
May 13 09:42:20.240: INFO: logging-elk-elasticsearch-curator-1557703800-7c8c9 from kube-system started at 2019-05-12 23:30:51 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.240: INFO: 	Container curator ready: false, restart count 0
May 13 09:42:20.240: INFO: logging-elk-kibana-init-j9nlg from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.240: INFO: 	Container init ready: false, restart count 6
May 13 09:42:20.240: INFO: monitoring-prometheus-nodeexporter-f97h9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:42:20.241: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.241: INFO: audit-logging-fluentd-ds-xmfv9 from kube-system started at 2019-05-08 03:33:53 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:42:20.241: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fskhj from heptio-sonobuoy started at 2019-05-13 08:34:47 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 09:42:20.241: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 09:42:20.241: INFO: calico-node-nqqhg from kube-system started at 2019-05-08 03:25:57 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:42:20.241: INFO: logging-elk-data-0 from kube-system started at 2019-05-08 03:33:04 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container es-data ready: true, restart count 0
May 13 09:42:20.241: INFO: logging-elk-kibana-558c869fd5-n8hq4 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container kibana ready: true, restart count 0
May 13 09:42:20.241: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.241: INFO: monitoring-prometheus-alertmanager-7fdbfd5559-589pt from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 09:42:20.241: INFO: 	Container alertmanager ready: true, restart count 0
May 13 09:42:20.241: INFO: 	Container configmap-reload ready: true, restart count 0
May 13 09:42:20.242: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.242: INFO: metering-reader-md2rh from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container metering-reader ready: true, restart count 4
May 13 09:42:20.242: INFO: logging-elk-filebeat-ds-z4wqk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:42:20.242: INFO: logging-elk-master-bf9784b7c-g2l6c from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container es-master ready: true, restart count 0
May 13 09:42:20.242: INFO: key-management-persistence-5c68555d9d-jlzj6 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container key-management-persistence ready: true, restart count 0
May 13 09:42:20.242: INFO: nvidia-device-plugin-lk7j7 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:42:20.242: INFO: metering-dm-767bd75d59-tdvcl from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container metering-dm ready: true, restart count 2
May 13 09:42:20.242: INFO: metrics-server-5bf88fc7bc-m9fzp from kube-system started at 2019-05-08 03:27:01 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container metrics-server ready: true, restart count 25
May 13 09:42:20.242: INFO: key-management-crypto-79d7dc5c95-dxfsh from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.242: INFO: 	Container key-management-crypto ready: true, restart count 0
May 13 09:42:20.242: INFO: monitoring-grafana-9d5f8498-j9hss from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 09:42:20.243: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
May 13 09:42:20.243: INFO: 	Container grafana ready: true, restart count 0
May 13 09:42:20.243: INFO: 	Container router ready: true, restart count 1
May 13 09:42:20.243: INFO: key-management-lifecycle-5d858b8c4f-5qd8h from kube-system started at 2019-05-08 03:33:47 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.243: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 09:42:20.243: INFO: 	Container key-management-lifecycle ready: true, restart count 0
May 13 09:42:20.243: INFO: logging-elk-elasticsearch-curator-1557617400-hlkf5 from kube-system started at 2019-05-11 23:29:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.243: INFO: 	Container curator ready: false, restart count 0
May 13 09:42:20.243: INFO: metering-ui-5856c4c88f-8c2sk from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.243: INFO: 	Container metering-ui ready: true, restart count 3
May 13 09:42:20.243: INFO: logging-elk-client-694fc7bd59-7s6rk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.243: INFO: 	Container es-client ready: true, restart count 0
May 13 09:42:20.243: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.244: INFO: monitoring-prometheus-collectdexporter-86465d9df-xhcb9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.244: INFO: 	Container collectd-exporter ready: true, restart count 0
May 13 09:42:20.244: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.244: INFO: monitoring-prometheus-kubestatemetrics-5f8846cf48-2qt72 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.244: INFO: 	Container kubestatemetrics ready: true, restart count 0
May 13 09:42:20.244: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.244: INFO: key-management-pep-6cf498f8-7kfl9 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.244: INFO: 	Container key-management-pep ready: true, restart count 0
May 13 09:42:20.244: INFO: logging-elk-elasticsearch-pki-init-96qch from kube-system started at 2019-05-08 03:32:13 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.244: INFO: 	Container kubectl ready: false, restart count 0
May 13 09:42:20.244: INFO: logging-elk-logstash-7f64bbdf8f-qbzn6 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.244: INFO: 	Container logstash ready: true, restart count 0
May 13 09:42:20.244: INFO: 
Logging pods the kubelet thinks is on node 172.16.176.226 before test
May 13 09:42:20.261: INFO: calico-node-vqnm2 from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.261: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:42:20.262: INFO: monitoring-prometheus-nodeexporter-997tc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.262: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:42:20.262: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.262: INFO: metering-reader-vh24d from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.262: INFO: 	Container metering-reader ready: true, restart count 2
May 13 09:42:20.262: INFO: image-manager-init-certs-tl8kt from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.262: INFO: 	Container init-certs ready: true, restart count 0
May 13 09:42:20.262: INFO: logging-elk-filebeat-ds-9nhbn from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.262: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:42:20.262: INFO: audit-logging-fluentd-ds-m65f6 from kube-system started at 2019-05-08 03:33:17 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.262: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:42:20.263: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-68z9l from heptio-sonobuoy started at 2019-05-13 08:33:06 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.263: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 09:42:20.263: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 09:42:20.263: INFO: k8s-proxy-44zsd from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.263: INFO: 	Container proxy ready: true, restart count 0
May 13 09:42:20.263: INFO: nvidia-device-plugin-d945k from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.263: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:42:20.263: INFO: php-apache-59dfbd948-bcn7g from ivt started at 2019-05-08 06:16:30 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.263: INFO: 	Container php-apache ready: true, restart count 0
May 13 09:42:20.264: INFO: web-terminal-5666fd8f98-fmtnz from kube-system started at 2019-05-10 07:45:34 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.264: INFO: 	Container web-terminal ready: true, restart count 0
May 13 09:42:20.264: INFO: 
Logging pods the kubelet thinks is on node 172.16.177.10 before test
May 13 09:42:20.282: INFO: nvidia-device-plugin-5ppzs from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.283: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 09:42:20.283: INFO: logging-elk-filebeat-ds-kp99k from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.283: INFO: 	Container filebeat ready: true, restart count 0
May 13 09:42:20.283: INFO: monitoring-prometheus-nodeexporter-bbgtc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.283: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 09:42:20.283: INFO: 	Container router ready: true, restart count 0
May 13 09:42:20.283: INFO: calico-node-vv57p from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.283: INFO: 	Container calico-node ready: true, restart count 0
May 13 09:42:20.284: INFO: k8s-proxy-dd6wd from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.284: INFO: 	Container proxy ready: true, restart count 0
May 13 09:42:20.284: INFO: metering-reader-t7zsm from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.284: INFO: 	Container metering-reader ready: true, restart count 5
May 13 09:42:20.284: INFO: audit-logging-fluentd-ds-hcn7v from kube-system started at 2019-05-10 07:49:28 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.284: INFO: 	Container fluentd ready: true, restart count 0
May 13 09:42:20.284: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 08:33:02 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.284: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 13 09:42:20.284: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-mk9hv from heptio-sonobuoy started at 2019-05-13 08:33:07 +0000 UTC (2 container statuses recorded)
May 13 09:42:20.284: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 09:42:20.285: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 09:42:20.285: INFO: image-manager-init-certs-rwqdh from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 09:42:20.285: INFO: 	Container init-certs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-69835514-7563-11e9-bbcc-d288ccfb79a4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-69835514-7563-11e9-bbcc-d288ccfb79a4 off the node 172.16.177.10
STEP: verifying the node doesn't have the label kubernetes.io/e2e-69835514-7563-11e9-bbcc-d288ccfb79a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:42:28.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-dnxkx" for this suite.
May 13 09:42:44.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:42:44.543: INFO: namespace: e2e-tests-sched-pred-dnxkx, resource: bindings, ignored listing per whitelist
May 13 09:42:44.770: INFO: namespace e2e-tests-sched-pred-dnxkx deletion completed in 16.283911239s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.898 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:42:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-4fcl6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 13 09:42:44.988: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:42:49.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4fcl6" for this suite.
May 13 09:42:55.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:42:55.474: INFO: namespace: e2e-tests-init-container-4fcl6, resource: bindings, ignored listing per whitelist
May 13 09:42:55.665: INFO: namespace e2e-tests-init-container-4fcl6 deletion completed in 6.278568607s

• [SLOW TEST:10.895 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:42:55.665: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lx655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
May 13 09:42:55.946: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 13 09:42:55.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:56.253: INFO: stderr: ""
May 13 09:42:56.253: INFO: stdout: "service/redis-slave created\n"
May 13 09:42:56.253: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 13 09:42:56.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:56.627: INFO: stderr: ""
May 13 09:42:56.627: INFO: stdout: "service/redis-master created\n"
May 13 09:42:56.627: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 13 09:42:56.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:56.904: INFO: stderr: ""
May 13 09:42:56.904: INFO: stdout: "service/frontend created\n"
May 13 09:42:56.905: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 13 09:42:56.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:57.292: INFO: stderr: ""
May 13 09:42:57.292: INFO: stdout: "deployment.extensions/frontend created\n"
May 13 09:42:57.293: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 13 09:42:57.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:57.700: INFO: stderr: ""
May 13 09:42:57.701: INFO: stdout: "deployment.extensions/redis-master created\n"
May 13 09:42:57.702: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 13 09:42:57.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:42:58.087: INFO: stderr: ""
May 13 09:42:58.087: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
May 13 09:42:58.087: INFO: Waiting for all frontend pods to be Running.
May 13 09:44:28.149: INFO: Waiting for frontend to serve content.
May 13 09:44:28.231: INFO: Trying to add a new entry to the guestbook.
May 13 09:44:28.255: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 13 09:44:28.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:28.513: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:28.513: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 13 09:44:28.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:28.688: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:28.688: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 13 09:44:28.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:28.894: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:28.894: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 13 09:44:28.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:29.036: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:29.036: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 13 09:44:29.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:29.202: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:29.202: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 13 09:44:29.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lx655'
May 13 09:44:29.376: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 09:44:29.376: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:44:29.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lx655" for this suite.
May 13 09:45:03.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:45:03.521: INFO: namespace: e2e-tests-kubectl-lx655, resource: bindings, ignored listing per whitelist
May 13 09:45:03.619: INFO: namespace e2e-tests-kubectl-lx655 deletion completed in 34.210102766s

• [SLOW TEST:127.954 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:45:03.619: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-rmzkj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:45:35.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-rmzkj" for this suite.
May 13 09:45:41.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:45:41.530: INFO: namespace: e2e-tests-container-runtime-rmzkj, resource: bindings, ignored listing per whitelist
May 13 09:45:41.701: INFO: namespace e2e-tests-container-runtime-rmzkj deletion completed in 6.347513143s

• [SLOW TEST:38.082 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:45:41.703: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-sxmgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-df56e58f-7563-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:45:42.194: INFO: Waiting up to 5m0s for pod "pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-sxmgt" to be "success or failure"
May 13 09:45:42.201: INFO: Pod "pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.636397ms
May 13 09:45:44.206: INFO: Pod "pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011244425s
May 13 09:45:46.210: INFO: Pod "pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015282734s
STEP: Saw pod success
May 13 09:45:46.210: INFO: Pod "pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:45:46.212: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:45:46.248: INFO: Waiting for pod pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:45:46.249: INFO: Pod pod-configmaps-df57bb06-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:45:46.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sxmgt" for this suite.
May 13 09:45:52.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:45:52.396: INFO: namespace: e2e-tests-configmap-sxmgt, resource: bindings, ignored listing per whitelist
May 13 09:45:52.714: INFO: namespace e2e-tests-configmap-sxmgt deletion completed in 6.459523013s

• [SLOW TEST:11.011 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:45:52.715: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7bmvz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e5e388e2-7563-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:45:53.241: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-7bmvz" to be "success or failure"
May 13 09:45:53.253: INFO: Pod "pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.803106ms
May 13 09:45:55.260: INFO: Pod "pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019439893s
May 13 09:45:57.269: INFO: Pod "pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027820766s
STEP: Saw pod success
May 13 09:45:57.269: INFO: Pod "pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:45:57.275: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:45:57.340: INFO: Waiting for pod pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:45:57.343: INFO: Pod pod-projected-configmaps-e5e4ad65-7563-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:45:57.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7bmvz" for this suite.
May 13 09:46:03.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:46:03.493: INFO: namespace: e2e-tests-projected-7bmvz, resource: bindings, ignored listing per whitelist
May 13 09:46:03.669: INFO: namespace e2e-tests-projected-7bmvz deletion completed in 6.303219516s

• [SLOW TEST:10.955 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:46:03.670: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-7mff2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-7mff2
May 13 09:46:27.984: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-7mff2
STEP: checking the pod's current state and verifying that restartCount is present
May 13 09:46:27.988: INFO: Initial restart count of pod liveness-http is 0
May 13 09:46:42.027: INFO: Restart count of pod e2e-tests-container-probe-7mff2/liveness-http is now 1 (14.038752138s elapsed)
May 13 09:47:04.091: INFO: Restart count of pod e2e-tests-container-probe-7mff2/liveness-http is now 2 (36.10234595s elapsed)
May 13 09:47:22.138: INFO: Restart count of pod e2e-tests-container-probe-7mff2/liveness-http is now 3 (54.14996684s elapsed)
May 13 09:47:42.184: INFO: Restart count of pod e2e-tests-container-probe-7mff2/liveness-http is now 4 (1m14.195444706s elapsed)
May 13 09:48:54.359: INFO: Restart count of pod e2e-tests-container-probe-7mff2/liveness-http is now 5 (2m26.371007571s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:48:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-7mff2" for this suite.
May 13 09:49:00.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:49:00.613: INFO: namespace: e2e-tests-container-probe-7mff2, resource: bindings, ignored listing per whitelist
May 13 09:49:00.704: INFO: namespace e2e-tests-container-probe-7mff2 deletion completed in 6.325512838s

• [SLOW TEST:177.034 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:49:00.705: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-rz2wt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-t5fnw
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
May 13 09:49:12.417: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-sjq2l
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:49:30.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-rz2wt" for this suite.
May 13 09:49:36.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:49:36.505: INFO: namespace: e2e-tests-namespaces-rz2wt, resource: bindings, ignored listing per whitelist
May 13 09:49:36.619: INFO: namespace e2e-tests-namespaces-rz2wt deletion completed in 6.197242256s
STEP: Destroying namespace "e2e-tests-nsdeletetest-t5fnw" for this suite.
May 13 09:49:36.621: INFO: Namespace e2e-tests-nsdeletetest-t5fnw was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-sjq2l" for this suite.
May 13 09:49:42.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:49:42.986: INFO: namespace: e2e-tests-nsdeletetest-sjq2l, resource: bindings, ignored listing per whitelist
May 13 09:49:42.988: INFO: namespace e2e-tests-nsdeletetest-sjq2l deletion completed in 6.367381397s

• [SLOW TEST:42.284 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:49:42.989: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lm4wz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 13 09:49:43.288: INFO: Waiting up to 5m0s for pod "pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-lm4wz" to be "success or failure"
May 13 09:49:43.295: INFO: Pod "pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.791266ms
May 13 09:49:45.298: INFO: Pod "pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010286881s
STEP: Saw pod success
May 13 09:49:45.298: INFO: Pod "pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:49:45.301: INFO: Trying to get logs from node 172.16.176.226 pod pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 09:49:45.325: INFO: Waiting for pod pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:49:45.328: INFO: Pod pod-6f1852d5-7564-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:49:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lm4wz" for this suite.
May 13 09:49:51.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:49:51.375: INFO: namespace: e2e-tests-emptydir-lm4wz, resource: bindings, ignored listing per whitelist
May 13 09:49:51.574: INFO: namespace e2e-tests-emptydir-lm4wz deletion completed in 6.240727461s

• [SLOW TEST:8.585 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:49:51.574: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sgczp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:49:51.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-sgczp" to be "success or failure"
May 13 09:49:51.887: INFO: Pod "downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387631ms
May 13 09:49:53.897: INFO: Pod "downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012573383s
May 13 09:49:55.900: INFO: Pod "downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01591221s
STEP: Saw pod success
May 13 09:49:55.900: INFO: Pod "downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:49:55.902: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:49:55.985: INFO: Waiting for pod downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:49:55.992: INFO: Pod downwardapi-volume-743187e6-7564-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:49:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sgczp" for this suite.
May 13 09:50:02.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:50:02.108: INFO: namespace: e2e-tests-downward-api-sgczp, resource: bindings, ignored listing per whitelist
May 13 09:50:02.662: INFO: namespace e2e-tests-downward-api-sgczp deletion completed in 6.665156732s

• [SLOW TEST:11.087 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:50:02.663: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-hbm87
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:50:02.946: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 13 09:50:03.003: INFO: Number of nodes with available pods: 0
May 13 09:50:03.003: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:50:04.023: INFO: Number of nodes with available pods: 0
May 13 09:50:04.023: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:50:05.013: INFO: Number of nodes with available pods: 0
May 13 09:50:05.013: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 09:50:06.013: INFO: Number of nodes with available pods: 4
May 13 09:50:06.013: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 13 09:50:06.092: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:06.092: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:06.092: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:06.092: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:07.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:07.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:07.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:07.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:08.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:08.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:08.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:08.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:09.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:09.105: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:09.105: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:09.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:10.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:10.109: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:10.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:10.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:11.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:11.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:11.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:11.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:12.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:12.109: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:12.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:12.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:13.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:13.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:13.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:13.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:14.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:14.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:14.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:14.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:15.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:15.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:15.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:15.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:16.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:16.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:16.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:16.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:17.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:17.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:17.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:17.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:18.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:18.108: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:18.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:18.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:19.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:19.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:19.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:19.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:20.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:20.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:20.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:20.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:21.118: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:21.118: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:21.118: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:21.118: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:22.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:22.113: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:22.113: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:22.113: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:23.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:23.108: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:23.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:23.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:24.119: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:24.119: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:24.119: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:24.119: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:25.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:25.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:25.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:25.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:26.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:26.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:26.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:26.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:27.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:27.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:27.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:27.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:28.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:28.108: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:28.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:28.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:29.115: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:29.116: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:29.116: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:29.116: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:30.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:30.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:30.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:30.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:31.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:31.110: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:31.110: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:31.110: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:32.111: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:32.111: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:32.111: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:32.111: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:33.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:33.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:33.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:33.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:34.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:34.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:34.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:34.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:35.116: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:35.116: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:35.116: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:35.116: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:36.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:36.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:36.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:36.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:37.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:37.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:37.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:37.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:38.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:38.108: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:38.108: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:38.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:38.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:39.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:39.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:39.106: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:39.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:39.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:40.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:40.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:40.107: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:40.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:40.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:41.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:41.108: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:41.109: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:41.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:41.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:42.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:42.106: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:42.107: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:42.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:42.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:43.126: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:43.126: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:43.126: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:43.126: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:43.126: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:44.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:44.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:44.107: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:44.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:44.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:45.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:45.107: INFO: Wrong image for pod: daemon-set-2b8cl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:45.107: INFO: Pod daemon-set-2b8cl is not available
May 13 09:50:45.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:45.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:46.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:46.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:46.109: INFO: Pod daemon-set-4m7tg is not available
May 13 09:50:46.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:47.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:47.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:47.107: INFO: Pod daemon-set-4m7tg is not available
May 13 09:50:47.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:48.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:48.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:48.109: INFO: Pod daemon-set-4m7tg is not available
May 13 09:50:48.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:49.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:49.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:49.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:50.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:50.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:50.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:51.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:51.105: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:51.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:52.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:52.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:52.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:53.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:53.113: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:53.113: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:54.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:54.105: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:54.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:55.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:55.113: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:55.113: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:56.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:56.110: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:56.110: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:57.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:57.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:57.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:58.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:58.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:58.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:59.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:59.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:50:59.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:00.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:00.105: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:00.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:01.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:01.108: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:01.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:02.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:02.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:02.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:03.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:03.110: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:03.110: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:04.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:04.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:04.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:05.116: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:05.116: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:05.116: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:06.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:06.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:06.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:07.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:07.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:07.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:08.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:08.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:08.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:09.120: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:09.121: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:09.121: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:10.112: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:10.112: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:10.112: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:11.119: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:11.119: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:11.119: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:12.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:12.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:12.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:13.117: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:13.117: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:13.117: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:14.130: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:14.130: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:14.130: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:15.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:15.107: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:15.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:16.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:16.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:16.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:17.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:17.106: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:17.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:18.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:18.105: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:18.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:19.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:19.109: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:19.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:20.114: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:20.114: INFO: Wrong image for pod: daemon-set-482lh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:20.114: INFO: Pod daemon-set-482lh is not available
May 13 09:51:20.114: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:21.119: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:21.119: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:22.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:22.106: INFO: Pod daemon-set-dgprb is not available
May 13 09:51:22.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:23.112: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:23.113: INFO: Pod daemon-set-dgprb is not available
May 13 09:51:23.113: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:24.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:24.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:25.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:25.110: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:26.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:26.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:27.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:27.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:28.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:28.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:29.114: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:29.114: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:30.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:30.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:31.117: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:31.117: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:32.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:32.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:33.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:33.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:34.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:34.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:35.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:35.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:36.117: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:36.117: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:37.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:37.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:38.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:38.113: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:39.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:39.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:40.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:40.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:41.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:41.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:42.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:42.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:43.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:43.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:44.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:44.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:45.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:45.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:46.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:46.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:47.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:47.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:48.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:48.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:49.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:49.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:50.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:50.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:51.117: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:51.117: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:52.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:52.106: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:53.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:53.108: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:54.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:54.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:55.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:55.105: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:55.105: INFO: Pod daemon-set-v77pr is not available
May 13 09:51:56.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:56.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:56.108: INFO: Pod daemon-set-v77pr is not available
May 13 09:51:57.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:57.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:57.107: INFO: Pod daemon-set-v77pr is not available
May 13 09:51:58.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:58.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:58.107: INFO: Pod daemon-set-v77pr is not available
May 13 09:51:59.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:59.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:51:59.107: INFO: Pod daemon-set-v77pr is not available
May 13 09:52:00.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:00.107: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:00.107: INFO: Pod daemon-set-v77pr is not available
May 13 09:52:01.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:01.109: INFO: Wrong image for pod: daemon-set-v77pr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:01.109: INFO: Pod daemon-set-v77pr is not available
May 13 09:52:02.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:02.108: INFO: Pod daemon-set-xkh89 is not available
May 13 09:52:03.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:03.107: INFO: Pod daemon-set-xkh89 is not available
May 13 09:52:04.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:04.111: INFO: Pod daemon-set-xkh89 is not available
May 13 09:52:05.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:05.107: INFO: Pod daemon-set-xkh89 is not available
May 13 09:52:06.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:07.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:08.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:09.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:10.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:11.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:12.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:13.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:14.108: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:15.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:16.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:17.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:18.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:19.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:20.105: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:21.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:22.112: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:23.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:24.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:25.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:26.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:27.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:28.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:29.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:30.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:31.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:32.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:33.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:34.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:35.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:36.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:37.113: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:38.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:38.106: INFO: Pod daemon-set-24hls is not available
May 13 09:52:39.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:39.106: INFO: Pod daemon-set-24hls is not available
May 13 09:52:40.110: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:40.110: INFO: Pod daemon-set-24hls is not available
May 13 09:52:41.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:41.107: INFO: Pod daemon-set-24hls is not available
May 13 09:52:42.106: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:42.106: INFO: Pod daemon-set-24hls is not available
May 13 09:52:43.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:43.107: INFO: Pod daemon-set-24hls is not available
May 13 09:52:44.107: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:44.107: INFO: Pod daemon-set-24hls is not available
May 13 09:52:45.109: INFO: Wrong image for pod: daemon-set-24hls. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 13 09:52:45.109: INFO: Pod daemon-set-24hls is not available
May 13 09:52:46.107: INFO: Pod daemon-set-rd5sb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 13 09:52:46.126: INFO: Number of nodes with available pods: 3
May 13 09:52:46.126: INFO: Node 172.16.175.32 is running more than one daemon pod
May 13 09:52:47.136: INFO: Number of nodes with available pods: 3
May 13 09:52:47.136: INFO: Node 172.16.175.32 is running more than one daemon pod
May 13 09:52:48.134: INFO: Number of nodes with available pods: 3
May 13 09:52:48.134: INFO: Node 172.16.175.32 is running more than one daemon pod
May 13 09:52:49.137: INFO: Number of nodes with available pods: 3
May 13 09:52:49.137: INFO: Node 172.16.175.32 is running more than one daemon pod
May 13 09:52:50.135: INFO: Number of nodes with available pods: 3
May 13 09:52:50.135: INFO: Node 172.16.175.32 is running more than one daemon pod
May 13 09:52:51.137: INFO: Number of nodes with available pods: 4
May 13 09:52:51.137: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-hbm87, will wait for the garbage collector to delete the pods
May 13 09:52:51.222: INFO: Deleting DaemonSet.extensions daemon-set took: 10.45434ms
May 13 09:52:51.323: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.513778ms
May 13 09:52:56.927: INFO: Number of nodes with available pods: 0
May 13 09:52:56.927: INFO: Number of running nodes: 0, number of available pods: 0
May 13 09:52:56.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-hbm87/daemonsets","resourceVersion":"860468"},"items":null}

May 13 09:52:56.932: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-hbm87/pods","resourceVersion":"860468"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:52:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-hbm87" for this suite.
May 13 09:53:04.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:53:05.172: INFO: namespace: e2e-tests-daemonsets-hbm87, resource: bindings, ignored listing per whitelist
May 13 09:53:05.272: INFO: namespace e2e-tests-daemonsets-hbm87 deletion completed in 8.319377549s

• [SLOW TEST:182.609 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:53:05.273: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-8kcl2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-8kcl2
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-8kcl2
STEP: Deleting pre-stop pod
May 13 09:53:54.758: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:53:54.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-8kcl2" for this suite.
May 13 09:54:34.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:54:34.852: INFO: namespace: e2e-tests-prestop-8kcl2, resource: bindings, ignored listing per whitelist
May 13 09:54:34.977: INFO: namespace e2e-tests-prestop-8kcl2 deletion completed in 40.201745531s

• [SLOW TEST:89.704 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:54:34.978: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-74j8n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-74j8n
May 13 09:54:37.291: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-74j8n
STEP: checking the pod's current state and verifying that restartCount is present
May 13 09:54:37.297: INFO: Initial restart count of pod liveness-http is 0
May 13 09:54:57.365: INFO: Restart count of pod e2e-tests-container-probe-74j8n/liveness-http is now 1 (20.068391647s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:54:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-74j8n" for this suite.
May 13 09:55:05.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:55:05.695: INFO: namespace: e2e-tests-container-probe-74j8n, resource: bindings, ignored listing per whitelist
May 13 09:55:05.732: INFO: namespace e2e-tests-container-probe-74j8n deletion completed in 8.346733769s

• [SLOW TEST:30.755 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:55:05.733: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-r8zhj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:55:05.994: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
May 13 09:55:06.006: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-r8zhj/daemonsets","resourceVersion":"860869"},"items":null}

May 13 09:55:06.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-r8zhj/pods","resourceVersion":"860869"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:55:06.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-r8zhj" for this suite.
May 13 09:55:12.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:55:12.176: INFO: namespace: e2e-tests-daemonsets-r8zhj, resource: bindings, ignored listing per whitelist
May 13 09:55:12.323: INFO: namespace e2e-tests-daemonsets-r8zhj deletion completed in 6.281887159s

S [SKIPPING] [6.589 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  May 13 09:55:05.994: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:55:12.324: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-b6lgr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-s7sbs
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-624l8
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:55:19.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-b6lgr" for this suite.
May 13 09:55:25.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:55:25.146: INFO: namespace: e2e-tests-namespaces-b6lgr, resource: bindings, ignored listing per whitelist
May 13 09:55:25.281: INFO: namespace e2e-tests-namespaces-b6lgr deletion completed in 6.266566899s
STEP: Destroying namespace "e2e-tests-nsdeletetest-s7sbs" for this suite.
May 13 09:55:25.286: INFO: Namespace e2e-tests-nsdeletetest-s7sbs was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-624l8" for this suite.
May 13 09:55:31.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:55:31.551: INFO: namespace: e2e-tests-nsdeletetest-624l8, resource: bindings, ignored listing per whitelist
May 13 09:55:31.663: INFO: namespace e2e-tests-nsdeletetest-624l8 deletion completed in 6.377656086s

• [SLOW TEST:19.340 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:55:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-l6tsm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 13 09:55:31.981: INFO: Pod name pod-release: Found 0 pods out of 1
May 13 09:55:36.986: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:55:38.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-l6tsm" for this suite.
May 13 09:55:44.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:55:44.191: INFO: namespace: e2e-tests-replication-controller-l6tsm, resource: bindings, ignored listing per whitelist
May 13 09:55:44.237: INFO: namespace e2e-tests-replication-controller-l6tsm deletion completed in 6.201017439s

• [SLOW TEST:12.573 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:55:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-46kg9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 13 09:55:50.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 09:55:50.756: INFO: Pod pod-with-poststart-http-hook still exists
May 13 09:55:52.757: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 09:55:52.762: INFO: Pod pod-with-poststart-http-hook still exists
May 13 09:55:54.759: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 09:55:54.776: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:55:54.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-46kg9" for this suite.
May 13 09:56:18.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:56:18.972: INFO: namespace: e2e-tests-container-lifecycle-hook-46kg9, resource: bindings, ignored listing per whitelist
May 13 09:56:19.097: INFO: namespace e2e-tests-container-lifecycle-hook-46kg9 deletion completed in 24.298961414s

• [SLOW TEST:34.860 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:56:19.098: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-s2cj7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:56:23.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-s2cj7" for this suite.
May 13 09:57:19.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:57:19.729: INFO: namespace: e2e-tests-kubelet-test-s2cj7, resource: bindings, ignored listing per whitelist
May 13 09:57:19.761: INFO: namespace e2e-tests-kubelet-test-s2cj7 deletion completed in 56.286616537s

• [SLOW TEST:60.663 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:57:19.764: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9jqd9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 09:57:20.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-9jqd9" to be "success or failure"
May 13 09:57:20.110: INFO: Pod "downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.544893ms
May 13 09:57:22.116: INFO: Pod "downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011015924s
STEP: Saw pod success
May 13 09:57:22.116: INFO: Pod "downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:57:22.123: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 09:57:22.160: INFO: Waiting for pod downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:57:22.162: INFO: Pod downwardapi-volume-7f5b323f-7565-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:57:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9jqd9" for this suite.
May 13 09:57:28.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:57:28.352: INFO: namespace: e2e-tests-projected-9jqd9, resource: bindings, ignored listing per whitelist
May 13 09:57:28.514: INFO: namespace e2e-tests-projected-9jqd9 deletion completed in 6.347802642s

• [SLOW TEST:8.751 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:57:28.516: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-jbwzm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4
May 13 09:57:28.802: INFO: Pod name my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4: Found 0 pods out of 1
May 13 09:57:33.826: INFO: Pod name my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4: Found 1 pods out of 1
May 13 09:57:33.826: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4" are running
May 13 09:57:33.830: INFO: Pod "my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4-qgjjz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:56:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:56:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:56:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:57:28 +0000 UTC Reason: Message:}])
May 13 09:57:33.830: INFO: Trying to dial the pod
May 13 09:57:38.869: INFO: Controller my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4: Got expected result from replica 1 [my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4-qgjjz]: "my-hostname-basic-848d79cd-7565-11e9-bbcc-d288ccfb79a4-qgjjz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:57:38.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-jbwzm" for this suite.
May 13 09:57:44.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:57:45.093: INFO: namespace: e2e-tests-replication-controller-jbwzm, resource: bindings, ignored listing per whitelist
May 13 09:57:45.172: INFO: namespace e2e-tests-replication-controller-jbwzm deletion completed in 6.271027023s

• [SLOW TEST:16.657 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:57:45.174: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-qdfxt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-qdfxt
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qdfxt to expose endpoints map[]
May 13 09:57:45.401: INFO: Get endpoints failed (4.640684ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 13 09:57:46.406: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qdfxt exposes endpoints map[] (1.009766311s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-qdfxt
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qdfxt to expose endpoints map[pod1:[80]]
May 13 09:57:49.518: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qdfxt exposes endpoints map[pod1:[80]] (3.038396598s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-qdfxt
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qdfxt to expose endpoints map[pod1:[80] pod2:[80]]
May 13 09:57:52.623: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qdfxt exposes endpoints map[pod1:[80] pod2:[80]] (3.040614899s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-qdfxt
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qdfxt to expose endpoints map[pod2:[80]]
May 13 09:57:53.645: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qdfxt exposes endpoints map[pod2:[80]] (1.01748104s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-qdfxt
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qdfxt to expose endpoints map[]
May 13 09:57:54.657: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qdfxt exposes endpoints map[] (1.007916039s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:57:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-qdfxt" for this suite.
May 13 09:58:18.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:58:18.846: INFO: namespace: e2e-tests-services-qdfxt, resource: bindings, ignored listing per whitelist
May 13 09:58:18.890: INFO: namespace e2e-tests-services-qdfxt deletion completed in 24.203220167s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:33.717 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:58:18.891: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gs7ml
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a290d1c6-7565-11e9-bbcc-d288ccfb79a4
STEP: Creating secret with name s-test-opt-upd-a290d21b-7565-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a290d1c6-7565-11e9-bbcc-d288ccfb79a4
STEP: Updating secret s-test-opt-upd-a290d21b-7565-11e9-bbcc-d288ccfb79a4
STEP: Creating secret with name s-test-opt-create-a290d236-7565-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:58:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gs7ml" for this suite.
May 13 09:58:49.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:58:50.018: INFO: namespace: e2e-tests-projected-gs7ml, resource: bindings, ignored listing per whitelist
May 13 09:58:50.055: INFO: namespace e2e-tests-projected-gs7ml deletion completed in 24.328696877s

• [SLOW TEST:31.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:58:50.056: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xbnl8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-b529277a-7565-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 09:58:50.380: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-xbnl8" to be "success or failure"
May 13 09:58:50.383: INFO: Pod "pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508719ms
May 13 09:58:52.387: INFO: Pod "pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007882802s
May 13 09:58:54.393: INFO: Pod "pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013038622s
STEP: Saw pod success
May 13 09:58:54.393: INFO: Pod "pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:58:54.398: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 09:58:54.453: INFO: Waiting for pod pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:58:54.472: INFO: Pod pod-projected-configmaps-b529ded1-7565-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:58:54.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xbnl8" for this suite.
May 13 09:59:00.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:59:00.784: INFO: namespace: e2e-tests-projected-xbnl8, resource: bindings, ignored listing per whitelist
May 13 09:59:00.792: INFO: namespace e2e-tests-projected-xbnl8 deletion completed in 6.299273604s

• [SLOW TEST:10.736 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:59:00.793: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-t8n5w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 13 09:59:01.087: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861829,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 09:59:01.087: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861830,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 13 09:59:01.087: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861831,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 13 09:59:11.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861853,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 09:59:11.121: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861854,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 13 09:59:11.121: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-t8n5w,SelfLink:/api/v1/namespaces/e2e-tests-watch-t8n5w/configmaps/e2e-watch-test-label-changed,UID:bb9466e3-7565-11e9-b9b7-00163e01adca,ResourceVersion:861855,Generation:0,CreationTimestamp:2019-05-13 09:59:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:59:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-t8n5w" for this suite.
May 13 09:59:17.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:59:17.208: INFO: namespace: e2e-tests-watch-t8n5w, resource: bindings, ignored listing per whitelist
May 13 09:59:17.412: INFO: namespace e2e-tests-watch-t8n5w deletion completed in 6.284810267s

• [SLOW TEST:16.619 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:59:17.415: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7tdgn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-c574bc5a-7565-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 09:59:17.687: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-7tdgn" to be "success or failure"
May 13 09:59:17.692: INFO: Pod "pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704481ms
May 13 09:59:19.696: INFO: Pod "pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008411984s
May 13 09:59:21.699: INFO: Pod "pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230042s
STEP: Saw pod success
May 13 09:59:21.699: INFO: Pod "pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:59:21.702: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 09:59:21.725: INFO: Waiting for pod pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:59:21.728: INFO: Pod pod-projected-secrets-c5755d16-7565-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:59:21.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7tdgn" for this suite.
May 13 09:59:27.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:59:27.775: INFO: namespace: e2e-tests-projected-7tdgn, resource: bindings, ignored listing per whitelist
May 13 09:59:27.983: INFO: namespace e2e-tests-projected-7tdgn deletion completed in 6.251032935s

• [SLOW TEST:10.569 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:59:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-69f5b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 13 09:59:32.899: INFO: Successfully updated pod "pod-update-cbc4ec91-7565-11e9-bbcc-d288ccfb79a4"
STEP: verifying the updated pod is in kubernetes
May 13 09:59:32.906: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:59:32.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-69f5b" for this suite.
May 13 09:59:44.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:59:45.217: INFO: namespace: e2e-tests-pods-69f5b, resource: bindings, ignored listing per whitelist
May 13 09:59:45.283: INFO: namespace e2e-tests-pods-69f5b deletion completed in 12.369425453s

• [SLOW TEST:17.299 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:59:45.284: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-kd7xk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 13 09:59:45.582: INFO: Waiting up to 5m0s for pod "downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-kd7xk" to be "success or failure"
May 13 09:59:45.584: INFO: Pod "downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015192ms
May 13 09:59:47.589: INFO: Pod "downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007062393s
May 13 09:59:49.593: INFO: Pod "downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011072801s
STEP: Saw pod success
May 13 09:59:49.593: INFO: Pod "downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 09:59:49.595: INFO: Trying to get logs from node 172.16.176.226 pod downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4 container dapi-container: <nil>
STEP: delete the pod
May 13 09:59:49.616: INFO: Waiting for pod downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4 to disappear
May 13 09:59:49.619: INFO: Pod downward-api-d60f6272-7565-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:59:49.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kd7xk" for this suite.
May 13 09:59:57.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 09:59:57.665: INFO: namespace: e2e-tests-downward-api-kd7xk, resource: bindings, ignored listing per whitelist
May 13 09:59:57.886: INFO: namespace e2e-tests-downward-api-kd7xk deletion completed in 8.261888439s

• [SLOW TEST:12.602 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 09:59:57.888: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-j2jqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 09:59:58.201: INFO: (0) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 20.218581ms)
May 13 09:59:58.206: INFO: (1) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.586828ms)
May 13 09:59:58.215: INFO: (2) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 8.249616ms)
May 13 09:59:58.221: INFO: (3) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.564257ms)
May 13 09:59:58.228: INFO: (4) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.42642ms)
May 13 09:59:58.233: INFO: (5) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.20449ms)
May 13 09:59:58.238: INFO: (6) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.377466ms)
May 13 09:59:58.243: INFO: (7) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 4.745069ms)
May 13 09:59:58.249: INFO: (8) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.83549ms)
May 13 09:59:58.255: INFO: (9) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.9696ms)
May 13 09:59:58.265: INFO: (10) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 9.398085ms)
May 13 09:59:58.271: INFO: (11) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.256458ms)
May 13 09:59:58.279: INFO: (12) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.846872ms)
May 13 09:59:58.286: INFO: (13) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.060827ms)
May 13 09:59:58.294: INFO: (14) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.750031ms)
May 13 09:59:58.303: INFO: (15) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 9.03121ms)
May 13 09:59:58.315: INFO: (16) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 11.725927ms)
May 13 09:59:58.333: INFO: (17) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 17.675725ms)
May 13 09:59:58.341: INFO: (18) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 8.622565ms)
May 13 09:59:58.349: INFO: (19) /api/v1/nodes/172.16.173.202/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.986461ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 09:59:58.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-j2jqp" for this suite.
May 13 10:00:04.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:00:04.693: INFO: namespace: e2e-tests-proxy-j2jqp, resource: bindings, ignored listing per whitelist
May 13 10:00:04.706: INFO: namespace e2e-tests-proxy-j2jqp deletion completed in 6.350864267s

• [SLOW TEST:6.818 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:00:04.706: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-r58d7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:00:05.095: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-r58d7" to be "success or failure"
May 13 10:00:05.099: INFO: Pod "downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159333ms
May 13 10:00:07.103: INFO: Pod "downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007924923s
May 13 10:00:09.109: INFO: Pod "downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014414402s
STEP: Saw pod success
May 13 10:00:09.109: INFO: Pod "downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:00:09.117: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:00:09.155: INFO: Waiting for pod downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:00:09.161: INFO: Pod downwardapi-volume-e1b07754-7565-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:00:09.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r58d7" for this suite.
May 13 10:00:17.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:00:17.304: INFO: namespace: e2e-tests-projected-r58d7, resource: bindings, ignored listing per whitelist
May 13 10:00:17.422: INFO: namespace e2e-tests-projected-r58d7 deletion completed in 8.255757416s

• [SLOW TEST:12.716 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:00:17.423: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-k9fmx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8tbbv in namespace e2e-tests-proxy-k9fmx
I0513 10:00:17.784136      21 runners.go:184] Created replication controller with name: proxy-service-8tbbv, namespace: e2e-tests-proxy-k9fmx, replica count: 1
I0513 10:00:18.834828      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:19.835678      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:20.836030      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:21.836256      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:22.840912      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:23.841215      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 10:00:24.841481      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 10:00:25.841724      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 10:00:26.841975      21 runners.go:184] proxy-service-8tbbv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 13 10:00:26.845: INFO: setup took 9.178270856s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 13 10:00:26.854: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 8.962203ms)
May 13 10:00:26.854: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 9.017857ms)
May 13 10:00:26.854: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.038571ms)
May 13 10:00:26.854: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 8.910119ms)
May 13 10:00:26.854: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 9.352608ms)
May 13 10:00:26.864: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 18.599248ms)
May 13 10:00:26.864: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 18.4647ms)
May 13 10:00:26.864: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 18.756902ms)
May 13 10:00:26.864: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 18.68472ms)
May 13 10:00:26.864: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 18.791467ms)
May 13 10:00:26.870: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 24.947694ms)
May 13 10:00:26.872: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 26.597493ms)
May 13 10:00:26.873: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 27.329272ms)
May 13 10:00:26.873: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 27.721715ms)
May 13 10:00:26.874: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 28.469835ms)
May 13 10:00:26.874: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 28.689735ms)
May 13 10:00:26.881: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 6.90825ms)
May 13 10:00:26.881: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.796911ms)
May 13 10:00:26.881: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 6.898122ms)
May 13 10:00:26.881: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.815262ms)
May 13 10:00:26.882: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 7.617621ms)
May 13 10:00:26.882: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 7.677096ms)
May 13 10:00:26.882: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.638406ms)
May 13 10:00:26.883: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 8.553094ms)
May 13 10:00:26.886: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 12.066603ms)
May 13 10:00:26.886: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 12.083964ms)
May 13 10:00:26.886: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 11.944604ms)
May 13 10:00:26.886: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 11.974376ms)
May 13 10:00:26.888: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 13.700715ms)
May 13 10:00:26.888: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 13.820483ms)
May 13 10:00:26.888: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 13.929705ms)
May 13 10:00:26.888: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 14.249764ms)
May 13 10:00:26.893: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 4.480023ms)
May 13 10:00:26.894: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 5.247726ms)
May 13 10:00:26.894: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 4.883095ms)
May 13 10:00:26.894: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 4.508996ms)
May 13 10:00:26.894: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 3.390633ms)
May 13 10:00:26.895: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 3.672798ms)
May 13 10:00:26.895: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 5.684111ms)
May 13 10:00:26.895: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 4.939071ms)
May 13 10:00:26.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 7.020854ms)
May 13 10:00:26.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 4.894782ms)
May 13 10:00:26.896: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.802455ms)
May 13 10:00:26.899: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.485261ms)
May 13 10:00:26.899: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 8.523717ms)
May 13 10:00:26.899: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 9.766906ms)
May 13 10:00:26.899: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 9.853212ms)
May 13 10:00:26.899: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 8.768681ms)
May 13 10:00:26.905: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.922417ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 7.230429ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.743659ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 7.691348ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 7.625162ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 8.035531ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 7.333091ms)
May 13 10:00:26.908: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.07309ms)
May 13 10:00:26.909: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.749624ms)
May 13 10:00:26.911: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 11.255476ms)
May 13 10:00:26.912: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 11.654802ms)
May 13 10:00:26.912: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 12.507919ms)
May 13 10:00:26.912: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 12.358733ms)
May 13 10:00:26.913: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 12.054186ms)
May 13 10:00:26.913: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 12.324891ms)
May 13 10:00:26.913: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 12.58589ms)
May 13 10:00:26.920: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.024298ms)
May 13 10:00:26.920: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 6.86181ms)
May 13 10:00:26.920: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 7.109893ms)
May 13 10:00:26.920: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.073164ms)
May 13 10:00:26.923: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 9.395463ms)
May 13 10:00:26.924: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 10.334182ms)
May 13 10:00:26.924: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 10.904777ms)
May 13 10:00:26.924: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 11.145529ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 11.32556ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 11.355342ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 11.38728ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 11.493983ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 11.433571ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 11.441228ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 11.443237ms)
May 13 10:00:26.925: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 11.680034ms)
May 13 10:00:26.930: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.178152ms)
May 13 10:00:26.930: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 5.178231ms)
May 13 10:00:26.930: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 5.096683ms)
May 13 10:00:26.931: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 4.927572ms)
May 13 10:00:26.933: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 7.003455ms)
May 13 10:00:26.933: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.914528ms)
May 13 10:00:26.933: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 6.833812ms)
May 13 10:00:26.933: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 6.577622ms)
May 13 10:00:26.933: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.671652ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 7.980193ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 7.721457ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 7.854848ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 8.175095ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 8.470943ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.646125ms)
May 13 10:00:26.934: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 8.85544ms)
May 13 10:00:26.938: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 3.810566ms)
May 13 10:00:26.938: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 3.730048ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 5.986475ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 5.808881ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 5.435669ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 6.142028ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 6.010323ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.784128ms)
May 13 10:00:26.940: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 5.739502ms)
May 13 10:00:26.941: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 7.069083ms)
May 13 10:00:26.941: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 6.717827ms)
May 13 10:00:26.941: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.479706ms)
May 13 10:00:26.941: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.913913ms)
May 13 10:00:26.942: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 7.127892ms)
May 13 10:00:26.942: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 7.479883ms)
May 13 10:00:26.942: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 7.812022ms)
May 13 10:00:26.949: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 6.496419ms)
May 13 10:00:26.949: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 7.340051ms)
May 13 10:00:26.949: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 7.166008ms)
May 13 10:00:26.950: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 7.152329ms)
May 13 10:00:26.950: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 7.587574ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 9.243605ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 9.161636ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.258666ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.817832ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 9.43026ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 9.681927ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.011032ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 8.917483ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 9.294571ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 9.384442ms)
May 13 10:00:26.952: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.421427ms)
May 13 10:00:26.957: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 4.953754ms)
May 13 10:00:26.959: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 6.509635ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 9.314033ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.286855ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 9.511219ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.370572ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 9.446215ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 9.480575ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 9.655125ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 9.474643ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 9.549913ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.628461ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 9.574725ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 9.846092ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 9.798479ms)
May 13 10:00:26.962: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 9.966267ms)
May 13 10:00:26.966: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 4.051733ms)
May 13 10:00:26.969: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 6.274349ms)
May 13 10:00:26.969: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.211646ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.590321ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.592235ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 9.083054ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 8.80787ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 9.494813ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 9.329021ms)
May 13 10:00:26.972: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 9.93513ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.617173ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 9.538241ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 10.13773ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 10.24409ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 10.58623ms)
May 13 10:00:26.973: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 10.392966ms)
May 13 10:00:26.979: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.228687ms)
May 13 10:00:26.981: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.083229ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 7.565357ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 7.07244ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 8.777555ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 7.508944ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 8.021338ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.417539ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 8.540418ms)
May 13 10:00:26.982: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 7.426726ms)
May 13 10:00:26.983: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 8.354813ms)
May 13 10:00:26.985: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 10.536369ms)
May 13 10:00:26.986: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 10.52205ms)
May 13 10:00:26.986: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 11.145994ms)
May 13 10:00:26.986: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 11.762914ms)
May 13 10:00:26.986: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 11.746561ms)
May 13 10:00:26.992: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 5.340714ms)
May 13 10:00:26.992: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 5.816911ms)
May 13 10:00:26.993: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.220637ms)
May 13 10:00:26.993: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 6.976193ms)
May 13 10:00:26.993: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 6.775546ms)
May 13 10:00:26.997: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 10.683943ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 11.355484ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 11.024106ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 10.541065ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 11.209651ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 11.434158ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 10.705473ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 10.794752ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 11.312582ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 11.063113ms)
May 13 10:00:26.998: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 11.356992ms)
May 13 10:00:27.003: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 5.023148ms)
May 13 10:00:27.005: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 5.901295ms)
May 13 10:00:27.005: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 6.39234ms)
May 13 10:00:27.006: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 6.574411ms)
May 13 10:00:27.007: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.718113ms)
May 13 10:00:27.007: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.267545ms)
May 13 10:00:27.007: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 9.188245ms)
May 13 10:00:27.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 8.259716ms)
May 13 10:00:27.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.809567ms)
May 13 10:00:27.008: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.12197ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 10.131343ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.631197ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 10.703994ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 9.764791ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 10.600196ms)
May 13 10:00:27.009: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 10.411506ms)
May 13 10:00:27.017: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 6.609899ms)
May 13 10:00:27.017: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 7.308707ms)
May 13 10:00:27.017: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.476584ms)
May 13 10:00:27.019: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 9.616037ms)
May 13 10:00:27.019: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 9.288516ms)
May 13 10:00:27.019: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 9.384386ms)
May 13 10:00:27.019: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 9.164847ms)
May 13 10:00:27.020: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 10.237716ms)
May 13 10:00:27.020: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 10.113015ms)
May 13 10:00:27.020: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 9.70827ms)
May 13 10:00:27.020: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 9.265235ms)
May 13 10:00:27.021: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 10.77448ms)
May 13 10:00:27.021: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 11.125273ms)
May 13 10:00:27.021: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 10.601033ms)
May 13 10:00:27.021: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 10.562137ms)
May 13 10:00:27.021: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 11.000898ms)
May 13 10:00:27.028: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.436419ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.475853ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 8.768395ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.624108ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 8.567578ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 8.802788ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 8.778825ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.953354ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 9.100187ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 8.66939ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 8.85057ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 8.918466ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 8.953377ms)
May 13 10:00:27.030: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 9.184975ms)
May 13 10:00:27.031: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 9.864638ms)
May 13 10:00:27.031: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 10.127258ms)
May 13 10:00:27.037: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 5.436856ms)
May 13 10:00:27.037: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 5.791385ms)
May 13 10:00:27.037: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 5.578475ms)
May 13 10:00:27.037: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 5.74511ms)
May 13 10:00:27.037: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 5.70997ms)
May 13 10:00:27.038: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 6.519373ms)
May 13 10:00:27.038: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 6.485088ms)
May 13 10:00:27.038: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 6.198007ms)
May 13 10:00:27.039: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 7.383458ms)
May 13 10:00:27.041: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.06646ms)
May 13 10:00:27.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 10.043058ms)
May 13 10:00:27.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 10.164642ms)
May 13 10:00:27.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 10.377756ms)
May 13 10:00:27.042: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 10.197941ms)
May 13 10:00:27.043: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 11.333781ms)
May 13 10:00:27.044: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 12.338532ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 13.299477ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 13.1931ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 13.160455ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 13.406678ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 13.283243ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 13.514049ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 13.331535ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 13.489472ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 13.669482ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 13.581044ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 13.357031ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 13.755337ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 13.767862ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 13.508306ms)
May 13 10:00:27.057: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 13.694307ms)
May 13 10:00:27.058: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 14.120578ms)
May 13 10:00:27.064: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 5.510651ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 9.608414ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 9.848266ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 10.233581ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 10.055543ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 10.1863ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 9.786838ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 9.742121ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 10.025635ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 10.087125ms)
May 13 10:00:27.068: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 9.928419ms)
May 13 10:00:27.069: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 10.934715ms)
May 13 10:00:27.070: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 12.137264ms)
May 13 10:00:27.071: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 13.202671ms)
May 13 10:00:27.072: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 13.624823ms)
May 13 10:00:27.072: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 13.368272ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 11.339658ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 11.459468ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 11.331312ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 11.245084ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 11.318713ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 11.434164ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 11.560621ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 11.773168ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 11.873428ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 12.110303ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 11.889679ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 11.909457ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 11.951379ms)
May 13 10:00:27.084: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 12.02754ms)
May 13 10:00:27.085: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 12.64691ms)
May 13 10:00:27.086: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 14.234017ms)
May 13 10:00:27.095: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 8.435129ms)
May 13 10:00:27.095: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:1080/proxy/rewri... (200; 8.327212ms)
May 13 10:00:27.095: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 8.488117ms)
May 13 10:00:27.095: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:443/proxy/... (200; 8.178679ms)
May 13 10:00:27.100: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname2/proxy/: bar (200; 13.622889ms)
May 13 10:00:27.100: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:462/proxy/: tls qux (200; 13.424647ms)
May 13 10:00:27.100: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv/proxy/rewriteme"... (200; 13.516745ms)
May 13 10:00:27.100: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/https:proxy-service-8tbbv-5hpvv:460/proxy/: tls baz (200; 13.418224ms)
May 13 10:00:27.101: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:160/proxy/: foo (200; 13.66415ms)
May 13 10:00:27.101: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/http:proxy-service-8tbbv-5hpvv:1080/proxy/... (200; 13.502548ms)
May 13 10:00:27.101: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/pods/proxy-service-8tbbv-5hpvv:162/proxy/: bar (200; 13.67077ms)
May 13 10:00:27.101: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname2/proxy/: tls qux (200; 13.807674ms)
May 13 10:00:27.101: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/https:proxy-service-8tbbv:tlsportname1/proxy/: tls baz (200; 14.208382ms)
May 13 10:00:27.102: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/proxy-service-8tbbv:portname1/proxy/: foo (200; 15.430767ms)
May 13 10:00:27.102: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname1/proxy/: foo (200; 15.127226ms)
May 13 10:00:27.103: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-k9fmx/services/http:proxy-service-8tbbv:portname2/proxy/: bar (200; 16.032267ms)
STEP: deleting ReplicationController proxy-service-8tbbv in namespace e2e-tests-proxy-k9fmx, will wait for the garbage collector to delete the pods
May 13 10:00:27.167: INFO: Deleting ReplicationController proxy-service-8tbbv took: 10.190672ms
May 13 10:00:27.267: INFO: Terminating ReplicationController proxy-service-8tbbv pods took: 100.278044ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:00:36.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-k9fmx" for this suite.
May 13 10:00:42.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:00:42.325: INFO: namespace: e2e-tests-proxy-k9fmx, resource: bindings, ignored listing per whitelist
May 13 10:00:42.499: INFO: namespace e2e-tests-proxy-k9fmx deletion completed in 6.325051704s

• [SLOW TEST:25.076 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:00:42.500: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-s6frf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:00:42.718: INFO: Creating ReplicaSet my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4
May 13 10:00:42.802: INFO: Pod name my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4: Found 0 pods out of 1
May 13 10:00:47.806: INFO: Pod name my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4: Found 1 pods out of 1
May 13 10:00:47.806: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4" is running
May 13 10:00:47.808: INFO: Pod "my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4-bk5cr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:59:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:59:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 09:59:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 10:00:42 +0000 UTC Reason: Message:}])
May 13 10:00:47.808: INFO: Trying to dial the pod
May 13 10:00:52.855: INFO: Controller my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4: Got expected result from replica 1 [my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4-bk5cr]: "my-hostname-basic-f82a57d2-7565-11e9-bbcc-d288ccfb79a4-bk5cr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:00:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-s6frf" for this suite.
May 13 10:01:00.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:01.318: INFO: namespace: e2e-tests-replicaset-s6frf, resource: bindings, ignored listing per whitelist
May 13 10:01:01.337: INFO: namespace e2e-tests-replicaset-s6frf deletion completed in 8.474447327s

• [SLOW TEST:18.837 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:01:01.338: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wm7lr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:01:01.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-wm7lr" to be "success or failure"
May 13 10:01:01.722: INFO: Pod "downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.306965ms
May 13 10:01:03.727: INFO: Pod "downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024046316s
May 13 10:01:05.732: INFO: Pod "downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029269006s
STEP: Saw pod success
May 13 10:01:05.732: INFO: Pod "downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:01:05.746: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:01:05.796: INFO: Waiting for pod downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:01:05.800: INFO: Pod downwardapi-volume-0367d548-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:01:05.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wm7lr" for this suite.
May 13 10:01:11.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:11.895: INFO: namespace: e2e-tests-projected-wm7lr, resource: bindings, ignored listing per whitelist
May 13 10:01:12.037: INFO: namespace e2e-tests-projected-wm7lr deletion completed in 6.229637004s

• [SLOW TEST:10.699 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:01:12.037: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-t8mx6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-vkjg7
STEP: Creating secret with name secret-test-09c6fbf2-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 10:01:12.497: INFO: Waiting up to 5m0s for pod "pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-t8mx6" to be "success or failure"
May 13 10:01:12.505: INFO: Pod "pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342064ms
May 13 10:01:14.518: INFO: Pod "pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01932088s
May 13 10:01:16.529: INFO: Pod "pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02960015s
STEP: Saw pod success
May 13 10:01:16.529: INFO: Pod "pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:01:16.533: INFO: Trying to get logs from node 172.16.177.10 pod pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 10:01:16.558: INFO: Waiting for pod pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:01:16.566: INFO: Pod pod-secrets-09e2be4a-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:01:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t8mx6" for this suite.
May 13 10:01:22.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:22.805: INFO: namespace: e2e-tests-secrets-t8mx6, resource: bindings, ignored listing per whitelist
May 13 10:01:22.850: INFO: namespace e2e-tests-secrets-t8mx6 deletion completed in 6.277750951s
STEP: Destroying namespace "e2e-tests-secret-namespace-vkjg7" for this suite.
May 13 10:01:28.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:29.034: INFO: namespace: e2e-tests-secret-namespace-vkjg7, resource: bindings, ignored listing per whitelist
May 13 10:01:29.094: INFO: namespace e2e-tests-secret-namespace-vkjg7 deletion completed in 6.243888413s

• [SLOW TEST:17.057 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:01:29.095: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-chwrq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:01:29.385: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-chwrq" to be "success or failure"
May 13 10:01:29.389: INFO: Pod "downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59458ms
May 13 10:01:31.394: INFO: Pod "downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009028014s
STEP: Saw pod success
May 13 10:01:31.394: INFO: Pod "downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:01:31.399: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:01:31.427: INFO: Waiting for pod downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:01:31.430: INFO: Pod downwardapi-volume-13f6752c-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:01:31.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-chwrq" for this suite.
May 13 10:01:37.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:37.696: INFO: namespace: e2e-tests-downward-api-chwrq, resource: bindings, ignored listing per whitelist
May 13 10:01:37.760: INFO: namespace e2e-tests-downward-api-chwrq deletion completed in 6.32250945s

• [SLOW TEST:8.665 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:01:37.762: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vhzfs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-vhzfs/configmap-test-191ed54f-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:01:38.091: INFO: Waiting up to 5m0s for pod "pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-vhzfs" to be "success or failure"
May 13 10:01:38.094: INFO: Pod "pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.001782ms
May 13 10:01:40.098: INFO: Pod "pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00694569s
May 13 10:01:42.102: INFO: Pod "pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01104889s
STEP: Saw pod success
May 13 10:01:42.103: INFO: Pod "pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:01:42.106: INFO: Trying to get logs from node 172.16.177.10 pod pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4 container env-test: <nil>
STEP: delete the pod
May 13 10:01:42.139: INFO: Waiting for pod pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:01:42.142: INFO: Pod pod-configmaps-191fbb16-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:01:42.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vhzfs" for this suite.
May 13 10:01:48.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:01:48.276: INFO: namespace: e2e-tests-configmap-vhzfs, resource: bindings, ignored listing per whitelist
May 13 10:01:48.310: INFO: namespace e2e-tests-configmap-vhzfs deletion completed in 6.161657174s

• [SLOW TEST:10.548 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:01:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cvqpk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
May 13 10:01:48.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:49.322: INFO: stderr: ""
May 13 10:01:49.322: INFO: stdout: "pod/pause created\n"
May 13 10:01:49.322: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 13 10:01:49.323: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-cvqpk" to be "running and ready"
May 13 10:01:49.326: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234084ms
May 13 10:01:51.332: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00946075s
May 13 10:01:53.338: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.014940939s
May 13 10:01:53.338: INFO: Pod "pause" satisfied condition "running and ready"
May 13 10:01:53.338: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
May 13 10:01:53.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:53.592: INFO: stderr: ""
May 13 10:01:53.592: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 13 10:01:53.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pod pause -L testing-label --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:53.769: INFO: stderr: ""
May 13 10:01:53.769: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 13 10:01:53.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 label pods pause testing-label- --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:53.885: INFO: stderr: ""
May 13 10:01:53.885: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 13 10:01:53.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pod pause -L testing-label --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:54.008: INFO: stderr: ""
May 13 10:01:54.008: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
May 13 10:01:54.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:54.116: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 10:01:54.116: INFO: stdout: "pod \"pause\" force deleted\n"
May 13 10:01:54.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-cvqpk'
May 13 10:01:54.247: INFO: stderr: "No resources found.\n"
May 13 10:01:54.247: INFO: stdout: ""
May 13 10:01:54.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -l name=pause --namespace=e2e-tests-kubectl-cvqpk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 10:01:54.360: INFO: stderr: ""
May 13 10:01:54.360: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:01:54.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cvqpk" for this suite.
May 13 10:02:00.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:02:00.393: INFO: namespace: e2e-tests-kubectl-cvqpk, resource: bindings, ignored listing per whitelist
May 13 10:02:00.635: INFO: namespace e2e-tests-kubectl-cvqpk deletion completed in 6.270135086s

• [SLOW TEST:12.325 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:02:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-75vxs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 10:02:00.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-75vxs'
May 13 10:02:01.114: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 10:02:01.114: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
May 13 10:02:01.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-75vxs'
May 13 10:02:01.262: INFO: stderr: ""
May 13 10:02:01.262: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:02:01.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-75vxs" for this suite.
May 13 10:02:07.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:02:07.379: INFO: namespace: e2e-tests-kubectl-75vxs, resource: bindings, ignored listing per whitelist
May 13 10:02:07.524: INFO: namespace e2e-tests-kubectl-75vxs deletion completed in 6.256533495s

• [SLOW TEST:6.888 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:02:07.524: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lfx9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:02:07.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-lfx9x" to be "success or failure"
May 13 10:02:07.796: INFO: Pod "downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204724ms
May 13 10:02:09.799: INFO: Pod "downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006267526s
May 13 10:02:11.803: INFO: Pod "downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010145039s
STEP: Saw pod success
May 13 10:02:11.803: INFO: Pod "downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:02:11.805: INFO: Trying to get logs from node 172.16.176.226 pod downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:02:11.827: INFO: Waiting for pod downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:02:11.830: INFO: Pod downwardapi-volume-2ad7124a-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:02:11.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lfx9x" for this suite.
May 13 10:02:17.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:02:17.949: INFO: namespace: e2e-tests-downward-api-lfx9x, resource: bindings, ignored listing per whitelist
May 13 10:02:18.012: INFO: namespace e2e-tests-downward-api-lfx9x deletion completed in 6.17705507s

• [SLOW TEST:10.488 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:02:18.013: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-gl6xd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:02:18.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gl6xd" for this suite.
May 13 10:02:42.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:02:42.500: INFO: namespace: e2e-tests-pods-gl6xd, resource: bindings, ignored listing per whitelist
May 13 10:02:42.536: INFO: namespace e2e-tests-pods-gl6xd deletion completed in 24.267868492s

• [SLOW TEST:24.524 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:02:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-gnzs8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:02:42.783: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 13 10:02:47.789: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 10:02:47.789: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 13 10:02:49.792: INFO: Creating deployment "test-rollover-deployment"
May 13 10:02:49.884: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 13 10:02:51.889: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 13 10:02:51.894: INFO: Ensure that both replica sets have 1 created replica
May 13 10:02:51.898: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 13 10:02:51.981: INFO: Updating deployment test-rollover-deployment
May 13 10:02:51.981: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 13 10:02:53.993: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 13 10:02:54.005: INFO: Make sure deployment "test-rollover-deployment" is complete
May 13 10:02:54.009: INFO: 
May 13 10:02:54.009: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 10:02:54.015: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-gnzs8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gnzs8/deployments/test-rollover-deployment,UID:43f4953e-7566-11e9-b9b7-00163e01adca,ResourceVersion:863000,Generation:2,CreationTimestamp:2019-05-13 10:02:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-13 10:02:49 +0000 UTC 2019-05-13 10:02:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-13 10:02:53 +0000 UTC 2019-05-13 10:02:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 13 10:02:54.019: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-gnzs8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gnzs8/replicasets/test-rollover-deployment-6b7f9d6597,UID:4536b47b-7566-11e9-b9b7-00163e01adca,ResourceVersion:862991,Generation:2,CreationTimestamp:2019-05-13 10:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 43f4953e-7566-11e9-b9b7-00163e01adca 0xc0013e6e97 0xc0013e6e98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 13 10:02:54.019: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 13 10:02:54.019: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-gnzs8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gnzs8/replicasets/test-rollover-controller,UID:3fb720c9-7566-11e9-b9b7-00163e01adca,ResourceVersion:862999,Generation:2,CreationTimestamp:2019-05-13 10:02:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 43f4953e-7566-11e9-b9b7-00163e01adca 0xc0013e6887 0xc0013e6888}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 10:02:54.026: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-gnzs8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-gnzs8/replicasets/test-rollover-deployment-6586df867b,UID:43f8eead-7566-11e9-b9b7-00163e01adca,ResourceVersion:862972,Generation:2,CreationTimestamp:2019-05-13 10:02:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 43f4953e-7566-11e9-b9b7-00163e01adca 0xc0013e6d67 0xc0013e6d68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 10:02:54.030: INFO: Pod "test-rollover-deployment-6b7f9d6597-92jpb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-92jpb,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-gnzs8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-gnzs8/pods/test-rollover-deployment-6b7f9d6597-92jpb,UID:453caeb6-7566-11e9-b9b7-00163e01adca,ResourceVersion:862990,Generation:0,CreationTimestamp:2019-05-13 10:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 4536b47b-7566-11e9-b9b7-00163e01adca 0xc000df7177 0xc000df7178}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cswfr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cswfr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cswfr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000df7390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000df73b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:01:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:01:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:01:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:02:52 +0000 UTC  }],Message:,Reason:,HostIP:172.16.176.226,PodIP:10.1.209.60,StartTime:2019-05-13 10:01:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-13 10:01:56 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2c55cdcba657073b66ccd1685084b5a2e87035eefe4426bd13a4751239ab61f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:02:54.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-gnzs8" for this suite.
May 13 10:03:00.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:03:00.189: INFO: namespace: e2e-tests-deployment-gnzs8, resource: bindings, ignored listing per whitelist
May 13 10:03:00.242: INFO: namespace e2e-tests-deployment-gnzs8 deletion completed in 6.208170735s

• [SLOW TEST:17.705 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:03:00.242: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-lggl6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 13 10:03:00.594: INFO: Number of nodes with available pods: 0
May 13 10:03:00.594: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:03:01.608: INFO: Number of nodes with available pods: 0
May 13 10:03:01.608: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:03:02.606: INFO: Number of nodes with available pods: 1
May 13 10:03:02.607: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:03:03.615: INFO: Number of nodes with available pods: 4
May 13 10:03:03.615: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 13 10:03:03.645: INFO: Number of nodes with available pods: 3
May 13 10:03:03.645: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:04.656: INFO: Number of nodes with available pods: 3
May 13 10:03:04.656: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:05.658: INFO: Number of nodes with available pods: 3
May 13 10:03:05.658: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:06.655: INFO: Number of nodes with available pods: 3
May 13 10:03:06.655: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:07.656: INFO: Number of nodes with available pods: 3
May 13 10:03:07.657: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:08.674: INFO: Number of nodes with available pods: 3
May 13 10:03:08.674: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:09.655: INFO: Number of nodes with available pods: 3
May 13 10:03:09.655: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:10.658: INFO: Number of nodes with available pods: 3
May 13 10:03:10.658: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:11.664: INFO: Number of nodes with available pods: 3
May 13 10:03:11.664: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:12.662: INFO: Number of nodes with available pods: 3
May 13 10:03:12.662: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:13.666: INFO: Number of nodes with available pods: 3
May 13 10:03:13.667: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:14.660: INFO: Number of nodes with available pods: 3
May 13 10:03:14.660: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:15.657: INFO: Number of nodes with available pods: 3
May 13 10:03:15.657: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:16.653: INFO: Number of nodes with available pods: 3
May 13 10:03:16.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:17.657: INFO: Number of nodes with available pods: 3
May 13 10:03:17.657: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:18.667: INFO: Number of nodes with available pods: 3
May 13 10:03:18.667: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:19.655: INFO: Number of nodes with available pods: 3
May 13 10:03:19.655: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:20.662: INFO: Number of nodes with available pods: 3
May 13 10:03:20.662: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:21.657: INFO: Number of nodes with available pods: 3
May 13 10:03:21.657: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:22.662: INFO: Number of nodes with available pods: 3
May 13 10:03:22.662: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:23.658: INFO: Number of nodes with available pods: 3
May 13 10:03:23.658: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:24.658: INFO: Number of nodes with available pods: 3
May 13 10:03:24.658: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:25.657: INFO: Number of nodes with available pods: 3
May 13 10:03:25.657: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:26.654: INFO: Number of nodes with available pods: 3
May 13 10:03:26.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:27.663: INFO: Number of nodes with available pods: 3
May 13 10:03:27.663: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:28.659: INFO: Number of nodes with available pods: 3
May 13 10:03:28.659: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:29.655: INFO: Number of nodes with available pods: 3
May 13 10:03:29.655: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:30.654: INFO: Number of nodes with available pods: 3
May 13 10:03:30.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:31.654: INFO: Number of nodes with available pods: 3
May 13 10:03:31.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:32.654: INFO: Number of nodes with available pods: 3
May 13 10:03:32.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:33.656: INFO: Number of nodes with available pods: 3
May 13 10:03:33.656: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:34.662: INFO: Number of nodes with available pods: 3
May 13 10:03:34.662: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:35.663: INFO: Number of nodes with available pods: 3
May 13 10:03:35.663: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:36.656: INFO: Number of nodes with available pods: 3
May 13 10:03:36.656: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:37.661: INFO: Number of nodes with available pods: 3
May 13 10:03:37.661: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:38.653: INFO: Number of nodes with available pods: 3
May 13 10:03:38.653: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:39.654: INFO: Number of nodes with available pods: 3
May 13 10:03:39.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:40.663: INFO: Number of nodes with available pods: 3
May 13 10:03:40.663: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:41.653: INFO: Number of nodes with available pods: 3
May 13 10:03:41.653: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:42.654: INFO: Number of nodes with available pods: 3
May 13 10:03:42.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:43.657: INFO: Number of nodes with available pods: 3
May 13 10:03:43.658: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:44.653: INFO: Number of nodes with available pods: 3
May 13 10:03:44.653: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:45.656: INFO: Number of nodes with available pods: 3
May 13 10:03:45.656: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:46.654: INFO: Number of nodes with available pods: 3
May 13 10:03:46.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:47.654: INFO: Number of nodes with available pods: 3
May 13 10:03:47.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:48.654: INFO: Number of nodes with available pods: 3
May 13 10:03:48.654: INFO: Node 172.16.176.226 is running more than one daemon pod
May 13 10:03:49.653: INFO: Number of nodes with available pods: 4
May 13 10:03:49.653: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-lggl6, will wait for the garbage collector to delete the pods
May 13 10:03:49.716: INFO: Deleting DaemonSet.extensions daemon-set took: 7.134802ms
May 13 10:03:49.816: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.274259ms
May 13 10:04:32.320: INFO: Number of nodes with available pods: 0
May 13 10:04:32.321: INFO: Number of running nodes: 0, number of available pods: 0
May 13 10:04:32.324: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-lggl6/daemonsets","resourceVersion":"863340"},"items":null}

May 13 10:04:32.327: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-lggl6/pods","resourceVersion":"863340"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:04:32.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-lggl6" for this suite.
May 13 10:04:38.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:04:38.541: INFO: namespace: e2e-tests-daemonsets-lggl6, resource: bindings, ignored listing per whitelist
May 13 10:04:38.701: INFO: namespace e2e-tests-daemonsets-lggl6 deletion completed in 6.308382595s

• [SLOW TEST:98.459 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:04:38.704: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2h85r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0513 10:04:49.234362      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 10:04:49.234: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:04:49.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2h85r" for this suite.
May 13 10:04:57.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:04:57.498: INFO: namespace: e2e-tests-gc-2h85r, resource: bindings, ignored listing per whitelist
May 13 10:04:57.580: INFO: namespace e2e-tests-gc-2h85r deletion completed in 8.332982364s

• [SLOW TEST:18.877 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:04:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-jmqnt
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:04:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:04:58.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-jmqnt" for this suite.
May 13 10:05:04.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:05:05.005: INFO: namespace: e2e-tests-custom-resource-definition-jmqnt, resource: bindings, ignored listing per whitelist
May 13 10:05:05.096: INFO: namespace e2e-tests-custom-resource-definition-jmqnt deletion completed in 6.231971108s

• [SLOW TEST:7.515 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:05:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hd67c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-94aeec3e-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:05:05.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-hd67c" to be "success or failure"
May 13 10:05:05.402: INFO: Pod "pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.917162ms
May 13 10:05:07.407: INFO: Pod "pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015957576s
May 13 10:05:09.412: INFO: Pod "pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020619149s
STEP: Saw pod success
May 13 10:05:09.412: INFO: Pod "pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:05:09.414: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 10:05:09.437: INFO: Waiting for pod pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:05:09.441: INFO: Pod pod-projected-configmaps-94af8f67-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:05:09.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hd67c" for this suite.
May 13 10:05:15.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:05:15.570: INFO: namespace: e2e-tests-projected-hd67c, resource: bindings, ignored listing per whitelist
May 13 10:05:15.687: INFO: namespace e2e-tests-projected-hd67c deletion completed in 6.242630597s

• [SLOW TEST:10.589 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:05:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-698qr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-6v2h
STEP: Creating a pod to test atomic-volume-subpath
May 13 10:05:15.989: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6v2h" in namespace "e2e-tests-subpath-698qr" to be "success or failure"
May 13 10:05:15.992: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479185ms
May 13 10:05:17.997: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007895993s
May 13 10:05:20.001: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 4.011791053s
May 13 10:05:22.008: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 6.01946501s
May 13 10:05:24.013: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 8.024312993s
May 13 10:05:26.018: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 10.028868661s
May 13 10:05:28.031: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 12.042140071s
May 13 10:05:30.042: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 14.053737416s
May 13 10:05:32.047: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 16.058179625s
May 13 10:05:34.060: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 18.071678906s
May 13 10:05:36.065: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 20.076112298s
May 13 10:05:38.069: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Running", Reason="", readiness=false. Elapsed: 22.080298022s
May 13 10:05:40.081: INFO: Pod "pod-subpath-test-secret-6v2h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.092705477s
STEP: Saw pod success
May 13 10:05:40.086: INFO: Pod "pod-subpath-test-secret-6v2h" satisfied condition "success or failure"
May 13 10:05:40.090: INFO: Trying to get logs from node 172.16.176.226 pod pod-subpath-test-secret-6v2h container test-container-subpath-secret-6v2h: <nil>
STEP: delete the pod
May 13 10:05:40.117: INFO: Waiting for pod pod-subpath-test-secret-6v2h to disappear
May 13 10:05:40.121: INFO: Pod pod-subpath-test-secret-6v2h no longer exists
STEP: Deleting pod pod-subpath-test-secret-6v2h
May 13 10:05:40.121: INFO: Deleting pod "pod-subpath-test-secret-6v2h" in namespace "e2e-tests-subpath-698qr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:05:40.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-698qr" for this suite.
May 13 10:05:46.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:05:46.328: INFO: namespace: e2e-tests-subpath-698qr, resource: bindings, ignored listing per whitelist
May 13 10:05:46.341: INFO: namespace e2e-tests-subpath-698qr deletion completed in 6.20879157s

• [SLOW TEST:30.654 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:05:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-qcqwc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:05:50.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-qcqwc" for this suite.
May 13 10:05:56.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:05:56.972: INFO: namespace: e2e-tests-emptydir-wrapper-qcqwc, resource: bindings, ignored listing per whitelist
May 13 10:05:57.142: INFO: namespace e2e-tests-emptydir-wrapper-qcqwc deletion completed in 6.38992123s

• [SLOW TEST:10.800 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:05:57.142: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-xnctb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 13 10:06:03.923: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:03.923: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.107: INFO: Exec stderr: ""
May 13 10:06:04.107: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.107: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.266: INFO: Exec stderr: ""
May 13 10:06:04.267: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.267: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.429: INFO: Exec stderr: ""
May 13 10:06:04.430: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.430: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.584: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 13 10:06:04.584: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.738: INFO: Exec stderr: ""
May 13 10:06:04.738: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:04.859: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 13 10:06:04.860: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:04.860: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:05.055: INFO: Exec stderr: ""
May 13 10:06:05.055: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:05.055: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:05.241: INFO: Exec stderr: ""
May 13 10:06:05.241: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:05.241: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:05.379: INFO: Exec stderr: ""
May 13 10:06:05.379: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xnctb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:06:05.379: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:06:05.543: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:06:05.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-xnctb" for this suite.
May 13 10:06:47.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:06:47.814: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-xnctb, resource: bindings, ignored listing per whitelist
May 13 10:06:47.844: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-xnctb deletion completed in 42.295167305s

• [SLOW TEST:50.702 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:06:47.844: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-krk4t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d1ee28fe-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 10:06:48.177: INFO: Waiting up to 5m0s for pod "pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-krk4t" to be "success or failure"
May 13 10:06:48.181: INFO: Pod "pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475954ms
May 13 10:06:50.202: INFO: Pod "pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025415946s
May 13 10:06:52.212: INFO: Pod "pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034720422s
STEP: Saw pod success
May 13 10:06:52.212: INFO: Pod "pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:06:52.215: INFO: Trying to get logs from node 172.16.176.226 pod pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 10:06:52.248: INFO: Waiting for pod pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:06:52.253: INFO: Pod pod-secrets-d1eeba17-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:06:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-krk4t" for this suite.
May 13 10:06:58.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:06:58.325: INFO: namespace: e2e-tests-secrets-krk4t, resource: bindings, ignored listing per whitelist
May 13 10:06:58.568: INFO: namespace e2e-tests-secrets-krk4t deletion completed in 6.307965667s

• [SLOW TEST:10.724 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:06:58.568: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8brss
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-d85b66cf-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating configMap with name cm-test-opt-upd-d85b6787-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d85b66cf-7566-11e9-bbcc-d288ccfb79a4
STEP: Updating configmap cm-test-opt-upd-d85b6787-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating configMap with name cm-test-opt-create-d85b67a8-7566-11e9-bbcc-d288ccfb79a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:07:07.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8brss" for this suite.
May 13 10:07:31.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:07:31.684: INFO: namespace: e2e-tests-projected-8brss, resource: bindings, ignored listing per whitelist
May 13 10:07:31.738: INFO: namespace e2e-tests-projected-8brss deletion completed in 24.23357868s

• [SLOW TEST:33.170 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:07:31.739: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-px9w7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 13 10:07:32.081: INFO: Waiting up to 5m0s for pod "pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-px9w7" to be "success or failure"
May 13 10:07:32.085: INFO: Pod "pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352244ms
May 13 10:07:34.091: INFO: Pod "pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009881034s
May 13 10:07:36.095: INFO: Pod "pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013830356s
STEP: Saw pod success
May 13 10:07:36.095: INFO: Pod "pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:07:36.098: INFO: Trying to get logs from node 172.16.176.226 pod pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:07:36.118: INFO: Waiting for pod pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:07:36.122: INFO: Pod pod-ec1b40ee-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:07:36.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-px9w7" for this suite.
May 13 10:07:42.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:07:42.253: INFO: namespace: e2e-tests-emptydir-px9w7, resource: bindings, ignored listing per whitelist
May 13 10:07:42.329: INFO: namespace e2e-tests-emptydir-px9w7 deletion completed in 6.199574644s

• [SLOW TEST:10.590 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:07:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rpq7l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-f27f789a-7566-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:07:42.785: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-rpq7l" to be "success or failure"
May 13 10:07:42.789: INFO: Pod "pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943408ms
May 13 10:07:44.792: INFO: Pod "pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006219465s
May 13 10:07:46.796: INFO: Pod "pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010095807s
STEP: Saw pod success
May 13 10:07:46.796: INFO: Pod "pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:07:46.798: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 10:07:46.822: INFO: Waiting for pod pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:07:46.825: INFO: Pod pod-projected-configmaps-f2808f52-7566-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:07:46.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rpq7l" for this suite.
May 13 10:07:52.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:07:53.096: INFO: namespace: e2e-tests-projected-rpq7l, resource: bindings, ignored listing per whitelist
May 13 10:07:53.147: INFO: namespace e2e-tests-projected-rpq7l deletion completed in 6.317242534s

• [SLOW TEST:10.818 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:07:53.147: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-f4mqf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:07:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-f4mqf" for this suite.
May 13 10:08:53.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:08:53.624: INFO: namespace: e2e-tests-kubelet-test-f4mqf, resource: bindings, ignored listing per whitelist
May 13 10:08:53.865: INFO: namespace e2e-tests-kubelet-test-f4mqf deletion completed in 56.293485688s

• [SLOW TEST:60.718 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:08:53.867: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rlnqd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 10:08:54.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-rlnqd'
May 13 10:08:54.310: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 10:08:54.310: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
May 13 10:08:58.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-rlnqd'
May 13 10:08:58.470: INFO: stderr: ""
May 13 10:08:58.470: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:08:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rlnqd" for this suite.
May 13 10:09:04.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:09:04.776: INFO: namespace: e2e-tests-kubectl-rlnqd, resource: bindings, ignored listing per whitelist
May 13 10:09:04.802: INFO: namespace e2e-tests-kubectl-rlnqd deletion completed in 6.311453196s

• [SLOW TEST:10.935 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:09:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rpqp8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 13 10:09:07.684: INFO: Successfully updated pod "annotationupdate23915e84-7567-11e9-bbcc-d288ccfb79a4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:09:09.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rpqp8" for this suite.
May 13 10:09:33.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:09:34.011: INFO: namespace: e2e-tests-downward-api-rpqp8, resource: bindings, ignored listing per whitelist
May 13 10:09:34.025: INFO: namespace e2e-tests-downward-api-rpqp8 deletion completed in 24.30795589s

• [SLOW TEST:29.220 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:09:34.026: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-8zrxk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-8zrxk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 10:09:34.261: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 10:09:59.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.22.222:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8zrxk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:09:59.722: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:09:59.915: INFO: Found all expected endpoints: [netserver-0]
May 13 10:09:59.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.15.61:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8zrxk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:09:59.919: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:10:00.082: INFO: Found all expected endpoints: [netserver-1]
May 13 10:10:00.089: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.209.17:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8zrxk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:10:00.089: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:10:00.300: INFO: Found all expected endpoints: [netserver-2]
May 13 10:10:00.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.197.238:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-8zrxk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 10:10:00.303: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
May 13 10:10:00.470: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:10:00.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-8zrxk" for this suite.
May 13 10:10:24.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:10:24.722: INFO: namespace: e2e-tests-pod-network-test-8zrxk, resource: bindings, ignored listing per whitelist
May 13 10:10:24.825: INFO: namespace e2e-tests-pod-network-test-8zrxk deletion completed in 24.348067393s

• [SLOW TEST:50.799 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:10:24.826: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-2s8m9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2s8m9
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
May 13 10:10:25.197: INFO: Found 0 stateful pods, waiting for 3
May 13 10:10:35.201: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:10:35.201: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:10:35.201: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 13 10:10:35.281: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 13 10:10:45.390: INFO: Updating stateful set ss2
May 13 10:10:45.395: INFO: Waiting for Pod e2e-tests-statefulset-2s8m9/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 13 10:10:55.444: INFO: Found 1 stateful pods, waiting for 3
May 13 10:11:05.452: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:11:05.452: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:11:05.452: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 13 10:11:05.587: INFO: Updating stateful set ss2
May 13 10:11:05.599: INFO: Waiting for Pod e2e-tests-statefulset-2s8m9/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 13 10:11:15.711: INFO: Updating stateful set ss2
May 13 10:11:15.727: INFO: Waiting for StatefulSet e2e-tests-statefulset-2s8m9/ss2 to complete update
May 13 10:11:15.727: INFO: Waiting for Pod e2e-tests-statefulset-2s8m9/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 13 10:11:25.741: INFO: Waiting for StatefulSet e2e-tests-statefulset-2s8m9/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 10:11:35.734: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2s8m9
May 13 10:11:35.737: INFO: Scaling statefulset ss2 to 0
May 13 10:11:55.768: INFO: Waiting for statefulset status.replicas updated to 0
May 13 10:11:55.773: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:11:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2s8m9" for this suite.
May 13 10:12:03.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:12:03.885: INFO: namespace: e2e-tests-statefulset-2s8m9, resource: bindings, ignored listing per whitelist
May 13 10:12:04.103: INFO: namespace e2e-tests-statefulset-2s8m9 deletion completed in 8.306045252s

• [SLOW TEST:99.277 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:12:04.109: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5pdwl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 13 10:12:04.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 create -f - --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:05.193: INFO: stderr: ""
May 13 10:12:05.193: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 10:12:05.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:05.358: INFO: stderr: ""
May 13 10:12:05.358: INFO: stdout: "update-demo-nautilus-7fqfj update-demo-nautilus-v74jf "
May 13 10:12:05.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-7fqfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:05.524: INFO: stderr: ""
May 13 10:12:05.524: INFO: stdout: ""
May 13 10:12:05.524: INFO: update-demo-nautilus-7fqfj is created but not running
May 13 10:12:10.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:10.648: INFO: stderr: ""
May 13 10:12:10.648: INFO: stdout: "update-demo-nautilus-7fqfj update-demo-nautilus-v74jf "
May 13 10:12:10.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-7fqfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:10.812: INFO: stderr: ""
May 13 10:12:10.812: INFO: stdout: "true"
May 13 10:12:10.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-7fqfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:10.966: INFO: stderr: ""
May 13 10:12:10.966: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 10:12:10.966: INFO: validating pod update-demo-nautilus-7fqfj
May 13 10:12:10.977: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 10:12:10.977: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 10:12:10.977: INFO: update-demo-nautilus-7fqfj is verified up and running
May 13 10:12:10.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:11.138: INFO: stderr: ""
May 13 10:12:11.138: INFO: stdout: "true"
May 13 10:12:11.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:11.286: INFO: stderr: ""
May 13 10:12:11.286: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 10:12:11.286: INFO: validating pod update-demo-nautilus-v74jf
May 13 10:12:11.296: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 10:12:11.296: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 10:12:11.296: INFO: update-demo-nautilus-v74jf is verified up and running
STEP: scaling down the replication controller
May 13 10:12:11.299: INFO: scanned /root for discovery docs: <nil>
May 13 10:12:11.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:12.644: INFO: stderr: ""
May 13 10:12:12.644: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 10:12:12.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:12.781: INFO: stderr: ""
May 13 10:12:12.781: INFO: stdout: "update-demo-nautilus-7fqfj update-demo-nautilus-v74jf "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 13 10:12:17.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:17.893: INFO: stderr: ""
May 13 10:12:17.893: INFO: stdout: "update-demo-nautilus-v74jf "
May 13 10:12:17.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:18.013: INFO: stderr: ""
May 13 10:12:18.013: INFO: stdout: "true"
May 13 10:12:18.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:18.124: INFO: stderr: ""
May 13 10:12:18.124: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 10:12:18.124: INFO: validating pod update-demo-nautilus-v74jf
May 13 10:12:18.129: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 10:12:18.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 10:12:18.129: INFO: update-demo-nautilus-v74jf is verified up and running
STEP: scaling up the replication controller
May 13 10:12:18.132: INFO: scanned /root for discovery docs: <nil>
May 13 10:12:18.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:19.309: INFO: stderr: ""
May 13 10:12:19.309: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 10:12:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:19.474: INFO: stderr: ""
May 13 10:12:19.474: INFO: stdout: "update-demo-nautilus-mdpdz update-demo-nautilus-v74jf "
May 13 10:12:19.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-mdpdz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:19.610: INFO: stderr: ""
May 13 10:12:19.610: INFO: stdout: ""
May 13 10:12:19.610: INFO: update-demo-nautilus-mdpdz is created but not running
May 13 10:12:24.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:24.743: INFO: stderr: ""
May 13 10:12:24.743: INFO: stdout: "update-demo-nautilus-mdpdz update-demo-nautilus-v74jf "
May 13 10:12:24.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-mdpdz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:24.907: INFO: stderr: ""
May 13 10:12:24.907: INFO: stdout: "true"
May 13 10:12:24.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-mdpdz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:25.032: INFO: stderr: ""
May 13 10:12:25.032: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 10:12:25.032: INFO: validating pod update-demo-nautilus-mdpdz
May 13 10:12:25.059: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 10:12:25.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 10:12:25.060: INFO: update-demo-nautilus-mdpdz is verified up and running
May 13 10:12:25.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:25.181: INFO: stderr: ""
May 13 10:12:25.181: INFO: stdout: "true"
May 13 10:12:25.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods update-demo-nautilus-v74jf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:25.288: INFO: stderr: ""
May 13 10:12:25.289: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 10:12:25.289: INFO: validating pod update-demo-nautilus-v74jf
May 13 10:12:25.294: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 10:12:25.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 10:12:25.294: INFO: update-demo-nautilus-v74jf is verified up and running
STEP: using delete to clean up resources
May 13 10:12:25.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:25.425: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 10:12:25.425: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 13 10:12:25.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-5pdwl'
May 13 10:12:25.578: INFO: stderr: "No resources found.\n"
May 13 10:12:25.578: INFO: stdout: ""
May 13 10:12:25.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 get pods -l name=update-demo --namespace=e2e-tests-kubectl-5pdwl -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 10:12:25.703: INFO: stderr: ""
May 13 10:12:25.703: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:12:25.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5pdwl" for this suite.
May 13 10:12:47.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:12:47.901: INFO: namespace: e2e-tests-kubectl-5pdwl, resource: bindings, ignored listing per whitelist
May 13 10:12:47.972: INFO: namespace e2e-tests-kubectl-5pdwl deletion completed in 22.263589744s

• [SLOW TEST:43.863 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:12:47.975: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dtv74
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 13 10:12:48.293: INFO: Waiting up to 5m0s for pod "pod-a899878f-7567-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-dtv74" to be "success or failure"
May 13 10:12:48.298: INFO: Pod "pod-a899878f-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.676715ms
May 13 10:12:50.303: INFO: Pod "pod-a899878f-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009367667s
May 13 10:12:52.306: INFO: Pod "pod-a899878f-7567-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012755637s
STEP: Saw pod success
May 13 10:12:52.306: INFO: Pod "pod-a899878f-7567-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:12:52.309: INFO: Trying to get logs from node 172.16.176.226 pod pod-a899878f-7567-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:12:52.341: INFO: Waiting for pod pod-a899878f-7567-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:12:52.343: INFO: Pod pod-a899878f-7567-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:12:52.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dtv74" for this suite.
May 13 10:12:58.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:12:58.561: INFO: namespace: e2e-tests-emptydir-dtv74, resource: bindings, ignored listing per whitelist
May 13 10:12:58.767: INFO: namespace e2e-tests-emptydir-dtv74 deletion completed in 6.417365018s

• [SLOW TEST:10.793 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:12:58.767: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7gxx2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-af066f20-7567-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 10:12:59.089: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-7gxx2" to be "success or failure"
May 13 10:12:59.096: INFO: Pod "pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.100584ms
May 13 10:13:01.121: INFO: Pod "pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03222349s
May 13 10:13:03.126: INFO: Pod "pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036692843s
STEP: Saw pod success
May 13 10:13:03.126: INFO: Pod "pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:13:03.129: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 10:13:03.149: INFO: Waiting for pod pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:13:03.152: INFO: Pod pod-projected-secrets-af0773e8-7567-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:13:03.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7gxx2" for this suite.
May 13 10:13:11.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:13:11.395: INFO: namespace: e2e-tests-projected-7gxx2, resource: bindings, ignored listing per whitelist
May 13 10:13:11.533: INFO: namespace e2e-tests-projected-7gxx2 deletion completed in 8.375960563s

• [SLOW TEST:12.766 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:13:11.540: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-25vps
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b6a694cf-7567-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:13:11.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-configmap-25vps" to be "success or failure"
May 13 10:13:11.890: INFO: Pod "pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.724937ms
May 13 10:13:13.893: INFO: Pod "pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011546954s
May 13 10:13:15.897: INFO: Pod "pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015112561s
STEP: Saw pod success
May 13 10:13:15.897: INFO: Pod "pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:13:15.900: INFO: Trying to get logs from node 172.16.176.226 pod pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 10:13:15.919: INFO: Waiting for pod pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:13:15.923: INFO: Pod pod-configmaps-b6a7403f-7567-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:13:15.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-25vps" for this suite.
May 13 10:13:21.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:13:22.010: INFO: namespace: e2e-tests-configmap-25vps, resource: bindings, ignored listing per whitelist
May 13 10:13:22.120: INFO: namespace e2e-tests-configmap-25vps deletion completed in 6.193294238s

• [SLOW TEST:10.580 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:13:22.120: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zs528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 10:13:22.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-zs528'
May 13 10:13:22.539: INFO: stderr: ""
May 13 10:13:22.539: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
May 13 10:13:22.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-zs528'
May 13 10:13:32.540: INFO: stderr: ""
May 13 10:13:32.540: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:13:32.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zs528" for this suite.
May 13 10:13:38.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:13:38.709: INFO: namespace: e2e-tests-kubectl-zs528, resource: bindings, ignored listing per whitelist
May 13 10:13:38.867: INFO: namespace e2e-tests-kubectl-zs528 deletion completed in 6.301713635s

• [SLOW TEST:16.747 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:13:38.868: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-drc89
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
May 13 10:13:39.640: INFO: created pod pod-service-account-defaultsa
May 13 10:13:39.640: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 13 10:13:39.929: INFO: created pod pod-service-account-mountsa
May 13 10:13:39.929: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 13 10:13:40.742: INFO: created pod pod-service-account-nomountsa
May 13 10:13:40.742: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 13 10:13:41.559: INFO: created pod pod-service-account-defaultsa-mountspec
May 13 10:13:41.559: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 13 10:13:42.339: INFO: created pod pod-service-account-mountsa-mountspec
May 13 10:13:42.339: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 13 10:13:43.133: INFO: created pod pod-service-account-nomountsa-mountspec
May 13 10:13:43.133: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 13 10:13:43.946: INFO: created pod pod-service-account-defaultsa-nomountspec
May 13 10:13:43.946: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 13 10:13:44.729: INFO: created pod pod-service-account-mountsa-nomountspec
May 13 10:13:44.729: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 13 10:13:45.572: INFO: created pod pod-service-account-nomountsa-nomountspec
May 13 10:13:45.572: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:13:45.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-drc89" for this suite.
May 13 10:13:53.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:13:54.534: INFO: namespace: e2e-tests-svcaccounts-drc89, resource: bindings, ignored listing per whitelist
May 13 10:13:54.545: INFO: namespace e2e-tests-svcaccounts-drc89 deletion completed in 8.967874268s

• [SLOW TEST:15.677 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:13:54.546: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4fvlc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:13:54.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-4fvlc" to be "success or failure"
May 13 10:13:54.899: INFO: Pod "downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007247ms
May 13 10:13:56.902: INFO: Pod "downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006580115s
May 13 10:13:58.911: INFO: Pod "downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015466379s
STEP: Saw pod success
May 13 10:13:58.911: INFO: Pod "downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:13:58.915: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:13:58.955: INFO: Waiting for pod downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:13:58.957: INFO: Pod downwardapi-volume-d04358fe-7567-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:13:58.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4fvlc" for this suite.
May 13 10:14:04.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:14:05.284: INFO: namespace: e2e-tests-downward-api-4fvlc, resource: bindings, ignored listing per whitelist
May 13 10:14:05.329: INFO: namespace e2e-tests-downward-api-4fvlc deletion completed in 6.356494343s

• [SLOW TEST:10.783 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:14:05.329: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-85pq7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-85pq7
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
May 13 10:14:05.604: INFO: Found 0 stateful pods, waiting for 3
May 13 10:14:15.609: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:14:15.609: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:14:15.609: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 13 10:14:15.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-85pq7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 10:14:15.994: INFO: stderr: ""
May 13 10:14:15.994: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 10:14:15.994: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 13 10:14:26.089: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 13 10:14:36.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-85pq7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 10:14:36.389: INFO: stderr: ""
May 13 10:14:36.390: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 10:14:36.390: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 10:14:56.410: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
May 13 10:14:56.410: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 13 10:15:06.424: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
May 13 10:15:06.424: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
May 13 10:15:16.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-85pq7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 10:15:16.733: INFO: stderr: ""
May 13 10:15:16.733: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 10:15:16.733: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 10:15:26.892: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 13 10:15:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 exec --namespace=e2e-tests-statefulset-85pq7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 10:15:37.264: INFO: stderr: ""
May 13 10:15:37.264: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 10:15:37.264: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 10:15:47.296: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
May 13 10:15:47.296: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 13 10:15:47.296: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May 13 10:15:57.306: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
May 13 10:15:57.306: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 13 10:15:57.306: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May 13 10:16:07.312: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
May 13 10:16:07.312: INFO: Waiting for Pod e2e-tests-statefulset-85pq7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 13 10:16:17.312: INFO: Waiting for StatefulSet e2e-tests-statefulset-85pq7/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 10:16:27.321: INFO: Deleting all statefulset in ns e2e-tests-statefulset-85pq7
May 13 10:16:27.324: INFO: Scaling statefulset ss2 to 0
May 13 10:16:47.390: INFO: Waiting for statefulset status.replicas updated to 0
May 13 10:16:47.394: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:16:47.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-85pq7" for this suite.
May 13 10:16:53.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:16:53.631: INFO: namespace: e2e-tests-statefulset-85pq7, resource: bindings, ignored listing per whitelist
May 13 10:16:53.673: INFO: namespace e2e-tests-statefulset-85pq7 deletion completed in 6.250045452s

• [SLOW TEST:168.344 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:16:53.673: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9m5qv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:16:53.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-9m5qv" to be "success or failure"
May 13 10:16:53.958: INFO: Pod "downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005256ms
May 13 10:16:55.963: INFO: Pod "downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010004004s
May 13 10:16:57.968: INFO: Pod "downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015116584s
STEP: Saw pod success
May 13 10:16:57.968: INFO: Pod "downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:16:57.971: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:16:58.011: INFO: Waiting for pod downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:16:58.014: INFO: Pod downwardapi-volume-3b0d02fa-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:16:58.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9m5qv" for this suite.
May 13 10:17:04.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:17:04.189: INFO: namespace: e2e-tests-projected-9m5qv, resource: bindings, ignored listing per whitelist
May 13 10:17:04.291: INFO: namespace e2e-tests-projected-9m5qv deletion completed in 6.270380275s

• [SLOW TEST:10.618 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:17:04.297: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z8wvm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 13 10:17:04.576: INFO: Waiting up to 5m0s for pod "pod-416023c1-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-z8wvm" to be "success or failure"
May 13 10:17:04.593: INFO: Pod "pod-416023c1-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704593ms
May 13 10:17:06.597: INFO: Pod "pod-416023c1-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020815416s
May 13 10:17:08.601: INFO: Pod "pod-416023c1-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024884361s
STEP: Saw pod success
May 13 10:17:08.601: INFO: Pod "pod-416023c1-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:17:08.603: INFO: Trying to get logs from node 172.16.176.226 pod pod-416023c1-7568-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:17:08.640: INFO: Waiting for pod pod-416023c1-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:17:08.643: INFO: Pod pod-416023c1-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:17:08.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z8wvm" for this suite.
May 13 10:17:14.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:17:14.811: INFO: namespace: e2e-tests-emptydir-z8wvm, resource: bindings, ignored listing per whitelist
May 13 10:17:14.929: INFO: namespace e2e-tests-emptydir-z8wvm deletion completed in 6.27760353s

• [SLOW TEST:10.633 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:17:14.929: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-tj6nc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:17:15.197: INFO: Creating deployment "nginx-deployment"
May 13 10:17:15.290: INFO: Waiting for observed generation 1
May 13 10:17:17.299: INFO: Waiting for all required pods to come up
May 13 10:17:17.309: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 13 10:17:19.329: INFO: Waiting for deployment "nginx-deployment" to complete
May 13 10:17:19.335: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 13 10:17:19.387: INFO: Updating deployment nginx-deployment
May 13 10:17:19.387: INFO: Waiting for observed generation 2
May 13 10:17:21.393: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 13 10:17:21.395: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 13 10:17:21.403: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 13 10:17:21.413: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 13 10:17:21.413: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 13 10:17:21.416: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 13 10:17:21.420: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 13 10:17:21.420: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 13 10:17:21.488: INFO: Updating deployment nginx-deployment
May 13 10:17:21.488: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 13 10:17:21.498: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 13 10:17:21.510: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 10:17:21.525: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tj6nc/deployments/nginx-deployment,UID:47c680d6-7568-11e9-b9b7-00163e01adca,ResourceVersion:867354,Generation:3,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-05-13 10:17:19 +0000 UTC 2019-05-13 10:17:15 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-05-13 10:17:21 +0000 UTC 2019-05-13 10:17:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 13 10:17:21.530: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tj6nc/replicasets/nginx-deployment-65bbdb5f8,UID:4a3a67f7-7568-11e9-b9b7-00163e01adca,ResourceVersion:867352,Generation:3,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 47c680d6-7568-11e9-b9b7-00163e01adca 0xc001d03357 0xc001d03358}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 10:17:21.530: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 13 10:17:21.530: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-tj6nc/replicasets/nginx-deployment-555b55d965,UID:47caeeb7-7568-11e9-b9b7-00163e01adca,ResourceVersion:867349,Generation:3,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 47c680d6-7568-11e9-b9b7-00163e01adca 0xc001d03157 0xc001d03158}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 13 10:17:21.546: INFO: Pod "nginx-deployment-555b55d965-5kjxz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5kjxz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-5kjxz,UID:47d01957-7568-11e9-b9b7-00163e01adca,ResourceVersion:867236,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002250727 0xc002250728}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002250820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002250840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.176.226,PodIP:10.1.209.34,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8dfd527fd1077f037b8375e1714426bfa53ad475e41c76df91838ba00a881d2b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.546: INFO: Pod "nginx-deployment-555b55d965-6fkwm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6fkwm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-6fkwm,UID:47d05a18-7568-11e9-b9b7-00163e01adca,ResourceVersion:867259,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002250900 0xc002250901}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.173.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002250b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002250b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.173.202,PodIP:10.1.197.241,StartTime:2019-05-13 10:17:15 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:17:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9d8da6c2902e6a530750ebe8f5bea174ad34f79b1fa473d0338a7d60830ecd71}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.546: INFO: Pod "nginx-deployment-555b55d965-8sp79" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8sp79,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-8sp79,UID:4b7fcb9d-7568-11e9-b9b7-00163e01adca,ResourceVersion:867358,Generation:0,CreationTimestamp:2019-05-13 10:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002250c37 0xc002250c38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002250ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002250ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.546: INFO: Pod "nginx-deployment-555b55d965-9txwh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9txwh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-9txwh,UID:47ce6c1b-7568-11e9-b9b7-00163e01adca,ResourceVersion:867250,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002250f40 0xc002250f41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002251050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002251070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:10.1.22.246,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://120652ccdf558eecbcb36260545c806e4f6e3cccd9612390f92bdfc58e11f8e5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.546: INFO: Pod "nginx-deployment-555b55d965-c6ll2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c6ll2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-c6ll2,UID:47d5cc4e-7568-11e9-b9b7-00163e01adca,ResourceVersion:867228,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251130 0xc002251131}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022511a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022511c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.176.226,PodIP:10.1.209.31,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://24019fc81088d910784a7c3e95bc9e33d69f262ad611312e8f54a187e9333845}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-cjjxd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cjjxd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-cjjxd,UID:4b7ff89f-7568-11e9-b9b7-00163e01adca,ResourceVersion:867359,Generation:0,CreationTimestamp:2019-05-13 10:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251400 0xc002251401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022514a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022514c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-d78wx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-d78wx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-d78wx,UID:47d66eac-7568-11e9-b9b7-00163e01adca,ResourceVersion:867253,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251530 0xc002251531}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.173.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022515a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022515c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.173.202,PodIP:10.1.197.248,StartTime:2019-05-13 10:17:15 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:17:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a4a9c509ad32d57958f7971569df404f4093eeab1831d2f2eba832bf1dba79c5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-g6jkm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-g6jkm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-g6jkm,UID:47d22027-7568-11e9-b9b7-00163e01adca,ResourceVersion:867233,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc0022516c7 0xc0022516c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002251930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002251950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.176.226,PodIP:10.1.209.32,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4c4f597d838bc2ce415188b4bfa7ade107a78164545c1a342f660fdb0394d39b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-q5rdk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-q5rdk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-q5rdk,UID:47d5fbe9-7568-11e9-b9b7-00163e01adca,ResourceVersion:867247,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251bd0 0xc002251bd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002251c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002251ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:10.1.22.240,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5e466643d25de43de1f55b0fb02acd035686ef96604924b8f2c1afc8975ff136}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-tt4pf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tt4pf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-tt4pf,UID:4b7c19e8-7568-11e9-b9b7-00163e01adca,ResourceVersion:867355,Generation:0,CreationTimestamp:2019-05-13 10:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251e70 0xc002251e71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.175.32,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002251ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002251f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.547: INFO: Pod "nginx-deployment-555b55d965-xg4cf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xg4cf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-555b55d965-xg4cf,UID:47d2c8ed-7568-11e9-b9b7-00163e01adca,ResourceVersion:867244,Generation:0,CreationTimestamp:2019-05-13 10:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 47caeeb7-7568-11e9-b9b7-00163e01adca 0xc002251f80 0xc002251f81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022783c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022783e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:10.1.22.239,StartTime:2019-05-13 10:16:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 10:16:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://428fb4947deb0c0cd80c2adf5c8e92130a61ddbe0e5a5bf7d4c55426390236b9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.548: INFO: Pod "nginx-deployment-65bbdb5f8-8cxrk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8cxrk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-8cxrk,UID:4b7ffecc-7568-11e9-b9b7-00163e01adca,ResourceVersion:867361,Generation:0,CreationTimestamp:2019-05-13 10:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc002278610 0xc002278611}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002278690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022786b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.548: INFO: Pod "nginx-deployment-65bbdb5f8-bhbl4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bhbl4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-bhbl4,UID:4a4f5ea8-7568-11e9-b9b7-00163e01adca,ResourceVersion:867317,Generation:0,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc002278720 0xc002278721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.175.32,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002278af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002278b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:18:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:18:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:18:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.175.32,PodIP:,StartTime:2019-05-13 10:18:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.550: INFO: Pod "nginx-deployment-65bbdb5f8-c92jf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-c92jf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-c92jf,UID:4a3b8db7-7568-11e9-b9b7-00163e01adca,ResourceVersion:867293,Generation:0,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc002278bd0 0xc002278bd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002278d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002278d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:,StartTime:2019-05-13 10:16:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.550: INFO: Pod "nginx-deployment-65bbdb5f8-kws8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kws8n,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-kws8n,UID:4a3d1631-7568-11e9-b9b7-00163e01adca,ResourceVersion:867297,Generation:0,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc002279060 0xc002279061}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.176.226,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002279120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002279150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.176.226,PodIP:,StartTime:2019-05-13 10:16:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.551: INFO: Pod "nginx-deployment-65bbdb5f8-rcr9m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rcr9m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-rcr9m,UID:4a50d932-7568-11e9-b9b7-00163e01adca,ResourceVersion:867315,Generation:0,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc0022798a0 0xc0022798a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.177.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002279940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002279960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:16:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.177.10,PodIP:,StartTime:2019-05-13 10:16:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 10:17:21.551: INFO: Pod "nginx-deployment-65bbdb5f8-z7x5b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-z7x5b,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-tj6nc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-tj6nc/pods/nginx-deployment-65bbdb5f8-z7x5b,UID:4a3df557-7568-11e9-b9b7-00163e01adca,ResourceVersion:867300,Generation:0,CreationTimestamp:2019-05-13 10:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 4a3a67f7-7568-11e9-b9b7-00163e01adca 0xc002279b30 0xc002279b31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wdb9b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdb9b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wdb9b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.16.173.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002279bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002279bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 10:17:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.173.202,PodIP:,StartTime:2019-05-13 10:17:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:17:21.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-tj6nc" for this suite.
May 13 10:17:29.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:17:29.683: INFO: namespace: e2e-tests-deployment-tj6nc, resource: bindings, ignored listing per whitelist
May 13 10:17:29.842: INFO: namespace e2e-tests-deployment-tj6nc deletion completed in 8.282082666s

• [SLOW TEST:14.913 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:17:29.842: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-btrpm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 13 10:17:30.058: INFO: PodSpec: initContainers in spec.initContainers
May 13 10:18:19.431: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-509630f5-7568-11e9-bbcc-d288ccfb79a4", GenerateName:"", Namespace:"e2e-tests-init-container-btrpm", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-btrpm/pods/pod-init-509630f5-7568-11e9-bbcc-d288ccfb79a4", UID:"50997151-7568-11e9-b9b7-00163e01adca", ResourceVersion:"867848", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693339450, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"57965735"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-g9dhd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0027ad680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g9dhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g9dhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g9dhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002bc6968), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"172.16.176.226", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001171500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002bc6a70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002bc6a90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002bc6a98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002bc6a9c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339392, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339392, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339392, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339450, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.176.226", PodIP:"10.1.209.37", StartTime:(*v1.Time)(0xc0028ae1e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000dc9ea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000dc9f10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ec32e49b54eabcebd1487593656c83ab3696f28c74fb29e59f6b90fd31cc9b04"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ae220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ae200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:18:19.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-btrpm" for this suite.
May 13 10:18:43.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:18:43.642: INFO: namespace: e2e-tests-init-container-btrpm, resource: bindings, ignored listing per whitelist
May 13 10:18:43.781: INFO: namespace e2e-tests-init-container-btrpm deletion completed in 24.333059925s

• [SLOW TEST:73.939 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:18:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-5v68k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
May 13 10:18:44.090: INFO: Waiting up to 5m0s for pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-containers-5v68k" to be "success or failure"
May 13 10:18:44.099: INFO: Pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.763321ms
May 13 10:18:46.108: INFO: Pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017784178s
May 13 10:18:48.119: INFO: Pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028171149s
May 13 10:18:50.130: INFO: Pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039330302s
STEP: Saw pod success
May 13 10:18:50.130: INFO: Pod "client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:18:50.135: INFO: Trying to get logs from node 172.16.177.10 pod client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:18:50.175: INFO: Waiting for pod client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:18:50.193: INFO: Pod client-containers-7cad4c03-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:18:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-5v68k" for this suite.
May 13 10:18:56.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:18:56.280: INFO: namespace: e2e-tests-containers-5v68k, resource: bindings, ignored listing per whitelist
May 13 10:18:56.401: INFO: namespace e2e-tests-containers-5v68k deletion completed in 6.201462523s

• [SLOW TEST:12.619 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:18:56.402: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-vg7hj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 10:18:56.608: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 10:18:56.617: INFO: Waiting for terminating namespaces to be deleted...
May 13 10:18:56.619: INFO: 
Logging pods the kubelet thinks is on node 172.16.173.202 before test
May 13 10:18:56.645: INFO: icp-mongodb-0 from kube-system started at 2019-05-08 03:27:17 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.645: INFO: 	Container icp-mongodb ready: true, restart count 0
May 13 10:18:56.645: INFO: 	Container metrics ready: true, restart count 0
May 13 10:18:56.646: INFO: service-catalog-apiserver-9dt69 from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container apiserver ready: true, restart count 4
May 13 10:18:56.646: INFO: auth-pap-9bjv2 from kube-system started at 2019-05-08 03:29:58 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container auth-pap ready: true, restart count 2
May 13 10:18:56.646: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.646: INFO: helm-repo-548dbc4c5c-lj4r2 from kube-system started at 2019-05-08 03:32:19 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container helm-repo ready: true, restart count 0
May 13 10:18:56.646: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.646: INFO: platform-api-994c46799-5f4ml from kube-system started at 2019-05-08 03:27:31 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container audit-service ready: true, restart count 1
May 13 10:18:56.646: INFO: 	Container platform-api ready: true, restart count 10
May 13 10:18:56.646: INFO: monitoring-prometheus-nodeexporter-lwpx9 from kube-system started at 2019-05-08 03:33:46 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container nodeexporter ready: true, restart count 1
May 13 10:18:56.646: INFO: 	Container router ready: true, restart count 1
May 13 10:18:56.646: INFO: mgmt-repo-79f95d66b8-lp7sf from kube-system started at 2019-05-08 03:34:03 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.646: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.646: INFO: 	Container mgmt-repo ready: true, restart count 0
May 13 10:18:56.647: INFO: security-onboarding-89nmn from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container security-onboarding ready: false, restart count 0
May 13 10:18:56.647: INFO: image-manager-0 from kube-system started at 2019-05-08 03:24:10 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container icp-registry ready: true, restart count 1
May 13 10:18:56.647: INFO: 	Container image-manager ready: true, restart count 0
May 13 10:18:56.647: INFO: calico-node-krfk6 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container calico-node ready: true, restart count 0
May 13 10:18:56.647: INFO: nvidia-device-plugin-vwsvk from kube-system started at 2019-05-08 03:27:07 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container nvidia-device-plugin ready: true, restart count 1
May 13 10:18:56.647: INFO: ibmcloud-image-enforcement-7c7d9688-tqjm9 from kube-system started at 2019-05-08 03:32:31 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
May 13 10:18:56.647: INFO: iam-onboarding-bptkl from kube-system started at 2019-05-08 03:34:14 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container iam-onboarding ready: false, restart count 0
May 13 10:18:56.647: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fv8lz from heptio-sonobuoy started at 2019-05-13 08:34:01 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 10:18:56.647: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 10:18:56.647: INFO: k8s-master-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 10:18:56.647: INFO: service-catalog-controller-manager-88d749c7d-6xqsk from kube-system started at 2019-05-08 03:27:22 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container controller-manager ready: true, restart count 1
May 13 10:18:56.647: INFO: nginx-ingress-controller-vf97z from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container nginx-ingress ready: true, restart count 1
May 13 10:18:56.647: INFO: metering-reader-46vqp from kube-system started at 2019-05-08 03:32:37 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container metering-reader ready: true, restart count 3
May 13 10:18:56.647: INFO: audit-logging-fluentd-ds-lr8k9 from kube-system started at 2019-05-08 03:34:29 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.647: INFO: 	Container fluentd ready: true, restart count 0
May 13 10:18:56.647: INFO: k8s-kmsplugin-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 10:18:56.648: INFO: k8s-proxy-kxd6g from kube-system started at 2019-05-08 03:23:38 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container proxy ready: true, restart count 0
May 13 10:18:56.648: INFO: cert-manager-ibm-cert-manager-7775495cb4-n84nn from cert-manager started at 2019-05-08 03:23:52 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container ibm-cert-manager ready: true, restart count 8
May 13 10:18:56.648: INFO: kube-dns-28wg9 from kube-system started at 2019-05-08 03:26:55 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container kube-dns ready: true, restart count 1
May 13 10:18:56.648: INFO: oidc-client-registration-htdq6 from kube-system started at 2019-05-08 03:29:48 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container oidc-client-registration ready: false, restart count 0
May 13 10:18:56.648: INFO: secret-watcher-54fd9cfc6d-gw9pw from kube-system started at 2019-05-08 03:34:09 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container secret-watcher ready: true, restart count 1
May 13 10:18:56.648: INFO: image-manager-init-certs-5cjmr from kube-system started at 2019-05-08 03:24:09 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container init-certs ready: true, restart count 1
May 13 10:18:56.648: INFO: auth-idp-cqjpn from kube-system started at 2019-05-08 03:29:48 +0000 UTC (4 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.648: INFO: 	Container platform-auth-service ready: true, restart count 0
May 13 10:18:56.648: INFO: 	Container platform-identity-manager ready: true, restart count 1
May 13 10:18:56.648: INFO: 	Container platform-identity-provider ready: true, restart count 1
May 13 10:18:56.648: INFO: platform-ui-sfdsp from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container platform-ui ready: true, restart count 0
May 13 10:18:56.648: INFO: platform-header-5hnzj from kube-system started at 2019-05-08 03:32:42 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container platform-header ready: true, restart count 1
May 13 10:18:56.648: INFO: helm-api-67456c54d4-rf5rv from kube-system started at 2019-05-08 03:33:58 +0000 UTC (3 container statuses recorded)
May 13 10:18:56.648: INFO: 	Container helmapi ready: true, restart count 0
May 13 10:18:56.648: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.648: INFO: 	Container rudder ready: true, restart count 0
May 13 10:18:56.649: INFO: calico-kube-controllers-7f949c47f-sgggp from kube-system started at 2019-05-08 03:26:42 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 13 10:18:56.649: INFO: default-http-backend-567686995f-psknt from kube-system started at 2019-05-08 03:27:27 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container default-http-backend ready: true, restart count 1
May 13 10:18:56.649: INFO: sonobuoy-e2e-job-9886345f531c45a5 from heptio-sonobuoy started at 2019-05-13 08:33:59 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container e2e ready: true, restart count 0
May 13 10:18:56.649: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 10:18:56.649: INFO: k8s-etcd-172.16.173.202 from kube-system started at <nil> (0 container statuses recorded)
May 13 10:18:56.649: INFO: tiller-deploy-57774fdbd5-t4zqz from kube-system started at 2019-05-08 03:23:25 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container tiller ready: true, restart count 1
May 13 10:18:56.649: INFO: auth-pdp-82gc5 from kube-system started at 2019-05-08 03:30:03 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container auth-pdp ready: true, restart count 0
May 13 10:18:56.649: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.649: INFO: icp-management-ingress-8jgdq from kube-system started at 2019-05-08 03:30:09 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container icp-management-ingress ready: true, restart count 0
May 13 10:18:56.649: INFO: logging-elk-filebeat-ds-tlc56 from kube-system started at 2019-05-08 03:33:38 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.649: INFO: 	Container filebeat ready: true, restart count 1
May 13 10:18:56.649: INFO: 
Logging pods the kubelet thinks is on node 172.16.175.32 before test
May 13 10:18:56.682: INFO: metrics-server-5bf88fc7bc-m9fzp from kube-system started at 2019-05-08 03:27:01 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.682: INFO: 	Container metrics-server ready: true, restart count 25
May 13 10:18:56.683: INFO: key-management-crypto-79d7dc5c95-dxfsh from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container key-management-crypto ready: true, restart count 0
May 13 10:18:56.683: INFO: key-management-lifecycle-5d858b8c4f-5qd8h from kube-system started at 2019-05-08 03:33:47 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container icp-audit-service ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container key-management-lifecycle ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-elasticsearch-curator-1557617400-hlkf5 from kube-system started at 2019-05-11 23:29:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container curator ready: false, restart count 0
May 13 10:18:56.683: INFO: metering-ui-5856c4c88f-8c2sk from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container metering-ui ready: true, restart count 3
May 13 10:18:56.683: INFO: logging-elk-client-694fc7bd59-7s6rk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container es-client ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: monitoring-grafana-9d5f8498-j9hss from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container grafana ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 1
May 13 10:18:56.683: INFO: monitoring-prometheus-kubestatemetrics-5f8846cf48-2qt72 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container kubestatemetrics ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: key-management-pep-6cf498f8-7kfl9 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container key-management-pep ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-elasticsearch-pki-init-96qch from kube-system started at 2019-05-08 03:32:13 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container kubectl ready: false, restart count 0
May 13 10:18:56.683: INFO: logging-elk-logstash-7f64bbdf8f-qbzn6 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container logstash ready: true, restart count 0
May 13 10:18:56.683: INFO: monitoring-prometheus-collectdexporter-86465d9df-xhcb9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container collectd-exporter ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: key-management-api-595d8867f9-v86wj from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container key-management-api ready: true, restart count 0
May 13 10:18:56.683: INFO: image-manager-init-certs-lc5z5 from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container init-certs ready: true, restart count 0
May 13 10:18:56.683: INFO: k8s-proxy-dbfrf from kube-system started at 2019-05-08 03:25:06 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container proxy ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-elasticsearch-tls-init-dndvq from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container searchguard-init ready: false, restart count 1
May 13 10:18:56.683: INFO: monitoring-prometheus-78d4f54b74-nhxvb from kube-system started at 2019-05-08 03:33:10 +0000 UTC (4 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container alert-rule-controller ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container configmap-reload-prometheus ready: true, restart count 16
May 13 10:18:56.683: INFO: 	Container prometheus ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 14
May 13 10:18:56.683: INFO: key-management-onboarding-x7xbz from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container key-management-onboarding ready: false, restart count 0
May 13 10:18:56.683: INFO: logging-elk-elasticsearch-curator-1557703800-7c8c9 from kube-system started at 2019-05-12 23:30:51 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container curator ready: false, restart count 0
May 13 10:18:56.683: INFO: logging-elk-kibana-init-j9nlg from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container init ready: false, restart count 6
May 13 10:18:56.683: INFO: monitoring-prometheus-nodeexporter-f97h9 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: monitoring-prometheus-elasticsearchexporter-799646cdd4-pstw6 from kube-system started at 2019-05-08 03:33:10 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container elasticsearchexporter ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fskhj from heptio-sonobuoy started at 2019-05-13 08:34:47 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 10:18:56.683: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 10:18:56.683: INFO: calico-node-nqqhg from kube-system started at 2019-05-08 03:25:57 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container calico-node ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-data-0 from kube-system started at 2019-05-08 03:33:04 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container es-data ready: true, restart count 0
May 13 10:18:56.683: INFO: audit-logging-fluentd-ds-xmfv9 from kube-system started at 2019-05-08 03:33:53 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container fluentd ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-kibana-558c869fd5-n8hq4 from kube-system started at 2019-05-08 03:33:02 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container kibana ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: monitoring-prometheus-alertmanager-7fdbfd5559-589pt from kube-system started at 2019-05-08 03:33:10 +0000 UTC (3 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container alertmanager ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container configmap-reload ready: true, restart count 0
May 13 10:18:56.683: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-filebeat-ds-z4wqk from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container filebeat ready: true, restart count 0
May 13 10:18:56.683: INFO: logging-elk-master-bf9784b7c-g2l6c from kube-system started at 2019-05-08 03:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container es-master ready: true, restart count 0
May 13 10:18:56.683: INFO: key-management-persistence-5c68555d9d-jlzj6 from kube-system started at 2019-05-08 03:33:47 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container key-management-persistence ready: true, restart count 0
May 13 10:18:56.683: INFO: nvidia-device-plugin-lk7j7 from kube-system started at 2019-05-08 03:26:32 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 10:18:56.683: INFO: metering-dm-767bd75d59-tdvcl from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container metering-dm ready: true, restart count 2
May 13 10:18:56.683: INFO: metering-reader-md2rh from kube-system started at 2019-05-08 03:32:01 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.683: INFO: 	Container metering-reader ready: true, restart count 4
May 13 10:18:56.683: INFO: 
Logging pods the kubelet thinks is on node 172.16.176.226 before test
May 13 10:18:56.701: INFO: calico-node-vqnm2 from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.701: INFO: 	Container calico-node ready: true, restart count 0
May 13 10:18:56.701: INFO: monitoring-prometheus-nodeexporter-997tc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.701: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 10:18:56.701: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.702: INFO: metering-reader-vh24d from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container metering-reader ready: true, restart count 2
May 13 10:18:56.702: INFO: image-manager-init-certs-tl8kt from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container init-certs ready: true, restart count 0
May 13 10:18:56.702: INFO: logging-elk-filebeat-ds-9nhbn from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container filebeat ready: true, restart count 0
May 13 10:18:56.702: INFO: audit-logging-fluentd-ds-m65f6 from kube-system started at 2019-05-08 03:33:17 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container fluentd ready: true, restart count 0
May 13 10:18:56.702: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-68z9l from heptio-sonobuoy started at 2019-05-13 08:33:06 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 10:18:56.702: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 10:18:56.702: INFO: k8s-proxy-44zsd from kube-system started at 2019-05-08 03:24:30 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container proxy ready: true, restart count 0
May 13 10:18:56.702: INFO: nvidia-device-plugin-d945k from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 10:18:56.702: INFO: php-apache-59dfbd948-bcn7g from ivt started at 2019-05-08 06:16:30 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container php-apache ready: true, restart count 0
May 13 10:18:56.702: INFO: web-terminal-5666fd8f98-fmtnz from kube-system started at 2019-05-10 07:45:34 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.702: INFO: 	Container web-terminal ready: true, restart count 0
May 13 10:18:56.702: INFO: 
Logging pods the kubelet thinks is on node 172.16.177.10 before test
May 13 10:18:56.715: INFO: metering-reader-t7zsm from kube-system started at 2019-05-08 03:31:24 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container metering-reader ready: true, restart count 5
May 13 10:18:56.715: INFO: audit-logging-fluentd-ds-hcn7v from kube-system started at 2019-05-10 07:49:28 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container fluentd ready: true, restart count 0
May 13 10:18:56.715: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 08:33:02 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 13 10:18:56.715: INFO: sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-mk9hv from heptio-sonobuoy started at 2019-05-13 08:33:07 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 10:18:56.715: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 10:18:56.715: INFO: calico-node-vv57p from kube-system started at 2019-05-08 03:25:20 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container calico-node ready: true, restart count 0
May 13 10:18:56.715: INFO: monitoring-prometheus-nodeexporter-bbgtc from kube-system started at 2019-05-08 03:32:33 +0000 UTC (2 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container nodeexporter ready: true, restart count 0
May 13 10:18:56.715: INFO: 	Container router ready: true, restart count 0
May 13 10:18:56.715: INFO: image-manager-init-certs-rwqdh from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container init-certs ready: true, restart count 0
May 13 10:18:56.715: INFO: k8s-proxy-dd6wd from kube-system started at 2019-05-08 03:24:29 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container proxy ready: true, restart count 0
May 13 10:18:56.715: INFO: nvidia-device-plugin-5ppzs from kube-system started at 2019-05-08 03:25:55 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.715: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
May 13 10:18:56.715: INFO: logging-elk-filebeat-ds-kp99k from kube-system started at 2019-05-08 03:32:25 +0000 UTC (1 container statuses recorded)
May 13 10:18:56.716: INFO: 	Container filebeat ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node 172.16.173.202
STEP: verifying the node has the label node 172.16.175.32
STEP: verifying the node has the label node 172.16.176.226
STEP: verifying the node has the label node 172.16.177.10
May 13 10:18:56.861: INFO: Pod cert-manager-ibm-cert-manager-7775495cb4-n84nn requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.861: INFO: Pod sonobuoy requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.861: INFO: Pod sonobuoy-e2e-job-9886345f531c45a5 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.861: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-68z9l requesting resource cpu=0m on Node 172.16.176.226
May 13 10:18:56.861: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fskhj requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.861: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-fv8lz requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.861: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b79e2cdd5264a9d-mk9hv requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.861: INFO: Pod php-apache-59dfbd948-bcn7g requesting resource cpu=200m on Node 172.16.176.226
May 13 10:18:56.861: INFO: Pod audit-logging-fluentd-ds-hcn7v requesting resource cpu=300m on Node 172.16.177.10
May 13 10:18:56.861: INFO: Pod audit-logging-fluentd-ds-lr8k9 requesting resource cpu=300m on Node 172.16.173.202
May 13 10:18:56.861: INFO: Pod audit-logging-fluentd-ds-m65f6 requesting resource cpu=300m on Node 172.16.176.226
May 13 10:18:56.861: INFO: Pod audit-logging-fluentd-ds-xmfv9 requesting resource cpu=300m on Node 172.16.175.32
May 13 10:18:56.862: INFO: Pod auth-idp-cqjpn requesting resource cpu=300m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod auth-pap-9bjv2 requesting resource cpu=150m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod auth-pdp-82gc5 requesting resource cpu=600m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod calico-kube-controllers-7f949c47f-sgggp requesting resource cpu=250m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod calico-node-krfk6 requesting resource cpu=250m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod calico-node-nqqhg requesting resource cpu=250m on Node 172.16.175.32
May 13 10:18:56.862: INFO: Pod calico-node-vqnm2 requesting resource cpu=250m on Node 172.16.176.226
May 13 10:18:56.862: INFO: Pod calico-node-vv57p requesting resource cpu=250m on Node 172.16.177.10
May 13 10:18:56.862: INFO: Pod default-http-backend-567686995f-psknt requesting resource cpu=20m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod helm-api-67456c54d4-rf5rv requesting resource cpu=350m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod helm-repo-548dbc4c5c-lj4r2 requesting resource cpu=150m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod ibmcloud-image-enforcement-7c7d9688-tqjm9 requesting resource cpu=128m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod icp-management-ingress-8jgdq requesting resource cpu=200m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod icp-mongodb-0 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod image-manager-0 requesting resource cpu=110m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod image-manager-init-certs-5cjmr requesting resource cpu=10m on Node 172.16.173.202
May 13 10:18:56.862: INFO: Pod image-manager-init-certs-lc5z5 requesting resource cpu=10m on Node 172.16.175.32
May 13 10:18:56.862: INFO: Pod image-manager-init-certs-rwqdh requesting resource cpu=10m on Node 172.16.177.10
May 13 10:18:56.862: INFO: Pod image-manager-init-certs-tl8kt requesting resource cpu=10m on Node 172.16.176.226
May 13 10:18:56.863: INFO: Pod k8s-etcd-172.16.173.202 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod k8s-kmsplugin-172.16.173.202 requesting resource cpu=5m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod k8s-master-172.16.173.202 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod k8s-proxy-44zsd requesting resource cpu=0m on Node 172.16.176.226
May 13 10:18:56.863: INFO: Pod k8s-proxy-dbfrf requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod k8s-proxy-dd6wd requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.863: INFO: Pod k8s-proxy-kxd6g requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod key-management-api-595d8867f9-v86wj requesting resource cpu=400m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod key-management-crypto-79d7dc5c95-dxfsh requesting resource cpu=300m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod key-management-lifecycle-5d858b8c4f-5qd8h requesting resource cpu=300m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod key-management-pep-6cf498f8-7kfl9 requesting resource cpu=300m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod key-management-persistence-5c68555d9d-jlzj6 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod kube-dns-28wg9 requesting resource cpu=100m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod logging-elk-client-694fc7bd59-7s6rk requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod logging-elk-data-0 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod logging-elk-filebeat-ds-9nhbn requesting resource cpu=0m on Node 172.16.176.226
May 13 10:18:56.863: INFO: Pod logging-elk-filebeat-ds-kp99k requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.863: INFO: Pod logging-elk-filebeat-ds-tlc56 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod logging-elk-filebeat-ds-z4wqk requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod logging-elk-kibana-558c869fd5-n8hq4 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod logging-elk-logstash-7f64bbdf8f-qbzn6 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod logging-elk-master-bf9784b7c-g2l6c requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod metering-dm-767bd75d59-tdvcl requesting resource cpu=250m on Node 172.16.175.32
May 13 10:18:56.863: INFO: Pod metering-reader-46vqp requesting resource cpu=250m on Node 172.16.173.202
May 13 10:18:56.863: INFO: Pod metering-reader-md2rh requesting resource cpu=250m on Node 172.16.175.32
May 13 10:18:56.864: INFO: Pod metering-reader-t7zsm requesting resource cpu=250m on Node 172.16.177.10
May 13 10:18:56.864: INFO: Pod metering-reader-vh24d requesting resource cpu=250m on Node 172.16.176.226
May 13 10:18:56.864: INFO: Pod metering-ui-5856c4c88f-8c2sk requesting resource cpu=250m on Node 172.16.175.32
May 13 10:18:56.864: INFO: Pod metrics-server-5bf88fc7bc-m9fzp requesting resource cpu=20m on Node 172.16.175.32
May 13 10:18:56.864: INFO: Pod mgmt-repo-79f95d66b8-lp7sf requesting resource cpu=150m on Node 172.16.173.202
May 13 10:18:56.864: INFO: Pod monitoring-grafana-9d5f8498-j9hss requesting resource cpu=100m on Node 172.16.175.32
May 13 10:18:56.864: INFO: Pod monitoring-prometheus-78d4f54b74-nhxvb requesting resource cpu=100m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-alertmanager-7fdbfd5559-589pt requesting resource cpu=10m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-collectdexporter-86465d9df-xhcb9 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-elasticsearchexporter-799646cdd4-pstw6 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-kubestatemetrics-5f8846cf48-2qt72 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-nodeexporter-997tc requesting resource cpu=0m on Node 172.16.176.226
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-nodeexporter-bbgtc requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-nodeexporter-f97h9 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod monitoring-prometheus-nodeexporter-lwpx9 requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod nginx-ingress-controller-vf97z requesting resource cpu=50m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod nvidia-device-plugin-5ppzs requesting resource cpu=0m on Node 172.16.177.10
May 13 10:18:56.865: INFO: Pod nvidia-device-plugin-d945k requesting resource cpu=0m on Node 172.16.176.226
May 13 10:18:56.865: INFO: Pod nvidia-device-plugin-lk7j7 requesting resource cpu=0m on Node 172.16.175.32
May 13 10:18:56.865: INFO: Pod nvidia-device-plugin-vwsvk requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod platform-api-994c46799-5f4ml requesting resource cpu=50m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod platform-header-5hnzj requesting resource cpu=300m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod platform-ui-sfdsp requesting resource cpu=300m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod secret-watcher-54fd9cfc6d-gw9pw requesting resource cpu=0m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod service-catalog-apiserver-9dt69 requesting resource cpu=100m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod service-catalog-controller-manager-88d749c7d-6xqsk requesting resource cpu=100m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod tiller-deploy-57774fdbd5-t4zqz requesting resource cpu=100m on Node 172.16.173.202
May 13 10:18:56.865: INFO: Pod web-terminal-5666fd8f98-fmtnz requesting resource cpu=10m on Node 172.16.176.226
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84541091-7568-11e9-bbcc-d288ccfb79a4.159e36c41a0d9115], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vg7hj/filler-pod-84541091-7568-11e9-bbcc-d288ccfb79a4 to 172.16.173.202]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84541091-7568-11e9-bbcc-d288ccfb79a4.159e36c462757ccf], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84541091-7568-11e9-bbcc-d288ccfb79a4.159e36c46a21d9c9], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84541091-7568-11e9-bbcc-d288ccfb79a4.159e36c47fa9a1a2], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8466fd28-7568-11e9-bbcc-d288ccfb79a4.159e36c41c314760], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vg7hj/filler-pod-8466fd28-7568-11e9-bbcc-d288ccfb79a4 to 172.16.175.32]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8466fd28-7568-11e9-bbcc-d288ccfb79a4.159e36cf1432c07d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8466fd28-7568-11e9-bbcc-d288ccfb79a4.159e36cf1dbb76ee], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8466fd28-7568-11e9-bbcc-d288ccfb79a4.159e36cf2e5d9198], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-846c313e-7568-11e9-bbcc-d288ccfb79a4.159e36b6feb012ba], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-846c313e-7568-11e9-bbcc-d288ccfb79a4.159e36b70547ed3c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-846c313e-7568-11e9-bbcc-d288ccfb79a4.159e36b716f7ba73], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-846c313e-7568-11e9-bbcc-d288ccfb79a4.159e36c42ca483f2], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vg7hj/filler-pod-846c313e-7568-11e9-bbcc-d288ccfb79a4 to 172.16.176.226]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8495b02b-7568-11e9-bbcc-d288ccfb79a4.159e36b73d67e7b5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8495b02b-7568-11e9-bbcc-d288ccfb79a4.159e36b74518da70], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8495b02b-7568-11e9-bbcc-d288ccfb79a4.159e36b7597d2b20], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8495b02b-7568-11e9-bbcc-d288ccfb79a4.159e36c461f84f2d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-vg7hj/filler-pod-8495b02b-7568-11e9-bbcc-d288ccfb79a4 to 172.16.177.10]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159e36c4de4de753], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node 172.16.173.202
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 172.16.175.32
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 172.16.176.226
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 172.16.177.10
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:01.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vg7hj" for this suite.
May 13 10:19:07.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:07.552: INFO: namespace: e2e-tests-sched-pred-vg7hj, resource: bindings, ignored listing per whitelist
May 13 10:19:07.568: INFO: namespace e2e-tests-sched-pred-vg7hj deletion completed in 6.149301211s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.166 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:07.569: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-7mdgq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 13 10:19:10.380: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4"
May 13 10:19:10.380: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-pods-7mdgq" to be "terminated due to deadline exceeded"
May 13 10:19:10.383: INFO: Pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4": Phase="Running", Reason="", readiness=true. Elapsed: 3.525701ms
May 13 10:19:12.388: INFO: Pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00773588s
May 13 10:19:14.392: INFO: Pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012367335s
May 13 10:19:14.392: INFO: Pod "pod-update-activedeadlineseconds-8ad5c06b-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:14.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7mdgq" for this suite.
May 13 10:19:20.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:20.533: INFO: namespace: e2e-tests-pods-7mdgq, resource: bindings, ignored listing per whitelist
May 13 10:19:20.690: INFO: namespace e2e-tests-pods-7mdgq deletion completed in 6.293346669s

• [SLOW TEST:13.122 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-v9h47
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:19:20.911: INFO: (0) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.764612ms)
May 13 10:19:20.919: INFO: (1) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.320753ms)
May 13 10:19:20.927: INFO: (2) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.9129ms)
May 13 10:19:20.933: INFO: (3) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.641091ms)
May 13 10:19:20.941: INFO: (4) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.830926ms)
May 13 10:19:20.954: INFO: (5) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 12.564672ms)
May 13 10:19:20.962: INFO: (6) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.513863ms)
May 13 10:19:20.969: INFO: (7) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.780596ms)
May 13 10:19:20.976: INFO: (8) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.066512ms)
May 13 10:19:20.982: INFO: (9) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.052534ms)
May 13 10:19:20.989: INFO: (10) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.792483ms)
May 13 10:19:20.994: INFO: (11) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.107519ms)
May 13 10:19:21.000: INFO: (12) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 6.323636ms)
May 13 10:19:21.008: INFO: (13) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 7.536649ms)
May 13 10:19:21.013: INFO: (14) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 4.735198ms)
May 13 10:19:21.019: INFO: (15) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 5.954573ms)
May 13 10:19:21.042: INFO: (16) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 23.479722ms)
May 13 10:19:21.057: INFO: (17) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 14.39632ms)
May 13 10:19:21.069: INFO: (18) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 11.941091ms)
May 13 10:19:21.105: INFO: (19) /api/v1/nodes/172.16.173.202:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="alternat... (200; 36.225325ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:21.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-v9h47" for this suite.
May 13 10:19:27.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:27.290: INFO: namespace: e2e-tests-proxy-v9h47, resource: bindings, ignored listing per whitelist
May 13 10:19:27.380: INFO: namespace e2e-tests-proxy-v9h47 deletion completed in 6.26711695s

• [SLOW TEST:6.689 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:27.380: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-82qxv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-96a5f3c8-7568-11e9-bbcc-d288ccfb79a4
STEP: Creating secret with name secret-projected-all-test-volume-96a5f3a4-7568-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test Check all projections for projected volume plugin
May 13 10:19:27.684: INFO: Waiting up to 5m0s for pod "projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-82qxv" to be "success or failure"
May 13 10:19:27.689: INFO: Pod "projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.577415ms
May 13 10:19:29.694: INFO: Pod "projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01030569s
STEP: Saw pod success
May 13 10:19:29.694: INFO: Pod "projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:19:29.698: INFO: Trying to get logs from node 172.16.176.226 pod projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4 container projected-all-volume-test: <nil>
STEP: delete the pod
May 13 10:19:29.722: INFO: Waiting for pod projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:19:29.725: INFO: Pod projected-volume-96a5f31c-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:29.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-82qxv" for this suite.
May 13 10:19:35.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:35.976: INFO: namespace: e2e-tests-projected-82qxv, resource: bindings, ignored listing per whitelist
May 13 10:19:36.022: INFO: namespace e2e-tests-projected-82qxv deletion completed in 6.291406588s

• [SLOW TEST:8.642 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:36.023: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8d7qz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9bcd415c-7568-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:19:36.289: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-8d7qz" to be "success or failure"
May 13 10:19:36.296: INFO: Pod "pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457919ms
May 13 10:19:38.300: INFO: Pod "pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010367838s
STEP: Saw pod success
May 13 10:19:38.300: INFO: Pod "pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:19:38.303: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 10:19:38.327: INFO: Waiting for pod pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:19:38.330: INFO: Pod pod-projected-configmaps-9bcdcc1a-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:38.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8d7qz" for this suite.
May 13 10:19:44.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:44.502: INFO: namespace: e2e-tests-projected-8d7qz, resource: bindings, ignored listing per whitelist
May 13 10:19:44.504: INFO: namespace e2e-tests-projected-8d7qz deletion completed in 6.166152861s

• [SLOW TEST:8.482 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:44.505: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-k2dvl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:19:44.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-downward-api-k2dvl" to be "success or failure"
May 13 10:19:44.774: INFO: Pod "downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41324ms
May 13 10:19:46.782: INFO: Pod "downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010248741s
STEP: Saw pod success
May 13 10:19:46.782: INFO: Pod "downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:19:46.786: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:19:46.815: INFO: Waiting for pod downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:19:46.817: INFO: Pod downwardapi-volume-a0da81de-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:46.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-k2dvl" for this suite.
May 13 10:19:52.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:19:52.987: INFO: namespace: e2e-tests-downward-api-k2dvl, resource: bindings, ignored listing per whitelist
May 13 10:19:53.135: INFO: namespace e2e-tests-downward-api-k2dvl deletion completed in 6.314461092s

• [SLOW TEST:8.631 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:19:53.137: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-966km
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 13 10:19:53.482: INFO: Waiting up to 5m0s for pod "pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-966km" to be "success or failure"
May 13 10:19:53.488: INFO: Pod "pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66074ms
May 13 10:19:55.490: INFO: Pod "pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008247997s
May 13 10:19:57.496: INFO: Pod "pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014323816s
STEP: Saw pod success
May 13 10:19:57.496: INFO: Pod "pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:19:57.499: INFO: Trying to get logs from node 172.16.176.226 pod pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:19:57.517: INFO: Waiting for pod pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:19:57.519: INFO: Pod pod-a604eb15-7568-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:19:57.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-966km" for this suite.
May 13 10:20:03.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:20:03.884: INFO: namespace: e2e-tests-emptydir-966km, resource: bindings, ignored listing per whitelist
May 13 10:20:03.957: INFO: namespace e2e-tests-emptydir-966km deletion completed in 6.433998447s

• [SLOW TEST:10.820 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:20:03.957: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kwd56
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-kwd56
May 13 10:20:08.304: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-kwd56
STEP: checking the pod's current state and verifying that restartCount is present
May 13 10:20:08.307: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:24:09.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kwd56" for this suite.
May 13 10:24:17.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:24:17.238: INFO: namespace: e2e-tests-container-probe-kwd56, resource: bindings, ignored listing per whitelist
May 13 10:24:17.377: INFO: namespace e2e-tests-container-probe-kwd56 deletion completed in 8.306030517s

• [SLOW TEST:253.419 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:24:17.378: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6b9tq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-438b1acc-7569-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 10:24:17.795: INFO: Waiting up to 5m0s for pod "pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-secrets-6b9tq" to be "success or failure"
May 13 10:24:17.797: INFO: Pod "pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680583ms
May 13 10:24:19.811: INFO: Pod "pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016441347s
STEP: Saw pod success
May 13 10:24:19.811: INFO: Pod "pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:24:19.816: INFO: Trying to get logs from node 172.16.176.226 pod pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4 container secret-volume-test: <nil>
STEP: delete the pod
May 13 10:24:19.845: INFO: Waiting for pod pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:24:19.848: INFO: Pod pod-secrets-438c26d6-7569-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:24:19.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6b9tq" for this suite.
May 13 10:24:25.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:24:26.056: INFO: namespace: e2e-tests-secrets-6b9tq, resource: bindings, ignored listing per whitelist
May 13 10:24:26.195: INFO: namespace e2e-tests-secrets-6b9tq deletion completed in 6.342454288s

• [SLOW TEST:8.818 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:24:26.196: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dw2pl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
May 13 10:24:30.494: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-48c0d1c6-7569-11e9-bbcc-d288ccfb79a4", GenerateName:"", Namespace:"e2e-tests-pods-dw2pl", SelfLink:"/api/v1/namespaces/e2e-tests-pods-dw2pl/pods/pod-submit-remove-48c0d1c6-7569-11e9-bbcc-d288ccfb79a4", UID:"48c43830-7569-11e9-b9b7-00163e01adca", ResourceVersion:"869037", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693339866, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"412290584"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-v6zpw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0016f9440), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v6zpw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000a5b0d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"172.16.177.10", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b4d1a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000a5b120)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000a5b910)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000a5b918), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000a5b91c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339808, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339810, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339810, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693339866, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.177.10", PodIP:"10.1.22.253", StartTime:(*v1.Time)(0xc001d9fda0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001d9fdc0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7", ContainerID:"docker://32dd1ba21ec7b5207ef365e5c36f07c6524531ebd2b1679e2ca139bd59b81b55"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:24:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dw2pl" for this suite.
May 13 10:24:48.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:24:49.041: INFO: namespace: e2e-tests-pods-dw2pl, resource: bindings, ignored listing per whitelist
May 13 10:24:49.077: INFO: namespace e2e-tests-pods-dw2pl deletion completed in 6.188248228s

• [SLOW TEST:22.881 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:24:49.078: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gtd82
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 10:24:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-gtd82'
May 13 10:24:49.897: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 10:24:49.897: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 13 10:24:49.912: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-mh7r4]
May 13 10:24:49.912: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-mh7r4" in namespace "e2e-tests-kubectl-gtd82" to be "running and ready"
May 13 10:24:49.916: INFO: Pod "e2e-test-nginx-rc-mh7r4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696719ms
May 13 10:24:51.920: INFO: Pod "e2e-test-nginx-rc-mh7r4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007763541s
May 13 10:24:51.920: INFO: Pod "e2e-test-nginx-rc-mh7r4" satisfied condition "running and ready"
May 13 10:24:51.920: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-mh7r4]
May 13 10:24:51.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gtd82'
May 13 10:24:52.133: INFO: stderr: ""
May 13 10:24:52.133: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
May 13 10:24:52.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gtd82'
May 13 10:24:52.277: INFO: stderr: ""
May 13 10:24:52.277: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:24:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gtd82" for this suite.
May 13 10:25:16.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:25:16.348: INFO: namespace: e2e-tests-kubectl-gtd82, resource: bindings, ignored listing per whitelist
May 13 10:25:16.612: INFO: namespace e2e-tests-kubectl-gtd82 deletion completed in 24.330007531s

• [SLOW TEST:27.535 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:25:16.614: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pwpk5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:25:16.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-870666073 version'
May 13 10:25:16.982: INFO: stderr: ""
May 13 10:25:16.982: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.5+icp-ee\", GitCommit:\"eb4df6c6fb47f5b4fd1ed8bfbfe2d0ed5ea636e1\", GitTreeState:\"clean\", BuildDate:\"2019-05-08T02:18:32Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:25:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pwpk5" for this suite.
May 13 10:25:23.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:25:23.049: INFO: namespace: e2e-tests-kubectl-pwpk5, resource: bindings, ignored listing per whitelist
May 13 10:25:23.255: INFO: namespace e2e-tests-kubectl-pwpk5 deletion completed in 6.26363355s

• [SLOW TEST:6.641 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:25:23.256: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t8dsn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-6acbabe5-7569-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume secrets
May 13 10:25:23.559: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-t8dsn" to be "success or failure"
May 13 10:25:23.569: INFO: Pod "pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.612305ms
May 13 10:25:25.575: INFO: Pod "pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016048101s
May 13 10:25:27.579: INFO: Pod "pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019896918s
STEP: Saw pod success
May 13 10:25:27.579: INFO: Pod "pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:25:27.581: INFO: Trying to get logs from node 172.16.177.10 pod pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 10:25:27.630: INFO: Waiting for pod pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:25:27.633: INFO: Pod pod-projected-secrets-6accf7df-7569-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:25:27.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t8dsn" for this suite.
May 13 10:25:33.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:25:33.927: INFO: namespace: e2e-tests-projected-t8dsn, resource: bindings, ignored listing per whitelist
May 13 10:25:34.066: INFO: namespace e2e-tests-projected-t8dsn deletion completed in 6.427746263s

• [SLOW TEST:10.810 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:25:34.067: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-64ksz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-7134e0ba-7569-11e9-bbcc-d288ccfb79a4
STEP: Creating a pod to test consume configMaps
May 13 10:25:34.375: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-64ksz" to be "success or failure"
May 13 10:25:34.390: INFO: Pod "pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.945912ms
May 13 10:25:36.394: INFO: Pod "pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018929151s
STEP: Saw pod success
May 13 10:25:36.394: INFO: Pod "pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:25:36.396: INFO: Trying to get logs from node 172.16.176.226 pod pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 10:25:36.436: INFO: Waiting for pod pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:25:36.441: INFO: Pod pod-projected-configmaps-7135787b-7569-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:25:36.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-64ksz" for this suite.
May 13 10:25:42.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:25:42.591: INFO: namespace: e2e-tests-projected-64ksz, resource: bindings, ignored listing per whitelist
May 13 10:25:42.656: INFO: namespace e2e-tests-projected-64ksz deletion completed in 6.20963368s

• [SLOW TEST:8.589 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:25:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-x45tg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 13 10:25:42.991: INFO: Waiting up to 5m0s for pod "pod-76567e90-7569-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-emptydir-x45tg" to be "success or failure"
May 13 10:25:42.996: INFO: Pod "pod-76567e90-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970458ms
May 13 10:25:45.000: INFO: Pod "pod-76567e90-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008614949s
May 13 10:25:47.005: INFO: Pod "pod-76567e90-7569-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013408459s
STEP: Saw pod success
May 13 10:25:47.005: INFO: Pod "pod-76567e90-7569-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:25:47.007: INFO: Trying to get logs from node 172.16.177.10 pod pod-76567e90-7569-11e9-bbcc-d288ccfb79a4 container test-container: <nil>
STEP: delete the pod
May 13 10:25:47.037: INFO: Waiting for pod pod-76567e90-7569-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:25:47.043: INFO: Pod pod-76567e90-7569-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:25:47.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-x45tg" for this suite.
May 13 10:25:53.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:25:53.292: INFO: namespace: e2e-tests-emptydir-x45tg, resource: bindings, ignored listing per whitelist
May 13 10:25:53.401: INFO: namespace e2e-tests-emptydir-x45tg deletion completed in 6.351975139s

• [SLOW TEST:10.745 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:25:53.402: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fbhzc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 13 10:25:53.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4" in namespace "e2e-tests-projected-fbhzc" to be "success or failure"
May 13 10:25:53.703: INFO: Pod "downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.615721ms
May 13 10:25:55.708: INFO: Pod "downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01255785s
May 13 10:25:57.712: INFO: Pod "downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016336789s
STEP: Saw pod success
May 13 10:25:57.712: INFO: Pod "downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4" satisfied condition "success or failure"
May 13 10:25:57.715: INFO: Trying to get logs from node 172.16.177.10 pod downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4 container client-container: <nil>
STEP: delete the pod
May 13 10:25:57.739: INFO: Waiting for pod downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4 to disappear
May 13 10:25:57.748: INFO: Pod downwardapi-volume-7cbcc030-7569-11e9-bbcc-d288ccfb79a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:25:57.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fbhzc" for this suite.
May 13 10:26:03.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:26:03.870: INFO: namespace: e2e-tests-projected-fbhzc, resource: bindings, ignored listing per whitelist
May 13 10:26:04.139: INFO: namespace e2e-tests-projected-fbhzc deletion completed in 6.386068062s

• [SLOW TEST:10.737 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
May 13 10:26:04.139: INFO: >>> kubeConfig: /tmp/kubeconfig-870666073
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-5h69m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 13 10:26:04.443: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 13 10:26:04.509: INFO: Number of nodes with available pods: 0
May 13 10:26:04.509: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 13 10:26:04.529: INFO: Number of nodes with available pods: 0
May 13 10:26:04.529: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:05.533: INFO: Number of nodes with available pods: 0
May 13 10:26:05.533: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:06.544: INFO: Number of nodes with available pods: 0
May 13 10:26:06.544: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:07.533: INFO: Number of nodes with available pods: 1
May 13 10:26:07.533: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 13 10:26:07.553: INFO: Number of nodes with available pods: 1
May 13 10:26:07.554: INFO: Number of running nodes: 0, number of available pods: 1
May 13 10:26:08.559: INFO: Number of nodes with available pods: 0
May 13 10:26:08.559: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 13 10:26:08.639: INFO: Number of nodes with available pods: 0
May 13 10:26:08.639: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:09.646: INFO: Number of nodes with available pods: 0
May 13 10:26:09.646: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:10.648: INFO: Number of nodes with available pods: 0
May 13 10:26:10.648: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:11.645: INFO: Number of nodes with available pods: 0
May 13 10:26:11.645: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:12.643: INFO: Number of nodes with available pods: 0
May 13 10:26:12.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:13.645: INFO: Number of nodes with available pods: 0
May 13 10:26:13.645: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:14.656: INFO: Number of nodes with available pods: 0
May 13 10:26:14.656: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:15.645: INFO: Number of nodes with available pods: 0
May 13 10:26:15.645: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:16.653: INFO: Number of nodes with available pods: 0
May 13 10:26:16.653: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:17.643: INFO: Number of nodes with available pods: 0
May 13 10:26:17.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:18.644: INFO: Number of nodes with available pods: 0
May 13 10:26:18.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:19.643: INFO: Number of nodes with available pods: 0
May 13 10:26:19.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:20.649: INFO: Number of nodes with available pods: 0
May 13 10:26:20.649: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:21.664: INFO: Number of nodes with available pods: 0
May 13 10:26:21.664: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:22.645: INFO: Number of nodes with available pods: 0
May 13 10:26:22.645: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:23.643: INFO: Number of nodes with available pods: 0
May 13 10:26:23.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:24.642: INFO: Number of nodes with available pods: 0
May 13 10:26:24.642: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:25.649: INFO: Number of nodes with available pods: 0
May 13 10:26:25.649: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:26.643: INFO: Number of nodes with available pods: 0
May 13 10:26:26.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:27.643: INFO: Number of nodes with available pods: 0
May 13 10:26:27.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:28.643: INFO: Number of nodes with available pods: 0
May 13 10:26:28.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:29.644: INFO: Number of nodes with available pods: 0
May 13 10:26:29.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:30.644: INFO: Number of nodes with available pods: 0
May 13 10:26:30.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:31.644: INFO: Number of nodes with available pods: 0
May 13 10:26:31.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:32.643: INFO: Number of nodes with available pods: 0
May 13 10:26:32.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:33.644: INFO: Number of nodes with available pods: 0
May 13 10:26:33.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:34.664: INFO: Number of nodes with available pods: 0
May 13 10:26:34.665: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:35.649: INFO: Number of nodes with available pods: 0
May 13 10:26:35.649: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:36.644: INFO: Number of nodes with available pods: 0
May 13 10:26:36.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:37.644: INFO: Number of nodes with available pods: 0
May 13 10:26:37.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:38.646: INFO: Number of nodes with available pods: 0
May 13 10:26:38.646: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:39.644: INFO: Number of nodes with available pods: 0
May 13 10:26:39.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:40.655: INFO: Number of nodes with available pods: 0
May 13 10:26:40.655: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:41.648: INFO: Number of nodes with available pods: 0
May 13 10:26:41.648: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:42.643: INFO: Number of nodes with available pods: 0
May 13 10:26:42.643: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:43.643: INFO: Number of nodes with available pods: 0
May 13 10:26:43.644: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:44.645: INFO: Number of nodes with available pods: 0
May 13 10:26:44.645: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:45.647: INFO: Number of nodes with available pods: 0
May 13 10:26:45.647: INFO: Node 172.16.173.202 is running more than one daemon pod
May 13 10:26:46.643: INFO: Number of nodes with available pods: 1
May 13 10:26:46.643: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-5h69m, will wait for the garbage collector to delete the pods
May 13 10:26:46.719: INFO: Deleting DaemonSet.extensions daemon-set took: 5.96583ms
May 13 10:26:46.820: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.577813ms
May 13 10:27:18.927: INFO: Number of nodes with available pods: 0
May 13 10:27:18.927: INFO: Number of running nodes: 0, number of available pods: 0
May 13 10:27:18.933: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-5h69m/daemonsets","resourceVersion":"869684"},"items":null}

May 13 10:27:18.940: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-5h69m/pods","resourceVersion":"869685"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
May 13 10:27:19.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-5h69m" for this suite.
May 13 10:27:25.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 10:27:25.187: INFO: namespace: e2e-tests-daemonsets-5h69m, resource: bindings, ignored listing per whitelist
May 13 10:27:25.250: INFO: namespace e2e-tests-daemonsets-5h69m deletion completed in 6.240965236s

• [SLOW TEST:81.111 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSMay 13 10:27:25.251: INFO: Running AfterSuite actions on all nodes
May 13 10:27:25.251: INFO: Running AfterSuite actions on node 1
May 13 10:27:25.251: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6764.983 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h52m46.336717169s
Test Suite Passed
