I0304 23:03:47.937857      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-061951449
I0304 23:03:47.938041      15 e2e.go:224] Starting e2e run "c430e77d-3ed1-11e9-8a62-3ec24305971a" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551740627 - Will randomize all specs
Will run 201 of 1946 specs

Mar  4 23:03:48.108: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:03:48.118: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  4 23:03:48.202: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  4 23:03:48.279: INFO: 23 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  4 23:03:48.279: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Mar  4 23:03:48.279: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  4 23:03:48.318: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-kube-fluentd' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ibm-master-proxy' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Mar  4 23:03:48.318: INFO: e2e test version: v1.13.0
Mar  4 23:03:48.324: INFO: kube-apiserver version: v1.13.4+IKS
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:03:48.324: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
Mar  4 23:03:48.596: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  4 23:03:48.688: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-l928j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0304 23:03:58.876226      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  4 23:03:58.876: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:03:58.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-l928j" for this suite.
Mar  4 23:04:06.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:04:07.112: INFO: namespace: e2e-tests-gc-l928j, resource: bindings, ignored listing per whitelist
Mar  4 23:04:07.266: INFO: namespace e2e-tests-gc-l928j deletion completed in 8.382011489s

• [SLOW TEST:18.942 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:04:07.267: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-b4xvs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0304 23:04:47.630522      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  4 23:04:47.630: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:04:47.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-b4xvs" for this suite.
Mar  4 23:04:55.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:04:56.131: INFO: namespace: e2e-tests-gc-b4xvs, resource: bindings, ignored listing per whitelist
Mar  4 23:04:56.411: INFO: namespace e2e-tests-gc-b4xvs deletion completed in 8.702922206s

• [SLOW TEST:49.145 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:04:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2khsg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  4 23:04:56.711: INFO: namespace e2e-tests-kubectl-2khsg
Mar  4 23:04:56.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-2khsg'
Mar  4 23:04:57.242: INFO: stderr: ""
Mar  4 23:04:57.242: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  4 23:04:58.251: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:04:58.251: INFO: Found 0 / 1
Mar  4 23:04:59.250: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:04:59.250: INFO: Found 0 / 1
Mar  4 23:05:00.251: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:05:00.251: INFO: Found 1 / 1
Mar  4 23:05:00.251: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  4 23:05:00.308: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:05:00.308: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 23:05:00.308: INFO: wait on redis-master startup in e2e-tests-kubectl-2khsg 
Mar  4 23:05:00.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 logs redis-master-jqbh5 redis-master --namespace=e2e-tests-kubectl-2khsg'
Mar  4 23:05:00.453: INFO: stderr: ""
Mar  4 23:05:00.453: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Mar 23:04:59.567 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Mar 23:04:59.567 # Server started, Redis version 3.2.12\n1:M 04 Mar 23:04:59.567 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Mar 23:04:59.567 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  4 23:05:00.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-2khsg'
Mar  4 23:05:00.606: INFO: stderr: ""
Mar  4 23:05:00.606: INFO: stdout: "service/rm2 exposed\n"
Mar  4 23:05:00.700: INFO: Service rm2 in namespace e2e-tests-kubectl-2khsg found.
STEP: exposing service
Mar  4 23:05:02.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-2khsg'
Mar  4 23:05:02.867: INFO: stderr: ""
Mar  4 23:05:02.867: INFO: stdout: "service/rm3 exposed\n"
Mar  4 23:05:02.875: INFO: Service rm3 in namespace e2e-tests-kubectl-2khsg found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:05:04.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2khsg" for this suite.
Mar  4 23:05:28.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:05:29.008: INFO: namespace: e2e-tests-kubectl-2khsg, resource: bindings, ignored listing per whitelist
Mar  4 23:05:29.260: INFO: namespace e2e-tests-kubectl-2khsg deletion completed in 24.357273628s

• [SLOW TEST:32.848 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:05:29.260: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-htsn7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-012e99af-3ed2-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:05:29.563: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-htsn7" to be "success or failure"
Mar  4 23:05:29.607: INFO: Pod "pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.107364ms
Mar  4 23:05:31.617: INFO: Pod "pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0545075s
Mar  4 23:05:33.628: INFO: Pod "pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065227559s
STEP: Saw pod success
Mar  4 23:05:33.628: INFO: Pod "pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:05:33.639: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:05:33.702: INFO: Waiting for pod pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a to disappear
Mar  4 23:05:33.712: INFO: Pod pod-projected-secrets-012fc805-3ed2-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:05:33.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-htsn7" for this suite.
Mar  4 23:05:39.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:05:39.826: INFO: namespace: e2e-tests-projected-htsn7, resource: bindings, ignored listing per whitelist
Mar  4 23:05:40.133: INFO: namespace e2e-tests-projected-htsn7 deletion completed in 6.409147939s

• [SLOW TEST:10.873 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:05:40.133: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cnxd7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-07ba30b5-3ed2-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:05:40.547: INFO: Waiting up to 5m0s for pod "pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-cnxd7" to be "success or failure"
Mar  4 23:05:40.556: INFO: Pod "pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.165537ms
Mar  4 23:05:42.564: INFO: Pod "pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016863373s
Mar  4 23:05:44.574: INFO: Pod "pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02617611s
STEP: Saw pod success
Mar  4 23:05:44.574: INFO: Pod "pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:05:44.583: INFO: Trying to get logs from node 10.190.208.159 pod pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:05:44.665: INFO: Waiting for pod pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a to disappear
Mar  4 23:05:44.673: INFO: Pod pod-configmaps-07bbcd77-3ed2-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:05:44.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cnxd7" for this suite.
Mar  4 23:05:52.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:05:52.914: INFO: namespace: e2e-tests-configmap-cnxd7, resource: bindings, ignored listing per whitelist
Mar  4 23:05:53.078: INFO: namespace e2e-tests-configmap-cnxd7 deletion completed in 8.393041765s

• [SLOW TEST:12.945 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:05:53.078: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-r5njv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar  4 23:05:55.452: INFO: Pod pod-hostip-0f625022-3ed2-11e9-8a62-3ec24305971a has hostIP: 10.190.208.161
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:05:55.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-r5njv" for this suite.
Mar  4 23:06:19.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:06:19.664: INFO: namespace: e2e-tests-pods-r5njv, resource: bindings, ignored listing per whitelist
Mar  4 23:06:19.951: INFO: namespace e2e-tests-pods-r5njv deletion completed in 24.440050119s

• [SLOW TEST:26.872 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:06:19.951: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wcvt6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  4 23:06:20.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-wcvt6'
Mar  4 23:06:20.401: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 23:06:20.401: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  4 23:06:22.427: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-br279]
Mar  4 23:06:22.427: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-br279" in namespace "e2e-tests-kubectl-wcvt6" to be "running and ready"
Mar  4 23:06:22.435: INFO: Pod "e2e-test-nginx-rc-br279": Phase="Running", Reason="", readiness=true. Elapsed: 7.737988ms
Mar  4 23:06:22.435: INFO: Pod "e2e-test-nginx-rc-br279" satisfied condition "running and ready"
Mar  4 23:06:22.435: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-br279]
Mar  4 23:06:22.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wcvt6'
Mar  4 23:06:22.634: INFO: stderr: ""
Mar  4 23:06:22.634: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Mar  4 23:06:22.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wcvt6'
Mar  4 23:06:22.768: INFO: stderr: ""
Mar  4 23:06:22.768: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:06:22.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wcvt6" for this suite.
Mar  4 23:06:28.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:06:29.167: INFO: namespace: e2e-tests-kubectl-wcvt6, resource: bindings, ignored listing per whitelist
Mar  4 23:06:29.186: INFO: namespace e2e-tests-kubectl-wcvt6 deletion completed in 6.406502419s

• [SLOW TEST:9.235 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:06:29.187: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-55psj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Mar  4 23:06:29.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:29.710: INFO: stderr: ""
Mar  4 23:06:29.710: INFO: stdout: "pod/pause created\n"
Mar  4 23:06:29.710: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  4 23:06:29.711: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-55psj" to be "running and ready"
Mar  4 23:06:29.800: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.233137ms
Mar  4 23:06:31.808: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.097554276s
Mar  4 23:06:31.808: INFO: Pod "pause" satisfied condition "running and ready"
Mar  4 23:06:31.808: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  4 23:06:31.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.014: INFO: stderr: ""
Mar  4 23:06:32.014: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  4 23:06:32.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pod pause -L testing-label --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.120: INFO: stderr: ""
Mar  4 23:06:32.120: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  4 23:06:32.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 label pods pause testing-label- --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.253: INFO: stderr: ""
Mar  4 23:06:32.253: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  4 23:06:32.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pod pause -L testing-label --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.357: INFO: stderr: ""
Mar  4 23:06:32.357: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Mar  4 23:06:32.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.502: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 23:06:32.502: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  4 23:06:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-55psj'
Mar  4 23:06:32.684: INFO: stderr: "No resources found.\n"
Mar  4 23:06:32.684: INFO: stdout: ""
Mar  4 23:06:32.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -l name=pause --namespace=e2e-tests-kubectl-55psj -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 23:06:32.825: INFO: stderr: ""
Mar  4 23:06:32.825: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:06:32.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-55psj" for this suite.
Mar  4 23:06:38.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:06:39.118: INFO: namespace: e2e-tests-kubectl-55psj, resource: bindings, ignored listing per whitelist
Mar  4 23:06:39.253: INFO: namespace e2e-tests-kubectl-55psj deletion completed in 6.317029476s

• [SLOW TEST:10.066 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:06:39.253: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-zt2sr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-zt2sr
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-zt2sr
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-zt2sr
Mar  4 23:06:39.642: INFO: Found 0 stateful pods, waiting for 1
Mar  4 23:06:49.651: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  4 23:06:49.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:06:50.006: INFO: stderr: ""
Mar  4 23:06:50.006: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:06:50.006: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  4 23:06:50.108: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  4 23:07:00.117: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 23:07:00.117: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 23:07:00.151: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:00.152: INFO: ss-0  10.190.208.159  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:00.152: INFO: 
Mar  4 23:07:00.152: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  4 23:07:01.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990437405s
Mar  4 23:07:02.171: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981174449s
Mar  4 23:07:03.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970897902s
Mar  4 23:07:04.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962245874s
Mar  4 23:07:05.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.94977595s
Mar  4 23:07:06.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941088403s
Mar  4 23:07:07.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930612837s
Mar  4 23:07:08.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.749974235s
Mar  4 23:07:09.411: INFO: Verifying statefulset ss doesn't scale past 3 for another 740.798591ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-zt2sr
Mar  4 23:07:10.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:10.794: INFO: stderr: ""
Mar  4 23:07:10.794: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  4 23:07:10.794: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  4 23:07:10.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:11.087: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  4 23:07:11.087: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  4 23:07:11.087: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  4 23:07:11.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:11.400: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar  4 23:07:11.400: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  4 23:07:11.400: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  4 23:07:11.410: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:07:11.410: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:07:11.410: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  4 23:07:11.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:07:11.733: INFO: stderr: ""
Mar  4 23:07:11.733: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:07:11.733: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  4 23:07:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:07:12.023: INFO: stderr: ""
Mar  4 23:07:12.023: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:07:12.023: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  4 23:07:12.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:07:12.327: INFO: stderr: ""
Mar  4 23:07:12.327: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:07:12.327: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  4 23:07:12.327: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 23:07:12.335: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  4 23:07:22.352: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 23:07:22.352: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 23:07:22.352: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 23:07:22.400: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:22.400: INFO: ss-0  10.190.208.159  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:22.400: INFO: ss-1  10.190.208.161  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:22.400: INFO: ss-2  10.190.208.164  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:22.400: INFO: 
Mar  4 23:07:22.400: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  4 23:07:23.409: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:23.409: INFO: ss-0  10.190.208.159  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:23.409: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:23.409: INFO: ss-2  10.190.208.164  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:23.409: INFO: 
Mar  4 23:07:23.409: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  4 23:07:24.418: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:24.418: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:24.418: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:24.418: INFO: 
Mar  4 23:07:24.418: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:25.428: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:25.428: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:25.428: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:25.428: INFO: 
Mar  4 23:07:25.428: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:26.437: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:26.437: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:26.437: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:26.437: INFO: 
Mar  4 23:07:26.437: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:27.447: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:27.447: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:27.447: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:27.447: INFO: 
Mar  4 23:07:27.447: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:28.456: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:28.456: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:28.456: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:28.456: INFO: 
Mar  4 23:07:28.456: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:29.470: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:29.470: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:29.470: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:29.470: INFO: 
Mar  4 23:07:29.470: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:30.484: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:30.484: INFO: ss-0  10.190.208.159  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:06:39 +0000 UTC  }]
Mar  4 23:07:30.484: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:30.484: INFO: 
Mar  4 23:07:30.484: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  4 23:07:31.494: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar  4 23:07:31.494: INFO: ss-1  10.190.208.161  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:07:00 +0000 UTC  }]
Mar  4 23:07:31.494: INFO: 
Mar  4 23:07:31.494: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-zt2sr
Mar  4 23:07:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:32.830: INFO: rc: 1
Mar  4 23:07:32.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001caa3f0 exit status 1 <nil> <nil> true [0xc000cb5000 0xc000cb5018 0xc000cb5030] [0xc000cb5000 0xc000cb5018 0xc000cb5030] [0xc000cb5010 0xc000cb5028] [0x92f8e0 0x92f8e0] 0xc001884de0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Mar  4 23:07:42.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:42.931: INFO: rc: 1
Mar  4 23:07:42.931: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170a390 exit status 1 <nil> <nil> true [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a030 0xc000d4a050] [0x92f8e0 0x92f8e0] 0xc0012f8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:07:52.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:07:53.056: INFO: rc: 1
Mar  4 23:07:53.056: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019403c0 exit status 1 <nil> <nil> true [0xc0005170a0 0xc0005171d0 0xc000517300] [0xc0005170a0 0xc0005171d0 0xc000517300] [0xc0005171c0 0xc000517260] [0x92f8e0 0x92f8e0] 0xc001948240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:03.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:03.148: INFO: rc: 1
Mar  4 23:08:03.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170a780 exit status 1 <nil> <nil> true [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a0b0 0xc000d4a0f8] [0x92f8e0 0x92f8e0] 0xc0012f8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:13.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:13.264: INFO: rc: 1
Mar  4 23:08:13.264: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019407b0 exit status 1 <nil> <nil> true [0xc000517460 0xc0005174d8 0xc000517528] [0xc000517460 0xc0005174d8 0xc000517528] [0xc0005174d0 0xc000517500] [0x92f8e0 0x92f8e0] 0xc001948540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:23.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:23.357: INFO: rc: 1
Mar  4 23:08:23.357: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170ab70 exit status 1 <nil> <nil> true [0xc000d4a130 0xc000d4a160 0xc000d4a1a0] [0xc000d4a130 0xc000d4a160 0xc000d4a1a0] [0xc000d4a158 0xc000d4a188] [0x92f8e0 0x92f8e0] 0xc0012f8a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:33.485: INFO: rc: 1
Mar  4 23:08:33.485: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001940ba0 exit status 1 <nil> <nil> true [0xc000517570 0xc000517638 0xc000517718] [0xc000517570 0xc000517638 0xc000517718] [0xc0005175f0 0xc0005176b8] [0x92f8e0 0x92f8e0] 0xc001948840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:43.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:43.593: INFO: rc: 1
Mar  4 23:08:43.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001940f90 exit status 1 <nil> <nil> true [0xc000517738 0xc000517758 0xc0005177f0] [0xc000517738 0xc000517758 0xc0005177f0] [0xc000517750 0xc0005177d0] [0x92f8e0 0x92f8e0] 0xc001948b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:08:53.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:08:53.808: INFO: rc: 1
Mar  4 23:08:53.808: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941380 exit status 1 <nil> <nil> true [0xc000517860 0xc0005178a0 0xc0005179f8] [0xc000517860 0xc0005178a0 0xc0005179f8] [0xc000517880 0xc0005179d0] [0x92f8e0 0x92f8e0] 0xc001948e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:03.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:03.934: INFO: rc: 1
Mar  4 23:09:03.934: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019417a0 exit status 1 <nil> <nil> true [0xc000517a50 0xc000517ad8 0xc000517b00] [0xc000517a50 0xc000517ad8 0xc000517b00] [0xc000517aa8 0xc000517af8] [0x92f8e0 0x92f8e0] 0xc001949140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:13.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:14.042: INFO: rc: 1
Mar  4 23:09:14.042: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170af60 exit status 1 <nil> <nil> true [0xc000d4a1b0 0xc000d4a1f0 0xc000d4a208] [0xc000d4a1b0 0xc000d4a1f0 0xc000d4a208] [0xc000d4a1e8 0xc000d4a200] [0x92f8e0 0x92f8e0] 0xc0012f8d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:24.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:24.156: INFO: rc: 1
Mar  4 23:09:24.156: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941b90 exit status 1 <nil> <nil> true [0xc000517bc0 0xc000517c30 0xc000517c80] [0xc000517bc0 0xc000517c30 0xc000517c80] [0xc000517bf8 0xc000517c50] [0x92f8e0 0x92f8e0] 0xc001949440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:34.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:34.310: INFO: rc: 1
Mar  4 23:09:34.310: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019403f0 exit status 1 <nil> <nil> true [0xc000517110 0xc000517248 0xc000517460] [0xc000517110 0xc000517248 0xc000517460] [0xc0005171d0 0xc000517300] [0x92f8e0 0x92f8e0] 0xc001948240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:44.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:44.411: INFO: rc: 1
Mar  4 23:09:44.411: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001940810 exit status 1 <nil> <nil> true [0xc0005174b0 0xc0005174e8 0xc000517570] [0xc0005174b0 0xc0005174e8 0xc000517570] [0xc0005174d8 0xc000517528] [0x92f8e0 0x92f8e0] 0xc001948540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:09:54.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:09:54.504: INFO: rc: 1
Mar  4 23:09:54.504: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170a3f0 exit status 1 <nil> <nil> true [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a030 0xc000d4a050] [0x92f8e0 0x92f8e0] 0xc0012f8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:04.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:04.619: INFO: rc: 1
Mar  4 23:10:04.619: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001940c30 exit status 1 <nil> <nil> true [0xc0005175d0 0xc000517660 0xc000517738] [0xc0005175d0 0xc000517660 0xc000517738] [0xc000517638 0xc000517718] [0x92f8e0 0x92f8e0] 0xc001948840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:14.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:14.730: INFO: rc: 1
Mar  4 23:10:14.730: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941020 exit status 1 <nil> <nil> true [0xc000517748 0xc000517770 0xc000517860] [0xc000517748 0xc000517770 0xc000517860] [0xc000517758 0xc0005177f0] [0x92f8e0 0x92f8e0] 0xc001948b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:24.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:24.846: INFO: rc: 1
Mar  4 23:10:24.846: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941440 exit status 1 <nil> <nil> true [0xc000517868 0xc000517948 0xc000517a50] [0xc000517868 0xc000517948 0xc000517a50] [0xc0005178a0 0xc0005179f8] [0x92f8e0 0x92f8e0] 0xc001948e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:34.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:34.956: INFO: rc: 1
Mar  4 23:10:34.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170a840 exit status 1 <nil> <nil> true [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a0b0 0xc000d4a0f8] [0x92f8e0 0x92f8e0] 0xc0012f8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:44.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:45.060: INFO: rc: 1
Mar  4 23:10:45.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941890 exit status 1 <nil> <nil> true [0xc000517a90 0xc000517ae0 0xc000517bc0] [0xc000517a90 0xc000517ae0 0xc000517bc0] [0xc000517ad8 0xc000517b00] [0x92f8e0 0x92f8e0] 0xc001949140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:10:55.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:10:55.221: INFO: rc: 1
Mar  4 23:10:55.221: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170ac60 exit status 1 <nil> <nil> true [0xc000d4a130 0xc000d4a160 0xc000d4a1a0] [0xc000d4a130 0xc000d4a160 0xc000d4a1a0] [0xc000d4a158 0xc000d4a188] [0x92f8e0 0x92f8e0] 0xc0012f8a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:05.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:05.331: INFO: rc: 1
Mar  4 23:11:05.331: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001941ce0 exit status 1 <nil> <nil> true [0xc000517bd8 0xc000517c40 0xc000517ca0] [0xc000517bd8 0xc000517c40 0xc000517ca0] [0xc000517c30 0xc000517c80] [0x92f8e0 0x92f8e0] 0xc001949440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:15.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:15.443: INFO: rc: 1
Mar  4 23:11:15.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0012b80f0 exit status 1 <nil> <nil> true [0xc000517cb0 0xc000517cf0 0xc000517d08] [0xc000517cb0 0xc000517cf0 0xc000517d08] [0xc000517ce8 0xc000517d00] [0x92f8e0 0x92f8e0] 0xc001949740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:25.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:25.561: INFO: rc: 1
Mar  4 23:11:25.561: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00170b050 exit status 1 <nil> <nil> true [0xc000d4a1b0 0xc000d4a1f0 0xc000d4a208] [0xc000d4a1b0 0xc000d4a1f0 0xc000d4a208] [0xc000d4a1e8 0xc000d4a200] [0x92f8e0 0x92f8e0] 0xc0012f8d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:35.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:35.671: INFO: rc: 1
Mar  4 23:11:35.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0012b8150 exit status 1 <nil> <nil> true [0xc000d4a210 0xc000d4a228 0xc000d4a298] [0xc000d4a210 0xc000d4a228 0xc000d4a298] [0xc000d4a220 0xc000d4a280] [0x92f8e0 0x92f8e0] 0xc001949800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:45.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:45.796: INFO: rc: 1
Mar  4 23:11:45.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019403c0 exit status 1 <nil> <nil> true [0xc000517110 0xc000517248 0xc000517460] [0xc000517110 0xc000517248 0xc000517460] [0xc0005171d0 0xc000517300] [0x92f8e0 0x92f8e0] 0xc001948240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:11:55.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:11:55.904: INFO: rc: 1
Mar  4 23:11:55.904: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0012b8480 exit status 1 <nil> <nil> true [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a008 0xc000d4a038 0xc000d4a070] [0xc000d4a030 0xc000d4a050] [0x92f8e0 0x92f8e0] 0xc0012f8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:12:05.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:12:06.017: INFO: rc: 1
Mar  4 23:12:06.017: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019407e0 exit status 1 <nil> <nil> true [0xc0005174b0 0xc0005174e8 0xc000517570] [0xc0005174b0 0xc0005174e8 0xc000517570] [0xc0005174d8 0xc000517528] [0x92f8e0 0x92f8e0] 0xc001948540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:12:16.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:12:16.125: INFO: rc: 1
Mar  4 23:12:16.125: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001940c00 exit status 1 <nil> <nil> true [0xc0005175d0 0xc000517660 0xc000517738] [0xc0005175d0 0xc000517660 0xc000517738] [0xc000517638 0xc000517718] [0x92f8e0 0x92f8e0] 0xc001948840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:12:26.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:12:26.274: INFO: rc: 1
Mar  4 23:12:26.274: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0012b8900 exit status 1 <nil> <nil> true [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a088 0xc000d4a0c0 0xc000d4a118] [0xc000d4a0b0 0xc000d4a0f8] [0x92f8e0 0x92f8e0] 0xc0012f8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Mar  4 23:12:36.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-zt2sr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:12:36.401: INFO: rc: 1
Mar  4 23:12:36.401: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Mar  4 23:12:36.401: INFO: Scaling statefulset ss to 0
Mar  4 23:12:36.426: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  4 23:12:36.434: INFO: Deleting all statefulset in ns e2e-tests-statefulset-zt2sr
Mar  4 23:12:36.448: INFO: Scaling statefulset ss to 0
Mar  4 23:12:36.471: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 23:12:36.479: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:12:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-zt2sr" for this suite.
Mar  4 23:12:42.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:12:42.612: INFO: namespace: e2e-tests-statefulset-zt2sr, resource: bindings, ignored listing per whitelist
Mar  4 23:12:42.947: INFO: namespace e2e-tests-statefulset-zt2sr deletion completed in 6.412453253s

• [SLOW TEST:363.694 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:12:42.948: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-khln6
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-03b8d700-3ed3-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-03b8d700-3ed3-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:12:47.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-khln6" for this suite.
Mar  4 23:13:11.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:13:11.724: INFO: namespace: e2e-tests-configmap-khln6, resource: bindings, ignored listing per whitelist
Mar  4 23:13:11.837: INFO: namespace e2e-tests-configmap-khln6 deletion completed in 24.285100964s

• [SLOW TEST:28.889 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:13:11.838: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dhgqm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-14e5e776-3ed3-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:13:12.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-dhgqm" to be "success or failure"
Mar  4 23:13:12.147: INFO: Pod "pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216624ms
Mar  4 23:13:14.156: INFO: Pod "pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017020514s
STEP: Saw pod success
Mar  4 23:13:14.156: INFO: Pod "pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:13:14.164: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:13:14.216: INFO: Waiting for pod pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a to disappear
Mar  4 23:13:14.224: INFO: Pod pod-projected-secrets-14e715a6-3ed3-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:13:14.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dhgqm" for this suite.
Mar  4 23:13:22.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:13:22.610: INFO: namespace: e2e-tests-projected-dhgqm, resource: bindings, ignored listing per whitelist
Mar  4 23:13:22.794: INFO: namespace e2e-tests-projected-dhgqm deletion completed in 8.560337557s

• [SLOW TEST:10.956 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:13:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-t6d5d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:13:48.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-t6d5d" for this suite.
Mar  4 23:13:54.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:13:54.196: INFO: namespace: e2e-tests-container-runtime-t6d5d, resource: bindings, ignored listing per whitelist
Mar  4 23:13:54.352: INFO: namespace e2e-tests-container-runtime-t6d5d deletion completed in 6.309089572s

• [SLOW TEST:31.558 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:13:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tjjg6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  4 23:13:57.299: INFO: Successfully updated pod "annotationupdate2e426902-3ed3-11e9-8a62-3ec24305971a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:14:01.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tjjg6" for this suite.
Mar  4 23:14:25.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:14:25.589: INFO: namespace: e2e-tests-projected-tjjg6, resource: bindings, ignored listing per whitelist
Mar  4 23:14:25.767: INFO: namespace e2e-tests-projected-tjjg6 deletion completed in 24.39348378s

• [SLOW TEST:31.415 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:14:25.768: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2wmpn
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-40f8a96a-3ed3-11e9-8a62-3ec24305971a
STEP: Creating secret with name s-test-opt-upd-40f8a9c8-3ed3-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-40f8a96a-3ed3-11e9-8a62-3ec24305971a
STEP: Updating secret s-test-opt-upd-40f8a9c8-3ed3-11e9-8a62-3ec24305971a
STEP: Creating secret with name s-test-opt-create-40f8a9e9-3ed3-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:15:35.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2wmpn" for this suite.
Mar  4 23:15:59.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:15:59.957: INFO: namespace: e2e-tests-projected-2wmpn, resource: bindings, ignored listing per whitelist
Mar  4 23:16:01.012: INFO: namespace e2e-tests-projected-2wmpn deletion completed in 25.203187759s

• [SLOW TEST:95.245 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:16:01.012: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mh5qh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-79bdb0f1-3ed3-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:16:01.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-mh5qh" to be "success or failure"
Mar  4 23:16:01.332: INFO: Pod "pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.824071ms
Mar  4 23:16:03.344: INFO: Pod "pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019071755s
STEP: Saw pod success
Mar  4 23:16:03.344: INFO: Pod "pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:16:03.353: INFO: Trying to get logs from node 10.190.208.159 pod pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:16:03.401: INFO: Waiting for pod pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a to disappear
Mar  4 23:16:03.408: INFO: Pod pod-configmaps-79beff98-3ed3-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:16:03.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mh5qh" for this suite.
Mar  4 23:16:09.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:16:09.676: INFO: namespace: e2e-tests-configmap-mh5qh, resource: bindings, ignored listing per whitelist
Mar  4 23:16:09.753: INFO: namespace e2e-tests-configmap-mh5qh deletion completed in 6.333229925s

• [SLOW TEST:8.740 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:16:09.753: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-l2sxn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar  4 23:16:10.057: INFO: Waiting up to 5m0s for pod "var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a" in namespace "e2e-tests-var-expansion-l2sxn" to be "success or failure"
Mar  4 23:16:10.065: INFO: Pod "var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11778ms
Mar  4 23:16:12.075: INFO: Pod "var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a": Phase="Running", Reason="", readiness=true. Elapsed: 2.017728536s
Mar  4 23:16:14.083: INFO: Pod "var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026204842s
STEP: Saw pod success
Mar  4 23:16:14.083: INFO: Pod "var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:16:14.091: INFO: Trying to get logs from node 10.190.208.161 pod var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  4 23:16:14.200: INFO: Waiting for pod var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a to disappear
Mar  4 23:16:14.207: INFO: Pod var-expansion-7ef34f08-3ed3-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:16:14.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-l2sxn" for this suite.
Mar  4 23:16:20.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:16:20.519: INFO: namespace: e2e-tests-var-expansion-l2sxn, resource: bindings, ignored listing per whitelist
Mar  4 23:16:20.575: INFO: namespace e2e-tests-var-expansion-l2sxn deletion completed in 6.356415303s

• [SLOW TEST:10.822 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:16:20.575: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-f7nfx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  4 23:16:20.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:21.341: INFO: stderr: ""
Mar  4 23:16:21.341: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 23:16:21.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:21.515: INFO: stderr: ""
Mar  4 23:16:21.515: INFO: stdout: "update-demo-nautilus-6pttf update-demo-nautilus-mbtk4 "
Mar  4 23:16:21.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-6pttf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:21.630: INFO: stderr: ""
Mar  4 23:16:21.630: INFO: stdout: ""
Mar  4 23:16:21.630: INFO: update-demo-nautilus-6pttf is created but not running
Mar  4 23:16:26.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:26.876: INFO: stderr: ""
Mar  4 23:16:26.876: INFO: stdout: "update-demo-nautilus-6pttf update-demo-nautilus-mbtk4 "
Mar  4 23:16:26.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-6pttf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:26.999: INFO: stderr: ""
Mar  4 23:16:26.999: INFO: stdout: "true"
Mar  4 23:16:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-6pttf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:27.096: INFO: stderr: ""
Mar  4 23:16:27.096: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 23:16:27.096: INFO: validating pod update-demo-nautilus-6pttf
Mar  4 23:16:27.117: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 23:16:27.118: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 23:16:27.118: INFO: update-demo-nautilus-6pttf is verified up and running
Mar  4 23:16:27.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-mbtk4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:27.212: INFO: stderr: ""
Mar  4 23:16:27.212: INFO: stdout: "true"
Mar  4 23:16:27.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-mbtk4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:27.317: INFO: stderr: ""
Mar  4 23:16:27.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 23:16:27.317: INFO: validating pod update-demo-nautilus-mbtk4
Mar  4 23:16:27.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 23:16:27.335: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 23:16:27.335: INFO: update-demo-nautilus-mbtk4 is verified up and running
STEP: using delete to clean up resources
Mar  4 23:16:27.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:27.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 23:16:27.451: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  4 23:16:27.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-f7nfx'
Mar  4 23:16:27.570: INFO: stderr: "No resources found.\n"
Mar  4 23:16:27.570: INFO: stdout: ""
Mar  4 23:16:27.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -l name=update-demo --namespace=e2e-tests-kubectl-f7nfx -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 23:16:27.749: INFO: stderr: ""
Mar  4 23:16:27.749: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:16:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f7nfx" for this suite.
Mar  4 23:16:51.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:16:52.069: INFO: namespace: e2e-tests-kubectl-f7nfx, resource: bindings, ignored listing per whitelist
Mar  4 23:16:52.157: INFO: namespace e2e-tests-kubectl-f7nfx deletion completed in 24.398226732s

• [SLOW TEST:31.582 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:16:52.157: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-fmzbc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:16:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-fmzbc" for this suite.
Mar  4 23:17:44.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:17:45.005: INFO: namespace: e2e-tests-kubelet-test-fmzbc, resource: bindings, ignored listing per whitelist
Mar  4 23:17:45.112: INFO: namespace e2e-tests-kubelet-test-fmzbc deletion completed in 50.510765303s

• [SLOW TEST:52.954 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:17:45.112: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-9dfjk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-9dfjk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9dfjk to expose endpoints map[]
Mar  4 23:17:45.416: INFO: Get endpoints failed (7.13558ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  4 23:17:46.424: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9dfjk exposes endpoints map[] (1.015606233s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-9dfjk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9dfjk to expose endpoints map[pod1:[80]]
Mar  4 23:17:49.577: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9dfjk exposes endpoints map[pod1:[80]] (3.076552635s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-9dfjk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9dfjk to expose endpoints map[pod1:[80] pod2:[80]]
Mar  4 23:17:52.684: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9dfjk exposes endpoints map[pod1:[80] pod2:[80]] (3.096482456s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-9dfjk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9dfjk to expose endpoints map[pod2:[80]]
Mar  4 23:17:52.807: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9dfjk exposes endpoints map[pod2:[80]] (107.494465ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-9dfjk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9dfjk to expose endpoints map[]
Mar  4 23:17:52.829: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9dfjk exposes endpoints map[] (7.443613ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:17:52.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-9dfjk" for this suite.
Mar  4 23:18:12.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:18:13.240: INFO: namespace: e2e-tests-services-9dfjk, resource: bindings, ignored listing per whitelist
Mar  4 23:18:13.436: INFO: namespace e2e-tests-services-9dfjk deletion completed in 20.541965648s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:28.325 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:18:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-t5dnw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar  4 23:18:13.809: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-061951449 proxy --unix-socket=/tmp/kubectl-proxy-unix419274596/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:18:13.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t5dnw" for this suite.
Mar  4 23:18:19.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:18:20.186: INFO: namespace: e2e-tests-kubectl-t5dnw, resource: bindings, ignored listing per whitelist
Mar  4 23:18:20.186: INFO: namespace e2e-tests-kubectl-t5dnw deletion completed in 6.28772354s

• [SLOW TEST:6.747 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:18:20.186: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-wvcl6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-wvcl6
I0304 23:18:20.508071      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-wvcl6, replica count: 1
I0304 23:18:21.558673      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0304 23:18:22.559005      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 23:18:22.685: INFO: Created: latency-svc-hgqts
Mar  4 23:18:22.702: INFO: Got endpoints: latency-svc-hgqts [43.514125ms]
Mar  4 23:18:22.724: INFO: Created: latency-svc-cz5n5
Mar  4 23:18:22.732: INFO: Got endpoints: latency-svc-cz5n5 [29.086083ms]
Mar  4 23:18:22.737: INFO: Created: latency-svc-jm4cp
Mar  4 23:18:22.745: INFO: Got endpoints: latency-svc-jm4cp [41.783995ms]
Mar  4 23:18:22.748: INFO: Created: latency-svc-q8j9r
Mar  4 23:18:22.756: INFO: Got endpoints: latency-svc-q8j9r [53.523781ms]
Mar  4 23:18:22.761: INFO: Created: latency-svc-rjqwz
Mar  4 23:18:22.769: INFO: Got endpoints: latency-svc-rjqwz [65.873064ms]
Mar  4 23:18:22.774: INFO: Created: latency-svc-rs24w
Mar  4 23:18:22.781: INFO: Got endpoints: latency-svc-rs24w [76.242144ms]
Mar  4 23:18:22.790: INFO: Created: latency-svc-h8nkp
Mar  4 23:18:22.797: INFO: Got endpoints: latency-svc-h8nkp [93.030598ms]
Mar  4 23:18:22.799: INFO: Created: latency-svc-fwxrt
Mar  4 23:18:22.805: INFO: Got endpoints: latency-svc-fwxrt [100.738939ms]
Mar  4 23:18:22.811: INFO: Created: latency-svc-gnsh2
Mar  4 23:18:22.818: INFO: Got endpoints: latency-svc-gnsh2 [113.089878ms]
Mar  4 23:18:22.821: INFO: Created: latency-svc-sdjpf
Mar  4 23:18:22.828: INFO: Got endpoints: latency-svc-sdjpf [124.052103ms]
Mar  4 23:18:22.837: INFO: Created: latency-svc-548qx
Mar  4 23:18:22.844: INFO: Got endpoints: latency-svc-548qx [140.323088ms]
Mar  4 23:18:22.848: INFO: Created: latency-svc-n7z6s
Mar  4 23:18:22.855: INFO: Got endpoints: latency-svc-n7z6s [150.785093ms]
Mar  4 23:18:22.859: INFO: Created: latency-svc-s2mh7
Mar  4 23:18:22.866: INFO: Got endpoints: latency-svc-s2mh7 [162.503332ms]
Mar  4 23:18:22.871: INFO: Created: latency-svc-qsq6q
Mar  4 23:18:22.878: INFO: Got endpoints: latency-svc-qsq6q [173.546095ms]
Mar  4 23:18:22.882: INFO: Created: latency-svc-8zcg8
Mar  4 23:18:22.889: INFO: Got endpoints: latency-svc-8zcg8 [184.691714ms]
Mar  4 23:18:22.909: INFO: Created: latency-svc-vwsm6
Mar  4 23:18:22.916: INFO: Got endpoints: latency-svc-vwsm6 [212.124782ms]
Mar  4 23:18:22.927: INFO: Created: latency-svc-hhv6q
Mar  4 23:18:22.935: INFO: Got endpoints: latency-svc-hhv6q [203.504213ms]
Mar  4 23:18:22.939: INFO: Created: latency-svc-znl5q
Mar  4 23:18:22.947: INFO: Got endpoints: latency-svc-znl5q [201.878079ms]
Mar  4 23:18:22.953: INFO: Created: latency-svc-tjslk
Mar  4 23:18:22.964: INFO: Got endpoints: latency-svc-tjslk [207.664241ms]
Mar  4 23:18:22.968: INFO: Created: latency-svc-9hw2b
Mar  4 23:18:22.977: INFO: Created: latency-svc-657fn
Mar  4 23:18:22.980: INFO: Got endpoints: latency-svc-9hw2b [210.663172ms]
Mar  4 23:18:22.986: INFO: Got endpoints: latency-svc-657fn [205.578345ms]
Mar  4 23:18:22.991: INFO: Created: latency-svc-kfwht
Mar  4 23:18:23.000: INFO: Got endpoints: latency-svc-kfwht [203.410467ms]
Mar  4 23:18:23.003: INFO: Created: latency-svc-z6b4w
Mar  4 23:18:23.010: INFO: Got endpoints: latency-svc-z6b4w [204.995205ms]
Mar  4 23:18:23.015: INFO: Created: latency-svc-blb9p
Mar  4 23:18:23.022: INFO: Got endpoints: latency-svc-blb9p [204.092925ms]
Mar  4 23:18:23.029: INFO: Created: latency-svc-sz6gb
Mar  4 23:18:23.036: INFO: Got endpoints: latency-svc-sz6gb [207.816397ms]
Mar  4 23:18:23.057: INFO: Created: latency-svc-8qpt8
Mar  4 23:18:23.064: INFO: Got endpoints: latency-svc-8qpt8 [219.313824ms]
Mar  4 23:18:23.070: INFO: Created: latency-svc-bmmls
Mar  4 23:18:23.078: INFO: Got endpoints: latency-svc-bmmls [223.137349ms]
Mar  4 23:18:23.086: INFO: Created: latency-svc-5dr49
Mar  4 23:18:23.093: INFO: Got endpoints: latency-svc-5dr49 [226.786698ms]
Mar  4 23:18:23.097: INFO: Created: latency-svc-jgsd7
Mar  4 23:18:23.104: INFO: Got endpoints: latency-svc-jgsd7 [226.80144ms]
Mar  4 23:18:23.109: INFO: Created: latency-svc-wvg9s
Mar  4 23:18:23.117: INFO: Got endpoints: latency-svc-wvg9s [227.748996ms]
Mar  4 23:18:23.124: INFO: Created: latency-svc-6zhr5
Mar  4 23:18:23.134: INFO: Got endpoints: latency-svc-6zhr5 [217.594134ms]
Mar  4 23:18:23.136: INFO: Created: latency-svc-4zzjt
Mar  4 23:18:23.145: INFO: Got endpoints: latency-svc-4zzjt [209.462196ms]
Mar  4 23:18:23.149: INFO: Created: latency-svc-9zfmd
Mar  4 23:18:23.157: INFO: Got endpoints: latency-svc-9zfmd [209.964943ms]
Mar  4 23:18:23.162: INFO: Created: latency-svc-4fb4r
Mar  4 23:18:23.170: INFO: Got endpoints: latency-svc-4fb4r [205.450405ms]
Mar  4 23:18:23.173: INFO: Created: latency-svc-sc428
Mar  4 23:18:23.180: INFO: Got endpoints: latency-svc-sc428 [200.314889ms]
Mar  4 23:18:23.183: INFO: Created: latency-svc-62tzg
Mar  4 23:18:23.191: INFO: Got endpoints: latency-svc-62tzg [204.487985ms]
Mar  4 23:18:23.195: INFO: Created: latency-svc-lz6vc
Mar  4 23:18:23.203: INFO: Got endpoints: latency-svc-lz6vc [203.317371ms]
Mar  4 23:18:23.207: INFO: Created: latency-svc-c2zhb
Mar  4 23:18:23.214: INFO: Got endpoints: latency-svc-c2zhb [203.912308ms]
Mar  4 23:18:23.217: INFO: Created: latency-svc-gsrnh
Mar  4 23:18:23.224: INFO: Got endpoints: latency-svc-gsrnh [202.032392ms]
Mar  4 23:18:23.230: INFO: Created: latency-svc-6hvs4
Mar  4 23:18:23.241: INFO: Got endpoints: latency-svc-6hvs4 [204.486036ms]
Mar  4 23:18:23.242: INFO: Created: latency-svc-csqq8
Mar  4 23:18:23.252: INFO: Got endpoints: latency-svc-csqq8 [187.817683ms]
Mar  4 23:18:23.253: INFO: Created: latency-svc-mvh8q
Mar  4 23:18:23.264: INFO: Created: latency-svc-gz525
Mar  4 23:18:23.276: INFO: Created: latency-svc-tv7vz
Mar  4 23:18:23.289: INFO: Created: latency-svc-n9mqs
Mar  4 23:18:23.292: INFO: Got endpoints: latency-svc-mvh8q [214.464671ms]
Mar  4 23:18:23.300: INFO: Created: latency-svc-4c52p
Mar  4 23:18:23.312: INFO: Created: latency-svc-pjrbf
Mar  4 23:18:23.326: INFO: Created: latency-svc-wwp5r
Mar  4 23:18:23.339: INFO: Created: latency-svc-rrzfx
Mar  4 23:18:23.343: INFO: Got endpoints: latency-svc-gz525 [249.81158ms]
Mar  4 23:18:23.350: INFO: Created: latency-svc-lbwnz
Mar  4 23:18:23.362: INFO: Created: latency-svc-cxzv9
Mar  4 23:18:23.375: INFO: Created: latency-svc-7988z
Mar  4 23:18:23.388: INFO: Created: latency-svc-zs9vd
Mar  4 23:18:23.391: INFO: Got endpoints: latency-svc-tv7vz [286.915543ms]
Mar  4 23:18:23.402: INFO: Created: latency-svc-rmqg9
Mar  4 23:18:23.412: INFO: Created: latency-svc-mtfdc
Mar  4 23:18:23.426: INFO: Created: latency-svc-bvtd7
Mar  4 23:18:23.438: INFO: Created: latency-svc-gjtw6
Mar  4 23:18:23.441: INFO: Got endpoints: latency-svc-n9mqs [324.02506ms]
Mar  4 23:18:23.451: INFO: Created: latency-svc-7f484
Mar  4 23:18:23.465: INFO: Created: latency-svc-n9wg7
Mar  4 23:18:23.474: INFO: Created: latency-svc-hmjjq
Mar  4 23:18:23.491: INFO: Got endpoints: latency-svc-4c52p [357.557484ms]
Mar  4 23:18:23.513: INFO: Created: latency-svc-lrbhs
Mar  4 23:18:23.544: INFO: Got endpoints: latency-svc-pjrbf [399.561665ms]
Mar  4 23:18:23.565: INFO: Created: latency-svc-m7jsc
Mar  4 23:18:23.593: INFO: Got endpoints: latency-svc-wwp5r [436.048679ms]
Mar  4 23:18:23.614: INFO: Created: latency-svc-4z8w2
Mar  4 23:18:23.642: INFO: Got endpoints: latency-svc-rrzfx [472.53132ms]
Mar  4 23:18:23.664: INFO: Created: latency-svc-28gp4
Mar  4 23:18:23.692: INFO: Got endpoints: latency-svc-lbwnz [512.130774ms]
Mar  4 23:18:23.715: INFO: Created: latency-svc-n6svf
Mar  4 23:18:23.742: INFO: Got endpoints: latency-svc-cxzv9 [551.052547ms]
Mar  4 23:18:23.763: INFO: Created: latency-svc-pfndv
Mar  4 23:18:23.792: INFO: Got endpoints: latency-svc-7988z [588.550652ms]
Mar  4 23:18:23.842: INFO: Got endpoints: latency-svc-zs9vd [627.697899ms]
Mar  4 23:18:23.892: INFO: Got endpoints: latency-svc-rmqg9 [667.603529ms]
Mar  4 23:18:23.945: INFO: Got endpoints: latency-svc-mtfdc [704.131939ms]
Mar  4 23:18:23.992: INFO: Got endpoints: latency-svc-bvtd7 [740.547259ms]
Mar  4 23:18:24.043: INFO: Got endpoints: latency-svc-gjtw6 [750.398302ms]
Mar  4 23:18:24.046: INFO: Created: latency-svc-kdfrm
Mar  4 23:18:24.047: INFO: Created: latency-svc-jtwdj
Mar  4 23:18:24.050: INFO: Created: latency-svc-7k55g
Mar  4 23:18:24.050: INFO: Created: latency-svc-j9cjf
Mar  4 23:18:24.050: INFO: Created: latency-svc-tkwc2
Mar  4 23:18:24.064: INFO: Created: latency-svc-shg4d
Mar  4 23:18:24.091: INFO: Got endpoints: latency-svc-7f484 [748.557675ms]
Mar  4 23:18:24.111: INFO: Created: latency-svc-h7dmm
Mar  4 23:18:24.142: INFO: Got endpoints: latency-svc-n9wg7 [750.295102ms]
Mar  4 23:18:24.163: INFO: Created: latency-svc-b8r2s
Mar  4 23:18:24.192: INFO: Got endpoints: latency-svc-hmjjq [751.362273ms]
Mar  4 23:18:24.213: INFO: Created: latency-svc-xcdnt
Mar  4 23:18:24.241: INFO: Got endpoints: latency-svc-lrbhs [749.300555ms]
Mar  4 23:18:24.264: INFO: Created: latency-svc-29b58
Mar  4 23:18:24.294: INFO: Got endpoints: latency-svc-m7jsc [749.236844ms]
Mar  4 23:18:24.316: INFO: Created: latency-svc-d56r8
Mar  4 23:18:24.347: INFO: Got endpoints: latency-svc-4z8w2 [753.979032ms]
Mar  4 23:18:24.369: INFO: Created: latency-svc-9hmwh
Mar  4 23:18:24.392: INFO: Got endpoints: latency-svc-28gp4 [749.645586ms]
Mar  4 23:18:24.414: INFO: Created: latency-svc-tgbsc
Mar  4 23:18:24.442: INFO: Got endpoints: latency-svc-n6svf [749.797629ms]
Mar  4 23:18:24.465: INFO: Created: latency-svc-prpwz
Mar  4 23:18:24.491: INFO: Got endpoints: latency-svc-pfndv [749.033447ms]
Mar  4 23:18:24.521: INFO: Created: latency-svc-ndn88
Mar  4 23:18:24.542: INFO: Got endpoints: latency-svc-kdfrm [750.012064ms]
Mar  4 23:18:24.564: INFO: Created: latency-svc-mxp6w
Mar  4 23:18:24.593: INFO: Got endpoints: latency-svc-jtwdj [750.687828ms]
Mar  4 23:18:24.616: INFO: Created: latency-svc-49qth
Mar  4 23:18:24.642: INFO: Got endpoints: latency-svc-7k55g [749.992494ms]
Mar  4 23:18:24.662: INFO: Created: latency-svc-pbgdj
Mar  4 23:18:24.691: INFO: Got endpoints: latency-svc-tkwc2 [746.361911ms]
Mar  4 23:18:24.713: INFO: Created: latency-svc-vx4rq
Mar  4 23:18:24.741: INFO: Got endpoints: latency-svc-j9cjf [749.180416ms]
Mar  4 23:18:24.763: INFO: Created: latency-svc-nklfg
Mar  4 23:18:24.791: INFO: Got endpoints: latency-svc-shg4d [748.166441ms]
Mar  4 23:18:24.812: INFO: Created: latency-svc-dvjlm
Mar  4 23:18:24.843: INFO: Got endpoints: latency-svc-h7dmm [751.848444ms]
Mar  4 23:18:24.865: INFO: Created: latency-svc-6hwqp
Mar  4 23:18:24.892: INFO: Got endpoints: latency-svc-b8r2s [749.674927ms]
Mar  4 23:18:24.913: INFO: Created: latency-svc-4kvqw
Mar  4 23:18:24.942: INFO: Got endpoints: latency-svc-xcdnt [749.743693ms]
Mar  4 23:18:24.963: INFO: Created: latency-svc-hvp2v
Mar  4 23:18:24.991: INFO: Got endpoints: latency-svc-29b58 [750.0913ms]
Mar  4 23:18:25.014: INFO: Created: latency-svc-wnzcn
Mar  4 23:18:25.041: INFO: Got endpoints: latency-svc-d56r8 [747.604695ms]
Mar  4 23:18:25.062: INFO: Created: latency-svc-6vzfz
Mar  4 23:18:25.091: INFO: Got endpoints: latency-svc-9hmwh [744.393525ms]
Mar  4 23:18:25.116: INFO: Created: latency-svc-jlcft
Mar  4 23:18:25.141: INFO: Got endpoints: latency-svc-tgbsc [749.422799ms]
Mar  4 23:18:25.162: INFO: Created: latency-svc-xpfl4
Mar  4 23:18:25.192: INFO: Got endpoints: latency-svc-prpwz [749.783887ms]
Mar  4 23:18:25.215: INFO: Created: latency-svc-52vlz
Mar  4 23:18:25.242: INFO: Got endpoints: latency-svc-ndn88 [750.623481ms]
Mar  4 23:18:25.263: INFO: Created: latency-svc-4jppb
Mar  4 23:18:25.293: INFO: Got endpoints: latency-svc-mxp6w [750.764641ms]
Mar  4 23:18:25.315: INFO: Created: latency-svc-cr9xl
Mar  4 23:18:25.341: INFO: Got endpoints: latency-svc-49qth [748.701303ms]
Mar  4 23:18:25.362: INFO: Created: latency-svc-fcrnq
Mar  4 23:18:25.396: INFO: Got endpoints: latency-svc-pbgdj [753.728386ms]
Mar  4 23:18:25.416: INFO: Created: latency-svc-g6g6d
Mar  4 23:18:25.442: INFO: Got endpoints: latency-svc-vx4rq [750.387331ms]
Mar  4 23:18:25.463: INFO: Created: latency-svc-lbws7
Mar  4 23:18:25.492: INFO: Got endpoints: latency-svc-nklfg [750.497742ms]
Mar  4 23:18:25.514: INFO: Created: latency-svc-lrl6s
Mar  4 23:18:25.542: INFO: Got endpoints: latency-svc-dvjlm [750.322283ms]
Mar  4 23:18:25.563: INFO: Created: latency-svc-6pglh
Mar  4 23:18:25.592: INFO: Got endpoints: latency-svc-6hwqp [748.733918ms]
Mar  4 23:18:25.614: INFO: Created: latency-svc-np4gg
Mar  4 23:18:25.641: INFO: Got endpoints: latency-svc-4kvqw [749.426589ms]
Mar  4 23:18:25.663: INFO: Created: latency-svc-v48kc
Mar  4 23:18:25.693: INFO: Got endpoints: latency-svc-hvp2v [750.805767ms]
Mar  4 23:18:25.715: INFO: Created: latency-svc-vhxmj
Mar  4 23:18:25.743: INFO: Got endpoints: latency-svc-wnzcn [751.515834ms]
Mar  4 23:18:25.768: INFO: Created: latency-svc-svwfs
Mar  4 23:18:25.792: INFO: Got endpoints: latency-svc-6vzfz [750.483578ms]
Mar  4 23:18:25.815: INFO: Created: latency-svc-sc9tt
Mar  4 23:18:25.842: INFO: Got endpoints: latency-svc-jlcft [750.655388ms]
Mar  4 23:18:25.864: INFO: Created: latency-svc-j76k6
Mar  4 23:18:25.892: INFO: Got endpoints: latency-svc-xpfl4 [750.459581ms]
Mar  4 23:18:25.915: INFO: Created: latency-svc-9n2dp
Mar  4 23:18:25.942: INFO: Got endpoints: latency-svc-52vlz [749.942116ms]
Mar  4 23:18:25.968: INFO: Created: latency-svc-sdgl4
Mar  4 23:18:25.992: INFO: Got endpoints: latency-svc-4jppb [749.620089ms]
Mar  4 23:18:26.013: INFO: Created: latency-svc-jgsqn
Mar  4 23:18:26.042: INFO: Got endpoints: latency-svc-cr9xl [749.38119ms]
Mar  4 23:18:26.064: INFO: Created: latency-svc-kz9hq
Mar  4 23:18:26.092: INFO: Got endpoints: latency-svc-fcrnq [750.435214ms]
Mar  4 23:18:26.113: INFO: Created: latency-svc-xbjvb
Mar  4 23:18:26.142: INFO: Got endpoints: latency-svc-g6g6d [746.313332ms]
Mar  4 23:18:26.164: INFO: Created: latency-svc-2ssmd
Mar  4 23:18:26.192: INFO: Got endpoints: latency-svc-lbws7 [749.833068ms]
Mar  4 23:18:26.213: INFO: Created: latency-svc-5nxvt
Mar  4 23:18:26.243: INFO: Got endpoints: latency-svc-lrl6s [750.970962ms]
Mar  4 23:18:26.265: INFO: Created: latency-svc-5xbln
Mar  4 23:18:26.292: INFO: Got endpoints: latency-svc-6pglh [749.48127ms]
Mar  4 23:18:26.316: INFO: Created: latency-svc-tg2rn
Mar  4 23:18:26.341: INFO: Got endpoints: latency-svc-np4gg [749.340631ms]
Mar  4 23:18:26.363: INFO: Created: latency-svc-z64sm
Mar  4 23:18:26.392: INFO: Got endpoints: latency-svc-v48kc [750.216255ms]
Mar  4 23:18:26.413: INFO: Created: latency-svc-lqs9r
Mar  4 23:18:26.443: INFO: Got endpoints: latency-svc-vhxmj [749.542925ms]
Mar  4 23:18:26.464: INFO: Created: latency-svc-7hzc2
Mar  4 23:18:26.492: INFO: Got endpoints: latency-svc-svwfs [749.516116ms]
Mar  4 23:18:26.513: INFO: Created: latency-svc-gwv6k
Mar  4 23:18:26.542: INFO: Got endpoints: latency-svc-sc9tt [750.042172ms]
Mar  4 23:18:26.577: INFO: Created: latency-svc-jsgd5
Mar  4 23:18:26.592: INFO: Got endpoints: latency-svc-j76k6 [749.781566ms]
Mar  4 23:18:26.611: INFO: Created: latency-svc-tqdkm
Mar  4 23:18:26.642: INFO: Got endpoints: latency-svc-9n2dp [750.189232ms]
Mar  4 23:18:26.663: INFO: Created: latency-svc-nl9dg
Mar  4 23:18:26.692: INFO: Got endpoints: latency-svc-sdgl4 [749.635025ms]
Mar  4 23:18:26.712: INFO: Created: latency-svc-f5dcv
Mar  4 23:18:26.742: INFO: Got endpoints: latency-svc-jgsqn [749.91991ms]
Mar  4 23:18:26.763: INFO: Created: latency-svc-hzpz5
Mar  4 23:18:26.792: INFO: Got endpoints: latency-svc-kz9hq [749.336764ms]
Mar  4 23:18:26.818: INFO: Created: latency-svc-rqw9s
Mar  4 23:18:26.852: INFO: Got endpoints: latency-svc-xbjvb [759.618777ms]
Mar  4 23:18:26.885: INFO: Created: latency-svc-k2tr7
Mar  4 23:18:26.892: INFO: Got endpoints: latency-svc-2ssmd [750.109157ms]
Mar  4 23:18:26.915: INFO: Created: latency-svc-hgtp8
Mar  4 23:18:26.942: INFO: Got endpoints: latency-svc-5nxvt [750.43705ms]
Mar  4 23:18:26.964: INFO: Created: latency-svc-g9pzv
Mar  4 23:18:26.992: INFO: Got endpoints: latency-svc-5xbln [748.496756ms]
Mar  4 23:18:27.013: INFO: Created: latency-svc-fl5hp
Mar  4 23:18:27.041: INFO: Got endpoints: latency-svc-tg2rn [749.453416ms]
Mar  4 23:18:27.062: INFO: Created: latency-svc-l4wh5
Mar  4 23:18:27.091: INFO: Got endpoints: latency-svc-z64sm [749.752224ms]
Mar  4 23:18:27.143: INFO: Got endpoints: latency-svc-lqs9r [751.428518ms]
Mar  4 23:18:27.196: INFO: Got endpoints: latency-svc-7hzc2 [752.79233ms]
Mar  4 23:18:27.242: INFO: Got endpoints: latency-svc-gwv6k [749.473297ms]
Mar  4 23:18:27.300: INFO: Got endpoints: latency-svc-jsgd5 [757.573835ms]
Mar  4 23:18:27.343: INFO: Got endpoints: latency-svc-tqdkm [751.21386ms]
Mar  4 23:18:27.394: INFO: Got endpoints: latency-svc-nl9dg [751.154138ms]
Mar  4 23:18:27.437: INFO: Created: latency-svc-hcn4k
Mar  4 23:18:27.440: INFO: Created: latency-svc-m8tfm
Mar  4 23:18:27.440: INFO: Created: latency-svc-tmcnc
Mar  4 23:18:27.442: INFO: Created: latency-svc-z4tgj
Mar  4 23:18:27.442: INFO: Got endpoints: latency-svc-f5dcv [750.231877ms]
Mar  4 23:18:27.443: INFO: Created: latency-svc-48mhz
Mar  4 23:18:27.443: INFO: Created: latency-svc-f8bcz
Mar  4 23:18:27.443: INFO: Created: latency-svc-qx7gm
Mar  4 23:18:27.471: INFO: Created: latency-svc-pf26b
Mar  4 23:18:27.492: INFO: Got endpoints: latency-svc-hzpz5 [750.042689ms]
Mar  4 23:18:27.517: INFO: Created: latency-svc-s7jlh
Mar  4 23:18:27.546: INFO: Got endpoints: latency-svc-rqw9s [754.585716ms]
Mar  4 23:18:27.568: INFO: Created: latency-svc-k8rzj
Mar  4 23:18:27.592: INFO: Got endpoints: latency-svc-k2tr7 [740.664661ms]
Mar  4 23:18:27.614: INFO: Created: latency-svc-x9cr7
Mar  4 23:18:27.641: INFO: Got endpoints: latency-svc-hgtp8 [748.793002ms]
Mar  4 23:18:27.663: INFO: Created: latency-svc-2tz8k
Mar  4 23:18:27.697: INFO: Got endpoints: latency-svc-g9pzv [754.898283ms]
Mar  4 23:18:27.743: INFO: Got endpoints: latency-svc-fl5hp [750.959424ms]
Mar  4 23:18:27.760: INFO: Created: latency-svc-dkt8q
Mar  4 23:18:27.766: INFO: Created: latency-svc-r9lz6
Mar  4 23:18:27.792: INFO: Got endpoints: latency-svc-l4wh5 [750.876204ms]
Mar  4 23:18:27.815: INFO: Created: latency-svc-7hm5s
Mar  4 23:18:27.841: INFO: Got endpoints: latency-svc-hcn4k [749.492969ms]
Mar  4 23:18:27.861: INFO: Created: latency-svc-df8fc
Mar  4 23:18:27.892: INFO: Got endpoints: latency-svc-48mhz [749.041879ms]
Mar  4 23:18:27.916: INFO: Created: latency-svc-srqjz
Mar  4 23:18:27.944: INFO: Got endpoints: latency-svc-tmcnc [747.967738ms]
Mar  4 23:18:27.967: INFO: Created: latency-svc-j6894
Mar  4 23:18:27.992: INFO: Got endpoints: latency-svc-z4tgj [750.530871ms]
Mar  4 23:18:28.012: INFO: Created: latency-svc-fnkhh
Mar  4 23:18:28.043: INFO: Got endpoints: latency-svc-m8tfm [743.619115ms]
Mar  4 23:18:28.065: INFO: Created: latency-svc-pn72s
Mar  4 23:18:28.093: INFO: Got endpoints: latency-svc-f8bcz [749.254267ms]
Mar  4 23:18:28.116: INFO: Created: latency-svc-nn9pr
Mar  4 23:18:28.143: INFO: Got endpoints: latency-svc-qx7gm [749.270176ms]
Mar  4 23:18:28.170: INFO: Created: latency-svc-bkv29
Mar  4 23:18:28.193: INFO: Got endpoints: latency-svc-pf26b [751.093535ms]
Mar  4 23:18:28.216: INFO: Created: latency-svc-xkf4g
Mar  4 23:18:28.241: INFO: Got endpoints: latency-svc-s7jlh [749.384795ms]
Mar  4 23:18:28.264: INFO: Created: latency-svc-x4nh6
Mar  4 23:18:28.292: INFO: Got endpoints: latency-svc-k8rzj [745.473418ms]
Mar  4 23:18:28.314: INFO: Created: latency-svc-gmjs9
Mar  4 23:18:28.343: INFO: Got endpoints: latency-svc-x9cr7 [750.291766ms]
Mar  4 23:18:28.364: INFO: Created: latency-svc-xrtpb
Mar  4 23:18:28.398: INFO: Got endpoints: latency-svc-2tz8k [756.142904ms]
Mar  4 23:18:28.420: INFO: Created: latency-svc-5wpt2
Mar  4 23:18:28.443: INFO: Got endpoints: latency-svc-dkt8q [746.11514ms]
Mar  4 23:18:28.467: INFO: Created: latency-svc-wlhhf
Mar  4 23:18:28.492: INFO: Got endpoints: latency-svc-r9lz6 [749.299689ms]
Mar  4 23:18:28.524: INFO: Created: latency-svc-9r2b2
Mar  4 23:18:28.547: INFO: Got endpoints: latency-svc-7hm5s [754.614412ms]
Mar  4 23:18:28.569: INFO: Created: latency-svc-28xpf
Mar  4 23:18:28.592: INFO: Got endpoints: latency-svc-df8fc [751.493245ms]
Mar  4 23:18:28.614: INFO: Created: latency-svc-ksdgm
Mar  4 23:18:28.645: INFO: Got endpoints: latency-svc-srqjz [752.677968ms]
Mar  4 23:18:28.667: INFO: Created: latency-svc-cc9zs
Mar  4 23:18:28.691: INFO: Got endpoints: latency-svc-j6894 [747.287599ms]
Mar  4 23:18:28.713: INFO: Created: latency-svc-qgx5p
Mar  4 23:18:28.743: INFO: Got endpoints: latency-svc-fnkhh [750.997617ms]
Mar  4 23:18:28.765: INFO: Created: latency-svc-cwgrk
Mar  4 23:18:28.792: INFO: Got endpoints: latency-svc-pn72s [748.7651ms]
Mar  4 23:18:28.813: INFO: Created: latency-svc-gjcsl
Mar  4 23:18:28.842: INFO: Got endpoints: latency-svc-nn9pr [748.638947ms]
Mar  4 23:18:28.865: INFO: Created: latency-svc-x8cb6
Mar  4 23:18:28.893: INFO: Got endpoints: latency-svc-bkv29 [750.184161ms]
Mar  4 23:18:28.915: INFO: Created: latency-svc-shrnv
Mar  4 23:18:28.942: INFO: Got endpoints: latency-svc-xkf4g [748.588889ms]
Mar  4 23:18:28.965: INFO: Created: latency-svc-9l8gh
Mar  4 23:18:28.992: INFO: Got endpoints: latency-svc-x4nh6 [751.291114ms]
Mar  4 23:18:29.014: INFO: Created: latency-svc-qqmz8
Mar  4 23:18:29.044: INFO: Got endpoints: latency-svc-gmjs9 [751.671051ms]
Mar  4 23:18:29.071: INFO: Created: latency-svc-ngmrr
Mar  4 23:18:29.092: INFO: Got endpoints: latency-svc-xrtpb [749.59409ms]
Mar  4 23:18:29.116: INFO: Created: latency-svc-7rcr6
Mar  4 23:18:29.142: INFO: Got endpoints: latency-svc-5wpt2 [744.190628ms]
Mar  4 23:18:29.165: INFO: Created: latency-svc-t7w22
Mar  4 23:18:29.193: INFO: Got endpoints: latency-svc-wlhhf [749.424962ms]
Mar  4 23:18:29.216: INFO: Created: latency-svc-r776h
Mar  4 23:18:29.242: INFO: Got endpoints: latency-svc-9r2b2 [749.906774ms]
Mar  4 23:18:29.264: INFO: Created: latency-svc-t7swc
Mar  4 23:18:29.293: INFO: Got endpoints: latency-svc-28xpf [745.730739ms]
Mar  4 23:18:29.314: INFO: Created: latency-svc-v8dmb
Mar  4 23:18:29.342: INFO: Got endpoints: latency-svc-ksdgm [748.971412ms]
Mar  4 23:18:29.364: INFO: Created: latency-svc-l7nnx
Mar  4 23:18:29.393: INFO: Got endpoints: latency-svc-cc9zs [748.148054ms]
Mar  4 23:18:29.416: INFO: Created: latency-svc-k8zgj
Mar  4 23:18:29.444: INFO: Got endpoints: latency-svc-qgx5p [752.651256ms]
Mar  4 23:18:29.466: INFO: Created: latency-svc-bn6dt
Mar  4 23:18:29.494: INFO: Got endpoints: latency-svc-cwgrk [750.117458ms]
Mar  4 23:18:29.517: INFO: Created: latency-svc-hhglh
Mar  4 23:18:29.542: INFO: Got endpoints: latency-svc-gjcsl [749.830938ms]
Mar  4 23:18:29.567: INFO: Created: latency-svc-g62lw
Mar  4 23:18:29.592: INFO: Got endpoints: latency-svc-x8cb6 [750.182837ms]
Mar  4 23:18:29.614: INFO: Created: latency-svc-tcfsh
Mar  4 23:18:29.642: INFO: Got endpoints: latency-svc-shrnv [748.797645ms]
Mar  4 23:18:29.664: INFO: Created: latency-svc-5lh6b
Mar  4 23:18:29.692: INFO: Got endpoints: latency-svc-9l8gh [749.013803ms]
Mar  4 23:18:29.712: INFO: Created: latency-svc-fxq82
Mar  4 23:18:29.742: INFO: Got endpoints: latency-svc-qqmz8 [749.860282ms]
Mar  4 23:18:29.764: INFO: Created: latency-svc-gk86v
Mar  4 23:18:29.792: INFO: Got endpoints: latency-svc-ngmrr [747.730693ms]
Mar  4 23:18:29.843: INFO: Got endpoints: latency-svc-7rcr6 [750.568231ms]
Mar  4 23:18:29.892: INFO: Got endpoints: latency-svc-t7w22 [749.253544ms]
Mar  4 23:18:29.942: INFO: Got endpoints: latency-svc-r776h [749.037237ms]
Mar  4 23:18:29.993: INFO: Got endpoints: latency-svc-t7swc [750.920476ms]
Mar  4 23:18:30.042: INFO: Got endpoints: latency-svc-v8dmb [749.352668ms]
Mar  4 23:18:30.092: INFO: Got endpoints: latency-svc-l7nnx [750.694119ms]
Mar  4 23:18:30.147: INFO: Got endpoints: latency-svc-k8zgj [753.42441ms]
Mar  4 23:18:30.148: INFO: Created: latency-svc-h6hsn
Mar  4 23:18:30.162: INFO: Created: latency-svc-4q6k9
Mar  4 23:18:30.175: INFO: Created: latency-svc-t67s9
Mar  4 23:18:30.188: INFO: Created: latency-svc-8zqz4
Mar  4 23:18:30.192: INFO: Got endpoints: latency-svc-bn6dt [748.183317ms]
Mar  4 23:18:30.200: INFO: Created: latency-svc-m5vk2
Mar  4 23:18:30.212: INFO: Created: latency-svc-bsdzx
Mar  4 23:18:30.224: INFO: Created: latency-svc-qxwgw
Mar  4 23:18:30.237: INFO: Created: latency-svc-66zh8
Mar  4 23:18:30.243: INFO: Got endpoints: latency-svc-hhglh [748.652703ms]
Mar  4 23:18:30.248: INFO: Created: latency-svc-24w4t
Mar  4 23:18:30.262: INFO: Created: latency-svc-vk554
Mar  4 23:18:30.291: INFO: Got endpoints: latency-svc-g62lw [749.016498ms]
Mar  4 23:18:30.316: INFO: Created: latency-svc-n5l5c
Mar  4 23:18:30.366: INFO: Got endpoints: latency-svc-tcfsh [774.134028ms]
Mar  4 23:18:30.395: INFO: Created: latency-svc-5mqlx
Mar  4 23:18:30.396: INFO: Got endpoints: latency-svc-5lh6b [753.672709ms]
Mar  4 23:18:30.420: INFO: Created: latency-svc-xlb75
Mar  4 23:18:30.441: INFO: Got endpoints: latency-svc-fxq82 [749.79932ms]
Mar  4 23:18:30.463: INFO: Created: latency-svc-8mhwp
Mar  4 23:18:30.495: INFO: Got endpoints: latency-svc-gk86v [752.23876ms]
Mar  4 23:18:30.516: INFO: Created: latency-svc-52sbq
Mar  4 23:18:30.542: INFO: Got endpoints: latency-svc-h6hsn [750.236924ms]
Mar  4 23:18:30.592: INFO: Got endpoints: latency-svc-4q6k9 [749.332799ms]
Mar  4 23:18:30.645: INFO: Got endpoints: latency-svc-t67s9 [753.051091ms]
Mar  4 23:18:30.691: INFO: Got endpoints: latency-svc-8zqz4 [748.756126ms]
Mar  4 23:18:30.743: INFO: Got endpoints: latency-svc-m5vk2 [749.301135ms]
Mar  4 23:18:30.799: INFO: Got endpoints: latency-svc-bsdzx [757.457138ms]
Mar  4 23:18:30.843: INFO: Got endpoints: latency-svc-qxwgw [750.168794ms]
Mar  4 23:18:30.892: INFO: Got endpoints: latency-svc-66zh8 [744.764857ms]
Mar  4 23:18:30.942: INFO: Got endpoints: latency-svc-24w4t [749.579984ms]
Mar  4 23:18:30.996: INFO: Got endpoints: latency-svc-vk554 [753.649745ms]
Mar  4 23:18:31.043: INFO: Got endpoints: latency-svc-n5l5c [751.150473ms]
Mar  4 23:18:31.092: INFO: Got endpoints: latency-svc-5mqlx [726.224735ms]
Mar  4 23:18:31.142: INFO: Got endpoints: latency-svc-xlb75 [746.058085ms]
Mar  4 23:18:31.193: INFO: Got endpoints: latency-svc-8mhwp [751.131459ms]
Mar  4 23:18:31.241: INFO: Got endpoints: latency-svc-52sbq [746.621845ms]
Mar  4 23:18:31.242: INFO: Latencies: [29.086083ms 41.783995ms 53.523781ms 65.873064ms 76.242144ms 93.030598ms 100.738939ms 113.089878ms 124.052103ms 140.323088ms 150.785093ms 162.503332ms 173.546095ms 184.691714ms 187.817683ms 200.314889ms 201.878079ms 202.032392ms 203.317371ms 203.410467ms 203.504213ms 203.912308ms 204.092925ms 204.486036ms 204.487985ms 204.995205ms 205.450405ms 205.578345ms 207.664241ms 207.816397ms 209.462196ms 209.964943ms 210.663172ms 212.124782ms 214.464671ms 217.594134ms 219.313824ms 223.137349ms 226.786698ms 226.80144ms 227.748996ms 249.81158ms 286.915543ms 324.02506ms 357.557484ms 399.561665ms 436.048679ms 472.53132ms 512.130774ms 551.052547ms 588.550652ms 627.697899ms 667.603529ms 704.131939ms 726.224735ms 740.547259ms 740.664661ms 743.619115ms 744.190628ms 744.393525ms 744.764857ms 745.473418ms 745.730739ms 746.058085ms 746.11514ms 746.313332ms 746.361911ms 746.621845ms 747.287599ms 747.604695ms 747.730693ms 747.967738ms 748.148054ms 748.166441ms 748.183317ms 748.496756ms 748.557675ms 748.588889ms 748.638947ms 748.652703ms 748.701303ms 748.733918ms 748.756126ms 748.7651ms 748.793002ms 748.797645ms 748.971412ms 749.013803ms 749.016498ms 749.033447ms 749.037237ms 749.041879ms 749.180416ms 749.236844ms 749.253544ms 749.254267ms 749.270176ms 749.299689ms 749.300555ms 749.301135ms 749.332799ms 749.336764ms 749.340631ms 749.352668ms 749.38119ms 749.384795ms 749.422799ms 749.424962ms 749.426589ms 749.453416ms 749.473297ms 749.48127ms 749.492969ms 749.516116ms 749.542925ms 749.579984ms 749.59409ms 749.620089ms 749.635025ms 749.645586ms 749.674927ms 749.743693ms 749.752224ms 749.781566ms 749.783887ms 749.797629ms 749.79932ms 749.830938ms 749.833068ms 749.860282ms 749.906774ms 749.91991ms 749.942116ms 749.992494ms 750.012064ms 750.042172ms 750.042689ms 750.0913ms 750.109157ms 750.117458ms 750.168794ms 750.182837ms 750.184161ms 750.189232ms 750.216255ms 750.231877ms 750.236924ms 750.291766ms 750.295102ms 750.322283ms 750.387331ms 750.398302ms 750.435214ms 750.43705ms 750.459581ms 750.483578ms 750.497742ms 750.530871ms 750.568231ms 750.623481ms 750.655388ms 750.687828ms 750.694119ms 750.764641ms 750.805767ms 750.876204ms 750.920476ms 750.959424ms 750.970962ms 750.997617ms 751.093535ms 751.131459ms 751.150473ms 751.154138ms 751.21386ms 751.291114ms 751.362273ms 751.428518ms 751.493245ms 751.515834ms 751.671051ms 751.848444ms 752.23876ms 752.651256ms 752.677968ms 752.79233ms 753.051091ms 753.42441ms 753.649745ms 753.672709ms 753.728386ms 753.979032ms 754.585716ms 754.614412ms 754.898283ms 756.142904ms 757.457138ms 757.573835ms 759.618777ms 774.134028ms]
Mar  4 23:18:31.242: INFO: 50 %ile: 749.332799ms
Mar  4 23:18:31.242: INFO: 90 %ile: 751.671051ms
Mar  4 23:18:31.242: INFO: 99 %ile: 759.618777ms
Mar  4 23:18:31.242: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:18:31.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-wvcl6" for this suite.
Mar  4 23:18:49.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:18:49.353: INFO: namespace: e2e-tests-svc-latency-wvcl6, resource: bindings, ignored listing per whitelist
Mar  4 23:18:49.757: INFO: namespace e2e-tests-svc-latency-wvcl6 deletion completed in 18.506282644s

• [SLOW TEST:29.571 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:18:49.757: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-8572r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8572r
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar  4 23:18:50.138: INFO: Found 0 stateful pods, waiting for 3
Mar  4 23:19:00.147: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:19:00.147: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:19:00.147: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  4 23:19:00.201: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  4 23:19:10.267: INFO: Updating stateful set ss2
Mar  4 23:19:10.283: INFO: Waiting for Pod e2e-tests-statefulset-8572r/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar  4 23:19:20.363: INFO: Found 1 stateful pods, waiting for 3
Mar  4 23:19:30.373: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:19:30.373: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:19:30.373: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  4 23:19:30.424: INFO: Updating stateful set ss2
Mar  4 23:19:30.502: INFO: Waiting for Pod e2e-tests-statefulset-8572r/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:19:40.550: INFO: Updating stateful set ss2
Mar  4 23:19:40.565: INFO: Waiting for StatefulSet e2e-tests-statefulset-8572r/ss2 to complete update
Mar  4 23:19:40.566: INFO: Waiting for Pod e2e-tests-statefulset-8572r/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:19:50.583: INFO: Waiting for StatefulSet e2e-tests-statefulset-8572r/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  4 23:20:00.583: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8572r
Mar  4 23:20:00.590: INFO: Scaling statefulset ss2 to 0
Mar  4 23:20:10.643: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 23:20:10.653: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:20:10.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8572r" for this suite.
Mar  4 23:20:18.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:20:18.831: INFO: namespace: e2e-tests-statefulset-8572r, resource: bindings, ignored listing per whitelist
Mar  4 23:20:19.072: INFO: namespace e2e-tests-statefulset-8572r deletion completed in 8.371878101s

• [SLOW TEST:89.315 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:20:19.075: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nh42c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  4 23:20:19.388: INFO: Waiting up to 5m0s for pod "pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-nh42c" to be "success or failure"
Mar  4 23:20:19.398: INFO: Pod "pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.713577ms
Mar  4 23:20:21.407: INFO: Pod "pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018688727s
Mar  4 23:20:23.420: INFO: Pod "pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03141918s
STEP: Saw pod success
Mar  4 23:20:23.420: INFO: Pod "pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:20:23.432: INFO: Trying to get logs from node 10.190.208.161 pod pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:20:23.532: INFO: Waiting for pod pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:20:23.541: INFO: Pod pod-138fbeeb-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:20:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nh42c" for this suite.
Mar  4 23:20:29.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:20:29.788: INFO: namespace: e2e-tests-emptydir-nh42c, resource: bindings, ignored listing per whitelist
Mar  4 23:20:29.949: INFO: namespace e2e-tests-emptydir-nh42c deletion completed in 6.398027874s

• [SLOW TEST:10.875 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:20:29.950: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-p4tpx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:20:30.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-p4tpx" to be "success or failure"
Mar  4 23:20:30.256: INFO: Pod "downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.221481ms
Mar  4 23:20:32.267: INFO: Pod "downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020584293s
STEP: Saw pod success
Mar  4 23:20:32.267: INFO: Pod "downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:20:32.276: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:20:32.339: INFO: Waiting for pod downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:20:32.347: INFO: Pod downwardapi-volume-1a08e00d-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:20:32.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p4tpx" for this suite.
Mar  4 23:20:38.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:20:38.946: INFO: namespace: e2e-tests-downward-api-p4tpx, resource: bindings, ignored listing per whitelist
Mar  4 23:20:39.117: INFO: namespace e2e-tests-downward-api-p4tpx deletion completed in 6.759458866s

• [SLOW TEST:9.167 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:20:39.117: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-wf7nn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar  4 23:20:39.413: INFO: Waiting up to 5m0s for pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-containers-wf7nn" to be "success or failure"
Mar  4 23:20:39.422: INFO: Pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.96255ms
Mar  4 23:20:41.431: INFO: Pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01755169s
Mar  4 23:20:43.442: INFO: Pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029059823s
Mar  4 23:20:45.450: INFO: Pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037150649s
STEP: Saw pod success
Mar  4 23:20:45.450: INFO: Pod "client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:20:45.458: INFO: Trying to get logs from node 10.190.208.159 pod client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:20:45.510: INFO: Waiting for pod client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:20:45.518: INFO: Pod client-containers-1f7fc014-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:20:45.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wf7nn" for this suite.
Mar  4 23:20:51.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:20:51.733: INFO: namespace: e2e-tests-containers-wf7nn, resource: bindings, ignored listing per whitelist
Mar  4 23:20:51.855: INFO: namespace e2e-tests-containers-wf7nn deletion completed in 6.326101702s

• [SLOW TEST:12.738 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:20:51.856: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hcd9s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  4 23:20:52.220: INFO: Waiting up to 5m0s for pod "downward-api-27222585-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-hcd9s" to be "success or failure"
Mar  4 23:20:52.228: INFO: Pod "downward-api-27222585-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.699597ms
Mar  4 23:20:54.237: INFO: Pod "downward-api-27222585-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016837089s
STEP: Saw pod success
Mar  4 23:20:54.237: INFO: Pod "downward-api-27222585-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:20:54.247: INFO: Trying to get logs from node 10.190.208.161 pod downward-api-27222585-3ed4-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  4 23:20:54.301: INFO: Waiting for pod downward-api-27222585-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:20:54.309: INFO: Pod downward-api-27222585-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:20:54.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hcd9s" for this suite.
Mar  4 23:21:00.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:00.693: INFO: namespace: e2e-tests-downward-api-hcd9s, resource: bindings, ignored listing per whitelist
Mar  4 23:21:00.724: INFO: namespace e2e-tests-downward-api-hcd9s deletion completed in 6.389822067s

• [SLOW TEST:8.868 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:00.725: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5mbfp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:21:01.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-5mbfp" to be "success or failure"
Mar  4 23:21:01.142: INFO: Pod "downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.590096ms
Mar  4 23:21:03.152: INFO: Pod "downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01893961s
STEP: Saw pod success
Mar  4 23:21:03.153: INFO: Pod "downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:21:03.160: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:21:03.206: INFO: Waiting for pod downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:21:03.214: INFO: Pod downwardapi-volume-2c722b2c-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:21:03.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5mbfp" for this suite.
Mar  4 23:21:09.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:09.320: INFO: namespace: e2e-tests-projected-5mbfp, resource: bindings, ignored listing per whitelist
Mar  4 23:21:09.556: INFO: namespace e2e-tests-projected-5mbfp deletion completed in 6.331503123s

• [SLOW TEST:8.832 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:09.557: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-6hqnh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:21:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-6hqnh" for this suite.
Mar  4 23:21:15.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:16.275: INFO: namespace: e2e-tests-services-6hqnh, resource: bindings, ignored listing per whitelist
Mar  4 23:21:16.307: INFO: namespace e2e-tests-services-6hqnh deletion completed in 6.391251159s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.750 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:16.307: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-ghhwf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  4 23:21:16.609: INFO: Waiting up to 5m0s for pod "pod-35abaa79-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-ghhwf" to be "success or failure"
Mar  4 23:21:16.620: INFO: Pod "pod-35abaa79-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.406209ms
Mar  4 23:21:18.629: INFO: Pod "pod-35abaa79-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019436915s
STEP: Saw pod success
Mar  4 23:21:18.629: INFO: Pod "pod-35abaa79-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:21:18.637: INFO: Trying to get logs from node 10.190.208.161 pod pod-35abaa79-3ed4-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:21:18.687: INFO: Waiting for pod pod-35abaa79-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:21:18.697: INFO: Pod pod-35abaa79-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:21:18.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ghhwf" for this suite.
Mar  4 23:21:24.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:24.857: INFO: namespace: e2e-tests-emptydir-ghhwf, resource: bindings, ignored listing per whitelist
Mar  4 23:21:25.060: INFO: namespace e2e-tests-emptydir-ghhwf deletion completed in 6.352984156s

• [SLOW TEST:8.753 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:25.060: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-85qkt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  4 23:21:25.416: INFO: Waiting up to 5m0s for pod "pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-85qkt" to be "success or failure"
Mar  4 23:21:25.423: INFO: Pod "pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.763997ms
Mar  4 23:21:27.433: INFO: Pod "pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017046784s
Mar  4 23:21:29.441: INFO: Pod "pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025645364s
STEP: Saw pod success
Mar  4 23:21:29.441: INFO: Pod "pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:21:29.448: INFO: Trying to get logs from node 10.190.208.159 pod pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:21:29.499: INFO: Waiting for pod pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:21:29.507: INFO: Pod pod-3ae3e885-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:21:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-85qkt" for this suite.
Mar  4 23:21:35.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:35.813: INFO: namespace: e2e-tests-emptydir-85qkt, resource: bindings, ignored listing per whitelist
Mar  4 23:21:35.937: INFO: namespace e2e-tests-emptydir-85qkt deletion completed in 6.419763851s

• [SLOW TEST:10.877 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:35.938: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8v776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:21:36.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-8v776" to be "success or failure"
Mar  4 23:21:36.332: INFO: Pod "downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.523597ms
Mar  4 23:21:38.340: INFO: Pod "downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017958935s
STEP: Saw pod success
Mar  4 23:21:38.340: INFO: Pod "downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:21:38.350: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:21:38.392: INFO: Waiting for pod downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:21:38.400: INFO: Pod downwardapi-volume-416b6530-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:21:38.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8v776" for this suite.
Mar  4 23:21:44.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:21:44.794: INFO: namespace: e2e-tests-projected-8v776, resource: bindings, ignored listing per whitelist
Mar  4 23:21:44.814: INFO: namespace e2e-tests-projected-8v776 deletion completed in 6.396925477s

• [SLOW TEST:8.877 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:21:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-p75zs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  4 23:21:46.208: INFO: Pod name wrapped-volume-race-474e5247-3ed4-11e9-8a62-3ec24305971a: Found 0 pods out of 5
Mar  4 23:21:51.381: INFO: Pod name wrapped-volume-race-474e5247-3ed4-11e9-8a62-3ec24305971a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-474e5247-3ed4-11e9-8a62-3ec24305971a in namespace e2e-tests-emptydir-wrapper-p75zs, will wait for the garbage collector to delete the pods
Mar  4 23:22:01.600: INFO: Deleting ReplicationController wrapped-volume-race-474e5247-3ed4-11e9-8a62-3ec24305971a took: 16.759751ms
Mar  4 23:22:01.700: INFO: Terminating ReplicationController wrapped-volume-race-474e5247-3ed4-11e9-8a62-3ec24305971a pods took: 100.227913ms
STEP: Creating RC which spawns configmap-volume pods
Mar  4 23:22:41.113: INFO: Pod name wrapped-volume-race-67fb1837-3ed4-11e9-8a62-3ec24305971a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-67fb1837-3ed4-11e9-8a62-3ec24305971a in namespace e2e-tests-emptydir-wrapper-p75zs, will wait for the garbage collector to delete the pods
Mar  4 23:22:57.247: INFO: Deleting ReplicationController wrapped-volume-race-67fb1837-3ed4-11e9-8a62-3ec24305971a took: 16.509344ms
Mar  4 23:22:57.347: INFO: Terminating ReplicationController wrapped-volume-race-67fb1837-3ed4-11e9-8a62-3ec24305971a pods took: 100.215542ms
STEP: Creating RC which spawns configmap-volume pods
Mar  4 23:23:41.609: INFO: Pod name wrapped-volume-race-8c11cd86-3ed4-11e9-8a62-3ec24305971a: Found 1 pods out of 5
Mar  4 23:23:46.621: INFO: Pod name wrapped-volume-race-8c11cd86-3ed4-11e9-8a62-3ec24305971a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8c11cd86-3ed4-11e9-8a62-3ec24305971a in namespace e2e-tests-emptydir-wrapper-p75zs, will wait for the garbage collector to delete the pods
Mar  4 23:23:58.798: INFO: Deleting ReplicationController wrapped-volume-race-8c11cd86-3ed4-11e9-8a62-3ec24305971a took: 15.70391ms
Mar  4 23:23:58.898: INFO: Terminating ReplicationController wrapped-volume-race-8c11cd86-3ed4-11e9-8a62-3ec24305971a pods took: 100.202085ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:24:42.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-p75zs" for this suite.
Mar  4 23:24:52.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:24:52.577: INFO: namespace: e2e-tests-emptydir-wrapper-p75zs, resource: bindings, ignored listing per whitelist
Mar  4 23:24:52.661: INFO: namespace e2e-tests-emptydir-wrapper-p75zs deletion completed in 10.382088143s

• [SLOW TEST:187.846 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:24:52.661: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-s7pkg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:24:53.100: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-s7pkg" to be "success or failure"
Mar  4 23:24:53.109: INFO: Pod "downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051629ms
Mar  4 23:24:55.118: INFO: Pod "downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01835349s
STEP: Saw pod success
Mar  4 23:24:55.118: INFO: Pod "downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:24:55.127: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:24:55.200: INFO: Waiting for pod downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a to disappear
Mar  4 23:24:55.208: INFO: Pod downwardapi-volume-b6a95a97-3ed4-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:24:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-s7pkg" for this suite.
Mar  4 23:25:01.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:25:01.327: INFO: namespace: e2e-tests-downward-api-s7pkg, resource: bindings, ignored listing per whitelist
Mar  4 23:25:01.553: INFO: namespace e2e-tests-downward-api-s7pkg deletion completed in 6.33581803s

• [SLOW TEST:8.892 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:25:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-5d2pj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:25:03.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-5d2pj" for this suite.
Mar  4 23:25:44.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:25:44.204: INFO: namespace: e2e-tests-kubelet-test-5d2pj, resource: bindings, ignored listing per whitelist
Mar  4 23:25:44.356: INFO: namespace e2e-tests-kubelet-test-5d2pj deletion completed in 40.355760209s

• [SLOW TEST:42.803 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:25:44.359: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bts84
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-d5797548-3ed4-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:25:48.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bts84" for this suite.
Mar  4 23:26:12.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:26:13.072: INFO: namespace: e2e-tests-configmap-bts84, resource: bindings, ignored listing per whitelist
Mar  4 23:26:13.256: INFO: namespace e2e-tests-configmap-bts84 deletion completed in 24.388639925s

• [SLOW TEST:28.897 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:26:13.257: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ndc2f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-ndc2f
Mar  4 23:26:17.572: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-ndc2f
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 23:26:17.580: INFO: Initial restart count of pod liveness-http is 0
Mar  4 23:26:35.674: INFO: Restart count of pod e2e-tests-container-probe-ndc2f/liveness-http is now 1 (18.093665238s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:26:35.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ndc2f" for this suite.
Mar  4 23:26:41.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:26:42.311: INFO: namespace: e2e-tests-container-probe-ndc2f, resource: bindings, ignored listing per whitelist
Mar  4 23:26:42.320: INFO: namespace e2e-tests-container-probe-ndc2f deletion completed in 6.610658281s

• [SLOW TEST:29.064 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:26:42.323: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-x5z29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  4 23:26:45.179: INFO: Successfully updated pod "pod-update-f7fd3e57-3ed4-11e9-8a62-3ec24305971a"
STEP: verifying the updated pod is in kubernetes
Mar  4 23:26:45.196: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:26:45.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-x5z29" for this suite.
Mar  4 23:27:09.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:27:09.421: INFO: namespace: e2e-tests-pods-x5z29, resource: bindings, ignored listing per whitelist
Mar  4 23:27:09.728: INFO: namespace e2e-tests-pods-x5z29 deletion completed in 24.521752166s

• [SLOW TEST:27.405 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:27:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-7dgf7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:27:10.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7dgf7" for this suite.
Mar  4 23:27:32.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:27:32.541: INFO: namespace: e2e-tests-pods-7dgf7, resource: bindings, ignored listing per whitelist
Mar  4 23:27:32.743: INFO: namespace e2e-tests-pods-7dgf7 deletion completed in 22.686657015s

• [SLOW TEST:23.014 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:27:32.745: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-b4tbk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-160dcd66-3ed5-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:27:33.117: INFO: Waiting up to 5m0s for pod "pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-b4tbk" to be "success or failure"
Mar  4 23:27:33.125: INFO: Pod "pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.209704ms
Mar  4 23:27:35.134: INFO: Pod "pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016900357s
STEP: Saw pod success
Mar  4 23:27:35.134: INFO: Pod "pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:27:35.141: INFO: Trying to get logs from node 10.190.208.159 pod pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:27:35.222: INFO: Waiting for pod pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a to disappear
Mar  4 23:27:35.230: INFO: Pod pod-secrets-1615e7c0-3ed5-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:27:35.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-b4tbk" for this suite.
Mar  4 23:27:41.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:27:41.466: INFO: namespace: e2e-tests-secrets-b4tbk, resource: bindings, ignored listing per whitelist
Mar  4 23:27:41.712: INFO: namespace e2e-tests-secrets-b4tbk deletion completed in 6.41241347s

• [SLOW TEST:8.967 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:27:41.713: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-hsvfz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:28:42.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hsvfz" for this suite.
Mar  4 23:29:06.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:29:06.651: INFO: namespace: e2e-tests-container-probe-hsvfz, resource: bindings, ignored listing per whitelist
Mar  4 23:29:06.861: INFO: namespace e2e-tests-container-probe-hsvfz deletion completed in 24.724654837s

• [SLOW TEST:85.147 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:29:06.863: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-gcdzb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:29:11.227: INFO: Waiting up to 5m0s for pod "client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a" in namespace "e2e-tests-pods-gcdzb" to be "success or failure"
Mar  4 23:29:11.238: INFO: Pod "client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.418345ms
Mar  4 23:29:13.246: INFO: Pod "client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019042753s
STEP: Saw pod success
Mar  4 23:29:13.247: INFO: Pod "client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:29:13.255: INFO: Trying to get logs from node 10.190.208.161 pod client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a container env3cont: <nil>
STEP: delete the pod
Mar  4 23:29:13.301: INFO: Waiting for pod client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a to disappear
Mar  4 23:29:13.309: INFO: Pod client-envvars-50919c2f-3ed5-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:29:13.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gcdzb" for this suite.
Mar  4 23:30:03.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:30:03.739: INFO: namespace: e2e-tests-pods-gcdzb, resource: bindings, ignored listing per whitelist
Mar  4 23:30:03.921: INFO: namespace e2e-tests-pods-gcdzb deletion completed in 50.600036625s

• [SLOW TEST:57.058 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:30:03.921: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-b78w9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-29jds
STEP: Creating secret with name secret-test-702763b6-3ed5-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:30:04.420: INFO: Waiting up to 5m0s for pod "pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-b78w9" to be "success or failure"
Mar  4 23:30:04.432: INFO: Pod "pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.540997ms
Mar  4 23:30:06.442: INFO: Pod "pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021588615s
STEP: Saw pod success
Mar  4 23:30:06.442: INFO: Pod "pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:30:06.450: INFO: Trying to get logs from node 10.190.208.159 pod pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:30:06.502: INFO: Waiting for pod pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a to disappear
Mar  4 23:30:06.509: INFO: Pod pod-secrets-70451df1-3ed5-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:30:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-b78w9" for this suite.
Mar  4 23:30:12.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:30:12.771: INFO: namespace: e2e-tests-secrets-b78w9, resource: bindings, ignored listing per whitelist
Mar  4 23:30:13.022: INFO: namespace e2e-tests-secrets-b78w9 deletion completed in 6.501974085s
STEP: Destroying namespace "e2e-tests-secret-namespace-29jds" for this suite.
Mar  4 23:30:19.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:30:19.137: INFO: namespace: e2e-tests-secret-namespace-29jds, resource: bindings, ignored listing per whitelist
Mar  4 23:30:19.450: INFO: namespace e2e-tests-secret-namespace-29jds deletion completed in 6.427411825s

• [SLOW TEST:15.529 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:30:19.450: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-jfr77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-jfr77
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 23:30:19.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 23:30:45.976: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.36:8080/dial?request=hostName&protocol=udp&host=172.30.111.35&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-jfr77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:30:45.976: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:30:46.202: INFO: Waiting for endpoints: map[]
Mar  4 23:30:46.210: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.36:8080/dial?request=hostName&protocol=udp&host=172.30.252.209&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-jfr77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:30:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:30:46.483: INFO: Waiting for endpoints: map[]
Mar  4 23:30:46.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.36:8080/dial?request=hostName&protocol=udp&host=172.30.189.236&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-jfr77 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:30:46.491: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:30:46.672: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:30:46.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-jfr77" for this suite.
Mar  4 23:31:10.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:31:10.871: INFO: namespace: e2e-tests-pod-network-test-jfr77, resource: bindings, ignored listing per whitelist
Mar  4 23:31:11.047: INFO: namespace e2e-tests-pod-network-test-jfr77 deletion completed in 24.363258151s

• [SLOW TEST:51.597 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:31:11.048: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-sgjbs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  4 23:31:11.333: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:31:14.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-sgjbs" for this suite.
Mar  4 23:31:20.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:31:20.855: INFO: namespace: e2e-tests-init-container-sgjbs, resource: bindings, ignored listing per whitelist
Mar  4 23:31:21.079: INFO: namespace e2e-tests-init-container-sgjbs deletion completed in 6.353380675s

• [SLOW TEST:10.031 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:31:21.081: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-p6j9d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-p6j9d
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar  4 23:31:21.417: INFO: Found 0 stateful pods, waiting for 3
Mar  4 23:31:31.427: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:31:31.427: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:31:31.427: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 23:31:31.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-p6j9d ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:31:31.790: INFO: stderr: ""
Mar  4 23:31:31.790: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:31:31.790: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar  4 23:31:41.865: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  4 23:31:51.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-p6j9d ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:31:52.263: INFO: stderr: ""
Mar  4 23:31:52.263: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  4 23:31:52.263: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  4 23:32:02.311: INFO: Waiting for StatefulSet e2e-tests-statefulset-p6j9d/ss2 to complete update
Mar  4 23:32:02.311: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:32:02.311: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:32:02.311: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:32:12.329: INFO: Waiting for StatefulSet e2e-tests-statefulset-p6j9d/ss2 to complete update
Mar  4 23:32:12.329: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:32:12.329: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar  4 23:32:22.328: INFO: Waiting for StatefulSet e2e-tests-statefulset-p6j9d/ss2 to complete update
Mar  4 23:32:22.328: INFO: Waiting for Pod e2e-tests-statefulset-p6j9d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Mar  4 23:32:32.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-p6j9d ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  4 23:32:33.016: INFO: stderr: ""
Mar  4 23:32:33.016: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  4 23:32:33.016: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  4 23:32:43.218: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  4 23:32:53.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-p6j9d ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  4 23:32:53.640: INFO: stderr: ""
Mar  4 23:32:53.640: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  4 23:32:53.640: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  4 23:33:04.016: INFO: Waiting for StatefulSet e2e-tests-statefulset-p6j9d/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  4 23:33:14.033: INFO: Deleting all statefulset in ns e2e-tests-statefulset-p6j9d
Mar  4 23:33:14.041: INFO: Scaling statefulset ss2 to 0
Mar  4 23:33:44.125: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 23:33:44.133: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:33:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-p6j9d" for this suite.
Mar  4 23:33:52.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:33:53.103: INFO: namespace: e2e-tests-statefulset-p6j9d, resource: bindings, ignored listing per whitelist
Mar  4 23:33:53.140: INFO: namespace e2e-tests-statefulset-p6j9d deletion completed in 8.938129884s

• [SLOW TEST:152.059 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:33:53.140: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nhpsl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:33:53.517: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-nhpsl" to be "success or failure"
Mar  4 23:33:53.526: INFO: Pod "downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.548769ms
Mar  4 23:33:56.007: INFO: Pod "downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.489532755s
STEP: Saw pod success
Mar  4 23:33:56.007: INFO: Pod "downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:33:56.015: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:33:56.061: INFO: Waiting for pod downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a to disappear
Mar  4 23:33:56.070: INFO: Pod downwardapi-volume-f8d27a32-3ed5-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:33:56.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nhpsl" for this suite.
Mar  4 23:34:02.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:34:02.247: INFO: namespace: e2e-tests-projected-nhpsl, resource: bindings, ignored listing per whitelist
Mar  4 23:34:02.457: INFO: namespace e2e-tests-projected-nhpsl deletion completed in 6.377784028s

• [SLOW TEST:9.317 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:34:02.458: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-kj7tn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar  4 23:34:02.800: INFO: Waiting up to 5m0s for pod "client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a" in namespace "e2e-tests-containers-kj7tn" to be "success or failure"
Mar  4 23:34:02.809: INFO: Pod "client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.501843ms
Mar  4 23:34:04.818: INFO: Pod "client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018282041s
STEP: Saw pod success
Mar  4 23:34:04.818: INFO: Pod "client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:34:04.826: INFO: Trying to get logs from node 10.190.208.159 pod client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:34:04.927: INFO: Waiting for pod client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a to disappear
Mar  4 23:34:04.935: INFO: Pod client-containers-fe53be4b-3ed5-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:34:04.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-kj7tn" for this suite.
Mar  4 23:34:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:34:11.001: INFO: namespace: e2e-tests-containers-kj7tn, resource: bindings, ignored listing per whitelist
Mar  4 23:34:11.607: INFO: namespace e2e-tests-containers-kj7tn deletion completed in 6.661358817s

• [SLOW TEST:9.150 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:34:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-f8wmt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-03d66b64-3ed6-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:34:12.007: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-f8wmt" to be "success or failure"
Mar  4 23:34:12.015: INFO: Pod "pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.235741ms
Mar  4 23:34:14.023: INFO: Pod "pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016710453s
Mar  4 23:34:16.032: INFO: Pod "pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025155052s
STEP: Saw pod success
Mar  4 23:34:16.032: INFO: Pod "pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:34:16.040: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:34:16.089: INFO: Waiting for pod pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a to disappear
Mar  4 23:34:16.099: INFO: Pod pod-projected-secrets-03d790d7-3ed6-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:34:16.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f8wmt" for this suite.
Mar  4 23:34:22.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:34:22.267: INFO: namespace: e2e-tests-projected-f8wmt, resource: bindings, ignored listing per whitelist
Mar  4 23:34:22.525: INFO: namespace e2e-tests-projected-f8wmt deletion completed in 6.41501551s

• [SLOW TEST:10.917 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:34:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-76pz6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-76pz6.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-76pz6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-76pz6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-76pz6.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-76pz6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-76pz6.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 23:34:37.288: INFO: DNS probes using e2e-tests-dns-76pz6/dns-test-0a5beaed-3ed6-11e9-8a62-3ec24305971a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:34:37.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-76pz6" for this suite.
Mar  4 23:34:43.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:34:43.581: INFO: namespace: e2e-tests-dns-76pz6, resource: bindings, ignored listing per whitelist
Mar  4 23:34:44.128: INFO: namespace e2e-tests-dns-76pz6 deletion completed in 6.804710133s

• [SLOW TEST:21.604 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:34:44.129: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-97nts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  4 23:34:48.464: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-172b8e82-3ed6-11e9-8a62-3ec24305971a,GenerateName:,Namespace:e2e-tests-events-97nts,SelfLink:/api/v1/namespaces/e2e-tests-events-97nts/pods/send-events-172b8e82-3ed6-11e9-8a62-3ec24305971a,UID:172c8b69-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:20491,Generation:0,CreationTimestamp:2019-03-04 23:34:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 416480432,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-qb2cg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qb2cg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-qb2cg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ce39c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ce39e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:34:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:34:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:34:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:34:44 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.43,StartTime:2019-03-04 23:34:44 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-04 23:34:47 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://1b5fa7da00ebaf2d8ae6452f88cffa6704b0627c4cd931db4ab46455b9294439}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  4 23:34:50.473: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  4 23:34:52.483: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:34:52.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-97nts" for this suite.
Mar  4 23:35:32.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:35:32.658: INFO: namespace: e2e-tests-events-97nts, resource: bindings, ignored listing per whitelist
Mar  4 23:35:32.852: INFO: namespace e2e-tests-events-97nts deletion completed in 40.336278025s

• [SLOW TEST:48.723 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:35:32.852: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-mz7wc
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:35:33.147: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:35:33.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-mz7wc" for this suite.
Mar  4 23:35:39.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:35:40.034: INFO: namespace: e2e-tests-custom-resource-definition-mz7wc, resource: bindings, ignored listing per whitelist
Mar  4 23:35:40.154: INFO: namespace e2e-tests-custom-resource-definition-mz7wc deletion completed in 6.292159523s

• [SLOW TEST:7.302 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:35:40.155: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-2tt6n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:35:40.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2tt6n" for this suite.
Mar  4 23:35:46.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:35:46.829: INFO: namespace: e2e-tests-kubelet-test-2tt6n, resource: bindings, ignored listing per whitelist
Mar  4 23:35:46.864: INFO: namespace e2e-tests-kubelet-test-2tt6n deletion completed in 6.364552839s

• [SLOW TEST:6.709 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:35:46.865: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-9frtq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-9frtq;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-9frtq;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-9frtq.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-9frtq.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 73.203.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.203.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.203.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.203.73_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-9frtq;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-9frtq;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-9frtq.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-9frtq.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-9frtq.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-9frtq.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-9frtq.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 73.203.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.203.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.203.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.203.73_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 23:35:59.283: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.468: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.482: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.495: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.509: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.522: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.534: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.547: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.559: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:35:59.635: INFO: Lookups using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-9frtq jessie_tcp@dns-test-service.e2e-tests-dns-9frtq jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc]

Mar  4 23:36:04.664: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.900: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.913: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.926: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.938: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.950: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.967: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.980: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:04.995: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:05.081: INFO: Lookups using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-9frtq jessie_tcp@dns-test-service.e2e-tests-dns-9frtq jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc]

Mar  4 23:36:09.740: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:09.918: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:09.931: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:09.999: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.012: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.025: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.038: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.052: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.065: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:10.158: INFO: Lookups using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-9frtq jessie_tcp@dns-test-service.e2e-tests-dns-9frtq jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc]

Mar  4 23:36:14.663: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.834: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.850: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.864: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.876: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.887: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.905: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.917: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:14.929: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:15.003: INFO: Lookups using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-9frtq jessie_tcp@dns-test-service.e2e-tests-dns-9frtq jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc]

Mar  4 23:36:19.662: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.823: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.835: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.848: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.864: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.876: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.888: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.900: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.913: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc from pod e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a: the server could not find the requested resource (get pods dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a)
Mar  4 23:36:19.989: INFO: Lookups using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-9frtq jessie_tcp@dns-test-service.e2e-tests-dns-9frtq jessie_udp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@dns-test-service.e2e-tests-dns-9frtq.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-9frtq.svc]

Mar  4 23:36:25.009: INFO: DNS probes using e2e-tests-dns-9frtq/dns-test-3c96dac2-3ed6-11e9-8a62-3ec24305971a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:36:25.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-9frtq" for this suite.
Mar  4 23:36:31.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:36:31.403: INFO: namespace: e2e-tests-dns-9frtq, resource: bindings, ignored listing per whitelist
Mar  4 23:36:31.474: INFO: namespace e2e-tests-dns-9frtq deletion completed in 6.346389479s

• [SLOW TEST:44.610 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:36:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-cvxhd
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  4 23:36:31.824: INFO: Waiting up to 5m0s for pod "pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-cvxhd" to be "success or failure"
Mar  4 23:36:31.834: INFO: Pod "pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.793936ms
Mar  4 23:36:33.843: INFO: Pod "pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019284682s
STEP: Saw pod success
Mar  4 23:36:33.843: INFO: Pod "pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:36:33.851: INFO: Trying to get logs from node 10.190.208.159 pod pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:36:34.271: INFO: Waiting for pod pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a to disappear
Mar  4 23:36:34.279: INFO: Pod pod-572e3bd3-3ed6-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:36:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cvxhd" for this suite.
Mar  4 23:36:40.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:36:40.521: INFO: namespace: e2e-tests-emptydir-cvxhd, resource: bindings, ignored listing per whitelist
Mar  4 23:36:40.733: INFO: namespace e2e-tests-emptydir-cvxhd deletion completed in 6.44453534s

• [SLOW TEST:9.259 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:36:40.735: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-f9gcq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-f9gcq
Mar  4 23:36:45.127: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-f9gcq
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 23:36:45.135: INFO: Initial restart count of pod liveness-http is 0
Mar  4 23:36:57.200: INFO: Restart count of pod e2e-tests-container-probe-f9gcq/liveness-http is now 1 (12.064677861s elapsed)
Mar  4 23:37:17.290: INFO: Restart count of pod e2e-tests-container-probe-f9gcq/liveness-http is now 2 (32.155027344s elapsed)
Mar  4 23:37:37.377: INFO: Restart count of pod e2e-tests-container-probe-f9gcq/liveness-http is now 3 (52.241645737s elapsed)
Mar  4 23:37:57.507: INFO: Restart count of pod e2e-tests-container-probe-f9gcq/liveness-http is now 4 (1m12.372118489s elapsed)
Mar  4 23:38:56.740: INFO: Restart count of pod e2e-tests-container-probe-f9gcq/liveness-http is now 5 (2m11.605535454s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:38:56.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-f9gcq" for this suite.
Mar  4 23:39:02.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:39:03.390: INFO: namespace: e2e-tests-container-probe-f9gcq, resource: bindings, ignored listing per whitelist
Mar  4 23:39:03.466: INFO: namespace e2e-tests-container-probe-f9gcq deletion completed in 6.684525713s

• [SLOW TEST:142.731 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:39:03.467: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-rtrsk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  4 23:39:03.802: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:39:08.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-rtrsk" for this suite.
Mar  4 23:39:32.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:39:32.956: INFO: namespace: e2e-tests-init-container-rtrsk, resource: bindings, ignored listing per whitelist
Mar  4 23:39:33.230: INFO: namespace e2e-tests-init-container-rtrsk deletion completed in 24.441311084s

• [SLOW TEST:29.764 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:39:33.231: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-d7xfh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c37d0cf9-3ed6-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:39:33.544: INFO: Waiting up to 5m0s for pod "pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-d7xfh" to be "success or failure"
Mar  4 23:39:33.552: INFO: Pod "pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.155598ms
Mar  4 23:39:35.570: INFO: Pod "pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025796878s
STEP: Saw pod success
Mar  4 23:39:35.570: INFO: Pod "pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:39:35.579: INFO: Trying to get logs from node 10.190.208.161 pod pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:39:35.629: INFO: Waiting for pod pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a to disappear
Mar  4 23:39:35.637: INFO: Pod pod-configmaps-c37e71a1-3ed6-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:39:35.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-d7xfh" for this suite.
Mar  4 23:39:41.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:39:41.836: INFO: namespace: e2e-tests-configmap-d7xfh, resource: bindings, ignored listing per whitelist
Mar  4 23:39:41.990: INFO: namespace e2e-tests-configmap-d7xfh deletion completed in 6.342951028s

• [SLOW TEST:8.759 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:39:41.990: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-z7dtm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  4 23:39:48.606: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:48.614: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:39:50.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:50.623: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:39:52.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:52.637: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:39:54.615: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:54.624: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:39:56.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:56.624: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:39:58.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:39:58.623: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:00.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:00.628: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:02.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:02.626: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:04.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:04.628: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:06.615: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:06.623: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:08.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:08.629: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:10.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:10.624: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:12.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:12.628: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:14.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:14.624: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 23:40:16.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 23:40:16.625: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:40:16.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-z7dtm" for this suite.
Mar  4 23:40:40.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:40:40.838: INFO: namespace: e2e-tests-container-lifecycle-hook-z7dtm, resource: bindings, ignored listing per whitelist
Mar  4 23:40:41.058: INFO: namespace e2e-tests-container-lifecycle-hook-z7dtm deletion completed in 24.392011626s

• [SLOW TEST:59.068 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:40:41.058: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-lphvn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:40:41.343: INFO: Creating deployment "nginx-deployment"
Mar  4 23:40:41.351: INFO: Waiting for observed generation 1
Mar  4 23:40:43.365: INFO: Waiting for all required pods to come up
Mar  4 23:40:43.376: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  4 23:40:45.398: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  4 23:40:45.412: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  4 23:40:45.428: INFO: Updating deployment nginx-deployment
Mar  4 23:40:45.428: INFO: Waiting for observed generation 2
Mar  4 23:40:47.443: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  4 23:40:47.450: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  4 23:40:47.456: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  4 23:40:47.526: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  4 23:40:47.526: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  4 23:40:47.535: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  4 23:40:47.550: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  4 23:40:47.550: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  4 23:40:47.565: INFO: Updating deployment nginx-deployment
Mar  4 23:40:47.565: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  4 23:40:47.580: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  4 23:40:47.589: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  4 23:40:47.613: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-lphvn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-lphvn/deployments/nginx-deployment,UID:ebea7212-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21774,Generation:3,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-03-04 23:40:45 +0000 UTC 2019-03-04 23:40:41 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-03-04 23:40:47 +0000 UTC 2019-03-04 23:40:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  4 23:40:47.628: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-lphvn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-lphvn/replicasets/nginx-deployment-65bbdb5f8,UID:ee598753-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21764,Generation:3,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ebea7212-3ed6-11e9-844e-4e8eff50a26d 0xc001401587 0xc001401588}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  4 23:40:47.628: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  4 23:40:47.629: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-lphvn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-lphvn/replicasets/nginx-deployment-555b55d965,UID:ebed6119-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21762,Generation:3,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ebea7212-3ed6-11e9-844e-4e8eff50a26d 0xc0014014c7 0xc0014014c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  4 23:40:47.644: INFO: Pod "nginx-deployment-555b55d965-26tjl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-26tjl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-26tjl,UID:efa0c0e3-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21803,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aaf887 0xc001aaf888}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aaf900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aaf920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.164,PodIP:,StartTime:2019-03-04 23:40:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-2kw9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2kw9h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-2kw9h,UID:efa42bf6-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21795,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aaf9d7 0xc001aaf9d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aafa50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aafa70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-2x968" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2x968,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-2x968,UID:ebf46d34-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21622,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aafae0 0xc001aafae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aafb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aafb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.50,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://7a3af0edb095e0c4b5c08e6000d70b9ac01e5f99227085bbdf44f0f5f5fed95b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-42nkt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-42nkt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-42nkt,UID:efa42794-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21791,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aafc37 0xc001aafc38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aafcb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aafcd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-45rnn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-45rnn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-45rnn,UID:ebf47402-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21637,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aafd40 0xc001aafd41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aafdb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aafdd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.247,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://f91bf45e812e07c6029954c32d0a56f47985f3f324e6cfb3e82154a6703f8b5b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-6mvmk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6mvmk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-6mvmk,UID:ebf30a2c-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21620,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aafe97 0xc001aafe98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aaff10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aaff30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.48,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://4d476d5fe55677db1d1eb3fbed43d7dd343b668ea91933f8b0f3d4a3035c2a08}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.645: INFO: Pod "nginx-deployment-555b55d965-87w4t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-87w4t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-87w4t,UID:ebf619d9-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21640,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc001aafff7 0xc001aafff8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.245,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://be4bd3a26231a8cdb02caa52ce930e401803b6fc9da482c00aaee92d25ed3b12}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-9w895" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9w895,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-9w895,UID:efa25906-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21776,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8157 0xc0018b8158}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b82f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-d5gqg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-d5gqg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-d5gqg,UID:efa25788-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21811,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8380 0xc0018b8381}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b83f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:,StartTime:2019-03-04 23:40:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-dz7d2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dz7d2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-dz7d2,UID:efa69f09-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21800,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b84c7 0xc0018b84c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-gr5st" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gr5st,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-gr5st,UID:efa43a42-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21794,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b85a7 0xc0018b85a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-hmpxx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hmpxx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-hmpxx,UID:efa6be2a-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21805,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b86b0 0xc0018b86b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-jpcdb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jpcdb,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-jpcdb,UID:ebf47805-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21615,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8787 0xc0018b8788}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.51,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://05f0b47f5315a8e6e6422f806243c7f0c8e709a033e32ceb52c2d15de32243e9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.646: INFO: Pod "nginx-deployment-555b55d965-kgqz2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kgqz2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-kgqz2,UID:ebf4735f-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21634,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b88e7 0xc0018b88e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.246,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://7e833519a6f715d55f4780689fbe289185e41bb2499aa55bf89b0fd170c3d9c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-kw25d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kw25d,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-kw25d,UID:ebf62509-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21625,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8a47 0xc0018b8a48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.49,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://704efded3737957264593ba8e4ce2306910a838e4e5b6cf970c91c29f7a8754f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-l9gpl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-l9gpl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-l9gpl,UID:efa69b5f-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21801,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8ba7 0xc0018b8ba8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-ldrtl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ldrtl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-ldrtl,UID:efa63a24-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21810,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8d27 0xc0018b8d28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-v9zrc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v9zrc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-v9zrc,UID:ebf1af96-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21631,Generation:0,CreationTimestamp:2019-03-04 23:40:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8e80 0xc0018b8e81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b8ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b8f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:41 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.248,StartTime:2019-03-04 23:40:41 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-04 23:40:42 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://ab12fe9f6e4381af810ad54c6537f5661da87c499e2b96c53d2347b85c3ebb64}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-x9hxm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x9hxm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-x9hxm,UID:efa6bb6e-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21804,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b8fd7 0xc0018b8fd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.647: INFO: Pod "nginx-deployment-555b55d965-xhwhx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xhwhx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-555b55d965-xhwhx,UID:efa43e20-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21792,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 ebed6119-3ed6-11e9-844e-4e8eff50a26d 0xc0018b90b7 0xc0018b90b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-26nq6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-26nq6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-26nq6,UID:efa24dd0-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21807,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b91c0 0xc0018b91c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b93a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:,StartTime:2019-03-04 23:40:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-4pk66" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4pk66,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-4pk66,UID:ee5c6ddb-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21759,Generation:0,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9460 0xc0018b9461}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b94e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.164,PodIP:172.30.252.215,StartTime:2019-03-04 23:40:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-bhlt4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bhlt4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-bhlt4,UID:ee5ac7ce-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21748,Generation:0,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b95e0 0xc0018b95e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.52,StartTime:2019-03-04 23:40:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-hqnp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hqnp2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-hqnp2,UID:efa51659-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21798,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b98c0 0xc0018b98c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-kr57n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kr57n,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-kr57n,UID:efa719b6-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21808,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b99d0 0xc0018b99d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.648: INFO: Pod "nginx-deployment-65bbdb5f8-l8hmf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l8hmf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-l8hmf,UID:efa52a41-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21799,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9ab7 0xc0018b9ab8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-q9xkj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-q9xkj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-q9xkj,UID:ee632c40-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21754,Generation:0,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9bc0 0xc0018b9bc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.250,StartTime:2019-03-04 23:40:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-qcbfr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qcbfr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-qcbfr,UID:efa3b443-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21785,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9dd0 0xc0018b9dd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.164,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-qckwq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-qckwq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-qckwq,UID:efa54d00-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21802,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9ee0 0xc0018b9ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018b9f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018b9f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-sflwh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sflwh,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-sflwh,UID:efa537a6-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21814,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc0018b9ff0 0xc0018b9ff1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001de6130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001de61b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-w45r2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-w45r2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-w45r2,UID:ee5c292a-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21752,Generation:0,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc001de64a0 0xc001de64a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001de6520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001de6540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.249,StartTime:2019-03-04 23:40:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.649: INFO: Pod "nginx-deployment-65bbdb5f8-wq8zt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wq8zt,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-wq8zt,UID:efa3c509-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21790,Generation:0,CreationTimestamp:2019-03-04 23:40:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc001de6630 0xc001de6631}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001de67d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001de6900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  4 23:40:47.650: INFO: Pod "nginx-deployment-65bbdb5f8-z4llr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-z4llr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-lphvn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-lphvn/pods/nginx-deployment-65bbdb5f8-z4llr,UID:ee61dba3-3ed6-11e9-844e-4e8eff50a26d,ResourceVersion:21746,Generation:0,CreationTimestamp:2019-03-04 23:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 ee598753-3ed6-11e9-844e-4e8eff50a26d 0xc001de6a20 0xc001de6a21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v2d9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v2d9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-v2d9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001de6aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001de6ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.53,StartTime:2019-03-04 23:40:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:40:47.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-lphvn" for this suite.
Mar  4 23:40:55.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:40:55.756: INFO: namespace: e2e-tests-deployment-lphvn, resource: bindings, ignored listing per whitelist
Mar  4 23:40:56.021: INFO: namespace e2e-tests-deployment-lphvn deletion completed in 8.357691651s

• [SLOW TEST:14.963 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:40:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-75jjt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar  4 23:40:56.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:40:57.047: INFO: stderr: ""
Mar  4 23:40:57.047: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 23:40:57.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:40:57.170: INFO: stderr: ""
Mar  4 23:40:57.170: INFO: stdout: "update-demo-nautilus-79zbv update-demo-nautilus-8f6ng "
Mar  4 23:40:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-79zbv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:40:57.308: INFO: stderr: ""
Mar  4 23:40:57.308: INFO: stdout: ""
Mar  4 23:40:57.308: INFO: update-demo-nautilus-79zbv is created but not running
Mar  4 23:41:02.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:02.504: INFO: stderr: ""
Mar  4 23:41:02.504: INFO: stdout: "update-demo-nautilus-79zbv update-demo-nautilus-8f6ng "
Mar  4 23:41:02.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-79zbv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:02.608: INFO: stderr: ""
Mar  4 23:41:02.608: INFO: stdout: "true"
Mar  4 23:41:02.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-79zbv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:02.715: INFO: stderr: ""
Mar  4 23:41:02.716: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 23:41:02.716: INFO: validating pod update-demo-nautilus-79zbv
Mar  4 23:41:02.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 23:41:02.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 23:41:02.734: INFO: update-demo-nautilus-79zbv is verified up and running
Mar  4 23:41:02.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-8f6ng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:02.844: INFO: stderr: ""
Mar  4 23:41:02.844: INFO: stdout: "true"
Mar  4 23:41:02.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-8f6ng -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:02.965: INFO: stderr: ""
Mar  4 23:41:02.965: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 23:41:02.965: INFO: validating pod update-demo-nautilus-8f6ng
Mar  4 23:41:02.987: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 23:41:02.987: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 23:41:02.987: INFO: update-demo-nautilus-8f6ng is verified up and running
STEP: rolling-update to new replication controller
Mar  4 23:41:02.989: INFO: scanned /root for discovery docs: <nil>
Mar  4 23:41:02.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:25.771: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  4 23:41:25.771: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 23:41:25.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:25.888: INFO: stderr: ""
Mar  4 23:41:25.888: INFO: stdout: "update-demo-kitten-fk988 update-demo-kitten-pz4f9 "
Mar  4 23:41:25.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-kitten-fk988 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:26.010: INFO: stderr: ""
Mar  4 23:41:26.010: INFO: stdout: "true"
Mar  4 23:41:26.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-kitten-fk988 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:26.106: INFO: stderr: ""
Mar  4 23:41:26.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  4 23:41:26.106: INFO: validating pod update-demo-kitten-fk988
Mar  4 23:41:26.124: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  4 23:41:26.124: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  4 23:41:26.124: INFO: update-demo-kitten-fk988 is verified up and running
Mar  4 23:41:26.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-kitten-pz4f9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:26.228: INFO: stderr: ""
Mar  4 23:41:26.228: INFO: stdout: "true"
Mar  4 23:41:26.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-kitten-pz4f9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-75jjt'
Mar  4 23:41:26.351: INFO: stderr: ""
Mar  4 23:41:26.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  4 23:41:26.351: INFO: validating pod update-demo-kitten-pz4f9
Mar  4 23:41:26.369: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  4 23:41:26.369: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  4 23:41:26.369: INFO: update-demo-kitten-pz4f9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:41:26.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-75jjt" for this suite.
Mar  4 23:41:50.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:41:50.645: INFO: namespace: e2e-tests-kubectl-75jjt, resource: bindings, ignored listing per whitelist
Mar  4 23:41:50.769: INFO: namespace e2e-tests-kubectl-75jjt deletion completed in 24.390318432s

• [SLOW TEST:54.747 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:41:50.769: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-wt47t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-wt47t/secret-test-1577d2b9-3ed7-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:41:51.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-wt47t" to be "success or failure"
Mar  4 23:41:51.102: INFO: Pod "pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.492668ms
Mar  4 23:41:53.110: INFO: Pod "pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01780529s
STEP: Saw pod success
Mar  4 23:41:53.110: INFO: Pod "pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:41:53.120: INFO: Trying to get logs from node 10.190.208.161 pod pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a container env-test: <nil>
STEP: delete the pod
Mar  4 23:41:53.200: INFO: Waiting for pod pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a to disappear
Mar  4 23:41:53.208: INFO: Pod pod-configmaps-1579030c-3ed7-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:41:53.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-wt47t" for this suite.
Mar  4 23:41:59.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:41:59.456: INFO: namespace: e2e-tests-secrets-wt47t, resource: bindings, ignored listing per whitelist
Mar  4 23:41:59.549: INFO: namespace e2e-tests-secrets-wt47t deletion completed in 6.330094851s

• [SLOW TEST:8.781 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:41:59.553: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vttz6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  4 23:41:59.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:00.017: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 23:42:00.017: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Mar  4 23:42:00.107: INFO: scanned /root for discovery docs: <nil>
Mar  4 23:42:00.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:15.998: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  4 23:42:15.999: INFO: stdout: "Created e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a\nScaling up e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  4 23:42:15.999: INFO: stdout: "Created e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a\nScaling up e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  4 23:42:15.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:16.114: INFO: stderr: ""
Mar  4 23:42:16.114: INFO: stdout: "e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a-dzzc4 "
Mar  4 23:42:16.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a-dzzc4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:16.240: INFO: stderr: ""
Mar  4 23:42:16.240: INFO: stdout: "true"
Mar  4 23:42:16.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a-dzzc4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:16.379: INFO: stderr: ""
Mar  4 23:42:16.379: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar  4 23:42:16.379: INFO: e2e-test-nginx-rc-d489a8838322aecf354505f2a7da014a-dzzc4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Mar  4 23:42:16.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vttz6'
Mar  4 23:42:16.517: INFO: stderr: ""
Mar  4 23:42:16.517: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:42:16.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vttz6" for this suite.
Mar  4 23:42:40.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:42:40.734: INFO: namespace: e2e-tests-kubectl-vttz6, resource: bindings, ignored listing per whitelist
Mar  4 23:42:41.072: INFO: namespace e2e-tests-kubectl-vttz6 deletion completed in 24.54077156s

• [SLOW TEST:41.520 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:42:41.074: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lmtfb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar  4 23:42:41.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 --namespace=e2e-tests-kubectl-lmtfb run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  4 23:42:43.861: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  4 23:42:43.861: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:42:45.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lmtfb" for this suite.
Mar  4 23:42:51.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:42:52.557: INFO: namespace: e2e-tests-kubectl-lmtfb, resource: bindings, ignored listing per whitelist
Mar  4 23:42:52.587: INFO: namespace e2e-tests-kubectl-lmtfb deletion completed in 6.687880569s

• [SLOW TEST:11.514 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:42:52.589: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-9bz54
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar  4 23:42:52.896: INFO: Waiting up to 5m0s for pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a" in namespace "e2e-tests-containers-9bz54" to be "success or failure"
Mar  4 23:42:52.903: INFO: Pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668296ms
Mar  4 23:42:54.912: INFO: Pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016461947s
Mar  4 23:42:56.920: INFO: Pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024684296s
Mar  4 23:42:58.937: INFO: Pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041551222s
STEP: Saw pod success
Mar  4 23:42:58.937: INFO: Pod "client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:42:58.947: INFO: Trying to get logs from node 10.190.208.161 pod client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:42:58.998: INFO: Waiting for pod client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a to disappear
Mar  4 23:42:59.009: INFO: Pod client-containers-3a510795-3ed7-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:42:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-9bz54" for this suite.
Mar  4 23:43:05.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:43:05.135: INFO: namespace: e2e-tests-containers-9bz54, resource: bindings, ignored listing per whitelist
Mar  4 23:43:05.363: INFO: namespace e2e-tests-containers-9bz54 deletion completed in 6.343981032s

• [SLOW TEST:12.774 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:43:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-f8vm6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:43:05.710: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"41f09bd8-3ed7-11e9-844e-4e8eff50a26d", Controller:(*bool)(0xc000ccdede), BlockOwnerDeletion:(*bool)(0xc000ccdedf)}}
Mar  4 23:43:05.722: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"41ed8a29-3ed7-11e9-844e-4e8eff50a26d", Controller:(*bool)(0xc001bfac9e), BlockOwnerDeletion:(*bool)(0xc001bfac9f)}}
Mar  4 23:43:05.732: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"41ef1fc6-3ed7-11e9-844e-4e8eff50a26d", Controller:(*bool)(0xc001efaa96), BlockOwnerDeletion:(*bool)(0xc001efaa97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:43:10.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-f8vm6" for this suite.
Mar  4 23:43:16.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:43:16.965: INFO: namespace: e2e-tests-gc-f8vm6, resource: bindings, ignored listing per whitelist
Mar  4 23:43:17.116: INFO: namespace e2e-tests-gc-f8vm6 deletion completed in 6.348437381s

• [SLOW TEST:11.753 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:43:17.116: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-j94sq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:43:17.437: INFO: (0) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.159305ms)
Mar  4 23:43:17.451: INFO: (1) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.014705ms)
Mar  4 23:43:17.467: INFO: (2) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 15.827807ms)
Mar  4 23:43:17.481: INFO: (3) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.15131ms)
Mar  4 23:43:17.494: INFO: (4) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.121972ms)
Mar  4 23:43:17.508: INFO: (5) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.40922ms)
Mar  4 23:43:17.522: INFO: (6) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.87401ms)
Mar  4 23:43:17.535: INFO: (7) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.772979ms)
Mar  4 23:43:17.547: INFO: (8) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.538541ms)
Mar  4 23:43:17.562: INFO: (9) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 15.14991ms)
Mar  4 23:43:17.575: INFO: (10) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.070523ms)
Mar  4 23:43:17.597: INFO: (11) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.96344ms)
Mar  4 23:43:17.611: INFO: (12) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.29369ms)
Mar  4 23:43:17.625: INFO: (13) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.048649ms)
Mar  4 23:43:17.640: INFO: (14) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.824974ms)
Mar  4 23:43:17.653: INFO: (15) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.097928ms)
Mar  4 23:43:17.670: INFO: (16) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.179774ms)
Mar  4 23:43:17.687: INFO: (17) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.745323ms)
Mar  4 23:43:17.701: INFO: (18) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.767128ms)
Mar  4 23:43:17.715: INFO: (19) /api/v1/nodes/10.190.208.159:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.647047ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:43:17.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-j94sq" for this suite.
Mar  4 23:43:23.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:43:23.788: INFO: namespace: e2e-tests-proxy-j94sq, resource: bindings, ignored listing per whitelist
Mar  4 23:43:24.059: INFO: namespace e2e-tests-proxy-j94sq deletion completed in 6.335373583s

• [SLOW TEST:6.942 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:43:24.061: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-t669t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  4 23:43:24.332: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 23:43:24.350: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 23:43:24.356: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.159 before test
Mar  4 23:43:24.387: INFO: calico-node-78pl9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:43:24.387: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-2lskk from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:43:24.387: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:43:24.387: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:43:24.387: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:43:24.387: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-fdlpz from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:43:24.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:43:24.387: INFO: ibm-keepalived-watcher-f79g9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:43:24.387: INFO: ibm-kube-fluentd-7dbzh from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:43:24.387: INFO: ibm-master-proxy-static-10.190.208.159 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:43:24.387: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-4cddx from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.387: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:43:24.387: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.161 before test
Mar  4 23:43:24.410: INFO: ibm-master-proxy-static-10.190.208.161 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:43:24.411: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-03-04 23:03:13 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  4 23:43:24.411: INFO: calico-node-lgpp2 from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:43:24.411: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-04 23:03:19 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 23:43:24.411: INFO: sonobuoy-e2e-job-903cc3ae0952453b from heptio-sonobuoy started at 2019-03-04 23:03:23 +0000 UTC (2 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container e2e ready: true, restart count 0
Mar  4 23:43:24.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:43:24.411: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-pm7j4 from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:43:24.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:43:24.411: INFO: ibm-kube-fluentd-9gf98 from kube-system started at 2019-03-04 21:42:06 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:43:24.411: INFO: ibm-keepalived-watcher-mnmzt from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.411: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:43:24.411: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.164 before test
Mar  4 23:43:24.489: INFO: ibm-storage-watcher-7f87684475-2hlnh from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  4 23:43:24.489: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-cz8tp from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:43:24.489: INFO: vpn-74bb4868b9-gwrxn from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container vpn ready: true, restart count 0
Mar  4 23:43:24.489: INFO: calico-kube-controllers-65868f965d-4fq52 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 23:43:24.489: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-gjlc9 from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:43:24.489: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:43:24.489: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:43:24.489: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:43:24.489: INFO: ibm-file-plugin-6578dcb564-vkqtt from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  4 23:43:24.489: INFO: coredns-autoscaler-64f9c5b4df-hd2v9 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container autoscaler ready: true, restart count 0
Mar  4 23:43:24.489: INFO: metrics-server-657cbf4579-s554h from kube-system started at 2019-03-04 21:33:04 +0000 UTC (2 container statuses recorded)
Mar  4 23:43:24.489: INFO: 	Container metrics-server ready: true, restart count 0
Mar  4 23:43:24.489: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  4 23:43:24.489: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-bp8dg from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:43:24.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:43:24.490: INFO: kubernetes-dashboard-7996b848f4-gbncd from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  4 23:43:24.490: INFO: ibm-kube-fluentd-4796f from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:43:24.490: INFO: ibm-master-proxy-static-10.190.208.164 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:43:24.490: INFO: ibm-keepalived-watcher-8m6km from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:43:24.490: INFO: coredns-58d696879-cck9d from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:43:24.490: INFO: coredns-58d696879-qwvbn from kube-system started at 2019-03-04 21:33:30 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:43:24.490: INFO: calico-node-26clt from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:43:24.490: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node 10.190.208.159
STEP: verifying the node has the label node 10.190.208.161
STEP: verifying the node has the label node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod sonobuoy-e2e-job-903cc3ae0952453b requesting resource cpu=0m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-bp8dg requesting resource cpu=0m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-fdlpz requesting resource cpu=0m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-pm7j4 requesting resource cpu=0m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-4cddx requesting resource cpu=5m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-cz8tp requesting resource cpu=5m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod calico-kube-controllers-65868f965d-4fq52 requesting resource cpu=10m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod calico-node-26clt requesting resource cpu=250m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod calico-node-78pl9 requesting resource cpu=250m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod calico-node-lgpp2 requesting resource cpu=250m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod coredns-58d696879-cck9d requesting resource cpu=100m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod coredns-58d696879-qwvbn requesting resource cpu=100m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod coredns-autoscaler-64f9c5b4df-hd2v9 requesting resource cpu=20m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod ibm-file-plugin-6578dcb564-vkqtt requesting resource cpu=50m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod ibm-keepalived-watcher-8m6km requesting resource cpu=5m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod ibm-keepalived-watcher-f79g9 requesting resource cpu=5m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod ibm-keepalived-watcher-mnmzt requesting resource cpu=5m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod ibm-kube-fluentd-4796f requesting resource cpu=25m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod ibm-kube-fluentd-7dbzh requesting resource cpu=25m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod ibm-kube-fluentd-9gf98 requesting resource cpu=25m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod ibm-master-proxy-static-10.190.208.159 requesting resource cpu=25m on Node 10.190.208.159
Mar  4 23:43:24.600: INFO: Pod ibm-master-proxy-static-10.190.208.161 requesting resource cpu=25m on Node 10.190.208.161
Mar  4 23:43:24.600: INFO: Pod ibm-master-proxy-static-10.190.208.164 requesting resource cpu=25m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod ibm-storage-watcher-7f87684475-2hlnh requesting resource cpu=50m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod kubernetes-dashboard-7996b848f4-gbncd requesting resource cpu=50m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod metrics-server-657cbf4579-s554h requesting resource cpu=53m on Node 10.190.208.164
Mar  4 23:43:24.600: INFO: Pod public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-2lskk requesting resource cpu=0m on Node 10.190.208.159
Mar  4 23:43:24.601: INFO: Pod public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-gjlc9 requesting resource cpu=0m on Node 10.190.208.164
Mar  4 23:43:24.601: INFO: Pod vpn-74bb4868b9-gwrxn requesting resource cpu=5m on Node 10.190.208.164
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d396f45-3ed7-11e9-8a62-3ec24305971a.1588e60a8d53c39b], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-t669t/filler-pod-4d396f45-3ed7-11e9-8a62-3ec24305971a to 10.190.208.159]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d396f45-3ed7-11e9-8a62-3ec24305971a.1588e60ac0571acc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d396f45-3ed7-11e9-8a62-3ec24305971a.1588e60ac320e2dc], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d396f45-3ed7-11e9-8a62-3ec24305971a.1588e60acaebc020], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3c45b2-3ed7-11e9-8a62-3ec24305971a.1588e60a8dfa6d6d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-t669t/filler-pod-4d3c45b2-3ed7-11e9-8a62-3ec24305971a to 10.190.208.161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3c45b2-3ed7-11e9-8a62-3ec24305971a.1588e60abb2ca310], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3c45b2-3ed7-11e9-8a62-3ec24305971a.1588e60abdaf7e93], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3c45b2-3ed7-11e9-8a62-3ec24305971a.1588e60ac588855f], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a.1588e60a8e982677], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-t669t/filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a to 10.190.208.164]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a.1588e60abcea5f32], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a.1588e60b0b19c067], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a.1588e60b0e82aa8c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3e5632-3ed7-11e9-8a62-3ec24305971a.1588e60b1b6312b6], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1588e60b80276ae2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.190.208.159
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.190.208.161
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.190.208.164
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:43:29.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-t669t" for this suite.
Mar  4 23:43:37.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:43:38.081: INFO: namespace: e2e-tests-sched-pred-t669t, resource: bindings, ignored listing per whitelist
Mar  4 23:43:38.252: INFO: namespace e2e-tests-sched-pred-t669t deletion completed in 8.380436067s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:14.192 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:43:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q2bcd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-558f2061-3ed7-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:43:38.619: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-q2bcd" to be "success or failure"
Mar  4 23:43:38.627: INFO: Pod "pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.934753ms
Mar  4 23:43:40.637: INFO: Pod "pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018317442s
STEP: Saw pod success
Mar  4 23:43:40.637: INFO: Pod "pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:43:40.646: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:43:40.699: INFO: Waiting for pod pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a to disappear
Mar  4 23:43:40.707: INFO: Pod pod-projected-configmaps-5591ec18-3ed7-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:43:40.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q2bcd" for this suite.
Mar  4 23:43:46.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:43:46.805: INFO: namespace: e2e-tests-projected-q2bcd, resource: bindings, ignored listing per whitelist
Mar  4 23:43:47.053: INFO: namespace e2e-tests-projected-q2bcd deletion completed in 6.334166644s

• [SLOW TEST:8.800 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:43:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-65d6r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-69s4
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 23:43:47.400: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-69s4" in namespace "e2e-tests-subpath-65d6r" to be "success or failure"
Mar  4 23:43:47.408: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.997193ms
Mar  4 23:43:49.416: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016798545s
Mar  4 23:43:51.425: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 4.025158014s
Mar  4 23:43:53.434: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 6.034078116s
Mar  4 23:43:55.445: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 8.045307039s
Mar  4 23:43:57.456: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 10.056829853s
Mar  4 23:43:59.466: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 12.065981466s
Mar  4 23:44:01.475: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 14.075493346s
Mar  4 23:44:03.484: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 16.084504716s
Mar  4 23:44:05.496: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 18.09586803s
Mar  4 23:44:07.505: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 20.105133177s
Mar  4 23:44:09.513: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Running", Reason="", readiness=false. Elapsed: 22.113798984s
Mar  4 23:44:11.522: INFO: Pod "pod-subpath-test-projected-69s4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.122313992s
STEP: Saw pod success
Mar  4 23:44:11.522: INFO: Pod "pod-subpath-test-projected-69s4" satisfied condition "success or failure"
Mar  4 23:44:11.531: INFO: Trying to get logs from node 10.190.208.159 pod pod-subpath-test-projected-69s4 container test-container-subpath-projected-69s4: <nil>
STEP: delete the pod
Mar  4 23:44:11.636: INFO: Waiting for pod pod-subpath-test-projected-69s4 to disappear
Mar  4 23:44:11.643: INFO: Pod pod-subpath-test-projected-69s4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-69s4
Mar  4 23:44:11.643: INFO: Deleting pod "pod-subpath-test-projected-69s4" in namespace "e2e-tests-subpath-65d6r"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:44:11.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-65d6r" for this suite.
Mar  4 23:44:17.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:44:17.839: INFO: namespace: e2e-tests-subpath-65d6r, resource: bindings, ignored listing per whitelist
Mar  4 23:44:17.957: INFO: namespace e2e-tests-subpath-65d6r deletion completed in 6.295635732s

• [SLOW TEST:30.905 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:44:17.958: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-lwtrh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  4 23:44:18.273: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-lwtrh,SelfLink:/api/v1/namespaces/e2e-tests-watch-lwtrh/configmaps/e2e-watch-test-watch-closed,UID:6d3335d4-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:23280,Generation:0,CreationTimestamp:2019-03-04 23:44:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 23:44:18.273: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-lwtrh,SelfLink:/api/v1/namespaces/e2e-tests-watch-lwtrh/configmaps/e2e-watch-test-watch-closed,UID:6d3335d4-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:23281,Generation:0,CreationTimestamp:2019-03-04 23:44:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  4 23:44:18.313: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-lwtrh,SelfLink:/api/v1/namespaces/e2e-tests-watch-lwtrh/configmaps/e2e-watch-test-watch-closed,UID:6d3335d4-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:23282,Generation:0,CreationTimestamp:2019-03-04 23:44:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 23:44:18.313: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-lwtrh,SelfLink:/api/v1/namespaces/e2e-tests-watch-lwtrh/configmaps/e2e-watch-test-watch-closed,UID:6d3335d4-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:23283,Generation:0,CreationTimestamp:2019-03-04 23:44:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:44:18.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-lwtrh" for this suite.
Mar  4 23:44:24.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:44:24.627: INFO: namespace: e2e-tests-watch-lwtrh, resource: bindings, ignored listing per whitelist
Mar  4 23:44:24.712: INFO: namespace e2e-tests-watch-lwtrh deletion completed in 6.384774728s

• [SLOW TEST:6.755 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:44:24.713: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-4gk4h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4gk4h
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 23:44:25.059: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 23:44:41.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.7:8080/dial?request=hostName&protocol=http&host=172.30.252.227&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4gk4h PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:44:41.241: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:44:41.498: INFO: Waiting for endpoints: map[]
Mar  4 23:44:41.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.7:8080/dial?request=hostName&protocol=http&host=172.30.111.8&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4gk4h PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:44:41.507: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:44:41.735: INFO: Waiting for endpoints: map[]
Mar  4 23:44:41.744: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.7:8080/dial?request=hostName&protocol=http&host=172.30.189.199&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-4gk4h PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 23:44:41.744: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  4 23:44:42.005: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:44:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4gk4h" for this suite.
Mar  4 23:45:06.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:45:06.305: INFO: namespace: e2e-tests-pod-network-test-4gk4h, resource: bindings, ignored listing per whitelist
Mar  4 23:45:06.396: INFO: namespace e2e-tests-pod-network-test-4gk4h deletion completed in 24.285185569s

• [SLOW TEST:41.683 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:45:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x79gr
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-8a16ac7f-3ed7-11e9-8a62-3ec24305971a
STEP: Creating configMap with name cm-test-opt-upd-8a16acc9-3ed7-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8a16ac7f-3ed7-11e9-8a62-3ec24305971a
STEP: Updating configmap cm-test-opt-upd-8a16acc9-3ed7-11e9-8a62-3ec24305971a
STEP: Creating configMap with name cm-test-opt-create-8a16ace7-3ed7-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:45:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x79gr" for this suite.
Mar  4 23:45:35.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:45:35.163: INFO: namespace: e2e-tests-projected-x79gr, resource: bindings, ignored listing per whitelist
Mar  4 23:45:35.567: INFO: namespace e2e-tests-projected-x79gr deletion completed in 24.56442894s

• [SLOW TEST:29.169 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:45:35.567: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-f5k9f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-cvzl
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 23:45:35.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cvzl" in namespace "e2e-tests-subpath-f5k9f" to be "success or failure"
Mar  4 23:45:35.908: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.929741ms
Mar  4 23:45:37.917: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016485853s
Mar  4 23:45:39.926: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 4.025546841s
Mar  4 23:45:41.935: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 6.03441305s
Mar  4 23:45:43.943: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 8.04305552s
Mar  4 23:45:45.953: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 10.052923756s
Mar  4 23:45:47.962: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 12.061704601s
Mar  4 23:45:49.971: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 14.070615494s
Mar  4 23:45:51.990: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 16.089592193s
Mar  4 23:45:54.000: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 18.099314557s
Mar  4 23:45:56.008: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 20.107709775s
Mar  4 23:45:58.017: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Running", Reason="", readiness=false. Elapsed: 22.11669221s
Mar  4 23:46:00.026: INFO: Pod "pod-subpath-test-configmap-cvzl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.125279368s
STEP: Saw pod success
Mar  4 23:46:00.026: INFO: Pod "pod-subpath-test-configmap-cvzl" satisfied condition "success or failure"
Mar  4 23:46:00.036: INFO: Trying to get logs from node 10.190.208.159 pod pod-subpath-test-configmap-cvzl container test-container-subpath-configmap-cvzl: <nil>
STEP: delete the pod
Mar  4 23:46:00.089: INFO: Waiting for pod pod-subpath-test-configmap-cvzl to disappear
Mar  4 23:46:00.099: INFO: Pod pod-subpath-test-configmap-cvzl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cvzl
Mar  4 23:46:00.100: INFO: Deleting pod "pod-subpath-test-configmap-cvzl" in namespace "e2e-tests-subpath-f5k9f"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:46:00.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-f5k9f" for this suite.
Mar  4 23:46:06.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:46:06.278: INFO: namespace: e2e-tests-subpath-f5k9f, resource: bindings, ignored listing per whitelist
Mar  4 23:46:06.473: INFO: namespace e2e-tests-subpath-f5k9f deletion completed in 6.353890223s

• [SLOW TEST:30.905 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:46:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-29fw9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  4 23:46:06.776: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  4 23:46:11.785: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:46:11.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-29fw9" for this suite.
Mar  4 23:46:17.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:46:17.912: INFO: namespace: e2e-tests-replication-controller-29fw9, resource: bindings, ignored listing per whitelist
Mar  4 23:46:18.157: INFO: namespace e2e-tests-replication-controller-29fw9 deletion completed in 6.332945962s

• [SLOW TEST:11.684 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:46:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-zcq5p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  4 23:46:18.451: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:46:21.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-zcq5p" for this suite.
Mar  4 23:46:27.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:46:27.727: INFO: namespace: e2e-tests-init-container-zcq5p, resource: bindings, ignored listing per whitelist
Mar  4 23:46:28.126: INFO: namespace e2e-tests-init-container-zcq5p deletion completed in 6.718458144s

• [SLOW TEST:9.969 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:46:28.126: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-n8sf2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-mm2nz
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Mar  4 23:46:37.821: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-bsrr8
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:46:55.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-n8sf2" for this suite.
Mar  4 23:47:01.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:47:02.079: INFO: namespace: e2e-tests-namespaces-n8sf2, resource: bindings, ignored listing per whitelist
Mar  4 23:47:02.305: INFO: namespace e2e-tests-namespaces-n8sf2 deletion completed in 6.427325816s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mm2nz" for this suite.
Mar  4 23:47:02.311: INFO: Namespace e2e-tests-nsdeletetest-mm2nz was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-bsrr8" for this suite.
Mar  4 23:47:08.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:47:08.488: INFO: namespace: e2e-tests-nsdeletetest-bsrr8, resource: bindings, ignored listing per whitelist
Mar  4 23:47:09.164: INFO: namespace e2e-tests-nsdeletetest-bsrr8 deletion completed in 6.852234521s

• [SLOW TEST:41.038 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:47:09.165: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-jnmbv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-d34653cb-3ed7-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:47:09.602: INFO: Waiting up to 5m0s for pod "pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-jnmbv" to be "success or failure"
Mar  4 23:47:09.610: INFO: Pod "pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.581201ms
Mar  4 23:47:11.710: INFO: Pod "pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107048622s
STEP: Saw pod success
Mar  4 23:47:11.710: INFO: Pod "pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:47:11.717: INFO: Trying to get logs from node 10.190.208.161 pod pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:47:12.100: INFO: Waiting for pod pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a to disappear
Mar  4 23:47:12.109: INFO: Pod pod-secrets-d34777dd-3ed7-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:47:12.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jnmbv" for this suite.
Mar  4 23:47:18.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:47:18.444: INFO: namespace: e2e-tests-secrets-jnmbv, resource: bindings, ignored listing per whitelist
Mar  4 23:47:18.461: INFO: namespace e2e-tests-secrets-jnmbv deletion completed in 6.340471595s

• [SLOW TEST:9.297 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:47:18.463: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-b6lzj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  4 23:47:23.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:23.306: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:25.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:25.317: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:27.307: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:27.316: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:29.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:29.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:31.307: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:31.316: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:33.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:33.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:35.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:35.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:37.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:37.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:39.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:39.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:41.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:41.315: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 23:47:43.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 23:47:43.315: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:47:43.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-b6lzj" for this suite.
Mar  4 23:48:07.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:48:08.126: INFO: namespace: e2e-tests-container-lifecycle-hook-b6lzj, resource: bindings, ignored listing per whitelist
Mar  4 23:48:08.134: INFO: namespace e2e-tests-container-lifecycle-hook-b6lzj deletion completed in 24.807152498s

• [SLOW TEST:49.671 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:48:08.135: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-jmmsl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  4 23:48:08.510: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24206,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 23:48:08.510: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24206,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  4 23:48:18.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24223,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  4 23:48:18.674: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24223,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  4 23:48:28.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24240,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 23:48:28.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24240,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  4 23:48:38.715: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24257,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 23:48:38.715: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-a,UID:f6711e0e-3ed7-11e9-844e-4e8eff50a26d,ResourceVersion:24257,Generation:0,CreationTimestamp:2019-03-04 23:48:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  4 23:48:48.730: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-b,UID:0e698f6b-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24274,Generation:0,CreationTimestamp:2019-03-04 23:48:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 23:48:48.730: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-b,UID:0e698f6b-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24274,Generation:0,CreationTimestamp:2019-03-04 23:48:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  4 23:48:58.751: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-b,UID:0e698f6b-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24291,Generation:0,CreationTimestamp:2019-03-04 23:48:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 23:48:58.752: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jmmsl,SelfLink:/api/v1/namespaces/e2e-tests-watch-jmmsl/configmaps/e2e-watch-test-configmap-b,UID:0e698f6b-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24291,Generation:0,CreationTimestamp:2019-03-04 23:48:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:49:08.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-jmmsl" for this suite.
Mar  4 23:49:14.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:49:15.124: INFO: namespace: e2e-tests-watch-jmmsl, resource: bindings, ignored listing per whitelist
Mar  4 23:49:15.153: INFO: namespace e2e-tests-watch-jmmsl deletion completed in 6.388623109s

• [SLOW TEST:67.019 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:49:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-72tzf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:49:15.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-72tzf" to be "success or failure"
Mar  4 23:49:15.640: INFO: Pod "downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.560286ms
Mar  4 23:49:17.650: INFO: Pod "downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017043886s
Mar  4 23:49:19.658: INFO: Pod "downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025724753s
STEP: Saw pod success
Mar  4 23:49:19.659: INFO: Pod "downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:49:19.666: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:49:19.710: INFO: Waiting for pod downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a to disappear
Mar  4 23:49:19.719: INFO: Pod downwardapi-volume-1e7232c5-3ed8-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:49:19.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-72tzf" for this suite.
Mar  4 23:49:25.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:49:25.783: INFO: namespace: e2e-tests-projected-72tzf, resource: bindings, ignored listing per whitelist
Mar  4 23:49:26.026: INFO: namespace e2e-tests-projected-72tzf deletion completed in 6.295941412s

• [SLOW TEST:10.873 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:49:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-zjfk2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  4 23:49:26.406: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 23:49:26.509: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 23:49:26.515: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.159 before test
Mar  4 23:49:26.542: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-fdlpz from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:49:26.543: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:49:26.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:49:26.543: INFO: calico-node-78pl9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.543: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:49:26.543: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-2lskk from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:49:26.543: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:49:26.543: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:49:26.543: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:49:26.543: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:49:26.543: INFO: ibm-keepalived-watcher-f79g9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.544: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:49:26.544: INFO: ibm-kube-fluentd-7dbzh from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.544: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:49:26.544: INFO: ibm-master-proxy-static-10.190.208.159 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:49:26.544: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-4cddx from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.544: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:49:26.544: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.161 before test
Mar  4 23:49:26.567: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-04 23:03:19 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 23:49:26.567: INFO: ibm-master-proxy-static-10.190.208.161 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:49:26.567: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-03-04 23:03:13 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  4 23:49:26.567: INFO: calico-node-lgpp2 from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:49:26.567: INFO: sonobuoy-e2e-job-903cc3ae0952453b from heptio-sonobuoy started at 2019-03-04 23:03:23 +0000 UTC (2 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container e2e ready: true, restart count 0
Mar  4 23:49:26.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:49:26.567: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-pm7j4 from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:49:26.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:49:26.567: INFO: ibm-kube-fluentd-9gf98 from kube-system started at 2019-03-04 21:42:06 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:49:26.567: INFO: ibm-keepalived-watcher-mnmzt from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.567: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:49:26.567: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.164 before test
Mar  4 23:49:26.600: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-gjlc9 from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:49:26.600: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:49:26.600: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:49:26.600: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:49:26.600: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:49:26.600: INFO: ibm-file-plugin-6578dcb564-vkqtt from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.600: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  4 23:49:26.600: INFO: coredns-autoscaler-64f9c5b4df-hd2v9 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.600: INFO: 	Container autoscaler ready: true, restart count 0
Mar  4 23:49:26.600: INFO: metrics-server-657cbf4579-s554h from kube-system started at 2019-03-04 21:33:04 +0000 UTC (2 container statuses recorded)
Mar  4 23:49:26.600: INFO: 	Container metrics-server ready: true, restart count 0
Mar  4 23:49:26.601: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  4 23:49:26.601: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-bp8dg from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:49:26.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:49:26.601: INFO: kubernetes-dashboard-7996b848f4-gbncd from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  4 23:49:26.601: INFO: ibm-kube-fluentd-4796f from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:49:26.601: INFO: ibm-master-proxy-static-10.190.208.164 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:49:26.601: INFO: ibm-keepalived-watcher-8m6km from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:49:26.601: INFO: coredns-58d696879-cck9d from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:49:26.601: INFO: coredns-58d696879-qwvbn from kube-system started at 2019-03-04 21:33:30 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:49:26.601: INFO: calico-node-26clt from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:49:26.601: INFO: ibm-storage-watcher-7f87684475-2hlnh from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  4 23:49:26.601: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-cz8tp from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:49:26.601: INFO: vpn-74bb4868b9-gwrxn from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container vpn ready: true, restart count 0
Mar  4 23:49:26.601: INFO: calico-kube-controllers-65868f965d-4fq52 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:49:26.601: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1588e65ed745310c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:49:27.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-zjfk2" for this suite.
Mar  4 23:49:33.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:49:34.050: INFO: namespace: e2e-tests-sched-pred-zjfk2, resource: bindings, ignored listing per whitelist
Mar  4 23:49:34.207: INFO: namespace e2e-tests-sched-pred-zjfk2 deletion completed in 6.538447285s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:8.179 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:49:34.211: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-zqthd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:49:34.495: INFO: Creating deployment "test-recreate-deployment"
Mar  4 23:49:34.503: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  4 23:49:34.518: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  4 23:49:36.534: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  4 23:49:36.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687340174, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687340174, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687340174, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687340174, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 23:49:38.552: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  4 23:49:38.570: INFO: Updating deployment test-recreate-deployment
Mar  4 23:49:38.570: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  4 23:49:38.679: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-zqthd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zqthd/deployments/test-recreate-deployment,UID:29b2f56d-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24470,Generation:2,CreationTimestamp:2019-03-04 23:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-04 23:49:38 +0000 UTC 2019-03-04 23:49:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-04 23:49:38 +0000 UTC 2019-03-04 23:49:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  4 23:49:38.688: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-zqthd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zqthd/replicasets/test-recreate-deployment-697fbf54bf,UID:2c2879bf-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24467,Generation:1,CreationTimestamp:2019-03-04 23:49:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 29b2f56d-3ed8-11e9-844e-4e8eff50a26d 0xc001bed687 0xc001bed688}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  4 23:49:38.688: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  4 23:49:38.688: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-zqthd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zqthd/replicasets/test-recreate-deployment-5dfdcc846d,UID:29b5a345-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24458,Generation:2,CreationTimestamp:2019-03-04 23:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 29b2f56d-3ed8-11e9-844e-4e8eff50a26d 0xc001bed5c7 0xc001bed5c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  4 23:49:38.698: INFO: Pod "test-recreate-deployment-697fbf54bf-69pgn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-69pgn,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-zqthd,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zqthd/pods/test-recreate-deployment-697fbf54bf-69pgn,UID:2c29be8d-3ed8-11e9-844e-4e8eff50a26d,ResourceVersion:24463,Generation:0,CreationTimestamp:2019-03-04 23:49:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 2c2879bf-3ed8-11e9-844e-4e8eff50a26d 0xc001e13c97 0xc001e13c98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-9tlp7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9tlp7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9tlp7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e13d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e13d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-04 23:49:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:49:38.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-zqthd" for this suite.
Mar  4 23:49:46.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:49:46.962: INFO: namespace: e2e-tests-deployment-zqthd, resource: bindings, ignored listing per whitelist
Mar  4 23:49:47.222: INFO: namespace e2e-tests-deployment-zqthd deletion completed in 8.51318257s

• [SLOW TEST:13.012 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:49:47.223: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-569ng
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-317638cf-3ed8-11e9-8a62-3ec24305971a
STEP: Creating configMap with name cm-test-opt-upd-317639df-3ed8-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-317638cf-3ed8-11e9-8a62-3ec24305971a
STEP: Updating configmap cm-test-opt-upd-317639df-3ed8-11e9-8a62-3ec24305971a
STEP: Creating configMap with name cm-test-opt-create-31763a00-3ed8-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:49:53.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-569ng" for this suite.
Mar  4 23:50:17.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:50:17.996: INFO: namespace: e2e-tests-configmap-569ng, resource: bindings, ignored listing per whitelist
Mar  4 23:50:18.226: INFO: namespace e2e-tests-configmap-569ng deletion completed in 24.377580888s

• [SLOW TEST:31.003 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:50:18.227: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-hsjql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-slck6
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-t9chw
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:50:25.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-hsjql" for this suite.
Mar  4 23:50:31.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:50:31.290: INFO: namespace: e2e-tests-namespaces-hsjql, resource: bindings, ignored listing per whitelist
Mar  4 23:50:31.431: INFO: namespace e2e-tests-namespaces-hsjql deletion completed in 6.395619393s
STEP: Destroying namespace "e2e-tests-nsdeletetest-slck6" for this suite.
Mar  4 23:50:31.438: INFO: Namespace e2e-tests-nsdeletetest-slck6 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-t9chw" for this suite.
Mar  4 23:50:37.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:50:39.373: INFO: namespace: e2e-tests-nsdeletetest-t9chw, resource: bindings, ignored listing per whitelist
Mar  4 23:50:39.456: INFO: namespace e2e-tests-nsdeletetest-t9chw deletion completed in 8.01765115s

• [SLOW TEST:21.229 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:50:39.456: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ld6rx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:51:05.962: INFO: Container started at 2019-03-04 23:50:41 +0000 UTC, pod became ready at 2019-03-04 23:51:05 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:51:05.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ld6rx" for this suite.
Mar  4 23:51:29.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:51:30.629: INFO: namespace: e2e-tests-container-probe-ld6rx, resource: bindings, ignored listing per whitelist
Mar  4 23:51:30.700: INFO: namespace e2e-tests-container-probe-ld6rx deletion completed in 24.727533894s

• [SLOW TEST:51.244 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:51:30.702: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-f96nv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:51:31.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-f96nv" to be "success or failure"
Mar  4 23:51:31.020: INFO: Pod "downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.834654ms
Mar  4 23:51:33.029: INFO: Pod "downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017688895s
STEP: Saw pod success
Mar  4 23:51:33.029: INFO: Pod "downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:51:33.038: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:51:33.149: INFO: Waiting for pod downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a to disappear
Mar  4 23:51:33.157: INFO: Pod downwardapi-volume-6f2233e8-3ed8-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:51:33.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f96nv" for this suite.
Mar  4 23:51:39.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:51:39.371: INFO: namespace: e2e-tests-downward-api-f96nv, resource: bindings, ignored listing per whitelist
Mar  4 23:51:39.638: INFO: namespace e2e-tests-downward-api-f96nv deletion completed in 6.469896252s

• [SLOW TEST:8.936 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:51:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-44sqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7472bca7-3ed8-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:51:39.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-44sqg" to be "success or failure"
Mar  4 23:51:39.940: INFO: Pod "pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.339676ms
Mar  4 23:51:41.954: INFO: Pod "pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023108102s
STEP: Saw pod success
Mar  4 23:51:41.954: INFO: Pod "pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:51:41.963: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:51:42.012: INFO: Waiting for pod pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a to disappear
Mar  4 23:51:42.020: INFO: Pod pod-projected-configmaps-747419f1-3ed8-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:51:42.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-44sqg" for this suite.
Mar  4 23:51:48.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:51:48.297: INFO: namespace: e2e-tests-projected-44sqg, resource: bindings, ignored listing per whitelist
Mar  4 23:51:48.351: INFO: namespace e2e-tests-projected-44sqg deletion completed in 6.320926241s

• [SLOW TEST:8.713 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:51:48.353: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-s8kgc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0304 23:51:49.746502      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  4 23:51:49.746: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:51:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-s8kgc" for this suite.
Mar  4 23:51:55.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:51:55.858: INFO: namespace: e2e-tests-gc-s8kgc, resource: bindings, ignored listing per whitelist
Mar  4 23:51:56.051: INFO: namespace e2e-tests-gc-s8kgc deletion completed in 6.296218295s

• [SLOW TEST:7.699 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:51:56.053: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-b69lr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-h2rx
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 23:51:56.416: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h2rx" in namespace "e2e-tests-subpath-b69lr" to be "success or failure"
Mar  4 23:51:56.425: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516891ms
Mar  4 23:51:58.434: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017039837s
Mar  4 23:52:00.444: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 4.027568214s
Mar  4 23:52:02.453: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 6.036449846s
Mar  4 23:52:04.736: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 8.319310827s
Mar  4 23:52:06.745: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 10.328142737s
Mar  4 23:52:08.753: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 12.336614307s
Mar  4 23:52:10.762: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 14.345294049s
Mar  4 23:52:13.125: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 16.708545002s
Mar  4 23:52:15.137: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 18.720034579s
Mar  4 23:52:17.148: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 20.731924437s
Mar  4 23:52:19.157: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Running", Reason="", readiness=false. Elapsed: 22.740653185s
Mar  4 23:52:21.165: INFO: Pod "pod-subpath-test-configmap-h2rx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.748763896s
STEP: Saw pod success
Mar  4 23:52:21.165: INFO: Pod "pod-subpath-test-configmap-h2rx" satisfied condition "success or failure"
Mar  4 23:52:21.173: INFO: Trying to get logs from node 10.190.208.161 pod pod-subpath-test-configmap-h2rx container test-container-subpath-configmap-h2rx: <nil>
STEP: delete the pod
Mar  4 23:52:21.234: INFO: Waiting for pod pod-subpath-test-configmap-h2rx to disappear
Mar  4 23:52:21.242: INFO: Pod pod-subpath-test-configmap-h2rx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h2rx
Mar  4 23:52:21.242: INFO: Deleting pod "pod-subpath-test-configmap-h2rx" in namespace "e2e-tests-subpath-b69lr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:52:21.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-b69lr" for this suite.
Mar  4 23:52:27.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:52:27.326: INFO: namespace: e2e-tests-subpath-b69lr, resource: bindings, ignored listing per whitelist
Mar  4 23:52:27.896: INFO: namespace e2e-tests-subpath-b69lr deletion completed in 6.636295438s

• [SLOW TEST:31.843 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:52:27.898: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-5qrk2
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-913da685-3ed8-11e9-8a62-3ec24305971a
STEP: Creating secret with name s-test-opt-upd-913da70f-3ed8-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-913da685-3ed8-11e9-8a62-3ec24305971a
STEP: Updating secret s-test-opt-upd-913da70f-3ed8-11e9-8a62-3ec24305971a
STEP: Creating secret with name s-test-opt-create-913da737-3ed8-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:53:41.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5qrk2" for this suite.
Mar  4 23:54:05.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:54:06.213: INFO: namespace: e2e-tests-secrets-5qrk2, resource: bindings, ignored listing per whitelist
Mar  4 23:54:06.326: INFO: namespace e2e-tests-secrets-5qrk2 deletion completed in 24.386692347s

• [SLOW TEST:98.428 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:54:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-5tsx2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:54:09.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-5tsx2" for this suite.
Mar  4 23:54:34.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:54:34.426: INFO: namespace: e2e-tests-replication-controller-5tsx2, resource: bindings, ignored listing per whitelist
Mar  4 23:54:34.465: INFO: namespace e2e-tests-replication-controller-5tsx2 deletion completed in 24.460551807s

• [SLOW TEST:28.138 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:54:34.465: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-m68xp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  4 23:54:34.822: INFO: Waiting up to 5m0s for pod "pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-m68xp" to be "success or failure"
Mar  4 23:54:34.831: INFO: Pod "pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.42201ms
Mar  4 23:54:36.841: INFO: Pod "pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018941555s
STEP: Saw pod success
Mar  4 23:54:36.841: INFO: Pod "pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:54:36.855: INFO: Trying to get logs from node 10.190.208.159 pod pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:54:36.903: INFO: Waiting for pod pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a to disappear
Mar  4 23:54:36.911: INFO: Pod pod-dcb253ba-3ed8-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:54:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m68xp" for this suite.
Mar  4 23:54:42.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:54:43.234: INFO: namespace: e2e-tests-emptydir-m68xp, resource: bindings, ignored listing per whitelist
Mar  4 23:54:43.313: INFO: namespace e2e-tests-emptydir-m68xp deletion completed in 6.387586862s

• [SLOW TEST:8.848 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:54:43.315: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-6klb2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-jv66
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 23:54:43.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jv66" in namespace "e2e-tests-subpath-6klb2" to be "success or failure"
Mar  4 23:54:43.643: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Pending", Reason="", readiness=false. Elapsed: 7.819883ms
Mar  4 23:54:45.652: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015912826s
Mar  4 23:54:47.718: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 4.082784646s
Mar  4 23:54:49.727: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 6.091711788s
Mar  4 23:54:51.736: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 8.099984215s
Mar  4 23:54:53.744: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 10.108663221s
Mar  4 23:54:55.761: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 12.125231215s
Mar  4 23:54:57.809: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 14.172839431s
Mar  4 23:54:59.818: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 16.182198776s
Mar  4 23:55:01.827: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 18.191353174s
Mar  4 23:55:03.835: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 20.199706926s
Mar  4 23:55:05.844: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Running", Reason="", readiness=false. Elapsed: 22.208699854s
Mar  4 23:55:08.077: INFO: Pod "pod-subpath-test-secret-jv66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.440997912s
STEP: Saw pod success
Mar  4 23:55:08.077: INFO: Pod "pod-subpath-test-secret-jv66" satisfied condition "success or failure"
Mar  4 23:55:08.087: INFO: Trying to get logs from node 10.190.208.161 pod pod-subpath-test-secret-jv66 container test-container-subpath-secret-jv66: <nil>
STEP: delete the pod
Mar  4 23:55:08.133: INFO: Waiting for pod pod-subpath-test-secret-jv66 to disappear
Mar  4 23:55:08.141: INFO: Pod pod-subpath-test-secret-jv66 no longer exists
STEP: Deleting pod pod-subpath-test-secret-jv66
Mar  4 23:55:08.141: INFO: Deleting pod "pod-subpath-test-secret-jv66" in namespace "e2e-tests-subpath-6klb2"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:55:08.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6klb2" for this suite.
Mar  4 23:55:16.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:55:16.725: INFO: namespace: e2e-tests-subpath-6klb2, resource: bindings, ignored listing per whitelist
Mar  4 23:55:17.036: INFO: namespace e2e-tests-subpath-6klb2 deletion completed in 8.789593793s

• [SLOW TEST:33.722 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:55:17.038: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-s5x8l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar  4 23:55:17.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-s5x8l'
Mar  4 23:55:17.732: INFO: stderr: ""
Mar  4 23:55:17.732: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  4 23:55:18.741: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:55:18.741: INFO: Found 0 / 1
Mar  4 23:55:19.742: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:55:19.742: INFO: Found 0 / 1
Mar  4 23:55:20.742: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:55:20.743: INFO: Found 1 / 1
Mar  4 23:55:20.743: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  4 23:55:20.761: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:55:20.761: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 23:55:20.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 patch pod redis-master-ln8r2 --namespace=e2e-tests-kubectl-s5x8l -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  4 23:55:21.201: INFO: stderr: ""
Mar  4 23:55:21.201: INFO: stdout: "pod/redis-master-ln8r2 patched\n"
STEP: checking annotations
Mar  4 23:55:21.209: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:55:21.209: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:55:21.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s5x8l" for this suite.
Mar  4 23:55:43.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:55:43.717: INFO: namespace: e2e-tests-kubectl-s5x8l, resource: bindings, ignored listing per whitelist
Mar  4 23:55:43.781: INFO: namespace e2e-tests-kubectl-s5x8l deletion completed in 22.481547677s

• [SLOW TEST:26.743 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:55:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pnp9g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  4 23:55:44.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-pnp9g" to be "success or failure"
Mar  4 23:55:44.102: INFO: Pod "downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643986ms
Mar  4 23:55:46.112: INFO: Pod "downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019432801s
STEP: Saw pod success
Mar  4 23:55:46.112: INFO: Pod "downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:55:46.123: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  4 23:55:46.169: INFO: Waiting for pod downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:55:46.177: INFO: Pod downwardapi-volume-05fc37d2-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:55:46.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pnp9g" for this suite.
Mar  4 23:55:52.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:55:52.562: INFO: namespace: e2e-tests-projected-pnp9g, resource: bindings, ignored listing per whitelist
Mar  4 23:55:52.571: INFO: namespace e2e-tests-projected-pnp9g deletion completed in 6.382838417s

• [SLOW TEST:8.789 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:55:52.574: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-czfbm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar  4 23:55:53.020: INFO: Waiting up to 5m0s for pod "var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-var-expansion-czfbm" to be "success or failure"
Mar  4 23:55:53.029: INFO: Pod "var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.22267ms
Mar  4 23:55:55.341: INFO: Pod "var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.321696437s
STEP: Saw pod success
Mar  4 23:55:55.342: INFO: Pod "var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:55:55.350: INFO: Trying to get logs from node 10.190.208.159 pod var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  4 23:55:55.398: INFO: Waiting for pod var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:55:55.406: INFO: Pod var-expansion-0b417242-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:55:55.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-czfbm" for this suite.
Mar  4 23:56:01.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:56:01.798: INFO: namespace: e2e-tests-var-expansion-czfbm, resource: bindings, ignored listing per whitelist
Mar  4 23:56:02.166: INFO: namespace e2e-tests-var-expansion-czfbm deletion completed in 6.748563182s

• [SLOW TEST:9.592 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:56:02.166: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-zmhsc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:56:04.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-zmhsc" for this suite.
Mar  4 23:56:10.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:56:11.108: INFO: namespace: e2e-tests-emptydir-wrapper-zmhsc, resource: bindings, ignored listing per whitelist
Mar  4 23:56:11.314: INFO: namespace e2e-tests-emptydir-wrapper-zmhsc deletion completed in 6.6782376s

• [SLOW TEST:9.148 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:56:11.315: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ljjpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-ljjpt/configmap-test-16637e28-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:56:11.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-ljjpt" to be "success or failure"
Mar  4 23:56:11.629: INFO: Pod "pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.243727ms
Mar  4 23:56:13.637: INFO: Pod "pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016458733s
STEP: Saw pod success
Mar  4 23:56:13.637: INFO: Pod "pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:56:13.658: INFO: Trying to get logs from node 10.190.208.159 pod pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a container env-test: <nil>
STEP: delete the pod
Mar  4 23:56:13.714: INFO: Waiting for pod pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:56:13.722: INFO: Pod pod-configmaps-1664f0b0-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:56:13.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ljjpt" for this suite.
Mar  4 23:56:19.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:56:19.982: INFO: namespace: e2e-tests-configmap-ljjpt, resource: bindings, ignored listing per whitelist
Mar  4 23:56:20.170: INFO: namespace e2e-tests-configmap-ljjpt deletion completed in 6.436874118s

• [SLOW TEST:8.855 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:56:20.170: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-bz4pn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar  4 23:56:21.019: INFO: created pod pod-service-account-defaultsa
Mar  4 23:56:21.019: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  4 23:56:21.031: INFO: created pod pod-service-account-mountsa
Mar  4 23:56:21.031: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  4 23:56:21.100: INFO: created pod pod-service-account-nomountsa
Mar  4 23:56:21.100: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  4 23:56:21.112: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  4 23:56:21.112: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  4 23:56:21.124: INFO: created pod pod-service-account-mountsa-mountspec
Mar  4 23:56:21.124: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  4 23:56:21.133: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  4 23:56:21.133: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  4 23:56:21.144: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  4 23:56:21.144: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  4 23:56:21.156: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  4 23:56:21.156: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  4 23:56:21.166: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  4 23:56:21.167: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:56:21.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-bz4pn" for this suite.
Mar  4 23:56:27.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:56:27.723: INFO: namespace: e2e-tests-svcaccounts-bz4pn, resource: bindings, ignored listing per whitelist
Mar  4 23:56:27.943: INFO: namespace e2e-tests-svcaccounts-bz4pn deletion completed in 6.763273205s

• [SLOW TEST:7.773 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:56:27.944: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4mwwl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:56:28.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 version --client'
Mar  4 23:56:28.342: INFO: stderr: ""
Mar  4 23:56:28.342: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar  4 23:56:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-4mwwl'
Mar  4 23:56:28.604: INFO: stderr: ""
Mar  4 23:56:28.604: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  4 23:56:28.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-4mwwl'
Mar  4 23:56:28.909: INFO: stderr: ""
Mar  4 23:56:28.909: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  4 23:56:29.995: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:56:29.996: INFO: Found 0 / 1
Mar  4 23:56:30.918: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:56:30.918: INFO: Found 1 / 1
Mar  4 23:56:30.918: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  4 23:56:30.926: INFO: Selector matched 1 pods for map[app:redis]
Mar  4 23:56:30.926: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 23:56:30.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 describe pod redis-master-jbmbd --namespace=e2e-tests-kubectl-4mwwl'
Mar  4 23:56:31.058: INFO: stderr: ""
Mar  4 23:56:31.058: INFO: stdout: "Name:               redis-master-jbmbd\nNamespace:          e2e-tests-kubectl-4mwwl\nPriority:           0\nPriorityClassName:  <none>\nNode:               10.190.208.159/10.190.208.159\nStart Time:         Mon, 04 Mar 2019 23:56:28 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 172.30.189.216\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://ade4c361afaa7d5e62b298115d3dba99fc6fbf1a6523b2a6cd15f2d4fd809062\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Mar 2019 23:56:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tlgzc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tlgzc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tlgzc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  3s    default-scheduler        Successfully assigned e2e-tests-kubectl-4mwwl/redis-master-jbmbd to 10.190.208.159\n  Normal  Pulled     2s    kubelet, 10.190.208.159  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, 10.190.208.159  Created container\n  Normal  Started    2s    kubelet, 10.190.208.159  Started container\n"
Mar  4 23:56:31.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 describe rc redis-master --namespace=e2e-tests-kubectl-4mwwl'
Mar  4 23:56:31.203: INFO: stderr: ""
Mar  4 23:56:31.203: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-4mwwl\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-jbmbd\n"
Mar  4 23:56:31.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 describe service redis-master --namespace=e2e-tests-kubectl-4mwwl'
Mar  4 23:56:31.414: INFO: stderr: ""
Mar  4 23:56:31.414: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-4mwwl\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.132.244\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.189.216:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  4 23:56:31.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 describe node 10.190.208.159'
Mar  4 23:56:31.585: INFO: stderr: ""
Mar  4 23:56:31.585: INFO: stdout: "Name:               10.190.208.159\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=u2c.2x4.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc07\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/machine-type=u2c.2x4.encrypted\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-pool-id=f5c01560778e42788b3a74d802406e9d-f6a947e\n                    ibm-cloud.kubernetes.io/worker-version=1.13.4_1510\n                    kubernetes.io/hostname=10.190.208.159\n                    privateVLAN=2561669\n                    publicVLAN=2561667\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Mar 2019 21:33:22 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 04 Mar 2019 23:56:29 +0000   Mon, 04 Mar 2019 21:33:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 04 Mar 2019 23:56:29 +0000   Mon, 04 Mar 2019 21:33:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 04 Mar 2019 23:56:29 +0000   Mon, 04 Mar 2019 21:33:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 04 Mar 2019 23:56:29 +0000   Mon, 04 Mar 2019 21:33:32 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.190.208.159\n  ExternalIP:  169.62.47.86\n  Hostname:    10.190.208.159\nCapacity:\n cpu:                2\n ephemeral-storage:  103079200Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4041552Ki\n pods:               110\nAllocatable:\n cpu:                1920m\n ephemeral-storage:  100275445682\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3535696Ki\n pods:               110\nSystem Info:\n Machine ID:                 266c2075dace453da02500b328c9e325\n System UUID:                F8CE50E9-36B8-8D23-FD03-175CC93EDEAD\n Boot ID:                    4eee33a3-d652-4819-b91d-f55ca4f3bd9c\n Kernel Version:             4.4.0-142-generic\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.4\n Kubelet Version:            v1.13.4+IKS\n Kube-Proxy Version:         v1.13.4+IKS\nProviderID:                  ibm://d18c889395112a40d2f4e3065f237a7d///f5c01560778e42788b3a74d802406e9d/kube-wdc07-crf5c01560778e42788b3a74d802406e9d-w1\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-4mwwl    redis-master-jbmbd                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-fdlpz            0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  ibm-system                 ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-4cddx                5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         141m\n  kube-system                calico-node-78pl9                                                  250m (13%)    0 (0%)      80Mi (2%)        0 (0%)         143m\n  kube-system                ibm-keepalived-watcher-f79g9                                       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         143m\n  kube-system                ibm-kube-fluentd-7dbzh                                             25m (1%)      300m (15%)  50Mi (1%)        800M (22%)     139m\n  kube-system                ibm-master-proxy-static-10.190.208.159                             25m (1%)      300m (15%)  32M (0%)         512M (14%)     143m\n  kube-system                public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-2lskk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         139m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                310m (16%)     600m (31%)\n  memory             184850Ki (5%)  1312M (36%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Mar  4 23:56:31.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 describe namespace e2e-tests-kubectl-4mwwl'
Mar  4 23:56:31.731: INFO: stderr: ""
Mar  4 23:56:31.731: INFO: stdout: "Name:         e2e-tests-kubectl-4mwwl\nLabels:       e2e-framework=kubectl\n              e2e-run=c430e77d-3ed1-11e9-8a62-3ec24305971a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:56:31.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4mwwl" for this suite.
Mar  4 23:56:56.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:56:56.222: INFO: namespace: e2e-tests-kubectl-4mwwl, resource: bindings, ignored listing per whitelist
Mar  4 23:56:56.372: INFO: namespace e2e-tests-kubectl-4mwwl deletion completed in 24.369669825s

• [SLOW TEST:28.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:56:56.372: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sl4d6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:56:56.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 version'
Mar  4 23:56:56.790: INFO: stderr: ""
Mar  4 23:56:56.790: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4+IKS\", GitCommit:\"30d33e6c72d6f771168f5a6ab10973590a044dc4\", GitTreeState:\"clean\", BuildDate:\"2019-03-01T15:25:52Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:56:56.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sl4d6" for this suite.
Mar  4 23:57:02.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:03.239: INFO: namespace: e2e-tests-kubectl-sl4d6, resource: bindings, ignored listing per whitelist
Mar  4 23:57:03.282: INFO: namespace e2e-tests-kubectl-sl4d6 deletion completed in 6.482525305s

• [SLOW TEST:6.910 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:03.282: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-g5m96
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  4 23:57:03.571: INFO: Waiting up to 5m0s for pod "downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-g5m96" to be "success or failure"
Mar  4 23:57:03.582: INFO: Pod "downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091555ms
Mar  4 23:57:05.590: INFO: Pod "downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019480321s
Mar  4 23:57:07.602: INFO: Pod "downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031331843s
STEP: Saw pod success
Mar  4 23:57:07.602: INFO: Pod "downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:57:07.613: INFO: Trying to get logs from node 10.190.208.161 pod downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  4 23:57:07.662: INFO: Waiting for pod downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:57:07.670: INFO: Pod downward-api-355b6818-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:07.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-g5m96" for this suite.
Mar  4 23:57:13.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:14.237: INFO: namespace: e2e-tests-downward-api-g5m96, resource: bindings, ignored listing per whitelist
Mar  4 23:57:14.271: INFO: namespace e2e-tests-downward-api-g5m96 deletion completed in 6.59079135s

• [SLOW TEST:10.989 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:14.273: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-4mt5d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-3beb9895-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:57:14.589: INFO: Waiting up to 5m0s for pod "pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-4mt5d" to be "success or failure"
Mar  4 23:57:14.597: INFO: Pod "pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.170629ms
Mar  4 23:57:16.607: INFO: Pod "pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017385702s
STEP: Saw pod success
Mar  4 23:57:16.607: INFO: Pod "pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:57:16.615: INFO: Trying to get logs from node 10.190.208.159 pod pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:57:16.700: INFO: Waiting for pod pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:57:16.708: INFO: Pod pod-secrets-3bece395-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:16.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4mt5d" for this suite.
Mar  4 23:57:22.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:23.132: INFO: namespace: e2e-tests-secrets-4mt5d, resource: bindings, ignored listing per whitelist
Mar  4 23:57:23.207: INFO: namespace e2e-tests-secrets-4mt5d deletion completed in 6.48626903s

• [SLOW TEST:8.934 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:23.207: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x8ljc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-413e534a-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  4 23:57:23.518: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-x8ljc" to be "success or failure"
Mar  4 23:57:23.528: INFO: Pod "pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.59649ms
Mar  4 23:57:25.537: INFO: Pod "pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019066906s
STEP: Saw pod success
Mar  4 23:57:25.537: INFO: Pod "pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:57:25.545: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 23:57:25.600: INFO: Waiting for pod pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:57:25.608: INFO: Pod pod-projected-secrets-413f8f37-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:25.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x8ljc" for this suite.
Mar  4 23:57:31.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:31.728: INFO: namespace: e2e-tests-projected-x8ljc, resource: bindings, ignored listing per whitelist
Mar  4 23:57:31.978: INFO: namespace e2e-tests-projected-x8ljc deletion completed in 6.359715313s

• [SLOW TEST:8.771 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:31.978: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s4hgj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-467f8a4a-3ed9-11e9-8a62-3ec24305971a
STEP: Creating secret with name secret-projected-all-test-volume-467f8a2a-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  4 23:57:32.344: INFO: Waiting up to 5m0s for pod "projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-s4hgj" to be "success or failure"
Mar  4 23:57:32.351: INFO: Pod "projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487967ms
Mar  4 23:57:34.360: INFO: Pod "projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016596501s
STEP: Saw pod success
Mar  4 23:57:34.360: INFO: Pod "projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:57:34.370: INFO: Trying to get logs from node 10.190.208.159 pod projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  4 23:57:34.421: INFO: Waiting for pod projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:57:34.429: INFO: Pod projected-volume-467f89e1-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:34.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s4hgj" for this suite.
Mar  4 23:57:40.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:40.627: INFO: namespace: e2e-tests-projected-s4hgj, resource: bindings, ignored listing per whitelist
Mar  4 23:57:40.867: INFO: namespace e2e-tests-projected-s4hgj deletion completed in 6.427636471s

• [SLOW TEST:8.888 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:40.867: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-h9nft
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar  4 23:57:41.171: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 23:57:41.195: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 23:57:41.202: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.159 before test
Mar  4 23:57:41.231: INFO: ibm-keepalived-watcher-f79g9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:57:41.231: INFO: ibm-kube-fluentd-7dbzh from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:57:41.231: INFO: ibm-master-proxy-static-10.190.208.159 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:57:41.231: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-4cddx from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:57:41.231: INFO: calico-node-78pl9 from kube-system started at 2019-03-04 21:33:22 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:57:41.231: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-2lskk from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:57:41.231: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:57:41.231: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:57:41.231: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:57:41.231: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-fdlpz from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:57:41.231: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:57:41.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:57:41.231: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.161 before test
Mar  4 23:57:41.258: INFO: calico-node-lgpp2 from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.258: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 23:57:41.258: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-04 23:03:19 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.258: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 23:57:41.258: INFO: ibm-master-proxy-static-10.190.208.161 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:57:41.258: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-03-04 23:03:13 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.258: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  4 23:57:41.258: INFO: sonobuoy-e2e-job-903cc3ae0952453b from heptio-sonobuoy started at 2019-03-04 23:03:23 +0000 UTC (2 container statuses recorded)
Mar  4 23:57:41.259: INFO: 	Container e2e ready: true, restart count 0
Mar  4 23:57:41.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:57:41.259: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-pm7j4 from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:57:41.259: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:57:41.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:57:41.259: INFO: ibm-kube-fluentd-9gf98 from kube-system started at 2019-03-04 21:42:06 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.259: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:57:41.259: INFO: ibm-keepalived-watcher-mnmzt from kube-system started at 2019-03-04 21:42:07 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.259: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:57:41.259: INFO: 
Logging pods the kubelet thinks is on node 10.190.208.164 before test
Mar  4 23:57:41.293: INFO: ibm-storage-watcher-7f87684475-2hlnh from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.293: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  4 23:57:41.293: INFO: ibm-cloud-provider-ip-169-62-47-38-5c5b88d844-cz8tp from ibm-system started at 2019-03-04 21:34:59 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.293: INFO: 	Container ibm-cloud-provider-ip-169-62-47-38 ready: true, restart count 0
Mar  4 23:57:41.293: INFO: vpn-74bb4868b9-gwrxn from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.293: INFO: 	Container vpn ready: true, restart count 0
Mar  4 23:57:41.294: INFO: calico-kube-controllers-65868f965d-4fq52 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 23:57:41.294: INFO: public-crf5c01560778e42788b3a74d802406e9d-alb1-558657d9fb-gjlc9 from kube-system started at 2019-03-04 21:37:14 +0000 UTC (4 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Mar  4 23:57:41.294: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Mar  4 23:57:41.294: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Mar  4 23:57:41.294: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  4 23:57:41.294: INFO: ibm-file-plugin-6578dcb564-vkqtt from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  4 23:57:41.294: INFO: coredns-autoscaler-64f9c5b4df-hd2v9 from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container autoscaler ready: true, restart count 0
Mar  4 23:57:41.294: INFO: metrics-server-657cbf4579-s554h from kube-system started at 2019-03-04 21:33:04 +0000 UTC (2 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container metrics-server ready: true, restart count 0
Mar  4 23:57:41.294: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  4 23:57:41.294: INFO: sonobuoy-systemd-logs-daemon-set-90d888cc88ae492f-bp8dg from heptio-sonobuoy started at 2019-03-04 23:03:24 +0000 UTC (2 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar  4 23:57:41.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 23:57:41.294: INFO: kubernetes-dashboard-7996b848f4-gbncd from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.294: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  4 23:57:41.294: INFO: ibm-kube-fluentd-4796f from kube-system started at 2019-03-04 21:37:08 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.295: INFO: 	Container fluentd ready: true, restart count 0
Mar  4 23:57:41.295: INFO: ibm-master-proxy-static-10.190.208.164 from kube-system started at <nil> (0 container statuses recorded)
Mar  4 23:57:41.295: INFO: ibm-keepalived-watcher-8m6km from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.295: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  4 23:57:41.295: INFO: coredns-58d696879-cck9d from kube-system started at 2019-03-04 21:32:47 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.295: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:57:41.295: INFO: coredns-58d696879-qwvbn from kube-system started at 2019-03-04 21:33:30 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.295: INFO: 	Container coredns ready: true, restart count 0
Mar  4 23:57:41.295: INFO: calico-node-26clt from kube-system started at 2019-03-04 21:32:37 +0000 UTC (1 container statuses recorded)
Mar  4 23:57:41.295: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4d17ca8c-3ed9-11e9-8a62-3ec24305971a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-4d17ca8c-3ed9-11e9-8a62-3ec24305971a off the node 10.190.208.161
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d17ca8c-3ed9-11e9-8a62-3ec24305971a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:45.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-h9nft" for this suite.
Mar  4 23:57:57.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:57:57.745: INFO: namespace: e2e-tests-sched-pred-h9nft, resource: bindings, ignored listing per whitelist
Mar  4 23:57:57.923: INFO: namespace e2e-tests-sched-pred-h9nft deletion completed in 12.395416139s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:17.056 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:57:57.925: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-ft85f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:57:58.330: INFO: (0) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.009532ms)
Mar  4 23:57:58.347: INFO: (1) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.541571ms)
Mar  4 23:57:58.360: INFO: (2) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.53358ms)
Mar  4 23:57:58.374: INFO: (3) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.620121ms)
Mar  4 23:57:58.388: INFO: (4) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.801985ms)
Mar  4 23:57:58.402: INFO: (5) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.507651ms)
Mar  4 23:57:58.414: INFO: (6) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.806092ms)
Mar  4 23:57:58.427: INFO: (7) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.601641ms)
Mar  4 23:57:58.440: INFO: (8) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.035804ms)
Mar  4 23:57:58.455: INFO: (9) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.477126ms)
Mar  4 23:57:58.469: INFO: (10) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.109815ms)
Mar  4 23:57:58.483: INFO: (11) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.268963ms)
Mar  4 23:57:58.497: INFO: (12) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.848581ms)
Mar  4 23:57:58.511: INFO: (13) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.282001ms)
Mar  4 23:57:58.524: INFO: (14) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.970511ms)
Mar  4 23:57:58.538: INFO: (15) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.470455ms)
Mar  4 23:57:58.552: INFO: (16) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.018774ms)
Mar  4 23:57:58.566: INFO: (17) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.475689ms)
Mar  4 23:57:58.586: INFO: (18) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 19.772496ms)
Mar  4 23:57:58.599: INFO: (19) /api/v1/nodes/10.190.208.159/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.270535ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:57:58.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ft85f" for this suite.
Mar  4 23:58:04.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:04.909: INFO: namespace: e2e-tests-proxy-ft85f, resource: bindings, ignored listing per whitelist
Mar  4 23:58:05.047: INFO: namespace e2e-tests-proxy-ft85f deletion completed in 6.437203506s

• [SLOW TEST:7.123 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:05.049: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7htck
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5a3785d0-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:58:05.419: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-7htck" to be "success or failure"
Mar  4 23:58:05.427: INFO: Pod "pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06011ms
Mar  4 23:58:07.437: INFO: Pod "pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018000488s
STEP: Saw pod success
Mar  4 23:58:07.437: INFO: Pod "pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:58:07.447: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:58:07.527: INFO: Waiting for pod pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:58:07.534: INFO: Pod pod-projected-configmaps-5a38e819-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7htck" for this suite.
Mar  4 23:58:13.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:13.870: INFO: namespace: e2e-tests-projected-7htck, resource: bindings, ignored listing per whitelist
Mar  4 23:58:13.884: INFO: namespace e2e-tests-projected-7htck deletion completed in 6.339239957s

• [SLOW TEST:8.835 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:13.885: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-szl84
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  4 23:58:14.201: INFO: Waiting up to 5m0s for pod "pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-szl84" to be "success or failure"
Mar  4 23:58:14.209: INFO: Pod "pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016219ms
Mar  4 23:58:16.247: INFO: Pod "pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045440585s
Mar  4 23:58:18.256: INFO: Pod "pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054249605s
STEP: Saw pod success
Mar  4 23:58:18.256: INFO: Pod "pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:58:18.264: INFO: Trying to get logs from node 10.190.208.161 pod pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:58:18.311: INFO: Waiting for pod pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:58:18.318: INFO: Pod pod-5f728cdf-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:18.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-szl84" for this suite.
Mar  4 23:58:24.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:24.763: INFO: namespace: e2e-tests-emptydir-szl84, resource: bindings, ignored listing per whitelist
Mar  4 23:58:24.764: INFO: namespace e2e-tests-emptydir-szl84 deletion completed in 6.435830344s

• [SLOW TEST:10.879 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:24.764: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-ltjn4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:29.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-ltjn4" for this suite.
Mar  4 23:58:35.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:35.515: INFO: namespace: e2e-tests-kubelet-test-ltjn4, resource: bindings, ignored listing per whitelist
Mar  4 23:58:35.579: INFO: namespace e2e-tests-kubelet-test-ltjn4 deletion completed in 6.481713937s

• [SLOW TEST:10.816 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vlcqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  4 23:58:35.877: INFO: Waiting up to 5m0s for pod "pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-vlcqg" to be "success or failure"
Mar  4 23:58:35.900: INFO: Pod "pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.856051ms
Mar  4 23:58:37.908: INFO: Pod "pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030476881s
STEP: Saw pod success
Mar  4 23:58:37.908: INFO: Pod "pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:58:37.919: INFO: Trying to get logs from node 10.190.208.161 pod pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  4 23:58:38.072: INFO: Waiting for pod pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:58:38.087: INFO: Pod pod-6c60aab5-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:38.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vlcqg" for this suite.
Mar  4 23:58:44.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:44.383: INFO: namespace: e2e-tests-emptydir-vlcqg, resource: bindings, ignored listing per whitelist
Mar  4 23:58:44.460: INFO: namespace e2e-tests-emptydir-vlcqg deletion completed in 6.362994024s

• [SLOW TEST:8.879 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:44.461: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8rhw2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-71b510fe-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  4 23:58:44.828: INFO: Waiting up to 5m0s for pod "pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-8rhw2" to be "success or failure"
Mar  4 23:58:44.839: INFO: Pod "pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.944416ms
Mar  4 23:58:46.848: INFO: Pod "pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019865301s
STEP: Saw pod success
Mar  4 23:58:46.848: INFO: Pod "pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  4 23:58:46.857: INFO: Trying to get logs from node 10.190.208.159 pod pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 23:58:46.926: INFO: Waiting for pod pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  4 23:58:46.939: INFO: Pod pod-configmaps-71b686a8-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:46.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8rhw2" for this suite.
Mar  4 23:58:52.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:58:53.324: INFO: namespace: e2e-tests-configmap-8rhw2, resource: bindings, ignored listing per whitelist
Mar  4 23:58:53.472: INFO: namespace e2e-tests-configmap-8rhw2 deletion completed in 6.520633182s

• [SLOW TEST:9.011 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:58:53.474: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-jh6q6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:58:53.853: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Mar  4 23:58:53.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jh6q6/daemonsets","resourceVersion":"26836"},"items":null}

Mar  4 23:58:53.877: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jh6q6/pods","resourceVersion":"26836"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  4 23:58:53.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jh6q6" for this suite.
Mar  4 23:58:59.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  4 23:59:00.034: INFO: namespace: e2e-tests-daemonsets-jh6q6, resource: bindings, ignored listing per whitelist
Mar  4 23:59:00.250: INFO: namespace e2e-tests-daemonsets-jh6q6 deletion completed in 6.322635103s

S [SKIPPING] [6.777 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar  4 23:58:53.853: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  4 23:59:00.250: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-bw29g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  4 23:59:00.647: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  4 23:59:00.664: INFO: Number of nodes with available pods: 0
Mar  4 23:59:00.664: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  4 23:59:00.702: INFO: Number of nodes with available pods: 0
Mar  4 23:59:00.702: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:01.711: INFO: Number of nodes with available pods: 0
Mar  4 23:59:01.711: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:02.712: INFO: Number of nodes with available pods: 1
Mar  4 23:59:02.712: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  4 23:59:02.820: INFO: Number of nodes with available pods: 1
Mar  4 23:59:02.820: INFO: Number of running nodes: 0, number of available pods: 1
Mar  4 23:59:03.829: INFO: Number of nodes with available pods: 0
Mar  4 23:59:03.829: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  4 23:59:03.850: INFO: Number of nodes with available pods: 0
Mar  4 23:59:03.850: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:04.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:04.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:05.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:05.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:06.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:06.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:07.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:07.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:08.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:08.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:09.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:09.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:10.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:10.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:11.858: INFO: Number of nodes with available pods: 0
Mar  4 23:59:11.858: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:12.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:12.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:13.858: INFO: Number of nodes with available pods: 0
Mar  4 23:59:13.858: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:14.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:14.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:15.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:15.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:16.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:16.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:17.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:17.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:18.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:18.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:19.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:19.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:20.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:20.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:21.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:21.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:22.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:22.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:23.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:23.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:24.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:24.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:25.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:25.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:26.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:26.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:27.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:27.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:28.862: INFO: Number of nodes with available pods: 0
Mar  4 23:59:28.862: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:29.900: INFO: Number of nodes with available pods: 0
Mar  4 23:59:29.900: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:30.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:30.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:31.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:31.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:32.860: INFO: Number of nodes with available pods: 0
Mar  4 23:59:32.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:33.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:33.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:34.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:34.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:35.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:35.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:36.889: INFO: Number of nodes with available pods: 0
Mar  4 23:59:36.889: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:37.864: INFO: Number of nodes with available pods: 0
Mar  4 23:59:37.864: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:38.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:38.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:39.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:39.859: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:40.859: INFO: Number of nodes with available pods: 0
Mar  4 23:59:40.860: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  4 23:59:41.859: INFO: Number of nodes with available pods: 1
Mar  4 23:59:41.859: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-bw29g, will wait for the garbage collector to delete the pods
Mar  4 23:59:41.954: INFO: Deleting DaemonSet.extensions daemon-set took: 19.748048ms
Mar  4 23:59:42.055: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.208627ms
Mar  5 00:00:20.663: INFO: Number of nodes with available pods: 0
Mar  5 00:00:20.663: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 00:00:20.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-bw29g/daemonsets","resourceVersion":"27053"},"items":null}

Mar  5 00:00:20.678: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-bw29g/pods","resourceVersion":"27053"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:00:20.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-bw29g" for this suite.
Mar  5 00:00:26.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:00:27.020: INFO: namespace: e2e-tests-daemonsets-bw29g, resource: bindings, ignored listing per whitelist
Mar  5 00:00:27.063: INFO: namespace e2e-tests-daemonsets-bw29g deletion completed in 6.312894598s

• [SLOW TEST:86.813 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:00:27.068: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6h9jj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-aed5066f-3ed9-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:00:27.385: INFO: Waiting up to 5m0s for pod "pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-6h9jj" to be "success or failure"
Mar  5 00:00:27.394: INFO: Pod "pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.839524ms
Mar  5 00:00:29.402: INFO: Pod "pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017203119s
STEP: Saw pod success
Mar  5 00:00:29.402: INFO: Pod "pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:00:29.410: INFO: Trying to get logs from node 10.190.208.161 pod pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 00:00:29.464: INFO: Waiting for pod pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  5 00:00:29.473: INFO: Pod pod-secrets-aed64f40-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:00:29.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6h9jj" for this suite.
Mar  5 00:00:35.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:00:35.836: INFO: namespace: e2e-tests-secrets-6h9jj, resource: bindings, ignored listing per whitelist
Mar  5 00:00:35.859: INFO: namespace e2e-tests-secrets-6h9jj deletion completed in 6.375080985s

• [SLOW TEST:8.792 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:00:35.859: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-wrnj2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar  5 00:00:36.821: INFO: Waiting up to 5m0s for pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77" in namespace "e2e-tests-svcaccounts-wrnj2" to be "success or failure"
Mar  5 00:00:36.829: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.546895ms
Mar  5 00:00:38.838: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017387799s
Mar  5 00:00:40.849: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028130476s
STEP: Saw pod success
Mar  5 00:00:40.849: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77" satisfied condition "success or failure"
Mar  5 00:00:40.857: INFO: Trying to get logs from node 10.190.208.159 pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77 container token-test: <nil>
STEP: delete the pod
Mar  5 00:00:40.910: INFO: Waiting for pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77 to disappear
Mar  5 00:00:40.918: INFO: Pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-dmf77 no longer exists
STEP: Creating a pod to test consume service account root CA
Mar  5 00:00:40.930: INFO: Waiting up to 5m0s for pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn" in namespace "e2e-tests-svcaccounts-wrnj2" to be "success or failure"
Mar  5 00:00:40.940: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.474909ms
Mar  5 00:00:42.949: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018766463s
Mar  5 00:00:44.959: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028415362s
STEP: Saw pod success
Mar  5 00:00:44.959: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn" satisfied condition "success or failure"
Mar  5 00:00:44.966: INFO: Trying to get logs from node 10.190.208.161 pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn container root-ca-test: <nil>
STEP: delete the pod
Mar  5 00:00:45.017: INFO: Waiting for pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn to disappear
Mar  5 00:00:45.025: INFO: Pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-gn7wn no longer exists
STEP: Creating a pod to test consume service account namespace
Mar  5 00:00:45.035: INFO: Waiting up to 5m0s for pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87" in namespace "e2e-tests-svcaccounts-wrnj2" to be "success or failure"
Mar  5 00:00:45.044: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.053394ms
Mar  5 00:00:47.053: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017719237s
STEP: Saw pod success
Mar  5 00:00:47.053: INFO: Pod "pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87" satisfied condition "success or failure"
Mar  5 00:00:47.062: INFO: Trying to get logs from node 10.190.208.159 pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87 container namespace-test: <nil>
STEP: delete the pod
Mar  5 00:00:47.124: INFO: Waiting for pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87 to disappear
Mar  5 00:00:47.131: INFO: Pod pod-service-account-b45f7660-3ed9-11e9-8a62-3ec24305971a-29v87 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:00:47.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-wrnj2" for this suite.
Mar  5 00:00:53.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:00:53.232: INFO: namespace: e2e-tests-svcaccounts-wrnj2, resource: bindings, ignored listing per whitelist
Mar  5 00:00:53.600: INFO: namespace e2e-tests-svcaccounts-wrnj2 deletion completed in 6.457299794s

• [SLOW TEST:17.740 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:00:53.600: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tmwqk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:00:54.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-tmwqk" to be "success or failure"
Mar  5 00:00:54.007: INFO: Pod "downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.59659ms
Mar  5 00:00:56.017: INFO: Pod "downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016638546s
STEP: Saw pod success
Mar  5 00:00:56.017: INFO: Pod "downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:00:56.024: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:00:56.070: INFO: Waiting for pod downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a to disappear
Mar  5 00:00:56.100: INFO: Pod downwardapi-volume-beabf6ea-3ed9-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:00:56.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tmwqk" for this suite.
Mar  5 00:01:02.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:01:02.251: INFO: namespace: e2e-tests-downward-api-tmwqk, resource: bindings, ignored listing per whitelist
Mar  5 00:01:02.621: INFO: namespace e2e-tests-downward-api-tmwqk deletion completed in 6.510936643s

• [SLOW TEST:9.021 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:01:02.621: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-4zh9p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-4zh9p
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-4zh9p
STEP: Deleting pre-stop pod
Mar  5 00:01:16.075: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:01:16.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-4zh9p" for this suite.
Mar  5 00:01:56.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:01:56.279: INFO: namespace: e2e-tests-prestop-4zh9p, resource: bindings, ignored listing per whitelist
Mar  5 00:01:56.453: INFO: namespace e2e-tests-prestop-4zh9p deletion completed in 40.353199309s

• [SLOW TEST:53.832 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:01:56.454: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-s7vpb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 00:01:56.815: INFO: Number of nodes with available pods: 0
Mar  5 00:01:56.815: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:01:57.835: INFO: Number of nodes with available pods: 0
Mar  5 00:01:57.835: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:01:58.836: INFO: Number of nodes with available pods: 0
Mar  5 00:01:58.836: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:01:59.834: INFO: Number of nodes with available pods: 2
Mar  5 00:01:59.834: INFO: Node 10.190.208.164 is running more than one daemon pod
Mar  5 00:02:00.834: INFO: Number of nodes with available pods: 3
Mar  5 00:02:00.834: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  5 00:02:00.926: INFO: Number of nodes with available pods: 2
Mar  5 00:02:00.926: INFO: Node 10.190.208.164 is running more than one daemon pod
Mar  5 00:02:01.948: INFO: Number of nodes with available pods: 2
Mar  5 00:02:01.948: INFO: Node 10.190.208.164 is running more than one daemon pod
Mar  5 00:02:02.947: INFO: Number of nodes with available pods: 3
Mar  5 00:02:02.947: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-s7vpb, will wait for the garbage collector to delete the pods
Mar  5 00:02:03.045: INFO: Deleting DaemonSet.extensions daemon-set took: 19.343042ms
Mar  5 00:02:03.145: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.366305ms
Mar  5 00:02:45.855: INFO: Number of nodes with available pods: 0
Mar  5 00:02:45.855: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 00:02:45.867: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-s7vpb/daemonsets","resourceVersion":"27655"},"items":null}

Mar  5 00:02:45.875: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-s7vpb/pods","resourceVersion":"27655"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:02:45.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-s7vpb" for this suite.
Mar  5 00:02:51.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:02:52.236: INFO: namespace: e2e-tests-daemonsets-s7vpb, resource: bindings, ignored listing per whitelist
Mar  5 00:02:52.257: INFO: namespace e2e-tests-daemonsets-s7vpb deletion completed in 6.33814047s

• [SLOW TEST:55.803 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:02:52.257: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-r77kh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  5 00:02:52.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27711,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  5 00:02:52.605: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27713,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  5 00:02:52.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27714,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  5 00:03:02.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27768,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 00:03:02.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27769,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  5 00:03:02.686: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-r77kh,SelfLink:/api/v1/namespaces/e2e-tests-watch-r77kh/configmaps/e2e-watch-test-label-changed,UID:0560fa43-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:27770,Generation:0,CreationTimestamp:2019-03-05 00:02:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:03:02.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-r77kh" for this suite.
Mar  5 00:03:08.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:03:08.925: INFO: namespace: e2e-tests-watch-r77kh, resource: bindings, ignored listing per whitelist
Mar  5 00:03:09.466: INFO: namespace e2e-tests-watch-r77kh deletion completed in 6.765292521s

• [SLOW TEST:17.209 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:03:09.467: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-j8vz6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  5 00:03:09.770: INFO: Waiting up to 5m0s for pod "downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-j8vz6" to be "success or failure"
Mar  5 00:03:09.807: INFO: Pod "downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 37.646233ms
Mar  5 00:03:11.816: INFO: Pod "downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045979526s
STEP: Saw pod success
Mar  5 00:03:11.816: INFO: Pod "downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:03:11.823: INFO: Trying to get logs from node 10.190.208.159 pod downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  5 00:03:11.874: INFO: Waiting for pod downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:03:11.882: INFO: Pod downward-api-0fa16c8c-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:03:11.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j8vz6" for this suite.
Mar  5 00:03:17.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:03:18.060: INFO: namespace: e2e-tests-downward-api-j8vz6, resource: bindings, ignored listing per whitelist
Mar  5 00:03:18.270: INFO: namespace e2e-tests-downward-api-j8vz6 deletion completed in 6.369707365s

• [SLOW TEST:8.803 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:03:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-hlj8w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:03:18.604: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:03:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hlj8w" for this suite.
Mar  5 00:04:07.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:04:07.159: INFO: namespace: e2e-tests-pods-hlj8w, resource: bindings, ignored listing per whitelist
Mar  5 00:04:07.603: INFO: namespace e2e-tests-pods-hlj8w deletion completed in 46.63610994s

• [SLOW TEST:49.332 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:04:07.603: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lgvx8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-324b3abd-3eda-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:04:08.024: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-lgvx8" to be "success or failure"
Mar  5 00:04:08.032: INFO: Pod "pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.448769ms
Mar  5 00:04:10.041: INFO: Pod "pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01751446s
Mar  5 00:04:12.050: INFO: Pod "pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0266737s
STEP: Saw pod success
Mar  5 00:04:12.051: INFO: Pod "pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:04:12.059: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 00:04:12.102: INFO: Waiting for pod pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:04:12.110: INFO: Pod pod-projected-secrets-325a4094-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:04:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lgvx8" for this suite.
Mar  5 00:04:18.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:04:18.308: INFO: namespace: e2e-tests-projected-lgvx8, resource: bindings, ignored listing per whitelist
Mar  5 00:04:18.451: INFO: namespace e2e-tests-projected-lgvx8 deletion completed in 6.328940915s

• [SLOW TEST:10.848 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:04:18.452: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-v9bxn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-v9bxn
Mar  5 00:04:20.815: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-v9bxn
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 00:04:20.823: INFO: Initial restart count of pod liveness-exec is 0
Mar  5 00:05:05.045: INFO: Restart count of pod e2e-tests-container-probe-v9bxn/liveness-exec is now 1 (44.222046109s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:05:05.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-v9bxn" for this suite.
Mar  5 00:05:11.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:05:11.354: INFO: namespace: e2e-tests-container-probe-v9bxn, resource: bindings, ignored listing per whitelist
Mar  5 00:05:11.587: INFO: namespace e2e-tests-container-probe-v9bxn deletion completed in 6.476348421s

• [SLOW TEST:53.135 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:05:11.588: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mvdxl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-586b01d9-3eda-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:05:11.916: INFO: Waiting up to 5m0s for pod "pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-mvdxl" to be "success or failure"
Mar  5 00:05:11.923: INFO: Pod "pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497932ms
Mar  5 00:05:13.932: INFO: Pod "pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015762112s
STEP: Saw pod success
Mar  5 00:05:13.932: INFO: Pod "pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:05:13.939: INFO: Trying to get logs from node 10.190.208.159 pod pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a container secret-env-test: <nil>
STEP: delete the pod
Mar  5 00:05:13.986: INFO: Waiting for pod pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:05:13.993: INFO: Pod pod-secrets-586f6614-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:05:13.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mvdxl" for this suite.
Mar  5 00:05:20.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:05:20.271: INFO: namespace: e2e-tests-secrets-mvdxl, resource: bindings, ignored listing per whitelist
Mar  5 00:05:20.463: INFO: namespace e2e-tests-secrets-mvdxl deletion completed in 6.458542618s

• [SLOW TEST:8.875 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:05:20.463: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-zdlqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 00:05:20.822: INFO: Number of nodes with available pods: 0
Mar  5 00:05:20.822: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:05:21.841: INFO: Number of nodes with available pods: 1
Mar  5 00:05:21.842: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:05:22.900: INFO: Number of nodes with available pods: 3
Mar  5 00:05:22.900: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  5 00:05:22.950: INFO: Number of nodes with available pods: 2
Mar  5 00:05:22.950: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:23.971: INFO: Number of nodes with available pods: 2
Mar  5 00:05:23.971: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:24.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:24.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:26.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:26.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:27.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:27.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:27.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:27.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:28.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:28.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:29.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:29.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:30.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:30.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:31.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:31.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:32.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:32.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:34.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:34.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:35.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:35.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:36.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:36.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:36.973: INFO: Number of nodes with available pods: 2
Mar  5 00:05:36.973: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:38.000: INFO: Number of nodes with available pods: 2
Mar  5 00:05:38.000: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:38.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:38.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:39.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:39.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:40.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:40.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:41.971: INFO: Number of nodes with available pods: 2
Mar  5 00:05:41.971: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:42.972: INFO: Number of nodes with available pods: 2
Mar  5 00:05:42.972: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:44.007: INFO: Number of nodes with available pods: 2
Mar  5 00:05:44.008: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:44.974: INFO: Number of nodes with available pods: 2
Mar  5 00:05:44.975: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:45.972: INFO: Number of nodes with available pods: 2
Mar  5 00:05:45.972: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:46.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:46.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:47.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:47.971: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:48.973: INFO: Number of nodes with available pods: 2
Mar  5 00:05:48.973: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:49.971: INFO: Number of nodes with available pods: 2
Mar  5 00:05:49.971: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:51.160: INFO: Number of nodes with available pods: 2
Mar  5 00:05:51.160: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:52.010: INFO: Number of nodes with available pods: 2
Mar  5 00:05:52.010: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:53.120: INFO: Number of nodes with available pods: 2
Mar  5 00:05:53.120: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:53.972: INFO: Number of nodes with available pods: 2
Mar  5 00:05:53.972: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:55.019: INFO: Number of nodes with available pods: 2
Mar  5 00:05:55.019: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:55.972: INFO: Number of nodes with available pods: 2
Mar  5 00:05:55.972: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:56.971: INFO: Number of nodes with available pods: 2
Mar  5 00:05:56.972: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:57.970: INFO: Number of nodes with available pods: 2
Mar  5 00:05:57.970: INFO: Node 10.190.208.161 is running more than one daemon pod
Mar  5 00:05:58.970: INFO: Number of nodes with available pods: 3
Mar  5 00:05:58.970: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-zdlqr, will wait for the garbage collector to delete the pods
Mar  5 00:05:59.054: INFO: Deleting DaemonSet.extensions daemon-set took: 18.592968ms
Mar  5 00:05:59.154: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.199679ms
Mar  5 00:06:35.963: INFO: Number of nodes with available pods: 0
Mar  5 00:06:35.963: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 00:06:35.970: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-zdlqr/daemonsets","resourceVersion":"28462"},"items":null}

Mar  5 00:06:35.978: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-zdlqr/pods","resourceVersion":"28462"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:06:36.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-zdlqr" for this suite.
Mar  5 00:06:42.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:06:42.209: INFO: namespace: e2e-tests-daemonsets-zdlqr, resource: bindings, ignored listing per whitelist
Mar  5 00:06:42.441: INFO: namespace e2e-tests-daemonsets-zdlqr deletion completed in 6.401480935s

• [SLOW TEST:81.978 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:06:42.441: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-lzmk6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8e96d763-3eda-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:06:42.785: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-lzmk6" to be "success or failure"
Mar  5 00:06:42.798: INFO: Pod "pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.670568ms
Mar  5 00:06:44.808: INFO: Pod "pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022238325s
STEP: Saw pod success
Mar  5 00:06:44.808: INFO: Pod "pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:06:44.818: INFO: Trying to get logs from node 10.190.208.161 pod pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:06:44.902: INFO: Waiting for pod pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:06:44.910: INFO: Pod pod-configmaps-8e986981-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:06:44.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lzmk6" for this suite.
Mar  5 00:06:51.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:06:51.301: INFO: namespace: e2e-tests-configmap-lzmk6, resource: bindings, ignored listing per whitelist
Mar  5 00:06:51.322: INFO: namespace e2e-tests-configmap-lzmk6 deletion completed in 6.304018667s

• [SLOW TEST:8.882 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:06:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-4phj9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  5 00:06:51.625: INFO: Waiting up to 5m0s for pod "pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-4phj9" to be "success or failure"
Mar  5 00:06:51.633: INFO: Pod "pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.194089ms
Mar  5 00:06:53.642: INFO: Pod "pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a": Phase="Running", Reason="", readiness=true. Elapsed: 2.016925792s
Mar  5 00:06:55.651: INFO: Pod "pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025819379s
STEP: Saw pod success
Mar  5 00:06:55.651: INFO: Pod "pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:06:55.659: INFO: Trying to get logs from node 10.190.208.159 pod pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:06:55.708: INFO: Waiting for pod pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:06:55.716: INFO: Pod pod-93dd7ee6-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:06:55.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4phj9" for this suite.
Mar  5 00:07:01.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:07:02.873: INFO: namespace: e2e-tests-emptydir-4phj9, resource: bindings, ignored listing per whitelist
Mar  5 00:07:02.895: INFO: namespace e2e-tests-emptydir-4phj9 deletion completed in 7.167387403s

• [SLOW TEST:11.572 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:07:02.895: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-b8xtm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar  5 00:07:03.203: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-b8xtm" to be "success or failure"
Mar  5 00:07:03.211: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.909801ms
Mar  5 00:07:05.219: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01659878s
STEP: Saw pod success
Mar  5 00:07:05.219: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  5 00:07:05.229: INFO: Trying to get logs from node 10.190.208.161 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  5 00:07:05.278: INFO: Waiting for pod pod-host-path-test to disappear
Mar  5 00:07:05.285: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:07:05.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-b8xtm" for this suite.
Mar  5 00:07:11.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:07:11.635: INFO: namespace: e2e-tests-hostpath-b8xtm, resource: bindings, ignored listing per whitelist
Mar  5 00:07:11.658: INFO: namespace e2e-tests-hostpath-b8xtm deletion completed in 6.3607881s

• [SLOW TEST:8.762 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:07:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6xsn4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 00:07:12.000: INFO: Waiting up to 5m0s for pod "pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-6xsn4" to be "success or failure"
Mar  5 00:07:12.009: INFO: Pod "pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.552382ms
Mar  5 00:07:14.018: INFO: Pod "pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017923239s
STEP: Saw pod success
Mar  5 00:07:14.018: INFO: Pod "pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:07:14.026: INFO: Trying to get logs from node 10.190.208.159 pod pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:07:14.123: INFO: Waiting for pod pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:07:14.134: INFO: Pod pod-9ffa8c29-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:07:14.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6xsn4" for this suite.
Mar  5 00:07:20.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:07:20.552: INFO: namespace: e2e-tests-emptydir-6xsn4, resource: bindings, ignored listing per whitelist
Mar  5 00:07:20.594: INFO: namespace e2e-tests-emptydir-6xsn4 deletion completed in 6.449150898s

• [SLOW TEST:8.936 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:07:20.597: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pqvfp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:07:20.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-pqvfp" to be "success or failure"
Mar  5 00:07:20.917: INFO: Pod "downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.523039ms
Mar  5 00:07:22.926: INFO: Pod "downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017458808s
STEP: Saw pod success
Mar  5 00:07:22.926: INFO: Pod "downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:07:22.934: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:07:22.986: INFO: Waiting for pod downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:07:22.994: INFO: Pod downwardapi-volume-a552144c-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:07:22.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pqvfp" for this suite.
Mar  5 00:07:29.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:07:29.171: INFO: namespace: e2e-tests-projected-pqvfp, resource: bindings, ignored listing per whitelist
Mar  5 00:07:29.412: INFO: namespace e2e-tests-projected-pqvfp deletion completed in 6.407994272s

• [SLOW TEST:8.815 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:07:29.413: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-4vcg5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:07:29.700: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  5 00:07:34.711: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 00:07:34.711: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  5 00:07:36.722: INFO: Creating deployment "test-rollover-deployment"
Mar  5 00:07:36.738: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  5 00:07:38.751: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  5 00:07:38.766: INFO: Ensure that both replica sets have 1 created replica
Mar  5 00:07:38.782: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  5 00:07:38.797: INFO: Updating deployment test-rollover-deployment
Mar  5 00:07:38.797: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  5 00:07:40.813: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  5 00:07:40.829: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  5 00:07:40.915: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 00:07:40.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341259, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 00:07:42.931: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 00:07:42.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341259, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 00:07:44.932: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 00:07:44.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341259, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 00:07:46.932: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 00:07:46.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341259, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 00:07:48.935: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 00:07:48.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341259, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341256, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 00:07:50.933: INFO: 
Mar  5 00:07:50.933: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  5 00:07:51.000: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-4vcg5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4vcg5/deployments/test-rollover-deployment,UID:aec1db11-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:28889,Generation:2,CreationTimestamp:2019-03-05 00:07:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-05 00:07:36 +0000 UTC 2019-03-05 00:07:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-05 00:07:49 +0000 UTC 2019-03-05 00:07:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 00:07:51.009: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-4vcg5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4vcg5/replicasets/test-rollover-deployment-6b7f9d6597,UID:affe2f46-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:28880,Generation:2,CreationTimestamp:2019-03-05 00:07:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aec1db11-3eda-11e9-844e-4e8eff50a26d 0xc001dbd617 0xc001dbd618}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 00:07:51.009: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  5 00:07:51.010: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-4vcg5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4vcg5/replicasets/test-rollover-controller,UID:aa8f94f9-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:28888,Generation:2,CreationTimestamp:2019-03-05 00:07:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aec1db11-3eda-11e9-844e-4e8eff50a26d 0xc001dbd487 0xc001dbd488}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 00:07:51.010: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-4vcg5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4vcg5/replicasets/test-rollover-deployment-6586df867b,UID:aec84ba2-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:28844,Generation:2,CreationTimestamp:2019-03-05 00:07:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment aec1db11-3eda-11e9-844e-4e8eff50a26d 0xc001dbd547 0xc001dbd548}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 00:07:51.023: INFO: Pod "test-rollover-deployment-6b7f9d6597-q2qkq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-q2qkq,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-4vcg5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4vcg5/pods/test-rollover-deployment-6b7f9d6597-q2qkq,UID:b003e30c-3eda-11e9-844e-4e8eff50a26d,ResourceVersion:28860,Generation:0,CreationTimestamp:2019-03-05 00:07:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 affe2f46-3eda-11e9-844e-4e8eff50a26d 0xc002606167 0xc002606168}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-2mgmh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2mgmh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-2mgmh true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.159,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026061e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002606200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:07:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:07:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:07:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:07:38 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.159,PodIP:172.30.189.237,StartTime:2019-03-05 00:07:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-05 00:07:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://6b1b1a884b158de6b7d4ed62891ae3d2ad771721b30d6fa1bb07bc8e82d27d93}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:07:51.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4vcg5" for this suite.
Mar  5 00:07:57.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:07:57.359: INFO: namespace: e2e-tests-deployment-4vcg5, resource: bindings, ignored listing per whitelist
Mar  5 00:07:57.447: INFO: namespace e2e-tests-deployment-4vcg5 deletion completed in 6.413356242s

• [SLOW TEST:28.034 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:07:57.448: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-ml5d4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0305 00:08:03.993305      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 00:08:03.993: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:08:03.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-ml5d4" for this suite.
Mar  5 00:08:12.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:08:12.201: INFO: namespace: e2e-tests-gc-ml5d4, resource: bindings, ignored listing per whitelist
Mar  5 00:08:12.358: INFO: namespace e2e-tests-gc-ml5d4 deletion completed in 8.34871363s

• [SLOW TEST:14.910 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:08:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tt9m7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar  5 00:08:12.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:13.236: INFO: stderr: ""
Mar  5 00:08:13.236: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 00:08:13.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:13.412: INFO: stderr: ""
Mar  5 00:08:13.412: INFO: stdout: "update-demo-nautilus-g4bbq update-demo-nautilus-zqrfk "
Mar  5 00:08:13.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-g4bbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:13.531: INFO: stderr: ""
Mar  5 00:08:13.531: INFO: stdout: ""
Mar  5 00:08:13.531: INFO: update-demo-nautilus-g4bbq is created but not running
Mar  5 00:08:18.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:18.635: INFO: stderr: ""
Mar  5 00:08:18.635: INFO: stdout: "update-demo-nautilus-g4bbq update-demo-nautilus-zqrfk "
Mar  5 00:08:18.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-g4bbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:18.740: INFO: stderr: ""
Mar  5 00:08:18.740: INFO: stdout: "true"
Mar  5 00:08:18.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-g4bbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:18.839: INFO: stderr: ""
Mar  5 00:08:18.840: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 00:08:18.840: INFO: validating pod update-demo-nautilus-g4bbq
Mar  5 00:08:18.857: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 00:08:18.857: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 00:08:18.857: INFO: update-demo-nautilus-g4bbq is verified up and running
Mar  5 00:08:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:18.957: INFO: stderr: ""
Mar  5 00:08:18.957: INFO: stdout: "true"
Mar  5 00:08:18.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:19.070: INFO: stderr: ""
Mar  5 00:08:19.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 00:08:19.070: INFO: validating pod update-demo-nautilus-zqrfk
Mar  5 00:08:19.092: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 00:08:19.092: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 00:08:19.092: INFO: update-demo-nautilus-zqrfk is verified up and running
STEP: scaling down the replication controller
Mar  5 00:08:19.094: INFO: scanned /root for discovery docs: <nil>
Mar  5 00:08:19.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:19.310: INFO: stderr: ""
Mar  5 00:08:19.310: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 00:08:19.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:19.474: INFO: stderr: ""
Mar  5 00:08:19.474: INFO: stdout: "update-demo-nautilus-g4bbq update-demo-nautilus-zqrfk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  5 00:08:24.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:24.595: INFO: stderr: ""
Mar  5 00:08:24.595: INFO: stdout: "update-demo-nautilus-g4bbq update-demo-nautilus-zqrfk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  5 00:08:29.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:29.727: INFO: stderr: ""
Mar  5 00:08:29.727: INFO: stdout: "update-demo-nautilus-zqrfk "
Mar  5 00:08:29.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:29.838: INFO: stderr: ""
Mar  5 00:08:29.838: INFO: stdout: "true"
Mar  5 00:08:29.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:29.954: INFO: stderr: ""
Mar  5 00:08:29.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 00:08:29.954: INFO: validating pod update-demo-nautilus-zqrfk
Mar  5 00:08:29.971: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 00:08:29.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 00:08:29.971: INFO: update-demo-nautilus-zqrfk is verified up and running
STEP: scaling up the replication controller
Mar  5 00:08:29.973: INFO: scanned /root for discovery docs: <nil>
Mar  5 00:08:29.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:31.154: INFO: stderr: ""
Mar  5 00:08:31.154: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 00:08:31.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:31.257: INFO: stderr: ""
Mar  5 00:08:31.257: INFO: stdout: "update-demo-nautilus-8fmtb update-demo-nautilus-zqrfk "
Mar  5 00:08:31.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-8fmtb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:31.386: INFO: stderr: ""
Mar  5 00:08:31.386: INFO: stdout: ""
Mar  5 00:08:31.386: INFO: update-demo-nautilus-8fmtb is created but not running
Mar  5 00:08:36.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:36.493: INFO: stderr: ""
Mar  5 00:08:36.493: INFO: stdout: "update-demo-nautilus-8fmtb update-demo-nautilus-zqrfk "
Mar  5 00:08:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-8fmtb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:36.600: INFO: stderr: ""
Mar  5 00:08:36.600: INFO: stdout: "true"
Mar  5 00:08:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-8fmtb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:36.700: INFO: stderr: ""
Mar  5 00:08:36.700: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 00:08:36.700: INFO: validating pod update-demo-nautilus-8fmtb
Mar  5 00:08:36.718: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 00:08:36.718: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 00:08:36.718: INFO: update-demo-nautilus-8fmtb is verified up and running
Mar  5 00:08:36.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:36.902: INFO: stderr: ""
Mar  5 00:08:36.902: INFO: stdout: "true"
Mar  5 00:08:36.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods update-demo-nautilus-zqrfk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:36.997: INFO: stderr: ""
Mar  5 00:08:36.997: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 00:08:36.997: INFO: validating pod update-demo-nautilus-zqrfk
Mar  5 00:08:37.011: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 00:08:37.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 00:08:37.011: INFO: update-demo-nautilus-zqrfk is verified up and running
STEP: using delete to clean up resources
Mar  5 00:08:37.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:37.130: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:08:37.130: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  5 00:08:37.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-tt9m7'
Mar  5 00:08:37.278: INFO: stderr: "No resources found.\n"
Mar  5 00:08:37.278: INFO: stdout: ""
Mar  5 00:08:37.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -l name=update-demo --namespace=e2e-tests-kubectl-tt9m7 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 00:08:37.437: INFO: stderr: ""
Mar  5 00:08:37.437: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:08:37.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tt9m7" for this suite.
Mar  5 00:08:43.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:08:43.972: INFO: namespace: e2e-tests-kubectl-tt9m7, resource: bindings, ignored listing per whitelist
Mar  5 00:08:44.062: INFO: namespace e2e-tests-kubectl-tt9m7 deletion completed in 6.372066454s

• [SLOW TEST:31.704 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:08:44.062: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mcm9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 00:08:44.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-mcm9t'
Mar  5 00:08:44.507: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 00:08:44.507: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Mar  5 00:08:46.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-mcm9t'
Mar  5 00:08:46.710: INFO: stderr: ""
Mar  5 00:08:46.710: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:08:46.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mcm9t" for this suite.
Mar  5 00:09:10.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:09:11.044: INFO: namespace: e2e-tests-kubectl-mcm9t, resource: bindings, ignored listing per whitelist
Mar  5 00:09:11.066: INFO: namespace e2e-tests-kubectl-mcm9t deletion completed in 24.345009924s

• [SLOW TEST:27.004 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:09:11.070: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kztxj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:09:11.416: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-kztxj" to be "success or failure"
Mar  5 00:09:11.426: INFO: Pod "downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.512491ms
Mar  5 00:09:13.434: INFO: Pod "downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018230456s
STEP: Saw pod success
Mar  5 00:09:13.435: INFO: Pod "downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:09:13.444: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:09:13.532: INFO: Waiting for pod downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a to disappear
Mar  5 00:09:13.540: INFO: Pod downwardapi-volume-e72c0724-3eda-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:09:13.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kztxj" for this suite.
Mar  5 00:09:19.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:09:19.849: INFO: namespace: e2e-tests-projected-kztxj, resource: bindings, ignored listing per whitelist
Mar  5 00:09:19.855: INFO: namespace e2e-tests-projected-kztxj deletion completed in 6.305664564s

• [SLOW TEST:8.786 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:09:19.861: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-kbjsf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-xknt
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 00:09:20.237: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xknt" in namespace "e2e-tests-subpath-kbjsf" to be "success or failure"
Mar  5 00:09:20.245: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.272328ms
Mar  5 00:09:22.254: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017754312s
Mar  5 00:09:24.263: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 4.026329765s
Mar  5 00:09:26.272: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 6.035551963s
Mar  5 00:09:28.281: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 8.044097963s
Mar  5 00:09:30.289: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 10.052386896s
Mar  5 00:09:32.299: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 12.062561202s
Mar  5 00:09:34.309: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 14.072209062s
Mar  5 00:09:36.318: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 16.08088793s
Mar  5 00:09:38.326: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 18.089489538s
Mar  5 00:09:40.336: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 20.099075323s
Mar  5 00:09:42.345: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Running", Reason="", readiness=false. Elapsed: 22.108529002s
Mar  5 00:09:44.355: INFO: Pod "pod-subpath-test-downwardapi-xknt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.11864333s
STEP: Saw pod success
Mar  5 00:09:44.355: INFO: Pod "pod-subpath-test-downwardapi-xknt" satisfied condition "success or failure"
Mar  5 00:09:44.363: INFO: Trying to get logs from node 10.190.208.159 pod pod-subpath-test-downwardapi-xknt container test-container-subpath-downwardapi-xknt: <nil>
STEP: delete the pod
Mar  5 00:09:44.419: INFO: Waiting for pod pod-subpath-test-downwardapi-xknt to disappear
Mar  5 00:09:44.427: INFO: Pod pod-subpath-test-downwardapi-xknt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xknt
Mar  5 00:09:44.427: INFO: Deleting pod "pod-subpath-test-downwardapi-xknt" in namespace "e2e-tests-subpath-kbjsf"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:09:44.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-kbjsf" for this suite.
Mar  5 00:09:50.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:09:50.878: INFO: namespace: e2e-tests-subpath-kbjsf, resource: bindings, ignored listing per whitelist
Mar  5 00:09:51.022: INFO: namespace e2e-tests-subpath-kbjsf deletion completed in 6.578220107s

• [SLOW TEST:31.161 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:09:51.023: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-l5z9d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-l5z9d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l5z9d to expose endpoints map[]
Mar  5 00:09:51.342: INFO: Get endpoints failed (7.475039ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  5 00:09:52.351: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l5z9d exposes endpoints map[] (1.015943804s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-l5z9d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l5z9d to expose endpoints map[pod1:[100]]
Mar  5 00:09:54.634: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l5z9d exposes endpoints map[pod1:[100]] (2.266501684s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-l5z9d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l5z9d to expose endpoints map[pod2:[101] pod1:[100]]
Mar  5 00:09:56.759: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l5z9d exposes endpoints map[pod1:[100] pod2:[101]] (2.101352237s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-l5z9d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l5z9d to expose endpoints map[pod2:[101]]
Mar  5 00:09:57.829: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l5z9d exposes endpoints map[pod2:[101]] (1.048873801s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-l5z9d
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l5z9d to expose endpoints map[]
Mar  5 00:09:57.853: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l5z9d exposes endpoints map[] (8.896106ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:09:57.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-l5z9d" for this suite.
Mar  5 00:10:21.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:10:22.431: INFO: namespace: e2e-tests-services-l5z9d, resource: bindings, ignored listing per whitelist
Mar  5 00:10:22.865: INFO: namespace e2e-tests-services-l5z9d deletion completed in 24.953773063s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:31.842 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:10:22.865: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jmhtx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-11f5deb6-3edb-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:10:23.186: INFO: Waiting up to 5m0s for pod "pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-jmhtx" to be "success or failure"
Mar  5 00:10:23.194: INFO: Pod "pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074901ms
Mar  5 00:10:25.203: INFO: Pod "pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01697768s
STEP: Saw pod success
Mar  5 00:10:25.203: INFO: Pod "pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:10:25.210: INFO: Trying to get logs from node 10.190.208.161 pod pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:10:25.262: INFO: Waiting for pod pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a to disappear
Mar  5 00:10:25.270: INFO: Pod pod-configmaps-11f755f7-3edb-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:10:25.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jmhtx" for this suite.
Mar  5 00:10:31.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:10:31.719: INFO: namespace: e2e-tests-configmap-jmhtx, resource: bindings, ignored listing per whitelist
Mar  5 00:10:31.754: INFO: namespace e2e-tests-configmap-jmhtx deletion completed in 6.473685772s

• [SLOW TEST:8.888 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:10:31.755: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-q4rff
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 00:10:32.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-q4rff'
Mar  5 00:10:32.220: INFO: stderr: ""
Mar  5 00:10:32.220: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  5 00:10:37.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-q4rff -o json'
Mar  5 00:10:37.381: INFO: stderr: ""
Mar  5 00:10:37.381: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-03-05T00:10:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-q4rff\",\n        \"resourceVersion\": \"29910\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-q4rff/pods/e2e-test-nginx-pod\",\n        \"uid\": \"1759b170-3edb-11e9-844e-4e8eff50a26d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4nxwf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.190.208.159\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4nxwf\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4nxwf\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-05T00:10:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-05T00:10:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-05T00:10:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-05T00:10:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://86b124f1b4ff0ed041191f3af9b5888d6a4efbfe84da5fe6970852e5d205fc84\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-05T00:10:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.190.208.159\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.189.253\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-05T00:10:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  5 00:10:37.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 replace -f - --namespace=e2e-tests-kubectl-q4rff'
Mar  5 00:10:37.726: INFO: stderr: ""
Mar  5 00:10:37.726: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Mar  5 00:10:37.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-q4rff'
Mar  5 00:10:50.851: INFO: stderr: ""
Mar  5 00:10:50.851: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:10:50.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q4rff" for this suite.
Mar  5 00:10:56.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:10:57.662: INFO: namespace: e2e-tests-kubectl-q4rff, resource: bindings, ignored listing per whitelist
Mar  5 00:10:57.685: INFO: namespace e2e-tests-kubectl-q4rff deletion completed in 6.784537297s

• [SLOW TEST:25.930 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:10:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-9qvt5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zrlwr in namespace e2e-tests-proxy-9qvt5
I0305 00:10:58.354921      15 runners.go:184] Created replication controller with name: proxy-service-zrlwr, namespace: e2e-tests-proxy-9qvt5, replica count: 1
I0305 00:10:59.405317      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 00:11:00.405537      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 00:11:01.405756      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 00:11:02.405952      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:03.406157      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:04.406389      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:05.406616      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:06.406859      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:07.407076      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:08.407250      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:09.407458      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 00:11:10.407728      15 runners.go:184] proxy-service-zrlwr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 00:11:10.460: INFO: setup took 12.450219228s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  5 00:11:10.482: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 21.233255ms)
Mar  5 00:11:10.486: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 25.693416ms)
Mar  5 00:11:10.486: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 25.158544ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 32.103111ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 32.411173ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 33.186552ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 33.055836ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 33.365397ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 33.663872ms)
Mar  5 00:11:10.494: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 33.380927ms)
Mar  5 00:11:10.498: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 37.504519ms)
Mar  5 00:11:10.500: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 38.764006ms)
Mar  5 00:11:10.503: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 41.558715ms)
Mar  5 00:11:10.503: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 42.187726ms)
Mar  5 00:11:10.504: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 42.931644ms)
Mar  5 00:11:10.510: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 47.913334ms)
Mar  5 00:11:10.522: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 12.665146ms)
Mar  5 00:11:10.527: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 16.662296ms)
Mar  5 00:11:10.527: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 17.145284ms)
Mar  5 00:11:10.527: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 17.388169ms)
Mar  5 00:11:10.528: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 17.708281ms)
Mar  5 00:11:10.530: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 20.391359ms)
Mar  5 00:11:10.531: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 20.517794ms)
Mar  5 00:11:10.531: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 21.113612ms)
Mar  5 00:11:10.531: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 21.043807ms)
Mar  5 00:11:10.531: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 21.129628ms)
Mar  5 00:11:10.531: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 21.07128ms)
Mar  5 00:11:10.532: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 21.749207ms)
Mar  5 00:11:10.533: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 22.974781ms)
Mar  5 00:11:10.533: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 23.159465ms)
Mar  5 00:11:10.534: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 23.511942ms)
Mar  5 00:11:10.534: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 23.924423ms)
Mar  5 00:11:10.550: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.768353ms)
Mar  5 00:11:10.555: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 21.063743ms)
Mar  5 00:11:10.556: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 21.59877ms)
Mar  5 00:11:10.556: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 21.137549ms)
Mar  5 00:11:10.556: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 21.399575ms)
Mar  5 00:11:10.556: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 20.978004ms)
Mar  5 00:11:10.556: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 21.830406ms)
Mar  5 00:11:10.557: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 20.751757ms)
Mar  5 00:11:10.557: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 22.083504ms)
Mar  5 00:11:10.557: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 22.481493ms)
Mar  5 00:11:10.557: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 21.479874ms)
Mar  5 00:11:10.561: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 25.981889ms)
Mar  5 00:11:10.561: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 25.252258ms)
Mar  5 00:11:10.561: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 25.681308ms)
Mar  5 00:11:10.561: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 25.639738ms)
Mar  5 00:11:10.562: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 27.421573ms)
Mar  5 00:11:10.577: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.073193ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 20.676893ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 20.323931ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 20.986503ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 20.57568ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 20.782304ms)
Mar  5 00:11:10.583: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 21.592358ms)
Mar  5 00:11:10.584: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 21.411641ms)
Mar  5 00:11:10.584: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 21.431664ms)
Mar  5 00:11:10.584: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 21.504645ms)
Mar  5 00:11:10.584: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 21.440005ms)
Mar  5 00:11:10.587: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.799209ms)
Mar  5 00:11:10.587: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 25.366538ms)
Mar  5 00:11:10.590: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 27.259354ms)
Mar  5 00:11:10.590: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 27.571375ms)
Mar  5 00:11:10.590: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 28.059858ms)
Mar  5 00:11:10.606: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.539273ms)
Mar  5 00:11:10.606: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.655089ms)
Mar  5 00:11:10.607: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 16.293735ms)
Mar  5 00:11:10.607: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.444802ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 17.044412ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 17.400649ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 17.119033ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 17.231609ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 17.437744ms)
Mar  5 00:11:10.608: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 17.289913ms)
Mar  5 00:11:10.610: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 19.292527ms)
Mar  5 00:11:10.613: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 22.435539ms)
Mar  5 00:11:10.617: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 26.156052ms)
Mar  5 00:11:10.617: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 26.551491ms)
Mar  5 00:11:10.617: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 26.651524ms)
Mar  5 00:11:10.618: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 26.96584ms)
Mar  5 00:11:10.630: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 11.525561ms)
Mar  5 00:11:10.634: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 13.917173ms)
Mar  5 00:11:10.634: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 15.505875ms)
Mar  5 00:11:10.634: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 15.638849ms)
Mar  5 00:11:10.634: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.33877ms)
Mar  5 00:11:10.634: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 16.583341ms)
Mar  5 00:11:10.635: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.668468ms)
Mar  5 00:11:10.635: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 15.528038ms)
Mar  5 00:11:10.635: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.643454ms)
Mar  5 00:11:10.635: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.807971ms)
Mar  5 00:11:10.640: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 22.324833ms)
Mar  5 00:11:10.643: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 25.050435ms)
Mar  5 00:11:10.643: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 24.969099ms)
Mar  5 00:11:10.644: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 25.159769ms)
Mar  5 00:11:10.644: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.748642ms)
Mar  5 00:11:10.644: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 25.435264ms)
Mar  5 00:11:10.656: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 11.798382ms)
Mar  5 00:11:10.659: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 14.673755ms)
Mar  5 00:11:10.659: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.039579ms)
Mar  5 00:11:10.661: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 16.44381ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.494974ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 16.416278ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.588397ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 17.326745ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 17.045877ms)
Mar  5 00:11:10.662: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 18.186202ms)
Mar  5 00:11:10.666: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 21.990311ms)
Mar  5 00:11:10.669: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 24.251066ms)
Mar  5 00:11:10.669: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.514071ms)
Mar  5 00:11:10.669: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 24.198275ms)
Mar  5 00:11:10.669: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 24.579935ms)
Mar  5 00:11:10.669: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 24.883333ms)
Mar  5 00:11:10.683: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 12.775134ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 15.565337ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.735732ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.526661ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.700758ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 15.744565ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.012148ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 16.533062ms)
Mar  5 00:11:10.686: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.412377ms)
Mar  5 00:11:10.688: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 18.279165ms)
Mar  5 00:11:10.690: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 19.596077ms)
Mar  5 00:11:10.692: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 21.698488ms)
Mar  5 00:11:10.692: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 21.576838ms)
Mar  5 00:11:10.692: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 21.751175ms)
Mar  5 00:11:10.692: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 21.838863ms)
Mar  5 00:11:10.692: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 21.787826ms)
Mar  5 00:11:11.079: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 386.509736ms)
Mar  5 00:11:11.079: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 386.396503ms)
Mar  5 00:11:11.080: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 387.055186ms)
Mar  5 00:11:11.080: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 386.673435ms)
Mar  5 00:11:11.080: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 386.774406ms)
Mar  5 00:11:11.082: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 389.287663ms)
Mar  5 00:11:11.082: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 389.572911ms)
Mar  5 00:11:11.082: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 389.193597ms)
Mar  5 00:11:11.083: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 390.707839ms)
Mar  5 00:11:11.083: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 390.367431ms)
Mar  5 00:11:11.084: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 391.060221ms)
Mar  5 00:11:11.084: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 390.646827ms)
Mar  5 00:11:11.084: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 390.501608ms)
Mar  5 00:11:11.086: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 392.873699ms)
Mar  5 00:11:11.088: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 394.905821ms)
Mar  5 00:11:11.088: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 394.856928ms)
Mar  5 00:11:11.102: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 13.98031ms)
Mar  5 00:11:11.105: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.479583ms)
Mar  5 00:11:11.106: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 17.55997ms)
Mar  5 00:11:11.106: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 17.643712ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 18.059571ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 18.730431ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 18.389849ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 18.140132ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 17.898814ms)
Mar  5 00:11:11.107: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 18.579606ms)
Mar  5 00:11:11.110: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 21.857721ms)
Mar  5 00:11:11.115: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 26.56222ms)
Mar  5 00:11:11.115: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 26.564129ms)
Mar  5 00:11:11.115: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 26.853319ms)
Mar  5 00:11:11.115: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 26.390212ms)
Mar  5 00:11:11.115: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 27.197634ms)
Mar  5 00:11:11.128: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 12.1285ms)
Mar  5 00:11:11.128: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 12.669843ms)
Mar  5 00:11:11.131: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 14.435254ms)
Mar  5 00:11:11.131: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 14.402589ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.392299ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 15.392614ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.954386ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.185691ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.657405ms)
Mar  5 00:11:11.132: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.785055ms)
Mar  5 00:11:11.134: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 18.875953ms)
Mar  5 00:11:11.138: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 21.280039ms)
Mar  5 00:11:11.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 24.852393ms)
Mar  5 00:11:11.141: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 24.426557ms)
Mar  5 00:11:11.141: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 25.13853ms)
Mar  5 00:11:11.141: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.806895ms)
Mar  5 00:11:11.153: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 12.020211ms)
Mar  5 00:11:11.156: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 14.460264ms)
Mar  5 00:11:11.156: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 14.618998ms)
Mar  5 00:11:11.156: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 14.574771ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.67433ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.558896ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 15.527431ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.56227ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.681062ms)
Mar  5 00:11:11.157: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 15.423613ms)
Mar  5 00:11:11.160: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 18.005652ms)
Mar  5 00:11:11.166: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 24.849776ms)
Mar  5 00:11:11.167: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 25.284928ms)
Mar  5 00:11:11.167: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 25.523483ms)
Mar  5 00:11:11.167: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 25.821201ms)
Mar  5 00:11:11.167: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 25.958419ms)
Mar  5 00:11:11.181: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 13.667361ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.812831ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 16.308272ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.078185ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.538394ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 16.351933ms)
Mar  5 00:11:11.184: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 16.337683ms)
Mar  5 00:11:11.185: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 16.779717ms)
Mar  5 00:11:11.185: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 16.927908ms)
Mar  5 00:11:11.185: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 17.050877ms)
Mar  5 00:11:11.185: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 17.500382ms)
Mar  5 00:11:11.188: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 20.245156ms)
Mar  5 00:11:11.191: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 23.119534ms)
Mar  5 00:11:11.191: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 23.27327ms)
Mar  5 00:11:11.191: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 23.753644ms)
Mar  5 00:11:11.192: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 23.69874ms)
Mar  5 00:11:11.204: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 11.950768ms)
Mar  5 00:11:11.205: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 13.444181ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.937864ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 17.417023ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 17.671551ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 17.631538ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 17.456936ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 17.7577ms)
Mar  5 00:11:11.210: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 18.849559ms)
Mar  5 00:11:11.211: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 17.614255ms)
Mar  5 00:11:11.211: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 18.911939ms)
Mar  5 00:11:11.214: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 21.625546ms)
Mar  5 00:11:11.214: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 21.822791ms)
Mar  5 00:11:11.214: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 21.96317ms)
Mar  5 00:11:11.215: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 21.985538ms)
Mar  5 00:11:11.215: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 22.333461ms)
Mar  5 00:11:11.227: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 12.141483ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.610027ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.824029ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.347531ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.493755ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.781493ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 16.552611ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 17.570522ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 17.336976ms)
Mar  5 00:11:11.232: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 16.791599ms)
Mar  5 00:11:11.236: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 20.726243ms)
Mar  5 00:11:11.237: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 20.635667ms)
Mar  5 00:11:11.237: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 21.098851ms)
Mar  5 00:11:11.238: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 22.742422ms)
Mar  5 00:11:11.240: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 23.512572ms)
Mar  5 00:11:11.240: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 24.223636ms)
Mar  5 00:11:11.253: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 12.252068ms)
Mar  5 00:11:11.256: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 14.9721ms)
Mar  5 00:11:11.256: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 15.194581ms)
Mar  5 00:11:11.256: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 15.386029ms)
Mar  5 00:11:11.256: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.242604ms)
Mar  5 00:11:11.257: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.739512ms)
Mar  5 00:11:11.257: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.828385ms)
Mar  5 00:11:11.257: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.050243ms)
Mar  5 00:11:11.257: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.859953ms)
Mar  5 00:11:11.257: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 16.10728ms)
Mar  5 00:11:11.262: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 21.042547ms)
Mar  5 00:11:11.266: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.778702ms)
Mar  5 00:11:11.269: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 28.338721ms)
Mar  5 00:11:11.269: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 28.20943ms)
Mar  5 00:11:11.269: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 28.320095ms)
Mar  5 00:11:11.270: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 28.389261ms)
Mar  5 00:11:11.282: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 12.016109ms)
Mar  5 00:11:11.282: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 12.099999ms)
Mar  5 00:11:11.289: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 19.001224ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 19.438635ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 19.407581ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 19.343235ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 19.356933ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 19.231033ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 19.31622ms)
Mar  5 00:11:11.290: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 19.328392ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 24.575558ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 24.213222ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 24.551579ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 25.196392ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 24.94194ms)
Mar  5 00:11:11.295: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 25.014493ms)
Mar  5 00:11:11.309: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 13.334443ms)
Mar  5 00:11:11.309: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 12.624137ms)
Mar  5 00:11:11.313: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.947844ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 17.761958ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 17.633212ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 18.034917ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 17.844303ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 17.836304ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 18.652516ms)
Mar  5 00:11:11.314: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 17.851587ms)
Mar  5 00:11:11.316: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 20.743477ms)
Mar  5 00:11:11.319: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 22.962455ms)
Mar  5 00:11:11.325: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 29.064918ms)
Mar  5 00:11:11.326: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 30.006319ms)
Mar  5 00:11:11.326: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 29.358869ms)
Mar  5 00:11:11.326: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 29.480737ms)
Mar  5 00:11:11.338: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 11.969556ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 15.092128ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 14.991267ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 14.983416ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 16.382587ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.13035ms)
Mar  5 00:11:11.342: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.058141ms)
Mar  5 00:11:11.343: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.129247ms)
Mar  5 00:11:11.343: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 15.929081ms)
Mar  5 00:11:11.343: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 17.227647ms)
Mar  5 00:11:11.349: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 23.025518ms)
Mar  5 00:11:11.350: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 22.689514ms)
Mar  5 00:11:11.350: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 23.18983ms)
Mar  5 00:11:11.353: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 26.091918ms)
Mar  5 00:11:11.354: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 26.608233ms)
Mar  5 00:11:11.354: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 26.457691ms)
Mar  5 00:11:11.366: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 12.040208ms)
Mar  5 00:11:11.370: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh/proxy/rewriteme"... (200; 15.094463ms)
Mar  5 00:11:11.370: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:460/proxy/: tls baz (200; 15.152952ms)
Mar  5 00:11:11.370: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:443/proxy/... (200; 15.675363ms)
Mar  5 00:11:11.370: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:1080/proxy/rewri... (200; 16.670867ms)
Mar  5 00:11:11.371: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 15.851675ms)
Mar  5 00:11:11.371: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:160/proxy/: foo (200; 16.869326ms)
Mar  5 00:11:11.371: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/proxy-service-zrlwr-l99dh:162/proxy/: bar (200; 16.103224ms)
Mar  5 00:11:11.371: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/https:proxy-service-zrlwr-l99dh:462/proxy/: tls qux (200; 16.187338ms)
Mar  5 00:11:11.371: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-9qvt5/pods/http:proxy-service-zrlwr-l99dh:1080/proxy/... (200; 16.440698ms)
Mar  5 00:11:11.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname2/proxy/: bar (200; 28.58228ms)
Mar  5 00:11:11.392: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname1/proxy/: tls baz (200; 36.907071ms)
Mar  5 00:11:11.392: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/proxy-service-zrlwr:portname1/proxy/: foo (200; 37.828037ms)
Mar  5 00:11:11.392: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/https:proxy-service-zrlwr:tlsportname2/proxy/: tls qux (200; 37.589495ms)
Mar  5 00:11:11.392: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname1/proxy/: foo (200; 37.371898ms)
Mar  5 00:11:11.392: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-9qvt5/services/http:proxy-service-zrlwr:portname2/proxy/: bar (200; 37.85502ms)
STEP: deleting ReplicationController proxy-service-zrlwr in namespace e2e-tests-proxy-9qvt5, will wait for the garbage collector to delete the pods
Mar  5 00:11:11.465: INFO: Deleting ReplicationController proxy-service-zrlwr took: 15.809566ms
Mar  5 00:11:11.566: INFO: Terminating ReplicationController proxy-service-zrlwr pods took: 101.023651ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:11:15.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-9qvt5" for this suite.
Mar  5 00:11:21.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:11:21.817: INFO: namespace: e2e-tests-proxy-9qvt5, resource: bindings, ignored listing per whitelist
Mar  5 00:11:21.981: INFO: namespace e2e-tests-proxy-9qvt5 deletion completed in 6.502085261s

• [SLOW TEST:24.294 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:11:21.982: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-qspxt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:11:22.282: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  5 00:11:22.302: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  5 00:11:27.311: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 00:11:27.311: INFO: Creating deployment "test-rolling-update-deployment"
Mar  5 00:11:27.320: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  5 00:11:27.336: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  5 00:11:29.353: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  5 00:11:29.359: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  5 00:11:29.407: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-qspxt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qspxt/deployments/test-rolling-update-deployment,UID:3832e142-3edb-11e9-844e-4e8eff50a26d,ResourceVersion:30135,Generation:1,CreationTimestamp:2019-03-05 00:11:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-05 00:11:27 +0000 UTC 2019-03-05 00:11:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-05 00:11:28 +0000 UTC 2019-03-05 00:11:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 00:11:29.416: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-qspxt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qspxt/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:383812e7-3edb-11e9-844e-4e8eff50a26d,ResourceVersion:30126,Generation:1,CreationTimestamp:2019-03-05 00:11:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3832e142-3edb-11e9-844e-4e8eff50a26d 0xc002167ee7 0xc002167ee8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 00:11:29.416: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  5 00:11:29.416: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-qspxt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qspxt/replicasets/test-rolling-update-controller,UID:353361bd-3edb-11e9-844e-4e8eff50a26d,ResourceVersion:30134,Generation:2,CreationTimestamp:2019-03-05 00:11:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3832e142-3edb-11e9-844e-4e8eff50a26d 0xc002167e27 0xc002167e28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  5 00:11:29.426: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-5xpcx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-5xpcx,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-qspxt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qspxt/pods/test-rolling-update-deployment-68b55d7bc6-5xpcx,UID:383979bd-3edb-11e9-844e-4e8eff50a26d,ResourceVersion:30125,Generation:0,CreationTimestamp:2019-03-05 00:11:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 383812e7-3edb-11e9-844e-4e8eff50a26d 0xc0026607a7 0xc0026607a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-2rjdj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2rjdj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-2rjdj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002660820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002660840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:11:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:11:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:11:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:11:27 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.49,StartTime:2019-03-05 00:11:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-05 00:11:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://00841e04d01b46a4e6b7049bfce9cb1ce6f5dece6f280db4bb73ad4872b09d05}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:11:29.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qspxt" for this suite.
Mar  5 00:11:35.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:11:35.870: INFO: namespace: e2e-tests-deployment-qspxt, resource: bindings, ignored listing per whitelist
Mar  5 00:11:35.959: INFO: namespace e2e-tests-deployment-qspxt deletion completed in 6.523093207s

• [SLOW TEST:13.978 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:11:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wgg62
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-3d857a80-3edb-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:11:36.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-wgg62" to be "success or failure"
Mar  5 00:11:36.276: INFO: Pod "pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.366686ms
Mar  5 00:11:38.285: INFO: Pod "pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017142415s
STEP: Saw pod success
Mar  5 00:11:38.285: INFO: Pod "pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:11:38.296: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:11:38.344: INFO: Waiting for pod pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a to disappear
Mar  5 00:11:38.354: INFO: Pod pod-projected-configmaps-3d86e4e9-3edb-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:11:38.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wgg62" for this suite.
Mar  5 00:11:44.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:11:44.671: INFO: namespace: e2e-tests-projected-wgg62, resource: bindings, ignored listing per whitelist
Mar  5 00:11:45.579: INFO: namespace e2e-tests-projected-wgg62 deletion completed in 7.179178676s

• [SLOW TEST:9.620 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:11:45.580: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-whbmg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0305 00:11:56.100215      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 00:11:56.100: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:11:56.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-whbmg" for this suite.
Mar  5 00:12:04.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:12:04.248: INFO: namespace: e2e-tests-gc-whbmg, resource: bindings, ignored listing per whitelist
Mar  5 00:12:04.444: INFO: namespace e2e-tests-gc-whbmg deletion completed in 8.333884806s

• [SLOW TEST:18.865 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:12:04.445: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-kpksk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  5 00:12:07.511: INFO: Successfully updated pod "labelsupdate4e9b6864-3edb-11e9-8a62-3ec24305971a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:12:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kpksk" for this suite.
Mar  5 00:12:35.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:12:35.816: INFO: namespace: e2e-tests-downward-api-kpksk, resource: bindings, ignored listing per whitelist
Mar  5 00:12:35.955: INFO: namespace e2e-tests-downward-api-kpksk deletion completed in 24.368572882s

• [SLOW TEST:31.510 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:12:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-gw7k7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a
Mar  5 00:12:36.520: INFO: Pod name my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a: Found 0 pods out of 1
Mar  5 00:12:41.529: INFO: Pod name my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a: Found 1 pods out of 1
Mar  5 00:12:41.529: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a" are running
Mar  5 00:12:41.537: INFO: Pod "my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a-k5255" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:12:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:12:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:12:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:12:36 +0000 UTC Reason: Message:}])
Mar  5 00:12:41.537: INFO: Trying to dial the pod
Mar  5 00:12:46.579: INFO: Controller my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a: Got expected result from replica 1 [my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a-k5255]: "my-hostname-basic-616ffee7-3edb-11e9-8a62-3ec24305971a-k5255", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:12:46.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-gw7k7" for this suite.
Mar  5 00:12:52.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:12:52.928: INFO: namespace: e2e-tests-replication-controller-gw7k7, resource: bindings, ignored listing per whitelist
Mar  5 00:12:53.031: INFO: namespace e2e-tests-replication-controller-gw7k7 deletion completed in 6.440410421s

• [SLOW TEST:17.075 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:12:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-2b5b4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar  5 00:12:53.516: INFO: PodSpec: initContainers in spec.initContainers
Mar  5 00:13:37.709: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6b947793-3edb-11e9-8a62-3ec24305971a", GenerateName:"", Namespace:"e2e-tests-init-container-2b5b4", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-2b5b4/pods/pod-init-6b947793-3edb-11e9-8a62-3ec24305971a", UID:"6b96606d-3edb-11e9-844e-4e8eff50a26d", ResourceVersion:"30871", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687341573, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"516283648"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-w2rzk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0009de8c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w2rzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w2rzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w2rzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001729488), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.190.208.161", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00247b9e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001729510)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001729530)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001729538), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00172953c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341573, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341573, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341573, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341573, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.190.208.161", PodIP:"172.30.111.63", StartTime:(*v1.Time)(0xc0029aae00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0029ab240), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002245f10)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://c670a612ca0b3f366ff0b0b1dbd4660093b9a2a0ca13c8378323ccc7551deb84"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029ab3c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029ab1c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:13:37.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2b5b4" for this suite.
Mar  5 00:14:01.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:14:01.897: INFO: namespace: e2e-tests-init-container-2b5b4, resource: bindings, ignored listing per whitelist
Mar  5 00:14:02.057: INFO: namespace e2e-tests-init-container-2b5b4 deletion completed in 24.330418642s

• [SLOW TEST:69.025 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:14:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2xdpm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0305 00:14:32.959186      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 00:14:32.959: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:14:32.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2xdpm" for this suite.
Mar  5 00:14:39.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:14:39.532: INFO: namespace: e2e-tests-gc-2xdpm, resource: bindings, ignored listing per whitelist
Mar  5 00:14:39.598: INFO: namespace e2e-tests-gc-2xdpm deletion completed in 6.59813528s

• [SLOW TEST:37.540 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:14:39.598: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6gnjn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-aafe4962-3edb-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:14:39.934: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-6gnjn" to be "success or failure"
Mar  5 00:14:39.942: INFO: Pod "pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.648373ms
Mar  5 00:14:42.385: INFO: Pod "pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.451182344s
Mar  5 00:14:44.399: INFO: Pod "pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.464382681s
STEP: Saw pod success
Mar  5 00:14:44.399: INFO: Pod "pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:14:44.407: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:14:44.458: INFO: Waiting for pod pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a to disappear
Mar  5 00:14:44.467: INFO: Pod pod-projected-configmaps-aaffdfca-3edb-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:14:44.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6gnjn" for this suite.
Mar  5 00:14:50.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:14:50.716: INFO: namespace: e2e-tests-projected-6gnjn, resource: bindings, ignored listing per whitelist
Mar  5 00:14:50.877: INFO: namespace e2e-tests-projected-6gnjn deletion completed in 6.400466594s

• [SLOW TEST:11.279 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:14:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rwgtt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 00:14:51.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-rwgtt'
Mar  5 00:14:51.303: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 00:14:51.303: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar  5 00:14:51.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-rwgtt'
Mar  5 00:14:51.557: INFO: stderr: ""
Mar  5 00:14:51.557: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:14:51.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rwgtt" for this suite.
Mar  5 00:14:57.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:14:57.921: INFO: namespace: e2e-tests-kubectl-rwgtt, resource: bindings, ignored listing per whitelist
Mar  5 00:14:58.101: INFO: namespace e2e-tests-kubectl-rwgtt deletion completed in 6.533287269s

• [SLOW TEST:7.223 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:14:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-km9l4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 00:14:58.400: INFO: Waiting up to 5m0s for pod "pod-b601c01a-3edb-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-km9l4" to be "success or failure"
Mar  5 00:14:58.408: INFO: Pod "pod-b601c01a-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255142ms
Mar  5 00:15:00.417: INFO: Pod "pod-b601c01a-3edb-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017010305s
STEP: Saw pod success
Mar  5 00:15:00.417: INFO: Pod "pod-b601c01a-3edb-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:15:00.425: INFO: Trying to get logs from node 10.190.208.159 pod pod-b601c01a-3edb-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:15:00.473: INFO: Waiting for pod pod-b601c01a-3edb-11e9-8a62-3ec24305971a to disappear
Mar  5 00:15:00.507: INFO: Pod pod-b601c01a-3edb-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:15:00.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-km9l4" for this suite.
Mar  5 00:15:06.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:15:06.851: INFO: namespace: e2e-tests-emptydir-km9l4, resource: bindings, ignored listing per whitelist
Mar  5 00:15:06.867: INFO: namespace e2e-tests-emptydir-km9l4 deletion completed in 6.343519613s

• [SLOW TEST:8.766 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:15:06.868: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-klhnp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar  5 00:15:09.272: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-bb44e969-3edb-11e9-8a62-3ec24305971a", GenerateName:"", Namespace:"e2e-tests-pods-klhnp", SelfLink:"/api/v1/namespaces/e2e-tests-pods-klhnp/pods/pod-submit-remove-bb44e969-3edb-11e9-8a62-3ec24305971a", UID:"bb47c986-3edb-11e9-844e-4e8eff50a26d", ResourceVersion:"31238", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63687341707, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"212638790"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-694x5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002a16240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-694x5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001336e68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.190.208.161", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000f2f140), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001336eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001336ed0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001336ed8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001336edc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341707, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341708, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341708, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63687341707, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.190.208.161", PodIP:"172.30.111.6", StartTime:(*v1.Time)(0xc0020e10c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0020e10e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"containerd://9ceb3eaebfad0b16a7b4c49763a5410ab60e366b417fba683afef1bca1886eec"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:15:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-klhnp" for this suite.
Mar  5 00:15:21.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:15:21.805: INFO: namespace: e2e-tests-pods-klhnp, resource: bindings, ignored listing per whitelist
Mar  5 00:15:21.859: INFO: namespace e2e-tests-pods-klhnp deletion completed in 6.349430996s

• [SLOW TEST:14.992 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:15:21.863: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qbr9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:15:22.229: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-qbr9x" to be "success or failure"
Mar  5 00:15:22.236: INFO: Pod "downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.353734ms
Mar  5 00:15:24.248: INFO: Pod "downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019698184s
STEP: Saw pod success
Mar  5 00:15:24.249: INFO: Pod "downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:15:24.257: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:15:24.301: INFO: Waiting for pod downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a to disappear
Mar  5 00:15:24.322: INFO: Pod downwardapi-volume-c434eccc-3edb-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:15:24.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qbr9x" for this suite.
Mar  5 00:15:30.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:15:30.550: INFO: namespace: e2e-tests-downward-api-qbr9x, resource: bindings, ignored listing per whitelist
Mar  5 00:15:30.728: INFO: namespace e2e-tests-downward-api-qbr9x deletion completed in 6.395049127s

• [SLOW TEST:8.866 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:15:30.731: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-ttgnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  5 00:15:35.193: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 00:15:35.200: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 00:15:37.201: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 00:15:37.209: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 00:15:39.201: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 00:15:39.220: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 00:15:41.201: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 00:15:41.210: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:15:41.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-ttgnq" for this suite.
Mar  5 00:16:05.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:16:05.357: INFO: namespace: e2e-tests-container-lifecycle-hook-ttgnq, resource: bindings, ignored listing per whitelist
Mar  5 00:16:05.579: INFO: namespace e2e-tests-container-lifecycle-hook-ttgnq deletion completed in 24.337599462s

• [SLOW TEST:34.849 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:16:05.579: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-s9fd4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  5 00:16:08.480: INFO: Successfully updated pod "annotationupdatede3ae3a3-3edb-11e9-8a62-3ec24305971a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:16:12.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-s9fd4" for this suite.
Mar  5 00:16:36.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:16:36.653: INFO: namespace: e2e-tests-downward-api-s9fd4, resource: bindings, ignored listing per whitelist
Mar  5 00:16:36.967: INFO: namespace e2e-tests-downward-api-s9fd4 deletion completed in 24.415951216s

• [SLOW TEST:31.387 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:16:36.968: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-bp8ll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-bp8ll
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 00:16:37.256: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 00:16:59.448: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.30.111.10 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bp8ll PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:16:59.448: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:00.621: INFO: Found all expected endpoints: [netserver-0]
Mar  5 00:17:00.631: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.30.252.235 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bp8ll PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:17:00.631: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:01.846: INFO: Found all expected endpoints: [netserver-1]
Mar  5 00:17:02.390: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 172.30.189.197 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-bp8ll PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:17:02.390: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:03.569: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:17:03.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-bp8ll" for this suite.
Mar  5 00:17:27.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:17:27.768: INFO: namespace: e2e-tests-pod-network-test-bp8ll, resource: bindings, ignored listing per whitelist
Mar  5 00:17:27.977: INFO: namespace e2e-tests-pod-network-test-bp8ll deletion completed in 24.367893098s

• [SLOW TEST:51.009 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:17:27.978: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bhbk5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-0f5d8eb2-3edc-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:17:28.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-bhbk5" to be "success or failure"
Mar  5 00:17:28.342: INFO: Pod "pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.525285ms
Mar  5 00:17:30.350: INFO: Pod "pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016331905s
Mar  5 00:17:32.361: INFO: Pod "pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026409855s
STEP: Saw pod success
Mar  5 00:17:32.361: INFO: Pod "pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:17:32.369: INFO: Trying to get logs from node 10.190.208.159 pod pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:17:32.430: INFO: Waiting for pod pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a to disappear
Mar  5 00:17:32.438: INFO: Pod pod-projected-configmaps-0f5efe11-3edc-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:17:32.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bhbk5" for this suite.
Mar  5 00:17:38.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:17:38.768: INFO: namespace: e2e-tests-projected-bhbk5, resource: bindings, ignored listing per whitelist
Mar  5 00:17:38.776: INFO: namespace e2e-tests-projected-bhbk5 deletion completed in 6.328070855s

• [SLOW TEST:10.799 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:17:38.776: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qwpm4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qwpm4
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 00:17:39.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  5 00:17:57.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.111.11:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qwpm4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:17:57.254: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:57.549: INFO: Found all expected endpoints: [netserver-0]
Mar  5 00:17:57.557: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.189.199:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qwpm4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:17:57.557: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:57.777: INFO: Found all expected endpoints: [netserver-1]
Mar  5 00:17:57.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.252.236:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qwpm4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:17:57.785: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:17:58.015: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:17:58.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qwpm4" for this suite.
Mar  5 00:18:14.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:18:14.228: INFO: namespace: e2e-tests-pod-network-test-qwpm4, resource: bindings, ignored listing per whitelist
Mar  5 00:18:14.554: INFO: namespace e2e-tests-pod-network-test-qwpm4 deletion completed in 16.45387761s

• [SLOW TEST:35.778 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:18:14.554: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lhdl2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:18:14.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-lhdl2" to be "success or failure"
Mar  5 00:18:14.903: INFO: Pod "downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.571333ms
Mar  5 00:18:16.911: INFO: Pod "downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056912843s
STEP: Saw pod success
Mar  5 00:18:16.911: INFO: Pod "downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:18:16.919: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:18:17.027: INFO: Waiting for pod downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a to disappear
Mar  5 00:18:17.035: INFO: Pod downwardapi-volume-2b1a3e35-3edc-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:18:17.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lhdl2" for this suite.
Mar  5 00:18:23.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:18:23.155: INFO: namespace: e2e-tests-projected-lhdl2, resource: bindings, ignored listing per whitelist
Mar  5 00:18:23.359: INFO: namespace e2e-tests-projected-lhdl2 deletion completed in 6.312729156s

• [SLOW TEST:8.805 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:18:23.359: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6lgwd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-30573356-3edc-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:18:23.649: INFO: Waiting up to 5m0s for pod "pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-6lgwd" to be "success or failure"
Mar  5 00:18:23.663: INFO: Pod "pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.117367ms
Mar  5 00:18:25.910: INFO: Pod "pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260888923s
Mar  5 00:18:27.918: INFO: Pod "pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.26902595s
STEP: Saw pod success
Mar  5 00:18:27.918: INFO: Pod "pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:18:27.926: INFO: Trying to get logs from node 10.190.208.159 pod pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 00:18:27.990: INFO: Waiting for pod pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a to disappear
Mar  5 00:18:27.998: INFO: Pod pod-secrets-30585dd6-3edc-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:18:27.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6lgwd" for this suite.
Mar  5 00:18:34.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:18:34.412: INFO: namespace: e2e-tests-secrets-6lgwd, resource: bindings, ignored listing per whitelist
Mar  5 00:18:34.530: INFO: namespace e2e-tests-secrets-6lgwd deletion completed in 6.521401419s

• [SLOW TEST:11.171 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:18:34.531: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rbfw2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar  5 00:18:34.908: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  5 00:18:34.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:35.444: INFO: stderr: ""
Mar  5 00:18:35.444: INFO: stdout: "service/redis-slave created\n"
Mar  5 00:18:35.444: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  5 00:18:35.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:35.765: INFO: stderr: ""
Mar  5 00:18:35.765: INFO: stdout: "service/redis-master created\n"
Mar  5 00:18:35.765: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  5 00:18:35.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:35.991: INFO: stderr: ""
Mar  5 00:18:35.991: INFO: stdout: "service/frontend created\n"
Mar  5 00:18:35.992: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  5 00:18:35.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:36.248: INFO: stderr: ""
Mar  5 00:18:36.248: INFO: stdout: "deployment.extensions/frontend created\n"
Mar  5 00:18:36.248: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  5 00:18:36.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:36.530: INFO: stderr: ""
Mar  5 00:18:36.530: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar  5 00:18:36.530: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  5 00:18:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:18:36.808: INFO: stderr: ""
Mar  5 00:18:36.808: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar  5 00:18:36.808: INFO: Waiting for all frontend pods to be Running.
Mar  5 00:18:51.860: INFO: Waiting for frontend to serve content.
Mar  5 00:18:56.892: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar  5 00:19:01.923: INFO: Trying to add a new entry to the guestbook.
Mar  5 00:19:01.953: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  5 00:19:01.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.155: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.155: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 00:19:02.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.302: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.302: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 00:19:02.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.451: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 00:19:02.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.561: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.561: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 00:19:02.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.694: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.694: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 00:19:02.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rbfw2'
Mar  5 00:19:02.941: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:19:02.941: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:19:02.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rbfw2" for this suite.
Mar  5 00:19:46.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:19:47.104: INFO: namespace: e2e-tests-kubectl-rbfw2, resource: bindings, ignored listing per whitelist
Mar  5 00:19:47.272: INFO: namespace e2e-tests-kubectl-rbfw2 deletion completed in 44.319893002s

• [SLOW TEST:72.742 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:19:47.272: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-8ltsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  5 00:19:51.828: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 00:19:51.836: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 00:19:53.837: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 00:19:53.845: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 00:19:55.837: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 00:19:55.845: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:19:55.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8ltsr" for this suite.
Mar  5 00:20:19.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:20:20.317: INFO: namespace: e2e-tests-container-lifecycle-hook-8ltsr, resource: bindings, ignored listing per whitelist
Mar  5 00:20:20.373: INFO: namespace e2e-tests-container-lifecycle-hook-8ltsr deletion completed in 24.516648561s

• [SLOW TEST:33.100 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:20:20.373: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cv4zp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:20:20.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-cv4zp" to be "success or failure"
Mar  5 00:20:20.741: INFO: Pod "downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.84491ms
Mar  5 00:20:22.750: INFO: Pod "downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018713865s
STEP: Saw pod success
Mar  5 00:20:22.750: INFO: Pod "downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:20:22.757: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:20:22.829: INFO: Waiting for pod downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a to disappear
Mar  5 00:20:22.837: INFO: Pod downwardapi-volume-7620e7ba-3edc-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:20:22.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cv4zp" for this suite.
Mar  5 00:20:28.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:20:29.383: INFO: namespace: e2e-tests-downward-api-cv4zp, resource: bindings, ignored listing per whitelist
Mar  5 00:20:29.525: INFO: namespace e2e-tests-downward-api-cv4zp deletion completed in 6.677386522s

• [SLOW TEST:9.152 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:20:29.525: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-rjdnd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:20:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:20:32.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rjdnd" for this suite.
Mar  5 00:21:28.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:21:28.679: INFO: namespace: e2e-tests-pods-rjdnd, resource: bindings, ignored listing per whitelist
Mar  5 00:21:28.805: INFO: namespace e2e-tests-pods-rjdnd deletion completed in 56.634696881s

• [SLOW TEST:59.280 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:21:28.805: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-pqgzb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  5 00:21:32.615: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:21:33.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-pqgzb" for this suite.
Mar  5 00:21:57.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:21:58.106: INFO: namespace: e2e-tests-replicaset-pqgzb, resource: bindings, ignored listing per whitelist
Mar  5 00:21:58.256: INFO: namespace e2e-tests-replicaset-pqgzb deletion completed in 24.51779788s

• [SLOW TEST:29.451 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:21:58.258: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-v4s5t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-v4s5t
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-v4s5t
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-v4s5t
Mar  5 00:21:59.118: INFO: Found 0 stateful pods, waiting for 1
Mar  5 00:22:09.127: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  5 00:22:09.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 00:22:09.491: INFO: stderr: ""
Mar  5 00:22:09.491: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 00:22:09.491: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 00:22:09.507: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  5 00:22:19.518: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 00:22:19.518: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 00:22:19.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998961s
Mar  5 00:22:20.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991364461s
Mar  5 00:22:21.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.630843373s
Mar  5 00:22:22.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.619970984s
Mar  5 00:22:23.941: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.61041626s
Mar  5 00:22:24.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.60127738s
Mar  5 00:22:25.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.591884798s
Mar  5 00:22:26.969: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.582425972s
Mar  5 00:22:27.978: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.573560773s
Mar  5 00:22:29.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 564.831441ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-v4s5t
Mar  5 00:22:30.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 00:22:30.427: INFO: stderr: ""
Mar  5 00:22:30.427: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 00:22:30.427: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 00:22:30.435: INFO: Found 1 stateful pods, waiting for 3
Mar  5 00:22:40.445: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 00:22:40.445: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 00:22:40.445: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  5 00:22:40.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 00:22:40.758: INFO: stderr: ""
Mar  5 00:22:40.758: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 00:22:40.758: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 00:22:40.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 00:22:41.082: INFO: stderr: ""
Mar  5 00:22:41.082: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 00:22:41.082: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 00:22:41.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  5 00:22:41.622: INFO: stderr: ""
Mar  5 00:22:41.622: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  5 00:22:41.622: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  5 00:22:41.622: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 00:22:41.631: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  5 00:22:51.648: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 00:22:51.648: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 00:22:51.648: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 00:22:51.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999886s
Mar  5 00:22:52.685: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991406483s
Mar  5 00:22:53.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981442851s
Mar  5 00:22:54.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972062444s
Mar  5 00:22:55.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962132195s
Mar  5 00:22:56.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952966216s
Mar  5 00:22:57.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943749446s
Mar  5 00:22:58.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93094241s
Mar  5 00:22:59.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921837016s
Mar  5 00:23:00.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 912.736732ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-v4s5t
Mar  5 00:23:01.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 00:23:02.080: INFO: stderr: ""
Mar  5 00:23:02.080: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 00:23:02.080: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 00:23:02.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 00:23:02.419: INFO: stderr: ""
Mar  5 00:23:02.419: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 00:23:02.419: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 00:23:02.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 exec --namespace=e2e-tests-statefulset-v4s5t ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  5 00:23:02.711: INFO: stderr: ""
Mar  5 00:23:02.711: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  5 00:23:02.711: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  5 00:23:02.711: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  5 00:23:32.795: INFO: Deleting all statefulset in ns e2e-tests-statefulset-v4s5t
Mar  5 00:23:32.803: INFO: Scaling statefulset ss to 0
Mar  5 00:23:32.827: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 00:23:32.907: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:23:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-v4s5t" for this suite.
Mar  5 00:23:40.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:23:41.047: INFO: namespace: e2e-tests-statefulset-v4s5t, resource: bindings, ignored listing per whitelist
Mar  5 00:23:41.271: INFO: namespace e2e-tests-statefulset-v4s5t deletion completed in 8.314461329s

• [SLOW TEST:103.014 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:23:41.272: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-vf562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  5 00:23:45.698: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:45.698: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:45.985: INFO: Exec stderr: ""
Mar  5 00:23:45.985: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:45.985: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:46.195: INFO: Exec stderr: ""
Mar  5 00:23:46.195: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:46.195: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:46.610: INFO: Exec stderr: ""
Mar  5 00:23:46.610: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:46.610: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:46.816: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  5 00:23:46.816: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:46.816: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:47.050: INFO: Exec stderr: ""
Mar  5 00:23:47.050: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:47.230: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  5 00:23:47.230: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:47.436: INFO: Exec stderr: ""
Mar  5 00:23:47.436: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:47.436: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:47.677: INFO: Exec stderr: ""
Mar  5 00:23:47.677: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:47.873: INFO: Exec stderr: ""
Mar  5 00:23:47.873: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-vf562 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 00:23:47.873: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
Mar  5 00:23:48.086: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:23:48.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-vf562" for this suite.
Mar  5 00:24:40.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:24:40.236: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-vf562, resource: bindings, ignored listing per whitelist
Mar  5 00:24:40.626: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-vf562 deletion completed in 52.515830739s

• [SLOW TEST:59.354 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:24:40.626: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7qtj5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  5 00:24:41.027: INFO: Waiting up to 5m0s for pod "downward-api-1147546c-3edd-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-7qtj5" to be "success or failure"
Mar  5 00:24:41.038: INFO: Pod "downward-api-1147546c-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.621149ms
Mar  5 00:24:43.046: INFO: Pod "downward-api-1147546c-3edd-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019065687s
STEP: Saw pod success
Mar  5 00:24:43.046: INFO: Pod "downward-api-1147546c-3edd-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:24:43.054: INFO: Trying to get logs from node 10.190.208.161 pod downward-api-1147546c-3edd-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  5 00:24:43.101: INFO: Waiting for pod downward-api-1147546c-3edd-11e9-8a62-3ec24305971a to disappear
Mar  5 00:24:43.108: INFO: Pod downward-api-1147546c-3edd-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:24:43.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7qtj5" for this suite.
Mar  5 00:24:49.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:24:49.716: INFO: namespace: e2e-tests-downward-api-7qtj5, resource: bindings, ignored listing per whitelist
Mar  5 00:24:49.776: INFO: namespace e2e-tests-downward-api-7qtj5 deletion completed in 6.658039892s

• [SLOW TEST:9.151 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:24:49.777: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-c7d6r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:24:50.078: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  5 00:24:55.086: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 00:24:55.086: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar  5 00:24:55.128: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-c7d6r,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-c7d6r/deployments/test-cleanup-deployment,UID:19ad207e-3edd-11e9-844e-4e8eff50a26d,ResourceVersion:33540,Generation:1,CreationTimestamp:2019-03-05 00:24:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  5 00:24:55.140: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar  5 00:24:55.141: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  5 00:24:55.141: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-c7d6r,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-c7d6r/replicasets/test-cleanup-controller,UID:16acb338-3edd-11e9-844e-4e8eff50a26d,ResourceVersion:33541,Generation:1,CreationTimestamp:2019-03-05 00:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 19ad207e-3edd-11e9-844e-4e8eff50a26d 0xc001ce2ab7 0xc001ce2ab8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  5 00:24:55.149: INFO: Pod "test-cleanup-controller-7vgnh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-7vgnh,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-c7d6r,SelfLink:/api/v1/namespaces/e2e-tests-deployment-c7d6r/pods/test-cleanup-controller-7vgnh,UID:16b08aec-3edd-11e9-844e-4e8eff50a26d,ResourceVersion:33532,Generation:0,CreationTimestamp:2019-03-05 00:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 16acb338-3edd-11e9-844e-4e8eff50a26d 0xc001bf8fb7 0xc001bf8fb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-ccmn4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ccmn4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ccmn4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.208.161,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bf9030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bf9050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:24:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:24:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:24:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-05 00:24:50 +0000 UTC  }],Message:,Reason:,HostIP:10.190.208.161,PodIP:172.30.111.21,StartTime:2019-03-05 00:24:50 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-05 00:24:51 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 containerd://be8c00800024a808bc0b9b01d44b0f4c357fd85948eaf1db46ae476362741cac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:24:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-c7d6r" for this suite.
Mar  5 00:25:01.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:25:01.660: INFO: namespace: e2e-tests-deployment-c7d6r, resource: bindings, ignored listing per whitelist
Mar  5 00:25:01.788: INFO: namespace e2e-tests-deployment-c7d6r deletion completed in 6.587952258s

• [SLOW TEST:12.011 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:25:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cntzr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar  5 00:25:02.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 api-versions'
Mar  5 00:25:02.222: INFO: stderr: ""
Mar  5 00:25:02.222: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:25:02.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cntzr" for this suite.
Mar  5 00:25:08.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:25:08.315: INFO: namespace: e2e-tests-kubectl-cntzr, resource: bindings, ignored listing per whitelist
Mar  5 00:25:08.649: INFO: namespace e2e-tests-kubectl-cntzr deletion completed in 6.415424631s

• [SLOW TEST:6.860 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:25:08.651: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hnzh5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:25:08.988: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-hnzh5" to be "success or failure"
Mar  5 00:25:09.014: INFO: Pod "downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.520049ms
Mar  5 00:25:11.024: INFO: Pod "downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036357683s
STEP: Saw pod success
Mar  5 00:25:11.024: INFO: Pod "downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:25:11.033: INFO: Trying to get logs from node 10.190.208.161 pod downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:25:11.081: INFO: Waiting for pod downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a to disappear
Mar  5 00:25:11.089: INFO: Pod downwardapi-volume-21f1eb39-3edd-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:25:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hnzh5" for this suite.
Mar  5 00:25:17.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:25:17.380: INFO: namespace: e2e-tests-downward-api-hnzh5, resource: bindings, ignored listing per whitelist
Mar  5 00:25:17.505: INFO: namespace e2e-tests-downward-api-hnzh5 deletion completed in 6.402477317s

• [SLOW TEST:8.854 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:25:17.505: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-55c98
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2735eaab-3edd-11e9-8a62-3ec24305971a
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2735eaab-3edd-11e9-8a62-3ec24305971a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:25:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-55c98" for this suite.
Mar  5 00:25:46.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:25:46.300: INFO: namespace: e2e-tests-projected-55c98, resource: bindings, ignored listing per whitelist
Mar  5 00:25:46.454: INFO: namespace e2e-tests-projected-55c98 deletion completed in 24.42197218s

• [SLOW TEST:28.949 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:25:46.456: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-5rh7l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:25:46.735: INFO: Creating ReplicaSet my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a
Mar  5 00:25:46.754: INFO: Pod name my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a: Found 0 pods out of 1
Mar  5 00:25:51.762: INFO: Pod name my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a: Found 1 pods out of 1
Mar  5 00:25:51.762: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a" is running
Mar  5 00:25:51.770: INFO: Pod "my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a-667np" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:25:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:25:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:25:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-05 00:25:46 +0000 UTC Reason: Message:}])
Mar  5 00:25:51.771: INFO: Trying to dial the pod
Mar  5 00:25:56.804: INFO: Controller my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a: Got expected result from replica 1 [my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a-667np]: "my-hostname-basic-38746b82-3edd-11e9-8a62-3ec24305971a-667np", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:25:56.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-5rh7l" for this suite.
Mar  5 00:26:02.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:26:03.066: INFO: namespace: e2e-tests-replicaset-5rh7l, resource: bindings, ignored listing per whitelist
Mar  5 00:26:03.294: INFO: namespace e2e-tests-replicaset-5rh7l deletion completed in 6.47888417s

• [SLOW TEST:16.839 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:26:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-sf5n5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar  5 00:26:03.775: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 00:26:03.800: INFO: Number of nodes with available pods: 0
Mar  5 00:26:03.800: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:26:04.823: INFO: Number of nodes with available pods: 1
Mar  5 00:26:04.824: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:26:05.822: INFO: Number of nodes with available pods: 3
Mar  5 00:26:05.822: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  5 00:26:05.878: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:05.878: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:05.878: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:06.997: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:06.997: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:06.997: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:07.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:07.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:07.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:08.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:08.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:08.901: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:09.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:09.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:09.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:10.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:10.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:10.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:11.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:11.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:11.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:12.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:12.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:12.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:13.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:13.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:13.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:14.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:14.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:14.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:15.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:15.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:15.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:16.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:16.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:16.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:17.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:17.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:17.900: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:18.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:18.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:18.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:19.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:19.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:19.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:20.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:20.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:20.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:21.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:21.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:21.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:22.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:22.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:22.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:23.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:23.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:23.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:24.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:24.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:24.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:25.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:25.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:25.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:26.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:26.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:26.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:27.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:27.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:27.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:28.903: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:28.903: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:28.903: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:29.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:29.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:29.901: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:30.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:30.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:30.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:31.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:31.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:31.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.279: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.279: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.279: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.902: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.902: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:33.902: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:34.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:34.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:34.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:35.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:35.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:35.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:36.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:36.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:36.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:37.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:37.902: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:37.902: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:37.902: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:38.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:38.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:38.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:38.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:39.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:39.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:39.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:39.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:40.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:40.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:40.898: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:40.898: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:41.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:41.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:41.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:41.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:42.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:42.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:42.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:42.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:43.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:43.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:43.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:43.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:44.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:44.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:44.899: INFO: Wrong image for pod: daemon-set-qngp7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:44.899: INFO: Pod daemon-set-qngp7 is not available
Mar  5 00:26:45.898: INFO: Pod daemon-set-8cp6j is not available
Mar  5 00:26:45.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:45.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:46.899: INFO: Pod daemon-set-8cp6j is not available
Mar  5 00:26:46.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:46.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:47.898: INFO: Pod daemon-set-8cp6j is not available
Mar  5 00:26:47.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:47.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:48.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:48.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:49.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:49.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:50.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:50.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:51.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:51.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:52.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:52.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:53.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:53.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:54.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:54.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:55.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:55.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:56.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:56.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:57.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:57.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:58.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:58.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:59.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:26:59.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:00.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:00.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:01.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:01.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:02.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:02.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:03.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:03.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:04.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:04.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:05.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:05.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:06.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:06.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:07.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:07.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:08.914: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:08.914: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:09.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:09.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:10.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:10.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:11.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:11.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:12.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:12.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:13.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:13.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:14.924: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:14.924: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:15.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:15.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:16.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:16.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:17.900: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:17.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:18.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:18.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:19.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:19.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:20.898: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:20.898: INFO: Pod daemon-set-jnp5q is not available
Mar  5 00:27:20.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:21.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:21.901: INFO: Pod daemon-set-jnp5q is not available
Mar  5 00:27:21.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:22.901: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:22.901: INFO: Pod daemon-set-jnp5q is not available
Mar  5 00:27:22.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:23.899: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:23.899: INFO: Pod daemon-set-jnp5q is not available
Mar  5 00:27:23.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:24.902: INFO: Wrong image for pod: daemon-set-jnp5q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:24.902: INFO: Pod daemon-set-jnp5q is not available
Mar  5 00:27:24.902: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:25.898: INFO: Pod daemon-set-lhlnt is not available
Mar  5 00:27:25.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:26.899: INFO: Pod daemon-set-lhlnt is not available
Mar  5 00:27:26.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:27.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:28.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:29.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:30.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:31.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:32.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:33.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:34.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:35.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:36.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:37.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:38.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:39.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:40.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:41.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:42.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:43.901: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:44.973: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:45.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:46.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:47.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:48.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:49.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:50.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:51.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:52.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:53.905: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:54.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:55.900: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:56.898: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:57.899: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:58.903: INFO: Wrong image for pod: daemon-set-lk7gd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar  5 00:27:58.903: INFO: Pod daemon-set-lk7gd is not available
Mar  5 00:27:59.899: INFO: Pod daemon-set-gvbtx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  5 00:28:00.008: INFO: Number of nodes with available pods: 2
Mar  5 00:28:00.008: INFO: Node 10.190.208.159 is running more than one daemon pod
Mar  5 00:28:01.040: INFO: Number of nodes with available pods: 3
Mar  5 00:28:01.040: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-sf5n5, will wait for the garbage collector to delete the pods
Mar  5 00:28:01.228: INFO: Deleting DaemonSet.extensions daemon-set took: 42.932437ms
Mar  5 00:28:01.328: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.284565ms
Mar  5 00:28:10.636: INFO: Number of nodes with available pods: 0
Mar  5 00:28:10.636: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 00:28:10.644: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-sf5n5/daemonsets","resourceVersion":"34229"},"items":null}

Mar  5 00:28:10.652: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-sf5n5/pods","resourceVersion":"34229"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:10.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-sf5n5" for this suite.
Mar  5 00:28:18.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:28:19.047: INFO: namespace: e2e-tests-daemonsets-sf5n5, resource: bindings, ignored listing per whitelist
Mar  5 00:28:19.059: INFO: namespace e2e-tests-daemonsets-sf5n5 deletion completed in 8.3218046s

• [SLOW TEST:135.763 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:28:19.061: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vlphh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:28:19.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-vlphh" to be "success or failure"
Mar  5 00:28:19.376: INFO: Pod "downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.8184ms
Mar  5 00:28:21.385: INFO: Pod "downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017759557s
Mar  5 00:28:23.429: INFO: Pod "downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061777472s
STEP: Saw pod success
Mar  5 00:28:23.429: INFO: Pod "downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:28:23.437: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:28:23.488: INFO: Waiting for pod downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a to disappear
Mar  5 00:28:23.498: INFO: Pod downwardapi-volume-936b9710-3edd-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:23.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vlphh" for this suite.
Mar  5 00:28:29.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:28:29.957: INFO: namespace: e2e-tests-downward-api-vlphh, resource: bindings, ignored listing per whitelist
Mar  5 00:28:30.116: INFO: namespace e2e-tests-downward-api-vlphh deletion completed in 6.605217s

• [SLOW TEST:11.055 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:28:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7n4fg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9a04195c-3edd-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:28:30.441: INFO: Waiting up to 5m0s for pod "pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a" in namespace "e2e-tests-secrets-7n4fg" to be "success or failure"
Mar  5 00:28:30.448: INFO: Pod "pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702267ms
Mar  5 00:28:32.458: INFO: Pod "pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017510172s
STEP: Saw pod success
Mar  5 00:28:32.458: INFO: Pod "pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:28:32.466: INFO: Trying to get logs from node 10.190.208.161 pod pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 00:28:32.540: INFO: Waiting for pod pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a to disappear
Mar  5 00:28:32.549: INFO: Pod pod-secrets-9a0556f9-3edd-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:32.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7n4fg" for this suite.
Mar  5 00:28:38.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:28:38.879: INFO: namespace: e2e-tests-secrets-7n4fg, resource: bindings, ignored listing per whitelist
Mar  5 00:28:39.016: INFO: namespace e2e-tests-secrets-7n4fg deletion completed in 6.454639956s

• [SLOW TEST:8.900 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:28:39.016: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-znwss
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar  5 00:28:39.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 cluster-info'
Mar  5 00:28:39.531: INFO: stderr: ""
Mar  5 00:28:39.531: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:39.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-znwss" for this suite.
Mar  5 00:28:45.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:28:46.212: INFO: namespace: e2e-tests-kubectl-znwss, resource: bindings, ignored listing per whitelist
Mar  5 00:28:46.212: INFO: namespace e2e-tests-kubectl-znwss deletion completed in 6.670075732s

• [SLOW TEST:7.196 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:28:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-trds9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  5 00:28:46.523: INFO: Waiting up to 5m0s for pod "pod-a39b50e6-3edd-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-trds9" to be "success or failure"
Mar  5 00:28:46.531: INFO: Pod "pod-a39b50e6-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.562835ms
Mar  5 00:28:48.540: INFO: Pod "pod-a39b50e6-3edd-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016044599s
Mar  5 00:28:50.548: INFO: Pod "pod-a39b50e6-3edd-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024453767s
STEP: Saw pod success
Mar  5 00:28:50.548: INFO: Pod "pod-a39b50e6-3edd-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:28:50.556: INFO: Trying to get logs from node 10.190.208.159 pod pod-a39b50e6-3edd-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:28:50.611: INFO: Waiting for pod pod-a39b50e6-3edd-11e9-8a62-3ec24305971a to disappear
Mar  5 00:28:50.619: INFO: Pod pod-a39b50e6-3edd-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:50.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-trds9" for this suite.
Mar  5 00:28:56.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:28:57.186: INFO: namespace: e2e-tests-emptydir-trds9, resource: bindings, ignored listing per whitelist
Mar  5 00:28:57.207: INFO: namespace e2e-tests-emptydir-trds9 deletion completed in 6.576586241s

• [SLOW TEST:10.994 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:28:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-rtgqq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:28:59.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-rtgqq" for this suite.
Mar  5 00:29:47.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:29:47.874: INFO: namespace: e2e-tests-kubelet-test-rtgqq, resource: bindings, ignored listing per whitelist
Mar  5 00:29:47.958: INFO: namespace e2e-tests-kubelet-test-rtgqq deletion completed in 48.328113234s

• [SLOW TEST:50.751 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:29:47.959: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-2w288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-2w288
Mar  5 00:29:50.270: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-2w288
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 00:29:50.277: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:33:50.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2w288" for this suite.
Mar  5 00:33:56.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:33:56.795: INFO: namespace: e2e-tests-container-probe-2w288, resource: bindings, ignored listing per whitelist
Mar  5 00:33:57.263: INFO: namespace e2e-tests-container-probe-2w288 deletion completed in 6.752065352s

• [SLOW TEST:249.304 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:33:57.263: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dq54f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar  5 00:33:57.574: INFO: Waiting up to 5m0s for pod "downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-dq54f" to be "success or failure"
Mar  5 00:33:57.582: INFO: Pod "downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.359865ms
Mar  5 00:33:59.608: INFO: Pod "downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033364589s
STEP: Saw pod success
Mar  5 00:33:59.608: INFO: Pod "downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:33:59.615: INFO: Trying to get logs from node 10.190.208.161 pod downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  5 00:33:59.666: INFO: Waiting for pod downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:33:59.675: INFO: Pod downward-api-5d019ae0-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:33:59.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dq54f" for this suite.
Mar  5 00:34:05.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:34:06.171: INFO: namespace: e2e-tests-downward-api-dq54f, resource: bindings, ignored listing per whitelist
Mar  5 00:34:06.200: INFO: namespace e2e-tests-downward-api-dq54f deletion completed in 6.513580251s

• [SLOW TEST:8.937 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:34:06.200: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8ctpl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 00:34:06.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-8ctpl'
Mar  5 00:34:06.621: INFO: stderr: ""
Mar  5 00:34:06.621: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Mar  5 00:34:06.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-8ctpl'
Mar  5 00:34:10.528: INFO: stderr: ""
Mar  5 00:34:10.528: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:34:10.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8ctpl" for this suite.
Mar  5 00:34:16.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:34:16.923: INFO: namespace: e2e-tests-kubectl-8ctpl, resource: bindings, ignored listing per whitelist
Mar  5 00:34:16.955: INFO: namespace e2e-tests-kubectl-8ctpl deletion completed in 6.355133925s

• [SLOW TEST:10.755 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:34:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-c2gs7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  5 00:34:17.271: INFO: Waiting up to 5m0s for pod "pod-68bf33c7-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-c2gs7" to be "success or failure"
Mar  5 00:34:17.280: INFO: Pod "pod-68bf33c7-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.959337ms
Mar  5 00:34:19.290: INFO: Pod "pod-68bf33c7-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018105257s
STEP: Saw pod success
Mar  5 00:34:19.290: INFO: Pod "pod-68bf33c7-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:34:19.298: INFO: Trying to get logs from node 10.190.208.161 pod pod-68bf33c7-3ede-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:34:19.345: INFO: Waiting for pod pod-68bf33c7-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:34:19.359: INFO: Pod pod-68bf33c7-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:34:19.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-c2gs7" for this suite.
Mar  5 00:34:25.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:34:25.597: INFO: namespace: e2e-tests-emptydir-c2gs7, resource: bindings, ignored listing per whitelist
Mar  5 00:34:26.236: INFO: namespace e2e-tests-emptydir-c2gs7 deletion completed in 6.865081991s

• [SLOW TEST:9.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:34:26.237: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-x9d9s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-x9d9s
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-x9d9s
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-x9d9s
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-x9d9s
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-x9d9s
Mar  5 00:34:28.651: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-x9d9s, name: ss-0, uid: 6e66f193-3ede-11e9-844e-4e8eff50a26d, status phase: Pending. Waiting for statefulset controller to delete.
Mar  5 00:34:30.508: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-x9d9s, name: ss-0, uid: 6e66f193-3ede-11e9-844e-4e8eff50a26d, status phase: Failed. Waiting for statefulset controller to delete.
Mar  5 00:34:30.522: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-x9d9s, name: ss-0, uid: 6e66f193-3ede-11e9-844e-4e8eff50a26d, status phase: Failed. Waiting for statefulset controller to delete.
Mar  5 00:34:30.533: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-x9d9s
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-x9d9s
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-x9d9s and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar  5 00:34:40.612: INFO: Deleting all statefulset in ns e2e-tests-statefulset-x9d9s
Mar  5 00:34:40.620: INFO: Scaling statefulset ss to 0
Mar  5 00:34:50.717: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 00:34:50.725: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:34:50.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-x9d9s" for this suite.
Mar  5 00:34:56.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:34:56.963: INFO: namespace: e2e-tests-statefulset-x9d9s, resource: bindings, ignored listing per whitelist
Mar  5 00:34:57.148: INFO: namespace e2e-tests-statefulset-x9d9s deletion completed in 6.374145641s

• [SLOW TEST:30.911 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:34:57.150: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-nnds5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  5 00:35:00.019: INFO: Successfully updated pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a"
Mar  5 00:35:00.019: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-pods-nnds5" to be "terminated due to deadline exceeded"
Mar  5 00:35:00.027: INFO: Pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a": Phase="Running", Reason="", readiness=true. Elapsed: 7.905813ms
Mar  5 00:35:02.035: INFO: Pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a": Phase="Running", Reason="", readiness=true. Elapsed: 2.016219983s
Mar  5 00:35:04.044: INFO: Pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.02466423s
Mar  5 00:35:04.044: INFO: Pod "pod-update-activedeadlineseconds-80b493e6-3ede-11e9-8a62-3ec24305971a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:04.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nnds5" for this suite.
Mar  5 00:35:12.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:35:12.288: INFO: namespace: e2e-tests-pods-nnds5, resource: bindings, ignored listing per whitelist
Mar  5 00:35:12.617: INFO: namespace e2e-tests-pods-nnds5 deletion completed in 8.516611857s

• [SLOW TEST:15.467 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:35:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bfg22
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-89f4efb8-3ede-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:35:13.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-bfg22" to be "success or failure"
Mar  5 00:35:13.028: INFO: Pod "pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.855349ms
Mar  5 00:35:15.036: INFO: Pod "pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018315305s
STEP: Saw pod success
Mar  5 00:35:15.036: INFO: Pod "pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:35:15.045: INFO: Trying to get logs from node 10.190.208.161 pod pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:35:15.100: INFO: Waiting for pod pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:35:15.107: INFO: Pod pod-configmaps-89f977e5-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:15.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bfg22" for this suite.
Mar  5 00:35:21.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:35:21.266: INFO: namespace: e2e-tests-configmap-bfg22, resource: bindings, ignored listing per whitelist
Mar  5 00:35:21.497: INFO: namespace e2e-tests-configmap-bfg22 deletion completed in 6.377979022s

• [SLOW TEST:8.880 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:35:21.497: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-47crh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar  5 00:35:21.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-47crh'
Mar  5 00:35:21.937: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  5 00:35:21.937: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Mar  5 00:35:25.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-47crh'
Mar  5 00:35:26.111: INFO: stderr: ""
Mar  5 00:35:26.111: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:26.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-47crh" for this suite.
Mar  5 00:35:32.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:35:32.280: INFO: namespace: e2e-tests-kubectl-47crh, resource: bindings, ignored listing per whitelist
Mar  5 00:35:32.475: INFO: namespace e2e-tests-kubectl-47crh deletion completed in 6.350811423s

• [SLOW TEST:10.978 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:35:32.477: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-w7kp4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 00:35:32.832: INFO: Waiting up to 5m0s for pod "pod-95c8ceda-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-w7kp4" to be "success or failure"
Mar  5 00:35:32.841: INFO: Pod "pod-95c8ceda-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.841019ms
Mar  5 00:35:34.849: INFO: Pod "pod-95c8ceda-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017546932s
STEP: Saw pod success
Mar  5 00:35:34.849: INFO: Pod "pod-95c8ceda-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:35:34.857: INFO: Trying to get logs from node 10.190.208.161 pod pod-95c8ceda-3ede-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:35:34.900: INFO: Waiting for pod pod-95c8ceda-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:35:34.908: INFO: Pod pod-95c8ceda-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:34.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-w7kp4" for this suite.
Mar  5 00:35:40.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:35:41.196: INFO: namespace: e2e-tests-emptydir-w7kp4, resource: bindings, ignored listing per whitelist
Mar  5 00:35:41.523: INFO: namespace e2e-tests-emptydir-w7kp4 deletion completed in 6.605161312s

• [SLOW TEST:9.046 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:35:41.524: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-4xtgk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-4xtgk/configmap-test-9b257b84-3ede-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:35:41.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-configmap-4xtgk" to be "success or failure"
Mar  5 00:35:41.843: INFO: Pod "pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.813575ms
Mar  5 00:35:43.857: INFO: Pod "pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021304209s
STEP: Saw pod success
Mar  5 00:35:43.857: INFO: Pod "pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:35:43.865: INFO: Trying to get logs from node 10.190.208.159 pod pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a container env-test: <nil>
STEP: delete the pod
Mar  5 00:35:43.916: INFO: Waiting for pod pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:35:43.924: INFO: Pod pod-configmaps-9b26e7cb-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:43.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4xtgk" for this suite.
Mar  5 00:35:50.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:35:50.580: INFO: namespace: e2e-tests-configmap-4xtgk, resource: bindings, ignored listing per whitelist
Mar  5 00:35:50.698: INFO: namespace e2e-tests-configmap-4xtgk deletion completed in 6.763493586s

• [SLOW TEST:9.175 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:35:50.698: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gnwqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-a0b1aba4-3ede-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume secrets
Mar  5 00:35:51.142: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-gnwqg" to be "success or failure"
Mar  5 00:35:51.150: INFO: Pod "pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109942ms
Mar  5 00:35:53.158: INFO: Pod "pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016624669s
Mar  5 00:35:55.167: INFO: Pod "pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02525117s
STEP: Saw pod success
Mar  5 00:35:55.167: INFO: Pod "pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:35:55.175: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 00:35:55.227: INFO: Waiting for pod pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:35:55.307: INFO: Pod pod-projected-secrets-a0b2eda6-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:35:55.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gnwqg" for this suite.
Mar  5 00:36:01.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:36:01.617: INFO: namespace: e2e-tests-projected-gnwqg, resource: bindings, ignored listing per whitelist
Mar  5 00:36:01.816: INFO: namespace e2e-tests-projected-gnwqg deletion completed in 6.497913983s

• [SLOW TEST:11.118 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:36:01.816: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sp7rp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:36:02.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-downward-api-sp7rp" to be "success or failure"
Mar  5 00:36:02.219: INFO: Pod "downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.745995ms
Mar  5 00:36:04.228: INFO: Pod "downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017379209s
STEP: Saw pod success
Mar  5 00:36:04.228: INFO: Pod "downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:36:04.236: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:36:04.282: INFO: Waiting for pod downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:36:04.290: INFO: Pod downwardapi-volume-a74bedb2-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:36:04.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sp7rp" for this suite.
Mar  5 00:36:10.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:36:10.512: INFO: namespace: e2e-tests-downward-api-sp7rp, resource: bindings, ignored listing per whitelist
Mar  5 00:36:10.828: INFO: namespace e2e-tests-downward-api-sp7rp deletion completed in 6.526908367s

• [SLOW TEST:9.012 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:36:10.829: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-l8jqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-aca9855b-3ede-11e9-8a62-3ec24305971a
STEP: Creating a pod to test consume configMaps
Mar  5 00:36:11.222: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-l8jqp" to be "success or failure"
Mar  5 00:36:11.231: INFO: Pod "pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.757625ms
Mar  5 00:36:13.241: INFO: Pod "pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018733292s
Mar  5 00:36:15.251: INFO: Pod "pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028983745s
STEP: Saw pod success
Mar  5 00:36:15.251: INFO: Pod "pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:36:15.259: INFO: Trying to get logs from node 10.190.208.161 pod pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 00:36:15.400: INFO: Waiting for pod pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:36:15.408: INFO: Pod pod-projected-configmaps-acab07dc-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:36:15.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l8jqp" for this suite.
Mar  5 00:36:21.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:36:21.645: INFO: namespace: e2e-tests-projected-l8jqp, resource: bindings, ignored listing per whitelist
Mar  5 00:36:21.765: INFO: namespace e2e-tests-projected-l8jqp deletion completed in 6.346893443s

• [SLOW TEST:10.937 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:36:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-zwd8h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar  5 00:36:22.121: INFO: Waiting up to 5m0s for pod "var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-var-expansion-zwd8h" to be "success or failure"
Mar  5 00:36:22.129: INFO: Pod "var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.953996ms
Mar  5 00:36:24.137: INFO: Pod "var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016223951s
STEP: Saw pod success
Mar  5 00:36:24.137: INFO: Pod "var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:36:24.145: INFO: Trying to get logs from node 10.190.208.159 pod var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a container dapi-container: <nil>
STEP: delete the pod
Mar  5 00:36:24.225: INFO: Waiting for pod var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:36:24.234: INFO: Pod var-expansion-b329c6b4-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:36:24.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-zwd8h" for this suite.
Mar  5 00:36:30.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:36:30.692: INFO: namespace: e2e-tests-var-expansion-zwd8h, resource: bindings, ignored listing per whitelist
Mar  5 00:36:30.726: INFO: namespace e2e-tests-var-expansion-zwd8h deletion completed in 6.481929884s

• [SLOW TEST:8.960 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:36:30.727: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xqtrj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Mar  5 00:36:31.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 create -f - --namespace=e2e-tests-kubectl-xqtrj'
Mar  5 00:36:31.350: INFO: stderr: ""
Mar  5 00:36:31.350: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar  5 00:36:32.360: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 00:36:32.360: INFO: Found 0 / 1
Mar  5 00:36:33.359: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 00:36:33.359: INFO: Found 1 / 1
Mar  5 00:36:33.359: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 00:36:33.367: INFO: Selector matched 1 pods for map[app:redis]
Mar  5 00:36:33.367: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  5 00:36:33.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 logs redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj'
Mar  5 00:36:33.668: INFO: stderr: ""
Mar  5 00:36:33.668: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Mar 00:36:32.363 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Mar 00:36:32.363 # Server started, Redis version 3.2.12\n1:M 05 Mar 00:36:32.363 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Mar 00:36:32.363 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  5 00:36:33.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 log redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj --tail=1'
Mar  5 00:36:33.840: INFO: stderr: ""
Mar  5 00:36:33.840: INFO: stdout: "1:M 05 Mar 00:36:32.363 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  5 00:36:33.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 log redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj --limit-bytes=1'
Mar  5 00:36:34.001: INFO: stderr: ""
Mar  5 00:36:34.001: INFO: stdout: " "
STEP: exposing timestamps
Mar  5 00:36:34.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 log redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj --tail=1 --timestamps'
Mar  5 00:36:34.139: INFO: stderr: ""
Mar  5 00:36:34.139: INFO: stdout: "2019-03-05T00:36:32.364075523Z 1:M 05 Mar 00:36:32.363 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  5 00:36:36.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 log redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj --since=1s'
Mar  5 00:36:36.837: INFO: stderr: ""
Mar  5 00:36:36.837: INFO: stdout: ""
Mar  5 00:36:36.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 log redis-master-gcx7b redis-master --namespace=e2e-tests-kubectl-xqtrj --since=24h'
Mar  5 00:36:36.954: INFO: stderr: ""
Mar  5 00:36:36.954: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Mar 00:36:32.363 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Mar 00:36:32.363 # Server started, Redis version 3.2.12\n1:M 05 Mar 00:36:32.363 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Mar 00:36:32.363 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Mar  5 00:36:36.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xqtrj'
Mar  5 00:36:37.068: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 00:36:37.068: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  5 00:36:37.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-xqtrj'
Mar  5 00:36:37.204: INFO: stderr: "No resources found.\n"
Mar  5 00:36:37.204: INFO: stdout: ""
Mar  5 00:36:37.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-061951449 get pods -l name=nginx --namespace=e2e-tests-kubectl-xqtrj -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 00:36:37.348: INFO: stderr: ""
Mar  5 00:36:37.348: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:36:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xqtrj" for this suite.
Mar  5 00:37:01.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:37:01.760: INFO: namespace: e2e-tests-kubectl-xqtrj, resource: bindings, ignored listing per whitelist
Mar  5 00:37:01.805: INFO: namespace e2e-tests-kubectl-xqtrj deletion completed in 24.446094419s

• [SLOW TEST:31.079 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:37:01.814: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bcbtz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar  5 00:37:04.697: INFO: Successfully updated pod "labelsupdatecb022540-3ede-11e9-8a62-3ec24305971a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:37:08.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bcbtz" for this suite.
Mar  5 00:37:32.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:37:33.050: INFO: namespace: e2e-tests-projected-bcbtz, resource: bindings, ignored listing per whitelist
Mar  5 00:37:33.256: INFO: namespace e2e-tests-projected-bcbtz deletion completed in 24.333009823s

• [SLOW TEST:31.442 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:37:33.258: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-mfmnz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 00:37:33.556: INFO: Waiting up to 5m0s for pod "pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a" in namespace "e2e-tests-emptydir-mfmnz" to be "success or failure"
Mar  5 00:37:33.565: INFO: Pod "pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.725921ms
Mar  5 00:37:35.573: INFO: Pod "pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017098294s
STEP: Saw pod success
Mar  5 00:37:35.573: INFO: Pod "pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:37:35.581: INFO: Trying to get logs from node 10.190.208.161 pod pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:37:35.631: INFO: Waiting for pod pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a to disappear
Mar  5 00:37:35.638: INFO: Pod pod-ddbdbc0b-3ede-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:37:35.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mfmnz" for this suite.
Mar  5 00:37:41.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:37:41.897: INFO: namespace: e2e-tests-emptydir-mfmnz, resource: bindings, ignored listing per whitelist
Mar  5 00:37:41.960: INFO: namespace e2e-tests-emptydir-mfmnz deletion completed in 6.311286484s

• [SLOW TEST:8.703 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:37:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sgmn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar  5 00:37:42.427: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-061951449 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:37:42.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sgmn9" for this suite.
Mar  5 00:37:48.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:37:48.785: INFO: namespace: e2e-tests-kubectl-sgmn9, resource: bindings, ignored listing per whitelist
Mar  5 00:37:48.917: INFO: namespace e2e-tests-kubectl-sgmn9 deletion completed in 6.362323233s

• [SLOW TEST:6.956 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:37:48.918: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-b4cmr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-b4cmr
Mar  5 00:37:51.478: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-b4cmr
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 00:37:51.486: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:41:52.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b4cmr" for this suite.
Mar  5 00:41:58.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:41:58.854: INFO: namespace: e2e-tests-container-probe-b4cmr, resource: bindings, ignored listing per whitelist
Mar  5 00:41:58.985: INFO: namespace e2e-tests-container-probe-b4cmr deletion completed in 6.4730014s

• [SLOW TEST:250.067 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:41:58.985: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-fbgfc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar  5 00:41:59.287: INFO: Waiting up to 5m0s for pod "client-containers-7c211423-3edf-11e9-8a62-3ec24305971a" in namespace "e2e-tests-containers-fbgfc" to be "success or failure"
Mar  5 00:41:59.295: INFO: Pod "client-containers-7c211423-3edf-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.306074ms
Mar  5 00:42:01.303: INFO: Pod "client-containers-7c211423-3edf-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016270228s
STEP: Saw pod success
Mar  5 00:42:01.303: INFO: Pod "client-containers-7c211423-3edf-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:42:01.312: INFO: Trying to get logs from node 10.190.208.161 pod client-containers-7c211423-3edf-11e9-8a62-3ec24305971a container test-container: <nil>
STEP: delete the pod
Mar  5 00:42:01.362: INFO: Waiting for pod client-containers-7c211423-3edf-11e9-8a62-3ec24305971a to disappear
Mar  5 00:42:01.370: INFO: Pod client-containers-7c211423-3edf-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:42:01.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-fbgfc" for this suite.
Mar  5 00:42:07.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:42:07.520: INFO: namespace: e2e-tests-containers-fbgfc, resource: bindings, ignored listing per whitelist
Mar  5 00:42:07.778: INFO: namespace e2e-tests-containers-fbgfc deletion completed in 6.399019098s

• [SLOW TEST:8.793 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:42:07.781: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-px728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  5 00:42:08.162: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-px728,SelfLink:/api/v1/namespaces/e2e-tests-watch-px728/configmaps/e2e-watch-test-resource-version,UID:81648a7a-3edf-11e9-844e-4e8eff50a26d,ResourceVersion:36650,Generation:0,CreationTimestamp:2019-03-05 00:42:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  5 00:42:08.162: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-px728,SelfLink:/api/v1/namespaces/e2e-tests-watch-px728/configmaps/e2e-watch-test-resource-version,UID:81648a7a-3edf-11e9-844e-4e8eff50a26d,ResourceVersion:36651,Generation:0,CreationTimestamp:2019-03-05 00:42:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:42:08.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-px728" for this suite.
Mar  5 00:42:14.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:42:14.365: INFO: namespace: e2e-tests-watch-px728, resource: bindings, ignored listing per whitelist
Mar  5 00:42:14.609: INFO: namespace e2e-tests-watch-px728 deletion completed in 6.438787332s

• [SLOW TEST:6.829 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Mar  5 00:42:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-061951449
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nksnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar  5 00:42:14.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a" in namespace "e2e-tests-projected-nksnq" to be "success or failure"
Mar  5 00:42:14.927: INFO: Pod "downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.071763ms
Mar  5 00:42:16.937: INFO: Pod "downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018317288s
STEP: Saw pod success
Mar  5 00:42:16.937: INFO: Pod "downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a" satisfied condition "success or failure"
Mar  5 00:42:16.945: INFO: Trying to get logs from node 10.190.208.159 pod downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a container client-container: <nil>
STEP: delete the pod
Mar  5 00:42:17.102: INFO: Waiting for pod downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a to disappear
Mar  5 00:42:17.109: INFO: Pod downwardapi-volume-85724a9b-3edf-11e9-8a62-3ec24305971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Mar  5 00:42:17.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nksnq" for this suite.
Mar  5 00:42:23.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  5 00:42:23.470: INFO: namespace: e2e-tests-projected-nksnq, resource: bindings, ignored listing per whitelist
Mar  5 00:42:23.491: INFO: namespace e2e-tests-projected-nksnq deletion completed in 6.367178098s

• [SLOW TEST:8.882 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSMar  5 00:42:23.492: INFO: Running AfterSuite actions on all nodes
Mar  5 00:42:23.492: INFO: Running AfterSuite actions on node 1
Mar  5 00:42:23.492: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5915.384 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h38m36.358520186s
Test Suite Passed
