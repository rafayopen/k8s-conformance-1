I1209 21:58:41.069798      17 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-348219877
I1209 21:58:41.069921      17 e2e.go:224] Starting e2e run "9698c90c-fbfd-11e8-a4b0-6e57f5092bbd" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1544392720 - Will randomize all specs
Will run 201 of 1946 specs

Dec  9 21:58:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 21:58:41.174: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  9 21:58:41.187: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  9 21:58:41.228: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  9 21:58:41.228: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec  9 21:58:41.228: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  9 21:58:41.236: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  9 21:58:41.236: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  9 21:58:41.236: INFO: e2e test version: v1.13.0
Dec  9 21:58:41.238: INFO: kube-apiserver version: v1.13.0
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:58:41.238: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
Dec  9 21:58:41.556: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Dec  9 21:58:41.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 --namespace=e2e-tests-kubectl-rvgwl run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  9 21:58:48.486: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  9 21:58:48.486: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 21:58:50.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rvgwl" for this suite.
Dec  9 21:58:56.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 21:58:56.610: INFO: namespace: e2e-tests-kubectl-rvgwl, resource: bindings, ignored listing per whitelist
Dec  9 21:58:56.685: INFO: namespace e2e-tests-kubectl-rvgwl deletion completed in 6.184270575s

• [SLOW TEST:15.447 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:58:56.685: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 21:58:56.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-22t4l" to be "success or failure"
Dec  9 21:58:57.057: INFO: Pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 80.680647ms
Dec  9 21:58:59.063: INFO: Pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086419029s
Dec  9 21:59:01.074: INFO: Pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09692828s
Dec  9 21:59:03.079: INFO: Pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102476986s
STEP: Saw pod success
Dec  9 21:59:03.079: INFO: Pod "downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 21:59:03.084: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 21:59:03.182: INFO: Waiting for pod downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 21:59:03.248: INFO: Pod downwardapi-volume-a0477577-fbfd-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 21:59:03.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-22t4l" for this suite.
Dec  9 21:59:09.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 21:59:09.534: INFO: namespace: e2e-tests-downward-api-22t4l, resource: bindings, ignored listing per whitelist
Dec  9 21:59:09.548: INFO: namespace e2e-tests-downward-api-22t4l deletion completed in 6.292196461s

• [SLOW TEST:12.863 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:59:09.548: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  9 21:59:09.835: INFO: Waiting up to 5m0s for pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-gnmrd" to be "success or failure"
Dec  9 21:59:09.891: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 55.720063ms
Dec  9 21:59:11.897: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061451842s
Dec  9 21:59:13.923: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08786593s
Dec  9 21:59:15.929: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093549334s
Dec  9 21:59:17.934: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099202568s
Dec  9 21:59:19.940: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104482076s
STEP: Saw pod success
Dec  9 21:59:19.940: INFO: Pod "downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 21:59:19.945: INFO: Trying to get logs from node k8s-g2 pod downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 21:59:20.026: INFO: Waiting for pod downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 21:59:20.031: INFO: Pod downward-api-a7f4971d-fbfd-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 21:59:20.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gnmrd" for this suite.
Dec  9 21:59:26.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 21:59:26.074: INFO: namespace: e2e-tests-downward-api-gnmrd, resource: bindings, ignored listing per whitelist
Dec  9 21:59:26.192: INFO: namespace e2e-tests-downward-api-gnmrd deletion completed in 6.15525842s

• [SLOW TEST:16.643 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:59:26.192: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 21:59:26.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-8nqlg'
Dec  9 21:59:26.498: INFO: stderr: ""
Dec  9 21:59:26.498: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  9 21:59:31.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-8nqlg -o json'
Dec  9 21:59:31.604: INFO: stderr: ""
Dec  9 21:59:31.604: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.4.28/32\"\n        },\n        \"creationTimestamp\": \"2018-12-09T21:59:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-8nqlg\",\n        \"resourceVersion\": \"36491\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-8nqlg/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b1e56435-fbfd-11e8-9ee5-54a05085d523\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5kn8m\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-g2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5kn8m\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5kn8m\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-09T21:59:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-09T21:59:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-09T21:59:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-09T21:59:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://5ebef5b7133728a8ec305b46f8b18e7c946f630f2dfe0d124c46b15a97d94cd5\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-12-09T21:59:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.22.132.14\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.28\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-12-09T21:59:26Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  9 21:59:31.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 replace -f - --namespace=e2e-tests-kubectl-8nqlg'
Dec  9 21:59:31.966: INFO: stderr: ""
Dec  9 21:59:31.966: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Dec  9 21:59:31.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-8nqlg'
Dec  9 21:59:41.314: INFO: stderr: ""
Dec  9 21:59:41.314: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 21:59:41.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8nqlg" for this suite.
Dec  9 21:59:47.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 21:59:47.388: INFO: namespace: e2e-tests-kubectl-8nqlg, resource: bindings, ignored listing per whitelist
Dec  9 21:59:47.481: INFO: namespace e2e-tests-kubectl-8nqlg deletion completed in 6.160243294s

• [SLOW TEST:21.289 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:59:47.481: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  9 21:59:47.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 21:59:47.736: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 21:59:47.759: INFO: 
Logging pods the kubelet thinks is on node k8s-g1 before test
Dec  9 21:59:47.778: INFO: kube-proxy-vv5ch from kube-system started at 2018-12-09 16:53:42 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 21:59:47.778: INFO: calico-node-xzzvt from kube-system started at 2018-12-09 16:52:48 +0000 UTC (2 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 21:59:47.778: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 21:59:47.778: INFO: sonobuoy-e2e-job-1185eb09e2a5466d from heptio-sonobuoy started at 2018-12-09 21:58:15 +0000 UTC (2 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container e2e ready: true, restart count 0
Dec  9 21:59:47.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 21:59:47.778: INFO: ssh-server-75fc845b9d-h6g6b from default started at 2018-12-09 17:11:36 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container ssh-server ready: true, restart count 1
Dec  9 21:59:47.778: INFO: coredns-5569467fdf-x57ml from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container coredns ready: true, restart count 1
Dec  9 21:59:47.778: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-8kkg6 from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 21:59:47.778: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  9 21:59:47.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 21:59:47.778: INFO: 
Logging pods the kubelet thinks is on node k8s-g2 before test
Dec  9 21:59:47.786: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-r6m2t from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 21:59:47.786: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  9 21:59:47.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 21:59:47.786: INFO: coredns-5569467fdf-cj6br from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.786: INFO: 	Container coredns ready: true, restart count 1
Dec  9 21:59:47.786: INFO: calico-node-29r2w from kube-system started at 2018-12-09 16:52:49 +0000 UTC (2 container statuses recorded)
Dec  9 21:59:47.786: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 21:59:47.786: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 21:59:47.786: INFO: kube-proxy-lw2nk from kube-system started at 2018-12-09 16:53:48 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.786: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 21:59:47.786: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-09 21:58:11 +0000 UTC (1 container statuses recorded)
Dec  9 21:59:47.786: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.156ec90edc98ae3e], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 21:59:48.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-db2hd" for this suite.
Dec  9 21:59:54.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 21:59:55.040: INFO: namespace: e2e-tests-sched-pred-db2hd, resource: bindings, ignored listing per whitelist
Dec  9 21:59:55.088: INFO: namespace e2e-tests-sched-pred-db2hd deletion completed in 6.176057211s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.607 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 21:59:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-wspff
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-wspff to expose endpoints map[]
Dec  9 21:59:55.447: INFO: Get endpoints failed (8.582135ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec  9 21:59:56.453: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-wspff exposes endpoints map[] (1.014899249s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-wspff
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-wspff to expose endpoints map[pod1:[80]]
Dec  9 21:59:59.653: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-wspff exposes endpoints map[pod1:[80]] (3.14064965s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-wspff
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-wspff to expose endpoints map[pod1:[80] pod2:[80]]
Dec  9 22:00:02.862: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-wspff exposes endpoints map[pod1:[80] pod2:[80]] (3.173090162s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-wspff
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-wspff to expose endpoints map[pod2:[80]]
Dec  9 22:00:02.950: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-wspff exposes endpoints map[pod2:[80]] (27.02835ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-wspff
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-wspff to expose endpoints map[]
Dec  9 22:00:03.061: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-wspff exposes endpoints map[] (22.705606ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:00:03.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-wspff" for this suite.
Dec  9 22:00:27.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:00:27.322: INFO: namespace: e2e-tests-services-wspff, resource: bindings, ignored listing per whitelist
Dec  9 22:00:27.411: INFO: namespace e2e-tests-services-wspff deletion completed in 24.185646156s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:32.322 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:00:27.411: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  9 22:00:37.977: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:37.983: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:39.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:39.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:41.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:41.990: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:43.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:43.990: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:45.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:45.990: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:47.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:47.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:49.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:49.988: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:51.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:51.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:53.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:53.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:55.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:55.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:57.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:00:57.989: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:00:59.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:01:00.019: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 22:01:01.983: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 22:01:01.988: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:01:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-ktsrw" for this suite.
Dec  9 22:01:26.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:01:26.189: INFO: namespace: e2e-tests-container-lifecycle-hook-ktsrw, resource: bindings, ignored listing per whitelist
Dec  9 22:01:26.206: INFO: namespace e2e-tests-container-lifecycle-hook-ktsrw deletion completed in 24.203463269s

• [SLOW TEST:58.796 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:01:26.206: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Dec  9 22:01:26.524: INFO: Waiting up to 5m0s for pod "client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-containers-rx4xj" to be "success or failure"
Dec  9 22:01:26.580: INFO: Pod "client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.210969ms
Dec  9 22:01:28.587: INFO: Pod "client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062698091s
Dec  9 22:01:30.607: INFO: Pod "client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083592303s
STEP: Saw pod success
Dec  9 22:01:30.607: INFO: Pod "client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:01:30.613: INFO: Trying to get logs from node k8s-g1 pod client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:01:30.734: INFO: Waiting for pod client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:01:30.739: INFO: Pod client-containers-f967c55e-fbfd-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:01:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-rx4xj" for this suite.
Dec  9 22:01:36.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:01:36.836: INFO: namespace: e2e-tests-containers-rx4xj, resource: bindings, ignored listing per whitelist
Dec  9 22:01:36.921: INFO: namespace e2e-tests-containers-rx4xj deletion completed in 6.176440695s

• [SLOW TEST:10.714 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:01:36.921: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:01:37.302: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ffcf1dbb-fbfd-11e8-9ee5-54a05085d523", Controller:(*bool)(0xc001bb61b6), BlockOwnerDeletion:(*bool)(0xc001bb61b7)}}
Dec  9 22:01:37.427: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ffc5eaf4-fbfd-11e8-9ee5-54a05085d523", Controller:(*bool)(0xc000b02e46), BlockOwnerDeletion:(*bool)(0xc000b02e47)}}
Dec  9 22:01:37.542: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ffc77e84-fbfd-11e8-9ee5-54a05085d523", Controller:(*bool)(0xc001bb6596), BlockOwnerDeletion:(*bool)(0xc001bb6597)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:01:42.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mnv7n" for this suite.
Dec  9 22:01:48.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:01:48.742: INFO: namespace: e2e-tests-gc-mnv7n, resource: bindings, ignored listing per whitelist
Dec  9 22:01:48.823: INFO: namespace e2e-tests-gc-mnv7n deletion completed in 6.198327451s

• [SLOW TEST:11.902 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:01:48.823: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5fssx
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  9 22:01:49.078: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  9 22:02:13.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.72:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5fssx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:02:13.601: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:02:13.671: INFO: Found all expected endpoints: [netserver-0]
Dec  9 22:02:13.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.31:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5fssx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:02:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:02:13.741: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:02:13.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5fssx" for this suite.
Dec  9 22:02:37.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:02:37.812: INFO: namespace: e2e-tests-pod-network-test-5fssx, resource: bindings, ignored listing per whitelist
Dec  9 22:02:37.904: INFO: namespace e2e-tests-pod-network-test-5fssx deletion completed in 24.157725423s

• [SLOW TEST:49.081 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:02:37.904: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  9 22:02:38.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xkbmb,SelfLink:/api/v1/namespaces/e2e-tests-watch-xkbmb/configmaps/e2e-watch-test-watch-closed,UID:241f2e67-fbfe-11e8-9ee5-54a05085d523,ResourceVersion:37121,Generation:0,CreationTimestamp:2018-12-09 22:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  9 22:02:38.159: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xkbmb,SelfLink:/api/v1/namespaces/e2e-tests-watch-xkbmb/configmaps/e2e-watch-test-watch-closed,UID:241f2e67-fbfe-11e8-9ee5-54a05085d523,ResourceVersion:37122,Generation:0,CreationTimestamp:2018-12-09 22:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  9 22:02:38.274: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xkbmb,SelfLink:/api/v1/namespaces/e2e-tests-watch-xkbmb/configmaps/e2e-watch-test-watch-closed,UID:241f2e67-fbfe-11e8-9ee5-54a05085d523,ResourceVersion:37123,Generation:0,CreationTimestamp:2018-12-09 22:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  9 22:02:38.274: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xkbmb,SelfLink:/api/v1/namespaces/e2e-tests-watch-xkbmb/configmaps/e2e-watch-test-watch-closed,UID:241f2e67-fbfe-11e8-9ee5-54a05085d523,ResourceVersion:37124,Generation:0,CreationTimestamp:2018-12-09 22:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:02:38.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-xkbmb" for this suite.
Dec  9 22:02:44.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:02:44.426: INFO: namespace: e2e-tests-watch-xkbmb, resource: bindings, ignored listing per whitelist
Dec  9 22:02:44.437: INFO: namespace e2e-tests-watch-xkbmb deletion completed in 6.158579197s

• [SLOW TEST:6.533 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:02:44.437: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Dec  9 22:02:44.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:44.972: INFO: stderr: ""
Dec  9 22:02:44.972: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 22:02:44.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:45.050: INFO: stderr: ""
Dec  9 22:02:45.050: INFO: stdout: "update-demo-nautilus-br46h "
STEP: Replicas for name=update-demo: expected=2 actual=1
Dec  9 22:02:50.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:50.107: INFO: stderr: ""
Dec  9 22:02:50.107: INFO: stdout: "update-demo-nautilus-6cb64 update-demo-nautilus-br46h "
Dec  9 22:02:50.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-6cb64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:50.160: INFO: stderr: ""
Dec  9 22:02:50.160: INFO: stdout: "true"
Dec  9 22:02:50.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-6cb64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:50.214: INFO: stderr: ""
Dec  9 22:02:50.214: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:02:50.214: INFO: validating pod update-demo-nautilus-6cb64
Dec  9 22:02:50.224: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:02:50.224: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:02:50.224: INFO: update-demo-nautilus-6cb64 is verified up and running
Dec  9 22:02:50.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-br46h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:50.280: INFO: stderr: ""
Dec  9 22:02:50.280: INFO: stdout: "true"
Dec  9 22:02:50.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-br46h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:02:50.334: INFO: stderr: ""
Dec  9 22:02:50.334: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:02:50.334: INFO: validating pod update-demo-nautilus-br46h
Dec  9 22:02:50.365: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:02:50.365: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:02:50.365: INFO: update-demo-nautilus-br46h is verified up and running
STEP: rolling-update to new replication controller
Dec  9 22:02:50.366: INFO: scanned /root for discovery docs: <nil>
Dec  9 22:02:50.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.309: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  9 22:03:13.309: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 22:03:13.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.414: INFO: stderr: ""
Dec  9 22:03:13.414: INFO: stdout: "update-demo-kitten-pnk6s update-demo-kitten-ptfl4 "
Dec  9 22:03:13.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-kitten-pnk6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.470: INFO: stderr: ""
Dec  9 22:03:13.470: INFO: stdout: "true"
Dec  9 22:03:13.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-kitten-pnk6s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.540: INFO: stderr: ""
Dec  9 22:03:13.540: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  9 22:03:13.540: INFO: validating pod update-demo-kitten-pnk6s
Dec  9 22:03:13.547: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  9 22:03:13.547: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  9 22:03:13.547: INFO: update-demo-kitten-pnk6s is verified up and running
Dec  9 22:03:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-kitten-ptfl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.601: INFO: stderr: ""
Dec  9 22:03:13.601: INFO: stdout: "true"
Dec  9 22:03:13.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-kitten-ptfl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ckbwb'
Dec  9 22:03:13.654: INFO: stderr: ""
Dec  9 22:03:13.654: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  9 22:03:13.654: INFO: validating pod update-demo-kitten-ptfl4
Dec  9 22:03:13.684: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  9 22:03:13.684: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  9 22:03:13.684: INFO: update-demo-kitten-ptfl4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:03:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ckbwb" for this suite.
Dec  9 22:03:37.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:03:37.746: INFO: namespace: e2e-tests-kubectl-ckbwb, resource: bindings, ignored listing per whitelist
Dec  9 22:03:37.876: INFO: namespace e2e-tests-kubectl-ckbwb deletion completed in 24.18526634s

• [SLOW TEST:53.439 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:03:37.876: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-47e41b47-fbfe-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:03:38.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-qxvb9" to be "success or failure"
Dec  9 22:03:38.285: INFO: Pod "pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 53.080826ms
Dec  9 22:03:40.291: INFO: Pod "pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058386882s
Dec  9 22:03:42.296: INFO: Pod "pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063987873s
STEP: Saw pod success
Dec  9 22:03:42.296: INFO: Pod "pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:03:42.302: INFO: Trying to get logs from node k8s-g1 pod pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:03:42.426: INFO: Waiting for pod pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:03:42.431: INFO: Pod pod-projected-configmaps-47e8b878-fbfe-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:03:42.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qxvb9" for this suite.
Dec  9 22:03:48.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:03:48.545: INFO: namespace: e2e-tests-projected-qxvb9, resource: bindings, ignored listing per whitelist
Dec  9 22:03:48.650: INFO: namespace e2e-tests-projected-qxvb9 deletion completed in 6.214261369s

• [SLOW TEST:10.774 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:03:48.650: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:03:48.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-pw797" to be "success or failure"
Dec  9 22:03:48.953: INFO: Pod "downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.274776ms
Dec  9 22:03:50.958: INFO: Pod "downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041588198s
Dec  9 22:03:52.964: INFO: Pod "downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04768476s
STEP: Saw pod success
Dec  9 22:03:52.964: INFO: Pod "downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:03:52.970: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:03:53.105: INFO: Waiting for pod downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:03:53.110: INFO: Pod downwardapi-volume-4e4c951f-fbfe-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:03:53.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pw797" for this suite.
Dec  9 22:03:59.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:03:59.287: INFO: namespace: e2e-tests-projected-pw797, resource: bindings, ignored listing per whitelist
Dec  9 22:03:59.304: INFO: namespace e2e-tests-projected-pw797 deletion completed in 6.188437249s

• [SLOW TEST:10.654 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:03:59.304: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:03:59.490: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:04:00.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-n6ln7" for this suite.
Dec  9 22:04:06.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:04:07.005: INFO: namespace: e2e-tests-custom-resource-definition-n6ln7, resource: bindings, ignored listing per whitelist
Dec  9 22:04:07.082: INFO: namespace e2e-tests-custom-resource-definition-n6ln7 deletion completed in 6.245143694s

• [SLOW TEST:7.778 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:04:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1209 22:04:17.391149      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 22:04:17.391: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:04:17.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-cmbtz" for this suite.
Dec  9 22:04:25.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:04:25.638: INFO: namespace: e2e-tests-gc-cmbtz, resource: bindings, ignored listing per whitelist
Dec  9 22:04:25.654: INFO: namespace e2e-tests-gc-cmbtz deletion completed in 8.259303739s

• [SLOW TEST:18.571 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:04:25.654: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:04:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:04:30.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-d4nht" for this suite.
Dec  9 22:05:14.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:05:14.253: INFO: namespace: e2e-tests-pods-d4nht, resource: bindings, ignored listing per whitelist
Dec  9 22:05:14.375: INFO: namespace e2e-tests-pods-d4nht deletion completed in 44.217509605s

• [SLOW TEST:48.721 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:05:14.375: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:05:14.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-2mjkf" to be "success or failure"
Dec  9 22:05:14.725: INFO: Pod "downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.971365ms
Dec  9 22:05:16.732: INFO: Pod "downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031130887s
Dec  9 22:05:18.737: INFO: Pod "downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036666699s
STEP: Saw pod success
Dec  9 22:05:18.737: INFO: Pod "downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:05:18.743: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:05:18.859: INFO: Waiting for pod downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:05:18.864: INFO: Pod downwardapi-volume-816ceb1a-fbfe-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:05:18.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2mjkf" for this suite.
Dec  9 22:05:24.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:05:25.039: INFO: namespace: e2e-tests-projected-2mjkf, resource: bindings, ignored listing per whitelist
Dec  9 22:05:25.058: INFO: namespace e2e-tests-projected-2mjkf deletion completed in 6.170518218s

• [SLOW TEST:10.683 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:05:25.058: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  9 22:05:25.364: INFO: Waiting up to 5m0s for pod "pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-cddr8" to be "success or failure"
Dec  9 22:05:25.389: INFO: Pod "pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.972073ms
Dec  9 22:05:27.395: INFO: Pod "pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030832259s
Dec  9 22:05:29.401: INFO: Pod "pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036808245s
STEP: Saw pod success
Dec  9 22:05:29.401: INFO: Pod "pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:05:29.406: INFO: Trying to get logs from node k8s-g1 pod pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:05:29.513: INFO: Waiting for pod pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:05:29.517: INFO: Pod pod-87c6326c-fbfe-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:05:29.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cddr8" for this suite.
Dec  9 22:05:35.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:05:35.598: INFO: namespace: e2e-tests-emptydir-cddr8, resource: bindings, ignored listing per whitelist
Dec  9 22:05:35.721: INFO: namespace e2e-tests-emptydir-cddr8 deletion completed in 6.198854529s

• [SLOW TEST:10.663 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:05:35.721: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:05:40.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-6b6gf" for this suite.
Dec  9 22:06:20.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:06:20.138: INFO: namespace: e2e-tests-kubelet-test-6b6gf, resource: bindings, ignored listing per whitelist
Dec  9 22:06:20.238: INFO: namespace e2e-tests-kubelet-test-6b6gf deletion completed in 40.166935669s

• [SLOW TEST:44.517 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:06:20.239: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:06:20.578: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  9 22:06:25.585: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  9 22:06:25.585: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  9 22:06:25.646: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-p67g5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-p67g5/deployments/test-cleanup-deployment,UID:abb6e7ab-fbfe-11e8-9ee5-54a05085d523,ResourceVersion:37916,Generation:1,CreationTimestamp:2018-12-09 22:06:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec  9 22:06:25.650: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:06:25.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-p67g5" for this suite.
Dec  9 22:06:31.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:06:31.809: INFO: namespace: e2e-tests-deployment-p67g5, resource: bindings, ignored listing per whitelist
Dec  9 22:06:31.904: INFO: namespace e2e-tests-deployment-p67g5 deletion completed in 6.2434227s

• [SLOW TEST:11.665 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:06:31.904: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Dec  9 22:06:32.190: INFO: Waiting up to 5m0s for pod "var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-var-expansion-76t4f" to be "success or failure"
Dec  9 22:06:32.194: INFO: Pod "var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998757ms
Dec  9 22:06:34.200: INFO: Pod "var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009742283s
Dec  9 22:06:36.206: INFO: Pod "var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015292187s
STEP: Saw pod success
Dec  9 22:06:36.206: INFO: Pod "var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:06:36.211: INFO: Trying to get logs from node k8s-g2 pod var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 22:06:36.348: INFO: Waiting for pod var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:06:36.352: INFO: Pod var-expansion-af9fb735-fbfe-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:06:36.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-76t4f" for this suite.
Dec  9 22:06:42.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:06:42.500: INFO: namespace: e2e-tests-var-expansion-76t4f, resource: bindings, ignored listing per whitelist
Dec  9 22:06:42.639: INFO: namespace e2e-tests-var-expansion-76t4f deletion completed in 6.282251796s

• [SLOW TEST:10.735 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:06:42.639: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  9 22:06:44.891: INFO: Pod name wrapped-volume-race-b7219436-fbfe-11e8-a4b0-6e57f5092bbd: Found 1 pods out of 5
Dec  9 22:06:49.903: INFO: Pod name wrapped-volume-race-b7219436-fbfe-11e8-a4b0-6e57f5092bbd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b7219436-fbfe-11e8-a4b0-6e57f5092bbd in namespace e2e-tests-emptydir-wrapper-n6whx, will wait for the garbage collector to delete the pods
Dec  9 22:07:04.043: INFO: Deleting ReplicationController wrapped-volume-race-b7219436-fbfe-11e8-a4b0-6e57f5092bbd took: 38.02243ms
Dec  9 22:07:04.444: INFO: Terminating ReplicationController wrapped-volume-race-b7219436-fbfe-11e8-a4b0-6e57f5092bbd pods took: 400.163411ms
STEP: Creating RC which spawns configmap-volume pods
Dec  9 22:07:52.115: INFO: Pod name wrapped-volume-race-df3def89-fbfe-11e8-a4b0-6e57f5092bbd: Found 0 pods out of 5
Dec  9 22:07:57.151: INFO: Pod name wrapped-volume-race-df3def89-fbfe-11e8-a4b0-6e57f5092bbd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-df3def89-fbfe-11e8-a4b0-6e57f5092bbd in namespace e2e-tests-emptydir-wrapper-n6whx, will wait for the garbage collector to delete the pods
Dec  9 22:08:11.303: INFO: Deleting ReplicationController wrapped-volume-race-df3def89-fbfe-11e8-a4b0-6e57f5092bbd took: 67.848611ms
Dec  9 22:08:11.604: INFO: Terminating ReplicationController wrapped-volume-race-df3def89-fbfe-11e8-a4b0-6e57f5092bbd pods took: 300.167987ms
STEP: Creating RC which spawns configmap-volume pods
Dec  9 22:08:47.988: INFO: Pod name wrapped-volume-race-00897d28-fbff-11e8-a4b0-6e57f5092bbd: Found 0 pods out of 5
Dec  9 22:08:53.013: INFO: Pod name wrapped-volume-race-00897d28-fbff-11e8-a4b0-6e57f5092bbd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-00897d28-fbff-11e8-a4b0-6e57f5092bbd in namespace e2e-tests-emptydir-wrapper-n6whx, will wait for the garbage collector to delete the pods
Dec  9 22:09:07.429: INFO: Deleting ReplicationController wrapped-volume-race-00897d28-fbff-11e8-a4b0-6e57f5092bbd took: 64.00551ms
Dec  9 22:09:07.829: INFO: Terminating ReplicationController wrapped-volume-race-00897d28-fbff-11e8-a4b0-6e57f5092bbd pods took: 400.151704ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:09:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-n6whx" for this suite.
Dec  9 22:10:01.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:10:01.295: INFO: namespace: e2e-tests-emptydir-wrapper-n6whx, resource: bindings, ignored listing per whitelist
Dec  9 22:10:01.406: INFO: namespace e2e-tests-emptydir-wrapper-n6whx deletion completed in 14.290930725s

• [SLOW TEST:198.766 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:10:01.406: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-2c8375b0-fbff-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:10:01.776: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-sx52l" to be "success or failure"
Dec  9 22:10:01.812: INFO: Pod "pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.103351ms
Dec  9 22:10:03.818: INFO: Pod "pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041704678s
Dec  9 22:10:05.823: INFO: Pod "pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047238915s
STEP: Saw pod success
Dec  9 22:10:05.823: INFO: Pod "pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:10:05.828: INFO: Trying to get logs from node k8s-g1 pod pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:10:05.951: INFO: Waiting for pod pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:10:05.956: INFO: Pod pod-configmaps-2c8d2863-fbff-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:10:05.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sx52l" for this suite.
Dec  9 22:10:11.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:10:12.049: INFO: namespace: e2e-tests-configmap-sx52l, resource: bindings, ignored listing per whitelist
Dec  9 22:10:12.120: INFO: namespace e2e-tests-configmap-sx52l deletion completed in 6.157857883s

• [SLOW TEST:10.714 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:10:12.120: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:10:16.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-sx25s" for this suite.
Dec  9 22:10:56.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:10:56.647: INFO: namespace: e2e-tests-kubelet-test-sx25s, resource: bindings, ignored listing per whitelist
Dec  9 22:10:56.739: INFO: namespace e2e-tests-kubelet-test-sx25s deletion completed in 40.196583125s

• [SLOW TEST:44.619 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:10:56.739: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-4d74cebb-fbff-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:10:56.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-6h62n" to be "success or failure"
Dec  9 22:10:57.028: INFO: Pod "pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 43.583956ms
Dec  9 22:10:59.034: INFO: Pod "pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049411353s
Dec  9 22:11:01.041: INFO: Pod "pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055977887s
STEP: Saw pod success
Dec  9 22:11:01.041: INFO: Pod "pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:11:01.046: INFO: Trying to get logs from node k8s-g1 pod pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:11:01.100: INFO: Waiting for pod pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:11:01.137: INFO: Pod pod-projected-secrets-4d771873-fbff-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:11:01.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6h62n" for this suite.
Dec  9 22:11:07.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:11:07.272: INFO: namespace: e2e-tests-projected-6h62n, resource: bindings, ignored listing per whitelist
Dec  9 22:11:07.359: INFO: namespace e2e-tests-projected-6h62n deletion completed in 6.216527643s

• [SLOW TEST:10.620 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:11:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-53c52220-fbff-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:11:07.643: INFO: Waiting up to 5m0s for pod "pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-45cq6" to be "success or failure"
Dec  9 22:11:07.667: INFO: Pod "pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.745412ms
Dec  9 22:11:09.687: INFO: Pod "pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044201331s
Dec  9 22:11:11.694: INFO: Pod "pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051474172s
STEP: Saw pod success
Dec  9 22:11:11.694: INFO: Pod "pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:11:11.701: INFO: Trying to get logs from node k8s-g2 pod pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:11:11.783: INFO: Waiting for pod pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:11:11.790: INFO: Pod pod-secrets-53cd5463-fbff-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:11:11.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-45cq6" for this suite.
Dec  9 22:11:17.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:11:17.957: INFO: namespace: e2e-tests-secrets-45cq6, resource: bindings, ignored listing per whitelist
Dec  9 22:11:18.027: INFO: namespace e2e-tests-secrets-45cq6 deletion completed in 6.218732351s

• [SLOW TEST:10.668 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:11:18.027: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:11:18.363: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  9 22:11:18.422: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:18.422: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:18.422: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:18.494: INFO: Number of nodes with available pods: 0
Dec  9 22:11:18.495: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:11:19.502: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:19.502: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:19.502: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:19.508: INFO: Number of nodes with available pods: 0
Dec  9 22:11:19.508: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:11:20.500: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:20.500: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:20.500: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:20.506: INFO: Number of nodes with available pods: 0
Dec  9 22:11:20.506: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:11:21.500: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:21.500: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:21.500: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:21.506: INFO: Number of nodes with available pods: 2
Dec  9 22:11:21.506: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  9 22:11:21.567: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:21.567: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:21.571: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:21.571: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:21.571: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:22.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:22.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:22.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:22.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:22.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:23.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:23.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:23.585: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:23.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:23.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:24.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:24.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:24.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:24.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:24.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:25.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:25.579: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:25.593: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:25.593: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:25.593: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:26.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:26.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:26.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:26.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:26.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:27.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:27.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:27.585: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:27.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:27.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:28.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:28.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:28.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:28.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:28.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:29.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:29.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:29.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:29.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:29.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:30.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:30.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:30.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:30.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:30.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:31.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:31.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:31.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:31.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:31.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:32.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:32.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:32.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:32.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:32.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:33.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:33.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:33.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:33.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:33.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:34.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:34.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:34.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:34.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:34.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:35.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:35.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:35.595: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:35.595: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:35.595: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:36.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:36.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:36.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:36.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:36.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:37.603: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:37.603: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:37.611: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:37.611: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:37.611: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:38.597: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:38.597: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:38.603: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:38.603: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:38.603: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:39.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:39.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:39.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:39.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:39.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:40.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:40.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:40.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:40.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:40.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:41.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:41.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:41.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:41.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:41.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:42.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:42.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:42.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:42.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:42.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:43.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:43.579: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:43.585: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:43.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:43.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:44.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:44.579: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:44.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:44.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:44.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:45.653: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:45.653: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:45.689: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:45.689: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:45.689: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:46.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:46.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:46.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:46.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:46.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:47.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:47.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:47.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:47.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:47.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:48.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:48.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:48.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:48.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:48.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:49.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:49.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:49.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:49.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:49.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:50.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:50.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:50.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:50.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:50.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:51.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:51.579: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:51.585: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:51.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:51.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:52.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:52.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:52.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:52.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:52.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:53.594: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:53.594: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:53.599: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:53.599: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:53.599: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:54.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:54.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:54.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:54.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:54.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:54.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:55.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:55.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:55.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:55.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:55.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:55.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:56.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:56.578: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:56.578: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:56.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:56.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:56.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:57.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:57.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:57.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:57.581: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:57.581: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:57.581: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:58.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:58.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:58.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:58.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:58.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:58.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:59.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:59.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:11:59.578: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:11:59.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:59.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:11:59.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:00.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:00.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:00.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:12:00.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:00.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:00.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:01.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:01.577: INFO: Wrong image for pod: daemon-set-qgpth. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:01.577: INFO: Pod daemon-set-qgpth is not available
Dec  9 22:12:01.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:01.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:01.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:02.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:02.577: INFO: Pod daemon-set-j6dw7 is not available
Dec  9 22:12:02.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:02.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:02.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:03.603: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:03.603: INFO: Pod daemon-set-j6dw7 is not available
Dec  9 22:12:03.609: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:03.609: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:03.609: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:04.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:04.597: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:04.597: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:04.597: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:05.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:05.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:05.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:05.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:06.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:06.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:06.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:06.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:07.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:07.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:07.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:07.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:08.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:08.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:08.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:08.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:09.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:09.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:09.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:09.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:10.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:10.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:10.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:10.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:11.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:11.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:11.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:11.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:12.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:12.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:12.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:12.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:13.585: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:13.623: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:13.623: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:13.623: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:14.593: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:14.598: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:14.598: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:14.598: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:15.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:15.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:15.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:15.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:16.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:16.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:16.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:16.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:17.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:17.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:17.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:17.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:18.579: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:18.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:18.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:18.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:19.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:19.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:19.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:19.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:20.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:20.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:20.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:20.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:21.595: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:21.600: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:21.600: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:21.600: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:22.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:22.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:22.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:22.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:23.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:23.585: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:23.585: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:23.585: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:24.588: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:24.593: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:24.593: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:24.593: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:25.596: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:25.602: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:25.602: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:25.602: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:26.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:26.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:26.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:26.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:27.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:27.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:27.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:27.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:28.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:28.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:28.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:28.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:29.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:29.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:29.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:29.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:30.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:30.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:30.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:30.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:31.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:31.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:31.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:31.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:32.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:32.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:32.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:32.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:33.593: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:33.600: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:33.600: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:33.600: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:34.641: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:34.647: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:34.647: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:34.647: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:35.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:35.578: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:35.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:35.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:35.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:36.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:36.578: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:36.584: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:36.584: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:36.584: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:37.578: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:37.578: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:37.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:37.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:37.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:38.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:38.577: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:38.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:38.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:38.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:39.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:39.577: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:39.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:39.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:39.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:40.577: INFO: Wrong image for pod: daemon-set-9lvfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  9 22:12:40.577: INFO: Pod daemon-set-9lvfq is not available
Dec  9 22:12:40.582: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:40.582: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:40.582: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.578: INFO: Pod daemon-set-vbp8r is not available
Dec  9 22:12:41.583: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.583: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.583: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  9 22:12:41.587: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.587: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.587: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:41.593: INFO: Number of nodes with available pods: 1
Dec  9 22:12:41.593: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:42.599: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:42.599: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:42.599: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:42.605: INFO: Number of nodes with available pods: 1
Dec  9 22:12:42.605: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:43.598: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:43.598: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:43.598: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:43.605: INFO: Number of nodes with available pods: 1
Dec  9 22:12:43.605: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:44.599: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:44.599: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:44.599: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:44.604: INFO: Number of nodes with available pods: 1
Dec  9 22:12:44.604: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:45.617: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:45.617: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:45.617: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:45.623: INFO: Number of nodes with available pods: 1
Dec  9 22:12:45.623: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:46.598: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:46.598: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:46.598: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:46.603: INFO: Number of nodes with available pods: 1
Dec  9 22:12:46.603: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:12:47.599: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:47.599: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:47.599: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:12:47.605: INFO: Number of nodes with available pods: 2
Dec  9 22:12:47.605: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-qfx78, will wait for the garbage collector to delete the pods
Dec  9 22:12:47.706: INFO: Deleting DaemonSet.extensions daemon-set took: 28.576774ms
Dec  9 22:12:47.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.132472ms
Dec  9 22:13:01.411: INFO: Number of nodes with available pods: 0
Dec  9 22:13:01.411: INFO: Number of running nodes: 0, number of available pods: 0
Dec  9 22:13:01.452: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qfx78/daemonsets","resourceVersion":"39711"},"items":null}

Dec  9 22:13:01.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qfx78/pods","resourceVersion":"39711"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:13:01.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qfx78" for this suite.
Dec  9 22:13:09.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:13:09.654: INFO: namespace: e2e-tests-daemonsets-qfx78, resource: bindings, ignored listing per whitelist
Dec  9 22:13:09.686: INFO: namespace e2e-tests-daemonsets-qfx78 deletion completed in 8.194760852s

• [SLOW TEST:111.660 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:13:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  9 22:13:10.044: INFO: Waiting up to 5m0s for pod "downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-gxnkq" to be "success or failure"
Dec  9 22:13:10.056: INFO: Pod "downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.210374ms
Dec  9 22:13:12.061: INFO: Pod "downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01732957s
Dec  9 22:13:14.066: INFO: Pod "downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022752252s
STEP: Saw pod success
Dec  9 22:13:14.066: INFO: Pod "downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:13:14.071: INFO: Trying to get logs from node k8s-g2 pod downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 22:13:14.177: INFO: Waiting for pod downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:13:14.182: INFO: Pod downward-api-9cc19814-fbff-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:13:14.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gxnkq" for this suite.
Dec  9 22:13:20.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:13:20.351: INFO: namespace: e2e-tests-downward-api-gxnkq, resource: bindings, ignored listing per whitelist
Dec  9 22:13:20.382: INFO: namespace e2e-tests-downward-api-gxnkq deletion completed in 6.195014451s

• [SLOW TEST:10.695 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:13:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Dec  9 22:13:20.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:23.760: INFO: stderr: ""
Dec  9 22:13:23.760: INFO: stdout: "pod/pause created\n"
Dec  9 22:13:23.760: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  9 22:13:23.760: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-wn7wj" to be "running and ready"
Dec  9 22:13:23.779: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.399866ms
Dec  9 22:13:25.785: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025074997s
Dec  9 22:13:27.791: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.030872876s
Dec  9 22:13:27.791: INFO: Pod "pause" satisfied condition "running and ready"
Dec  9 22:13:27.791: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  9 22:13:27.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:27.899: INFO: stderr: ""
Dec  9 22:13:27.899: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  9 22:13:27.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pod pause -L testing-label --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:27.961: INFO: stderr: ""
Dec  9 22:13:27.961: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  9 22:13:27.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 label pods pause testing-label- --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:28.078: INFO: stderr: ""
Dec  9 22:13:28.078: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  9 22:13:28.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pod pause -L testing-label --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:28.132: INFO: stderr: ""
Dec  9 22:13:28.132: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Dec  9 22:13:28.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:28.270: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 22:13:28.270: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  9 22:13:28.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-wn7wj'
Dec  9 22:13:28.391: INFO: stderr: "No resources found.\n"
Dec  9 22:13:28.391: INFO: stdout: ""
Dec  9 22:13:28.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -l name=pause --namespace=e2e-tests-kubectl-wn7wj -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 22:13:28.445: INFO: stderr: ""
Dec  9 22:13:28.445: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:13:28.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wn7wj" for this suite.
Dec  9 22:13:34.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:13:34.641: INFO: namespace: e2e-tests-kubectl-wn7wj, resource: bindings, ignored listing per whitelist
Dec  9 22:13:34.738: INFO: namespace e2e-tests-kubectl-wn7wj deletion completed in 6.287801838s

• [SLOW TEST:14.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:13:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-7ltph
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Dec  9 22:13:35.132: INFO: Found 0 stateful pods, waiting for 3
Dec  9 22:13:45.165: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:13:45.165: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:13:45.165: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:13:45.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-7ltph ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:13:45.386: INFO: stderr: ""
Dec  9 22:13:45.386: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:13:45.386: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  9 22:13:55.444: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  9 22:14:05.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-7ltph ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:14:05.624: INFO: stderr: ""
Dec  9 22:14:05.624: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:14:05.624: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:14:15.653: INFO: Waiting for StatefulSet e2e-tests-statefulset-7ltph/ss2 to complete update
Dec  9 22:14:15.653: INFO: Waiting for Pod e2e-tests-statefulset-7ltph/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  9 22:14:15.653: INFO: Waiting for Pod e2e-tests-statefulset-7ltph/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  9 22:14:25.674: INFO: Waiting for StatefulSet e2e-tests-statefulset-7ltph/ss2 to complete update
Dec  9 22:14:25.674: INFO: Waiting for Pod e2e-tests-statefulset-7ltph/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  9 22:14:35.664: INFO: Waiting for StatefulSet e2e-tests-statefulset-7ltph/ss2 to complete update
Dec  9 22:14:35.664: INFO: Waiting for Pod e2e-tests-statefulset-7ltph/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Dec  9 22:14:45.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-7ltph ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:14:45.845: INFO: stderr: ""
Dec  9 22:14:45.845: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:14:45.845: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:14:55.890: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  9 22:15:05.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-7ltph ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:15:06.042: INFO: stderr: ""
Dec  9 22:15:06.042: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:15:06.042: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:15:26.068: INFO: Waiting for StatefulSet e2e-tests-statefulset-7ltph/ss2 to complete update
Dec  9 22:15:26.069: INFO: Waiting for Pod e2e-tests-statefulset-7ltph/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  9 22:15:36.079: INFO: Deleting all statefulset in ns e2e-tests-statefulset-7ltph
Dec  9 22:15:36.083: INFO: Scaling statefulset ss2 to 0
Dec  9 22:16:06.157: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:16:06.171: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:16:06.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-7ltph" for this suite.
Dec  9 22:16:14.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:16:14.401: INFO: namespace: e2e-tests-statefulset-7ltph, resource: bindings, ignored listing per whitelist
Dec  9 22:16:14.541: INFO: namespace e2e-tests-statefulset-7ltph deletion completed in 8.332057759s

• [SLOW TEST:159.803 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:16:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:16:14.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-2f7vz" to be "success or failure"
Dec  9 22:16:14.898: INFO: Pod "downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 31.409502ms
Dec  9 22:16:16.904: INFO: Pod "downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036579197s
Dec  9 22:16:18.909: INFO: Pod "downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042065572s
STEP: Saw pod success
Dec  9 22:16:18.909: INFO: Pod "downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:16:18.914: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:16:19.006: INFO: Waiting for pod downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:16:19.011: INFO: Pod downwardapi-volume-0ae6e8eb-fc00-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:16:19.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2f7vz" for this suite.
Dec  9 22:16:25.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:16:25.094: INFO: namespace: e2e-tests-downward-api-2f7vz, resource: bindings, ignored listing per whitelist
Dec  9 22:16:25.341: INFO: namespace e2e-tests-downward-api-2f7vz deletion completed in 6.324677002s

• [SLOW TEST:10.800 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:16:25.341: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Dec  9 22:16:25.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 cluster-info'
Dec  9 22:16:25.702: INFO: stderr: ""
Dec  9 22:16:25.702: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:16:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fcqll" for this suite.
Dec  9 22:16:31.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:16:31.821: INFO: namespace: e2e-tests-kubectl-fcqll, resource: bindings, ignored listing per whitelist
Dec  9 22:16:31.889: INFO: namespace e2e-tests-kubectl-fcqll deletion completed in 6.183179106s

• [SLOW TEST:6.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:16:31.889: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-dxvhc
Dec  9 22:16:36.363: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-dxvhc
STEP: checking the pod's current state and verifying that restartCount is present
Dec  9 22:16:36.368: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:20:37.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-dxvhc" for this suite.
Dec  9 22:20:43.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:20:43.831: INFO: namespace: e2e-tests-container-probe-dxvhc, resource: bindings, ignored listing per whitelist
Dec  9 22:20:43.922: INFO: namespace e2e-tests-container-probe-dxvhc deletion completed in 6.21563764s

• [SLOW TEST:252.033 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:20:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Dec  9 22:20:48.311: INFO: Pod pod-hostip-ab729ee5-fc00-11e8-a4b0-6e57f5092bbd has hostIP: 172.22.132.14
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:20:48.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-9b64c" for this suite.
Dec  9 22:21:10.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:21:10.375: INFO: namespace: e2e-tests-pods-9b64c, resource: bindings, ignored listing per whitelist
Dec  9 22:21:10.505: INFO: namespace e2e-tests-pods-9b64c deletion completed in 22.188489131s

• [SLOW TEST:26.582 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:21:10.505: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-b299r;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-b299r;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-b299r.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-b299r.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 127.169.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.169.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.169.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.169.127_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-b299r;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-b299r;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-b299r.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-b299r.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-b299r.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-b299r.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-b299r.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 127.169.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.169.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.169.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.169.127_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  9 22:21:32.982: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:32.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.000: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.011: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.017: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.060: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.066: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.071: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.077: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.082: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.088: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.093: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.099: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:33.131: INFO: Lookups using e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-b299r wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-b299r jessie_tcp@dns-test-service.e2e-tests-dns-b299r jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc]

Dec  9 22:21:38.138: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.145: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.156: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.168: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.218: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.224: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.229: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.234: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.240: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.246: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.252: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.257: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:38.289: INFO: Lookups using e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-b299r wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-b299r jessie_tcp@dns-test-service.e2e-tests-dns-b299r jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc]

Dec  9 22:21:43.139: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.147: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.161: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.175: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.181: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.226: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.232: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.237: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.242: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.249: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.255: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.260: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.265: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:43.298: INFO: Lookups using e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-b299r wheezy_tcp@dns-test-service.e2e-tests-dns-b299r.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-b299r jessie_tcp@dns-test-service.e2e-tests-dns-b299r jessie_udp@dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@dns-test-service.e2e-tests-dns-b299r.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc]

Dec  9 22:21:48.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:48.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc from pod e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd: the server could not find the requested resource (get pods dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd)
Dec  9 22:21:48.289: INFO: Lookups using e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd failed for: [wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-b299r.svc]

Dec  9 22:21:53.301: INFO: DNS probes using e2e-tests-dns-b299r/dns-test-bb530674-fc00-11e8-a4b0-6e57f5092bbd succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:21:53.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-b299r" for this suite.
Dec  9 22:21:59.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:21:59.835: INFO: namespace: e2e-tests-dns-b299r, resource: bindings, ignored listing per whitelist
Dec  9 22:21:59.851: INFO: namespace e2e-tests-dns-b299r deletion completed in 6.197956511s

• [SLOW TEST:49.346 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:21:59.851: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:23:00.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jmpzx" for this suite.
Dec  9 22:23:24.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:23:24.238: INFO: namespace: e2e-tests-container-probe-jmpzx, resource: bindings, ignored listing per whitelist
Dec  9 22:23:24.301: INFO: namespace e2e-tests-container-probe-jmpzx deletion completed in 24.162462375s

• [SLOW TEST:84.450 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:23:24.302: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  9 22:23:24.558: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:24.558: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:24.558: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:24.634: INFO: Number of nodes with available pods: 0
Dec  9 22:23:24.634: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:23:25.640: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:25.640: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:25.640: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:25.645: INFO: Number of nodes with available pods: 0
Dec  9 22:23:25.645: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:23:26.639: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:26.639: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:26.639: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:26.661: INFO: Number of nodes with available pods: 0
Dec  9 22:23:26.661: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:23:27.641: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.641: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.641: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.646: INFO: Number of nodes with available pods: 2
Dec  9 22:23:27.646: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  9 22:23:27.785: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.785: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.785: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:27.818: INFO: Number of nodes with available pods: 1
Dec  9 22:23:27.818: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:23:28.825: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:28.825: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:28.825: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:28.830: INFO: Number of nodes with available pods: 1
Dec  9 22:23:28.830: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:23:29.853: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:29.853: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:29.853: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:29.858: INFO: Number of nodes with available pods: 1
Dec  9 22:23:29.858: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:23:30.824: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:30.824: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:30.824: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:23:30.859: INFO: Number of nodes with available pods: 2
Dec  9 22:23:30.859: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-4jtsh, will wait for the garbage collector to delete the pods
Dec  9 22:23:30.947: INFO: Deleting DaemonSet.extensions daemon-set took: 25.649661ms
Dec  9 22:23:31.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.12961ms
Dec  9 22:24:11.453: INFO: Number of nodes with available pods: 0
Dec  9 22:24:11.453: INFO: Number of running nodes: 0, number of available pods: 0
Dec  9 22:24:11.457: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4jtsh/daemonsets","resourceVersion":"41584"},"items":null}

Dec  9 22:24:11.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4jtsh/pods","resourceVersion":"41584"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:24:11.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4jtsh" for this suite.
Dec  9 22:24:19.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:24:19.876: INFO: namespace: e2e-tests-daemonsets-4jtsh, resource: bindings, ignored listing per whitelist
Dec  9 22:24:19.879: INFO: namespace e2e-tests-daemonsets-4jtsh deletion completed in 8.399320014s

• [SLOW TEST:55.578 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:24:19.879: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:24:20.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 version --client'
Dec  9 22:24:20.120: INFO: stderr: ""
Dec  9 22:24:20.120: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Dec  9 22:24:20.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-d52fw'
Dec  9 22:24:23.334: INFO: stderr: ""
Dec  9 22:24:23.334: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  9 22:24:23.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-d52fw'
Dec  9 22:24:23.828: INFO: stderr: ""
Dec  9 22:24:23.828: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  9 22:24:24.834: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:24:24.834: INFO: Found 0 / 1
Dec  9 22:24:25.833: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:24:25.833: INFO: Found 0 / 1
Dec  9 22:24:26.834: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:24:26.834: INFO: Found 1 / 1
Dec  9 22:24:26.834: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  9 22:24:26.839: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:24:26.839: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 22:24:26.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 describe pod redis-master-hc22r --namespace=e2e-tests-kubectl-d52fw'
Dec  9 22:24:26.911: INFO: stderr: ""
Dec  9 22:24:26.911: INFO: stdout: "Name:               redis-master-hc22r\nNamespace:          e2e-tests-kubectl-d52fw\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-g1/172.22.132.13\nStart Time:         Sun, 09 Dec 2018 22:24:23 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 10.244.3.108/32\nStatus:             Running\nIP:                 10.244.3.108\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://decacd1afd14cdb2e0e1bd4922e26128067c2d624b4f6875a988427d48f48370\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 09 Dec 2018 22:24:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tw2lx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tw2lx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tw2lx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned e2e-tests-kubectl-d52fw/redis-master-hc22r to k8s-g1\n  Normal  Pulled     2s    kubelet, k8s-g1    Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-g1    Created container\n  Normal  Started    1s    kubelet, k8s-g1    Started container\n"
Dec  9 22:24:26.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 describe rc redis-master --namespace=e2e-tests-kubectl-d52fw'
Dec  9 22:24:26.986: INFO: stderr: ""
Dec  9 22:24:26.986: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-d52fw\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-hc22r\n"
Dec  9 22:24:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 describe service redis-master --namespace=e2e-tests-kubectl-d52fw'
Dec  9 22:24:27.056: INFO: stderr: ""
Dec  9 22:24:27.056: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-d52fw\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.107.111.210\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.108:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  9 22:24:27.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 describe node k8s-g1'
Dec  9 22:24:27.188: INFO: stderr: ""
Dec  9 22:24:27.188: INFO: stdout: "Name:               k8s-g1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=k8s-g1\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.22.132.13/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 09 Dec 2018 16:52:45 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------    -----------------                 ------------------                ------                       -------\n  MemoryPressure   False     Sun, 09 Dec 2018 22:24:17 +0000   Sun, 09 Dec 2018 17:26:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False     Sun, 09 Dec 2018 22:24:17 +0000   Sun, 09 Dec 2018 17:26:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False     Sun, 09 Dec 2018 22:24:17 +0000   Sun, 09 Dec 2018 17:26:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True      Sun, 09 Dec 2018 22:24:17 +0000   Sun, 09 Dec 2018 17:26:12 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\n  OutOfDisk        Unknown   Sun, 09 Dec 2018 16:52:45 +0000   Sun, 09 Dec 2018 17:25:35 +0000   NodeStatusNeverUpdated       Kubelet never posted node status.\nAddresses:\n  InternalIP:  172.22.132.13\n  Hostname:    k8s-g1\nCapacity:\n cpu:                4\n ephemeral-storage:  961302540Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16365776Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  885936419398\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16263376Ki\n pods:               110\nSystem Info:\n Machine ID:                 5d9197e8949045c890fc367ea7126bc2\n System UUID:                00000000-0000-0000-0000-448A5BA4BD34\n Boot ID:                    4d4dead3-acfb-4455-b79c-c3700a42cd69\n Kernel Version:             4.4.0-139-generic\n OS Image:                   Ubuntu 16.04.5 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.3.0\n Kubelet Version:            v1.13.0\n Kube-Proxy Version:         v1.13.0\nPodCIDR:                     10.244.3.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    ssh-server-75fc845b9d-h6g6b                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h12m\n  e2e-tests-kubectl-d52fw    redis-master-hc22r                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  heptio-sonobuoy            sonobuoy-e2e-job-1185eb09e2a5466d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-8kkg6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                calico-node-xzzvt                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         5h31m\n  kube-system                coredns-5569467fdf-x57ml                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     5h1m\n  kube-system                kube-proxy-vv5ch                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                350m (8%)  0 (0%)\n  memory             70Mi (0%)  170Mi (1%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:              <none>\n"
Dec  9 22:24:27.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 describe namespace e2e-tests-kubectl-d52fw'
Dec  9 22:24:27.299: INFO: stderr: ""
Dec  9 22:24:27.299: INFO: stdout: "Name:         e2e-tests-kubectl-d52fw\nLabels:       e2e-framework=kubectl\n              e2e-run=9698c90c-fbfd-11e8-a4b0-6e57f5092bbd\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:24:27.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d52fw" for this suite.
Dec  9 22:24:51.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:24:51.474: INFO: namespace: e2e-tests-kubectl-d52fw, resource: bindings, ignored listing per whitelist
Dec  9 22:24:51.498: INFO: namespace e2e-tests-kubectl-d52fw deletion completed in 24.172647618s

• [SLOW TEST:31.619 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:24:51.498: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-3f122bf1-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:24:51.863: INFO: Waiting up to 5m0s for pod "pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-zs58j" to be "success or failure"
Dec  9 22:24:51.875: INFO: Pod "pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.825701ms
Dec  9 22:24:53.880: INFO: Pod "pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017016287s
Dec  9 22:24:55.893: INFO: Pod "pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03037868s
STEP: Saw pod success
Dec  9 22:24:55.894: INFO: Pod "pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:24:55.900: INFO: Trying to get logs from node k8s-g2 pod pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:24:56.040: INFO: Waiting for pod pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:24:56.046: INFO: Pod pod-secrets-3f17246c-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:24:56.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zs58j" for this suite.
Dec  9 22:25:02.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:25:02.187: INFO: namespace: e2e-tests-secrets-zs58j, resource: bindings, ignored listing per whitelist
Dec  9 22:25:02.229: INFO: namespace e2e-tests-secrets-zs58j deletion completed in 6.176904266s

• [SLOW TEST:10.731 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:25:02.229: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  9 22:25:10.679: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:10.685: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:12.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:12.692: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:14.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:14.692: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:16.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:16.691: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:18.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:18.691: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:20.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:20.690: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 22:25:22.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 22:25:22.691: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:25:22.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-tg2b5" for this suite.
Dec  9 22:25:44.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:25:44.833: INFO: namespace: e2e-tests-container-lifecycle-hook-tg2b5, resource: bindings, ignored listing per whitelist
Dec  9 22:25:45.013: INFO: namespace e2e-tests-container-lifecycle-hook-tg2b5 deletion completed in 22.304570551s

• [SLOW TEST:42.783 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:25:45.013: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  9 22:25:45.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-wbkpg'
Dec  9 22:25:45.573: INFO: stderr: ""
Dec  9 22:25:45.573: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  9 22:25:46.579: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:25:46.579: INFO: Found 0 / 1
Dec  9 22:25:47.612: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:25:47.612: INFO: Found 0 / 1
Dec  9 22:25:48.580: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:25:48.580: INFO: Found 1 / 1
Dec  9 22:25:48.580: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  9 22:25:48.586: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:25:48.586: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 22:25:48.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 patch pod redis-master-244cm --namespace=e2e-tests-kubectl-wbkpg -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  9 22:25:48.686: INFO: stderr: ""
Dec  9 22:25:48.686: INFO: stdout: "pod/redis-master-244cm patched\n"
STEP: checking annotations
Dec  9 22:25:48.692: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:25:48.692: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:25:48.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wbkpg" for this suite.
Dec  9 22:26:12.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:26:12.853: INFO: namespace: e2e-tests-kubectl-wbkpg, resource: bindings, ignored listing per whitelist
Dec  9 22:26:12.948: INFO: namespace e2e-tests-kubectl-wbkpg deletion completed in 24.251861803s

• [SLOW TEST:27.936 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:26:12.948: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-6f9d0e18-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:26:13.376: INFO: Waiting up to 5m0s for pod "pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-jwqfq" to be "success or failure"
Dec  9 22:26:13.385: INFO: Pod "pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996583ms
Dec  9 22:26:15.390: INFO: Pod "pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014453526s
Dec  9 22:26:17.397: INFO: Pod "pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020762952s
STEP: Saw pod success
Dec  9 22:26:17.397: INFO: Pod "pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:26:17.402: INFO: Trying to get logs from node k8s-g2 pod pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:26:17.494: INFO: Waiting for pod pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:26:17.498: INFO: Pod pod-secrets-6fac370c-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:26:17.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jwqfq" for this suite.
Dec  9 22:26:23.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:26:23.568: INFO: namespace: e2e-tests-secrets-jwqfq, resource: bindings, ignored listing per whitelist
Dec  9 22:26:23.672: INFO: namespace e2e-tests-secrets-jwqfq deletion completed in 6.160457777s

• [SLOW TEST:10.724 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:26:23.672: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  9 22:26:33.055: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:26:33.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-kplnj" for this suite.
Dec  9 22:26:57.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:26:57.439: INFO: namespace: e2e-tests-replicaset-kplnj, resource: bindings, ignored listing per whitelist
Dec  9 22:26:57.498: INFO: namespace e2e-tests-replicaset-kplnj deletion completed in 24.397897152s

• [SLOW TEST:33.825 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:26:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-8a20b655-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:26:57.868: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-dpdhl" to be "success or failure"
Dec  9 22:26:57.895: INFO: Pod "pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.645972ms
Dec  9 22:26:59.900: INFO: Pod "pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032145468s
Dec  9 22:27:01.906: INFO: Pod "pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038110008s
STEP: Saw pod success
Dec  9 22:27:01.906: INFO: Pod "pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:27:01.911: INFO: Trying to get logs from node k8s-g1 pod pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:27:02.024: INFO: Waiting for pod pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:27:02.030: INFO: Pod pod-projected-secrets-8a265bd4-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:27:02.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dpdhl" for this suite.
Dec  9 22:27:08.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:27:08.101: INFO: namespace: e2e-tests-projected-dpdhl, resource: bindings, ignored listing per whitelist
Dec  9 22:27:08.241: INFO: namespace e2e-tests-projected-dpdhl deletion completed in 6.205913682s

• [SLOW TEST:10.743 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:27:08.241: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:27:08.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-pm65k" to be "success or failure"
Dec  9 22:27:08.555: INFO: Pod "downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.725677ms
Dec  9 22:27:10.561: INFO: Pod "downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04235293s
Dec  9 22:27:12.567: INFO: Pod "downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048669735s
STEP: Saw pod success
Dec  9 22:27:12.567: INFO: Pod "downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:27:12.573: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:27:12.659: INFO: Waiting for pod downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:27:12.663: INFO: Pod downwardapi-volume-908a2439-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:27:12.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pm65k" for this suite.
Dec  9 22:27:18.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:27:18.966: INFO: namespace: e2e-tests-projected-pm65k, resource: bindings, ignored listing per whitelist
Dec  9 22:27:18.969: INFO: namespace e2e-tests-projected-pm65k deletion completed in 6.275580015s

• [SLOW TEST:10.728 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:27:18.969: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-96fccd90-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:27:19.360: INFO: Waiting up to 5m0s for pod "pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-2qjhj" to be "success or failure"
Dec  9 22:27:19.368: INFO: Pod "pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050114ms
Dec  9 22:27:21.373: INFO: Pod "pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013450306s
Dec  9 22:27:23.378: INFO: Pod "pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018425313s
STEP: Saw pod success
Dec  9 22:27:23.378: INFO: Pod "pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:27:23.383: INFO: Trying to get logs from node k8s-g1 pod pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd container secret-env-test: <nil>
STEP: delete the pod
Dec  9 22:27:23.475: INFO: Waiting for pod pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:27:23.481: INFO: Pod pod-secrets-9701e1f9-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:27:23.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2qjhj" for this suite.
Dec  9 22:27:29.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:27:29.707: INFO: namespace: e2e-tests-secrets-2qjhj, resource: bindings, ignored listing per whitelist
Dec  9 22:27:29.794: INFO: namespace e2e-tests-secrets-2qjhj deletion completed in 6.308201293s

• [SLOW TEST:10.825 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:27:29.794: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-v5z5l
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-v5z5l
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-v5z5l
Dec  9 22:27:30.221: INFO: Found 0 stateful pods, waiting for 1
Dec  9 22:27:40.227: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  9 22:27:40.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:27:40.388: INFO: stderr: ""
Dec  9 22:27:40.388: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:27:40.388: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:27:40.393: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  9 22:27:50.399: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:27:50.399: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:27:50.453: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Dec  9 22:27:50.453: INFO: ss-0  k8s-g2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  }]
Dec  9 22:27:50.453: INFO: 
Dec  9 22:27:50.453: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  9 22:27:51.459: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.977967852s
Dec  9 22:27:52.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972092737s
Dec  9 22:27:53.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965535139s
Dec  9 22:27:54.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950226819s
Dec  9 22:27:55.493: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944219534s
Dec  9 22:27:56.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.93861683s
Dec  9 22:27:57.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.932307392s
Dec  9 22:27:58.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925445387s
Dec  9 22:27:59.566: INFO: Verifying statefulset ss doesn't scale past 3 for another 898.527503ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-v5z5l
Dec  9 22:28:00.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:28:00.704: INFO: stderr: ""
Dec  9 22:28:00.705: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:28:00.705: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:28:00.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:28:00.838: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  9 22:28:00.839: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:28:00.839: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:28:00.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:28:00.954: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  9 22:28:00.954: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:28:00.954: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:28:00.959: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  9 22:28:10.965: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:28:10.965: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:28:10.965: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  9 22:28:10.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:28:11.085: INFO: stderr: ""
Dec  9 22:28:11.085: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:28:11.085: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:28:11.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:28:11.283: INFO: stderr: ""
Dec  9 22:28:11.283: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:28:11.283: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:28:11.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-v5z5l ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:28:11.484: INFO: stderr: ""
Dec  9 22:28:11.484: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:28:11.484: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:28:11.484: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:28:11.506: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  9 22:28:21.516: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:28:21.516: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:28:21.516: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:28:21.544: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Dec  9 22:28:21.544: INFO: ss-0  k8s-g2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  }]
Dec  9 22:28:21.544: INFO: ss-1  k8s-g1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:21.544: INFO: ss-2  k8s-g1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:21.544: INFO: 
Dec  9 22:28:21.544: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  9 22:28:22.551: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Dec  9 22:28:22.552: INFO: ss-0  k8s-g2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  }]
Dec  9 22:28:22.552: INFO: ss-1  k8s-g1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:22.552: INFO: ss-2  k8s-g1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:22.552: INFO: 
Dec  9 22:28:22.552: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  9 22:28:23.559: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Dec  9 22:28:23.559: INFO: ss-0  k8s-g2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:30 +0000 UTC  }]
Dec  9 22:28:23.559: INFO: ss-1  k8s-g1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:23.559: INFO: ss-2  k8s-g1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:28:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:27:50 +0000 UTC  }]
Dec  9 22:28:23.559: INFO: 
Dec  9 22:28:23.559: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  9 22:28:24.565: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.976584652s
Dec  9 22:28:25.572: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.970189247s
Dec  9 22:28:26.577: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963923871s
Dec  9 22:28:27.583: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.958137994s
Dec  9 22:28:28.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.952393412s
Dec  9 22:28:29.596: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.945173501s
Dec  9 22:28:30.601: INFO: Verifying statefulset ss doesn't scale past 0 for another 939.937005ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-v5z5l
Dec  9 22:28:31.608: INFO: Scaling statefulset ss to 0
Dec  9 22:28:31.622: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  9 22:28:31.625: INFO: Deleting all statefulset in ns e2e-tests-statefulset-v5z5l
Dec  9 22:28:31.629: INFO: Scaling statefulset ss to 0
Dec  9 22:28:31.641: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:28:31.645: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:28:31.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-v5z5l" for this suite.
Dec  9 22:28:39.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:28:39.966: INFO: namespace: e2e-tests-statefulset-v5z5l, resource: bindings, ignored listing per whitelist
Dec  9 22:28:40.142: INFO: namespace e2e-tests-statefulset-v5z5l deletion completed in 8.451347692s

• [SLOW TEST:70.348 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:28:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:28:40.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-xqh2g" to be "success or failure"
Dec  9 22:28:40.561: INFO: Pod "downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 29.185563ms
Dec  9 22:28:42.567: INFO: Pod "downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035912585s
Dec  9 22:28:44.573: INFO: Pod "downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04176092s
STEP: Saw pod success
Dec  9 22:28:44.573: INFO: Pod "downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:28:44.578: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:28:44.703: INFO: Waiting for pod downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:28:44.709: INFO: Pod downwardapi-volume-c758fbe5-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:28:44.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xqh2g" for this suite.
Dec  9 22:28:50.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:28:50.887: INFO: namespace: e2e-tests-projected-xqh2g, resource: bindings, ignored listing per whitelist
Dec  9 22:28:50.944: INFO: namespace e2e-tests-projected-xqh2g deletion completed in 6.223199573s

• [SLOW TEST:10.801 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:28:50.944: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-cdbeaf87-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:28:51.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-rj2c2" to be "success or failure"
Dec  9 22:28:51.339: INFO: Pod "pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 71.37259ms
Dec  9 22:28:53.345: INFO: Pod "pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077021171s
Dec  9 22:28:55.351: INFO: Pod "pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082855446s
STEP: Saw pod success
Dec  9 22:28:55.351: INFO: Pod "pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:28:55.356: INFO: Trying to get logs from node k8s-g2 pod pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:28:55.481: INFO: Waiting for pod pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:28:55.486: INFO: Pod pod-projected-secrets-cdc60575-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:28:55.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rj2c2" for this suite.
Dec  9 22:29:01.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:29:01.670: INFO: namespace: e2e-tests-projected-rj2c2, resource: bindings, ignored listing per whitelist
Dec  9 22:29:01.703: INFO: namespace e2e-tests-projected-rj2c2 deletion completed in 6.191244974s

• [SLOW TEST:10.760 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:29:01.704: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d423becd-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:29:01.978: INFO: Waiting up to 5m0s for pod "pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-sfmwf" to be "success or failure"
Dec  9 22:29:02.080: INFO: Pod "pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 102.247227ms
Dec  9 22:29:04.085: INFO: Pod "pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107464945s
Dec  9 22:29:06.090: INFO: Pod "pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112738153s
STEP: Saw pod success
Dec  9 22:29:06.090: INFO: Pod "pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:29:06.096: INFO: Trying to get logs from node k8s-g1 pod pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:29:06.165: INFO: Waiting for pod pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:29:06.171: INFO: Pod pod-secrets-d42bd62c-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:29:06.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sfmwf" for this suite.
Dec  9 22:29:12.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:29:12.291: INFO: namespace: e2e-tests-secrets-sfmwf, resource: bindings, ignored listing per whitelist
Dec  9 22:29:12.385: INFO: namespace e2e-tests-secrets-sfmwf deletion completed in 6.208511903s

• [SLOW TEST:10.681 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:29:12.385: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-da7d7b2e-fc01-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:29:12.850: INFO: Waiting up to 5m0s for pod "pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-xsjn2" to be "success or failure"
Dec  9 22:29:12.904: INFO: Pod "pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 53.90993ms
Dec  9 22:29:14.909: INFO: Pod "pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059022389s
Dec  9 22:29:16.915: INFO: Pod "pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064518211s
STEP: Saw pod success
Dec  9 22:29:16.915: INFO: Pod "pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:29:16.920: INFO: Trying to get logs from node k8s-g2 pod pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:29:16.975: INFO: Waiting for pod pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:29:16.980: INFO: Pod pod-secrets-daa2e569-fc01-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:29:16.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xsjn2" for this suite.
Dec  9 22:29:23.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:29:23.157: INFO: namespace: e2e-tests-secrets-xsjn2, resource: bindings, ignored listing per whitelist
Dec  9 22:29:23.231: INFO: namespace e2e-tests-secrets-xsjn2 deletion completed in 6.245955209s
STEP: Destroying namespace "e2e-tests-secret-namespace-hfldg" for this suite.
Dec  9 22:29:29.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:29:29.413: INFO: namespace: e2e-tests-secret-namespace-hfldg, resource: bindings, ignored listing per whitelist
Dec  9 22:29:29.504: INFO: namespace e2e-tests-secret-namespace-hfldg deletion completed in 6.272969522s

• [SLOW TEST:17.119 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:29:29.504: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:29:29.796: INFO: (0) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 8.639253ms)
Dec  9 22:29:29.802: INFO: (1) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.596614ms)
Dec  9 22:29:29.807: INFO: (2) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.547053ms)
Dec  9 22:29:29.813: INFO: (3) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.489719ms)
Dec  9 22:29:29.818: INFO: (4) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.612874ms)
Dec  9 22:29:29.824: INFO: (5) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.356102ms)
Dec  9 22:29:29.829: INFO: (6) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.619157ms)
Dec  9 22:29:29.835: INFO: (7) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.507585ms)
Dec  9 22:29:29.840: INFO: (8) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.598281ms)
Dec  9 22:29:29.846: INFO: (9) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.513885ms)
Dec  9 22:29:29.851: INFO: (10) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.475302ms)
Dec  9 22:29:29.857: INFO: (11) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.480602ms)
Dec  9 22:29:29.862: INFO: (12) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.494457ms)
Dec  9 22:29:29.868: INFO: (13) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.623339ms)
Dec  9 22:29:29.874: INFO: (14) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.528553ms)
Dec  9 22:29:29.879: INFO: (15) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.324216ms)
Dec  9 22:29:29.908: INFO: (16) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 29.216845ms)
Dec  9 22:29:29.914: INFO: (17) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.629518ms)
Dec  9 22:29:29.920: INFO: (18) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.746652ms)
Dec  9 22:29:29.925: INFO: (19) /api/v1/nodes/k8s-g1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.513552ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:29:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-gwpc5" for this suite.
Dec  9 22:29:35.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:29:36.098: INFO: namespace: e2e-tests-proxy-gwpc5, resource: bindings, ignored listing per whitelist
Dec  9 22:29:36.101: INFO: namespace e2e-tests-proxy-gwpc5 deletion completed in 6.171873837s

• [SLOW TEST:6.597 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:29:36.101: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-p6bnm
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-p6bnm
STEP: Deleting pre-stop pod
Dec  9 22:29:47.737: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:29:47.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-p6bnm" for this suite.
Dec  9 22:30:27.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:30:27.904: INFO: namespace: e2e-tests-prestop-p6bnm, resource: bindings, ignored listing per whitelist
Dec  9 22:30:28.012: INFO: namespace e2e-tests-prestop-p6bnm deletion completed in 40.195984417s

• [SLOW TEST:51.911 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:30:28.012: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-07951f32-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:30:28.252: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-jh74s" to be "success or failure"
Dec  9 22:30:28.327: INFO: Pod "pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 74.794139ms
Dec  9 22:30:30.334: INFO: Pod "pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081676311s
Dec  9 22:30:32.339: INFO: Pod "pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087283526s
STEP: Saw pod success
Dec  9 22:30:32.339: INFO: Pod "pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:30:32.344: INFO: Trying to get logs from node k8s-g1 pod pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:30:32.424: INFO: Waiting for pod pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:30:32.429: INFO: Pod pod-projected-configmaps-07980406-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:30:32.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jh74s" for this suite.
Dec  9 22:30:38.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:30:38.626: INFO: namespace: e2e-tests-projected-jh74s, resource: bindings, ignored listing per whitelist
Dec  9 22:30:38.653: INFO: namespace e2e-tests-projected-jh74s deletion completed in 6.218515539s

• [SLOW TEST:10.641 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:30:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:30:38.861: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:30:42.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wrhcp" for this suite.
Dec  9 22:31:35.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:31:35.185: INFO: namespace: e2e-tests-pods-wrhcp, resource: bindings, ignored listing per whitelist
Dec  9 22:31:35.193: INFO: namespace e2e-tests-pods-wrhcp deletion completed in 52.223997087s

• [SLOW TEST:56.540 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:31:35.193: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-2fa5c0c2-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:31:35.516: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-hpmwn" to be "success or failure"
Dec  9 22:31:35.544: INFO: Pod "pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 27.726214ms
Dec  9 22:31:37.549: INFO: Pod "pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033064887s
Dec  9 22:31:39.555: INFO: Pod "pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039300342s
STEP: Saw pod success
Dec  9 22:31:39.555: INFO: Pod "pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:31:39.560: INFO: Trying to get logs from node k8s-g1 pod pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:31:39.718: INFO: Waiting for pod pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:31:39.724: INFO: Pod pod-configmaps-2fa9c41b-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:31:39.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hpmwn" for this suite.
Dec  9 22:31:45.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:31:45.812: INFO: namespace: e2e-tests-configmap-hpmwn, resource: bindings, ignored listing per whitelist
Dec  9 22:31:45.896: INFO: namespace e2e-tests-configmap-hpmwn deletion completed in 6.166691286s

• [SLOW TEST:10.703 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:31:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-3604f146-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 22:31:46.224: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-dbgdf" to be "success or failure"
Dec  9 22:31:46.278: INFO: Pod "pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 53.89263ms
Dec  9 22:31:48.299: INFO: Pod "pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07547232s
Dec  9 22:31:50.306: INFO: Pod "pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082040748s
STEP: Saw pod success
Dec  9 22:31:50.306: INFO: Pod "pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:31:50.312: INFO: Trying to get logs from node k8s-g2 pod pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 22:31:50.419: INFO: Waiting for pod pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:31:50.425: INFO: Pod pod-projected-secrets-36081348-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:31:50.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dbgdf" for this suite.
Dec  9 22:31:56.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:31:56.546: INFO: namespace: e2e-tests-projected-dbgdf, resource: bindings, ignored listing per whitelist
Dec  9 22:31:56.679: INFO: namespace e2e-tests-projected-dbgdf deletion completed in 6.224947262s

• [SLOW TEST:10.782 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:31:56.679: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:32:06.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-5556v" for this suite.
Dec  9 22:32:30.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:32:30.217: INFO: namespace: e2e-tests-replication-controller-5556v, resource: bindings, ignored listing per whitelist
Dec  9 22:32:30.269: INFO: namespace e2e-tests-replication-controller-5556v deletion completed in 24.257305792s

• [SLOW TEST:33.590 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:32:30.269: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-507cd6aa-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:32:30.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-j7mbk" to be "success or failure"
Dec  9 22:32:30.633: INFO: Pod "pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.350508ms
Dec  9 22:32:32.639: INFO: Pod "pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013797554s
Dec  9 22:32:34.645: INFO: Pod "pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019519318s
STEP: Saw pod success
Dec  9 22:32:34.645: INFO: Pod "pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:32:34.649: INFO: Trying to get logs from node k8s-g2 pod pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:32:34.780: INFO: Waiting for pod pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:32:34.785: INFO: Pod pod-projected-configmaps-50874d54-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:32:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j7mbk" for this suite.
Dec  9 22:32:40.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:32:40.875: INFO: namespace: e2e-tests-projected-j7mbk, resource: bindings, ignored listing per whitelist
Dec  9 22:32:41.046: INFO: namespace e2e-tests-projected-j7mbk deletion completed in 6.23510221s

• [SLOW TEST:10.777 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:32:41.046: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  9 22:32:45.965: INFO: Successfully updated pod "pod-update-56e0f935-fc02-11e8-a4b0-6e57f5092bbd"
STEP: verifying the updated pod is in kubernetes
Dec  9 22:32:46.019: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:32:46.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6tlsl" for this suite.
Dec  9 22:33:10.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:33:10.277: INFO: namespace: e2e-tests-pods-6tlsl, resource: bindings, ignored listing per whitelist
Dec  9 22:33:10.320: INFO: namespace e2e-tests-pods-6tlsl deletion completed in 24.266367772s

• [SLOW TEST:29.274 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:33:10.320: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:33:10.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-s94r2" to be "success or failure"
Dec  9 22:33:10.620: INFO: Pod "downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219653ms
Dec  9 22:33:12.627: INFO: Pod "downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010884635s
Dec  9 22:33:14.634: INFO: Pod "downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017953579s
STEP: Saw pod success
Dec  9 22:33:14.634: INFO: Pod "downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:33:14.640: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:33:14.738: INFO: Waiting for pod downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:33:14.743: INFO: Pod downwardapi-volume-6859d1f6-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:33:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s94r2" for this suite.
Dec  9 22:33:20.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:33:20.926: INFO: namespace: e2e-tests-projected-s94r2, resource: bindings, ignored listing per whitelist
Dec  9 22:33:20.953: INFO: namespace e2e-tests-projected-s94r2 deletion completed in 6.203216615s

• [SLOW TEST:10.632 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:33:20.953: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-6ebcdb14-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating configMap with name cm-test-opt-upd-6ebcdb53-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6ebcdb14-fc02-11e8-a4b0-6e57f5092bbd
STEP: Updating configmap cm-test-opt-upd-6ebcdb53-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating configMap with name cm-test-opt-create-6ebcdb6c-fc02-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:33:27.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fbzr9" for this suite.
Dec  9 22:33:51.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:33:51.772: INFO: namespace: e2e-tests-configmap-fbzr9, resource: bindings, ignored listing per whitelist
Dec  9 22:33:51.795: INFO: namespace e2e-tests-configmap-fbzr9 deletion completed in 24.175738759s

• [SLOW TEST:30.843 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:33:51.795: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Dec  9 22:33:52.327: INFO: Waiting up to 5m0s for pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-containers-28cxq" to be "success or failure"
Dec  9 22:33:52.410: INFO: Pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 82.650249ms
Dec  9 22:33:54.415: INFO: Pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088460595s
Dec  9 22:33:56.421: INFO: Pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093864928s
Dec  9 22:33:58.429: INFO: Pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101749369s
STEP: Saw pod success
Dec  9 22:33:58.429: INFO: Pod "client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:33:58.436: INFO: Trying to get logs from node k8s-g2 pod client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:33:58.526: INFO: Waiting for pod client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:33:58.532: INFO: Pod client-containers-81340a6d-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:33:58.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-28cxq" for this suite.
Dec  9 22:34:04.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:34:04.609: INFO: namespace: e2e-tests-containers-28cxq, resource: bindings, ignored listing per whitelist
Dec  9 22:34:04.729: INFO: namespace e2e-tests-containers-28cxq deletion completed in 6.191827478s

• [SLOW TEST:12.934 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:34:04.729: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Dec  9 22:34:04.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 api-versions'
Dec  9 22:34:05.112: INFO: stderr: ""
Dec  9 22:34:05.112: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:34:05.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w9s98" for this suite.
Dec  9 22:34:11.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:34:11.186: INFO: namespace: e2e-tests-kubectl-w9s98, resource: bindings, ignored listing per whitelist
Dec  9 22:34:11.353: INFO: namespace e2e-tests-kubectl-w9s98 deletion completed in 6.236084333s

• [SLOW TEST:6.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:34:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  9 22:34:11.670: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:11.670: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:11.670: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:11.704: INFO: Number of nodes with available pods: 0
Dec  9 22:34:11.704: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:34:12.709: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:12.709: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:12.709: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:12.714: INFO: Number of nodes with available pods: 0
Dec  9 22:34:12.714: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:34:13.710: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:13.710: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:13.710: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:13.715: INFO: Number of nodes with available pods: 0
Dec  9 22:34:13.715: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 22:34:14.711: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.711: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.711: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.717: INFO: Number of nodes with available pods: 2
Dec  9 22:34:14.717: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  9 22:34:14.776: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.776: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.776: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:14.781: INFO: Number of nodes with available pods: 1
Dec  9 22:34:14.781: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:15.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:15.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:15.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:15.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:15.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:16.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:16.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:16.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:16.794: INFO: Number of nodes with available pods: 1
Dec  9 22:34:16.794: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:17.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:17.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:17.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:17.791: INFO: Number of nodes with available pods: 1
Dec  9 22:34:17.791: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:18.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:18.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:18.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:18.794: INFO: Number of nodes with available pods: 1
Dec  9 22:34:18.794: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:19.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:19.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:19.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:19.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:19.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:20.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:20.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:20.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:20.802: INFO: Number of nodes with available pods: 1
Dec  9 22:34:20.802: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:21.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:21.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:21.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:21.819: INFO: Number of nodes with available pods: 1
Dec  9 22:34:21.819: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:22.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:22.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:22.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:22.791: INFO: Number of nodes with available pods: 1
Dec  9 22:34:22.791: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:23.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:23.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:23.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:23.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:23.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:24.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:24.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:24.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:24.794: INFO: Number of nodes with available pods: 1
Dec  9 22:34:24.794: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:25.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:25.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:25.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:25.794: INFO: Number of nodes with available pods: 1
Dec  9 22:34:25.794: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:26.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:26.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:26.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:26.794: INFO: Number of nodes with available pods: 1
Dec  9 22:34:26.794: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:27.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:27.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:27.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:27.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:27.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:28.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:28.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:28.789: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:28.795: INFO: Number of nodes with available pods: 1
Dec  9 22:34:28.795: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:29.806: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:29.806: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:29.806: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:29.812: INFO: Number of nodes with available pods: 1
Dec  9 22:34:29.812: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:30.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:30.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:30.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:30.795: INFO: Number of nodes with available pods: 1
Dec  9 22:34:30.795: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:31.808: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:31.808: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:31.808: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:31.815: INFO: Number of nodes with available pods: 1
Dec  9 22:34:31.815: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:32.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:32.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:32.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:32.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:32.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:33.789: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:33.789: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:33.789: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:33.795: INFO: Number of nodes with available pods: 1
Dec  9 22:34:33.795: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:34.788: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:34.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:34.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:34.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:34.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:35.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:35.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:35.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:35.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:35.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:36.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:36.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:36.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:36.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:36.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:37.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:37.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:37.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:37.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:37.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:38.794: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:38.794: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:38.794: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:38.800: INFO: Number of nodes with available pods: 1
Dec  9 22:34:38.800: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:39.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:39.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:39.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:39.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:39.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:40.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:40.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:40.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:40.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:40.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:41.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:41.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:41.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:41.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:41.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:42.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:42.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:42.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:42.791: INFO: Number of nodes with available pods: 1
Dec  9 22:34:42.791: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:43.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:43.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:43.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:43.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:43.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:44.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:44.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:44.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:44.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:44.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:45.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:45.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:45.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:45.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:45.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:46.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:46.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:46.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:46.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:46.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:47.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:47.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:47.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:47.791: INFO: Number of nodes with available pods: 1
Dec  9 22:34:47.791: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:48.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:48.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:48.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:48.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:48.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:49.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:49.788: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:49.788: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:49.793: INFO: Number of nodes with available pods: 1
Dec  9 22:34:49.793: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:50.792: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:50.792: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:50.792: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:50.799: INFO: Number of nodes with available pods: 1
Dec  9 22:34:50.799: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:51.787: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:51.787: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:51.787: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:51.792: INFO: Number of nodes with available pods: 1
Dec  9 22:34:51.792: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:52.833: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:52.833: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:52.833: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:52.856: INFO: Number of nodes with available pods: 1
Dec  9 22:34:52.856: INFO: Node k8s-g2 is running more than one daemon pod
Dec  9 22:34:53.786: INFO: DaemonSet pods can't tolerate node k8s-m1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:53.786: INFO: DaemonSet pods can't tolerate node k8s-m2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:53.786: INFO: DaemonSet pods can't tolerate node k8s-m3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 22:34:53.809: INFO: Number of nodes with available pods: 2
Dec  9 22:34:53.809: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tvcm6, will wait for the garbage collector to delete the pods
Dec  9 22:34:53.892: INFO: Deleting DaemonSet.extensions daemon-set took: 25.286424ms
Dec  9 22:34:54.092: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.147864ms
Dec  9 22:35:27.998: INFO: Number of nodes with available pods: 0
Dec  9 22:35:27.998: INFO: Number of running nodes: 0, number of available pods: 0
Dec  9 22:35:28.001: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tvcm6/daemonsets","resourceVersion":"43809"},"items":null}

Dec  9 22:35:28.006: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tvcm6/pods","resourceVersion":"43809"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:35:28.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tvcm6" for this suite.
Dec  9 22:35:36.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:35:36.078: INFO: namespace: e2e-tests-daemonsets-tvcm6, resource: bindings, ignored listing per whitelist
Dec  9 22:35:36.296: INFO: namespace e2e-tests-daemonsets-tvcm6 deletion completed in 8.265461894s

• [SLOW TEST:84.942 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:35:36.296: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1209 22:35:42.554247      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 22:35:42.554: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:35:42.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6sfz6" for this suite.
Dec  9 22:35:50.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:35:50.711: INFO: namespace: e2e-tests-gc-6sfz6, resource: bindings, ignored listing per whitelist
Dec  9 22:35:50.819: INFO: namespace e2e-tests-gc-6sfz6 deletion completed in 8.217521486s

• [SLOW TEST:14.523 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:35:50.819: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  9 22:35:51.021: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 22:35:51.028: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 22:35:51.034: INFO: 
Logging pods the kubelet thinks is on node k8s-g1 before test
Dec  9 22:35:51.047: INFO: sonobuoy-e2e-job-1185eb09e2a5466d from heptio-sonobuoy started at 2018-12-09 21:58:15 +0000 UTC (2 container statuses recorded)
Dec  9 22:35:51.047: INFO: 	Container e2e ready: true, restart count 0
Dec  9 22:35:51.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 22:35:51.047: INFO: ssh-server-75fc845b9d-h6g6b from default started at 2018-12-09 17:11:36 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.047: INFO: 	Container ssh-server ready: true, restart count 1
Dec  9 22:35:51.047: INFO: coredns-5569467fdf-x57ml from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.047: INFO: 	Container coredns ready: true, restart count 1
Dec  9 22:35:51.047: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-8kkg6 from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 22:35:51.047: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  9 22:35:51.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 22:35:51.048: INFO: kube-proxy-vv5ch from kube-system started at 2018-12-09 16:53:42 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.048: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 22:35:51.048: INFO: calico-node-xzzvt from kube-system started at 2018-12-09 16:52:48 +0000 UTC (2 container statuses recorded)
Dec  9 22:35:51.048: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 22:35:51.048: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 22:35:51.048: INFO: 
Logging pods the kubelet thinks is on node k8s-g2 before test
Dec  9 22:35:51.056: INFO: kube-proxy-lw2nk from kube-system started at 2018-12-09 16:53:48 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.056: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 22:35:51.056: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-09 21:58:11 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.056: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 22:35:51.056: INFO: coredns-5569467fdf-cj6br from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 22:35:51.056: INFO: 	Container coredns ready: true, restart count 1
Dec  9 22:35:51.056: INFO: calico-node-29r2w from kube-system started at 2018-12-09 16:52:49 +0000 UTC (2 container statuses recorded)
Dec  9 22:35:51.056: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 22:35:51.056: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 22:35:51.056: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-r6m2t from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 22:35:51.056: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  9 22:35:51.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ca87d097-fc02-11e8-a4b0-6e57f5092bbd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ca87d097-fc02-11e8-a4b0-6e57f5092bbd off the node k8s-g1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ca87d097-fc02-11e8-a4b0-6e57f5092bbd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:35:59.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-z9sq2" for this suite.
Dec  9 22:36:13.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:36:13.642: INFO: namespace: e2e-tests-sched-pred-z9sq2, resource: bindings, ignored listing per whitelist
Dec  9 22:36:13.823: INFO: namespace e2e-tests-sched-pred-z9sq2 deletion completed in 14.273758044s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:23.005 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:36:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ktzp9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  9 22:36:14.074: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  9 22:36:38.413: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.79:8080/dial?request=hostName&protocol=udp&host=10.244.4.78&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktzp9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:36:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:36:38.506: INFO: Waiting for endpoints: map[]
Dec  9 22:36:38.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.79:8080/dial?request=hostName&protocol=udp&host=10.244.3.131&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-ktzp9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:36:38.511: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:36:38.572: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:36:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ktzp9" for this suite.
Dec  9 22:37:02.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:37:02.748: INFO: namespace: e2e-tests-pod-network-test-ktzp9, resource: bindings, ignored listing per whitelist
Dec  9 22:37:02.799: INFO: namespace e2e-tests-pod-network-test-ktzp9 deletion completed in 24.222475409s

• [SLOW TEST:48.976 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:37:02.800: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-gzsgn/configmap-test-f2ea5ec7-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:37:03.067: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-gzsgn" to be "success or failure"
Dec  9 22:37:03.129: INFO: Pod "pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 62.242647ms
Dec  9 22:37:05.135: INFO: Pod "pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067786637s
Dec  9 22:37:07.141: INFO: Pod "pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073935183s
STEP: Saw pod success
Dec  9 22:37:07.141: INFO: Pod "pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:37:07.146: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd container env-test: <nil>
STEP: delete the pod
Dec  9 22:37:07.270: INFO: Waiting for pod pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:37:07.276: INFO: Pod pod-configmaps-f2ec3fb6-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:37:07.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gzsgn" for this suite.
Dec  9 22:37:13.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:37:13.429: INFO: namespace: e2e-tests-configmap-gzsgn, resource: bindings, ignored listing per whitelist
Dec  9 22:37:13.540: INFO: namespace e2e-tests-configmap-gzsgn deletion completed in 6.259027779s

• [SLOW TEST:10.741 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:37:13.541: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  9 22:37:14.002: INFO: Waiting up to 5m0s for pod "pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-xzhct" to be "success or failure"
Dec  9 22:37:14.043: INFO: Pod "pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.671197ms
Dec  9 22:37:16.049: INFO: Pod "pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046361901s
Dec  9 22:37:18.059: INFO: Pod "pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056914304s
STEP: Saw pod success
Dec  9 22:37:18.059: INFO: Pod "pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:37:18.065: INFO: Trying to get logs from node k8s-g1 pod pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:37:18.162: INFO: Waiting for pod pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:37:18.168: INFO: Pod pod-f953c017-fc02-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:37:18.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xzhct" for this suite.
Dec  9 22:37:24.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:37:24.351: INFO: namespace: e2e-tests-emptydir-xzhct, resource: bindings, ignored listing per whitelist
Dec  9 22:37:24.379: INFO: namespace e2e-tests-emptydir-xzhct deletion completed in 6.205349705s

• [SLOW TEST:10.839 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:37:24.380: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-ffc6cb16-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating configMap with name cm-test-opt-upd-ffc6cb89-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ffc6cb16-fc02-11e8-a4b0-6e57f5092bbd
STEP: Updating configmap cm-test-opt-upd-ffc6cb89-fc02-11e8-a4b0-6e57f5092bbd
STEP: Creating configMap with name cm-test-opt-create-ffc6cbbc-fc02-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:37:33.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lbqb5" for this suite.
Dec  9 22:37:57.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:37:57.160: INFO: namespace: e2e-tests-projected-lbqb5, resource: bindings, ignored listing per whitelist
Dec  9 22:37:57.205: INFO: namespace e2e-tests-projected-lbqb5 deletion completed in 24.164913264s

• [SLOW TEST:32.825 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:37:57.205: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:37:57.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-4nr9r" to be "success or failure"
Dec  9 22:37:57.517: INFO: Pod "downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.659976ms
Dec  9 22:37:59.523: INFO: Pod "downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029204219s
Dec  9 22:38:01.529: INFO: Pod "downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036000939s
STEP: Saw pod success
Dec  9 22:38:01.529: INFO: Pod "downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:38:01.535: INFO: Trying to get logs from node k8s-g1 pod downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:38:01.696: INFO: Waiting for pod downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:38:01.701: INFO: Pod downwardapi-volume-1354d596-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:38:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4nr9r" for this suite.
Dec  9 22:38:07.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:38:07.840: INFO: namespace: e2e-tests-downward-api-4nr9r, resource: bindings, ignored listing per whitelist
Dec  9 22:38:07.888: INFO: namespace e2e-tests-downward-api-4nr9r deletion completed in 6.161759672s

• [SLOW TEST:10.684 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:38:07.889: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  9 22:38:08.150: INFO: Waiting up to 5m0s for pod "downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-httqq" to be "success or failure"
Dec  9 22:38:08.201: INFO: Pod "downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.950862ms
Dec  9 22:38:10.210: INFO: Pod "downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059682808s
Dec  9 22:38:12.215: INFO: Pod "downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065145336s
STEP: Saw pod success
Dec  9 22:38:12.215: INFO: Pod "downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:38:12.220: INFO: Trying to get logs from node k8s-g2 pod downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 22:38:12.284: INFO: Waiting for pod downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:38:12.308: INFO: Pod downward-api-19b5533e-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:38:12.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-httqq" for this suite.
Dec  9 22:38:18.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:38:18.435: INFO: namespace: e2e-tests-downward-api-httqq, resource: bindings, ignored listing per whitelist
Dec  9 22:38:18.538: INFO: namespace e2e-tests-downward-api-httqq deletion completed in 6.225468486s

• [SLOW TEST:10.650 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:38:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:38:23.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-cbfvz" for this suite.
Dec  9 22:38:29.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:38:29.263: INFO: namespace: e2e-tests-emptydir-wrapper-cbfvz, resource: bindings, ignored listing per whitelist
Dec  9 22:38:29.303: INFO: namespace e2e-tests-emptydir-wrapper-cbfvz deletion completed in 6.170588269s

• [SLOW TEST:10.764 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:38:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:38:29.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-mpdq9" to be "success or failure"
Dec  9 22:38:29.687: INFO: Pod "downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.54405ms
Dec  9 22:38:31.693: INFO: Pod "downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018776819s
Dec  9 22:38:33.699: INFO: Pod "downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024184233s
STEP: Saw pod success
Dec  9 22:38:33.699: INFO: Pod "downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:38:33.734: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:38:33.883: INFO: Waiting for pod downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:38:33.888: INFO: Pod downwardapi-volume-2688e2fb-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:38:33.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mpdq9" for this suite.
Dec  9 22:38:39.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:38:39.984: INFO: namespace: e2e-tests-downward-api-mpdq9, resource: bindings, ignored listing per whitelist
Dec  9 22:38:40.060: INFO: namespace e2e-tests-downward-api-mpdq9 deletion completed in 6.167536697s

• [SLOW TEST:10.757 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:38:40.060: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:38:40.258: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  9 22:38:40.284: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  9 22:38:45.291: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  9 22:38:45.291: INFO: Creating deployment "test-rolling-update-deployment"
Dec  9 22:38:45.441: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  9 22:38:45.456: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  9 22:38:47.465: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  9 22:38:47.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679991925, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679991925, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679991925, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679991925, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 22:38:49.473: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  9 22:38:49.484: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-g7zbm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g7zbm/deployments/test-rolling-update-deployment,UID:2fe7444f-fc03-11e8-9ee5-54a05085d523,ResourceVersion:44739,Generation:1,CreationTimestamp:2018-12-09 22:38:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-09 22:38:45 +0000 UTC 2018-12-09 22:38:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-09 22:38:48 +0000 UTC 2018-12-09 22:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 22:38:49.487: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-g7zbm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g7zbm/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:2ff7fae1-fc03-11e8-9811-448a5b81d79a,ResourceVersion:44730,Generation:1,CreationTimestamp:2018-12-09 22:38:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2fe7444f-fc03-11e8-9ee5-54a05085d523 0xc0023746f7 0xc0023746f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  9 22:38:49.487: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  9 22:38:49.488: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-g7zbm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-g7zbm/replicasets/test-rolling-update-controller,UID:2cdd4c0f-fc03-11e8-9ee5-54a05085d523,ResourceVersion:44738,Generation:2,CreationTimestamp:2018-12-09 22:38:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2fe7444f-fc03-11e8-9ee5-54a05085d523 0xc002374637 0xc002374638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 22:38:49.493: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-mhtxm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-mhtxm,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-g7zbm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-g7zbm/pods/test-rolling-update-deployment-68b55d7bc6-mhtxm,UID:2ffbd4e3-fc03-11e8-9811-448a5b81d79a,ResourceVersion:44729,Generation:0,CreationTimestamp:2018-12-09 22:38:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.84/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 2ff7fae1-fc03-11e8-9811-448a5b81d79a 0xc002426007 0xc002426008}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-d9dpw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9dpw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-d9dpw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002426080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024260a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:38:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:38:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:38:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:38:45 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.84,StartTime:2018-12-09 22:38:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-09 22:38:47 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ec11763c8f59ad5ddd6c975d683944bf6683fb0262b1fb01e323450e54461903}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:38:49.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-g7zbm" for this suite.
Dec  9 22:38:57.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:38:57.597: INFO: namespace: e2e-tests-deployment-g7zbm, resource: bindings, ignored listing per whitelist
Dec  9 22:38:57.657: INFO: namespace e2e-tests-deployment-g7zbm deletion completed in 8.15916022s

• [SLOW TEST:17.597 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:38:57.657: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-dtq7k.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-dtq7k.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dtq7k.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-dtq7k.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-dtq7k.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dtq7k.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  9 22:39:02.256: INFO: DNS probes using e2e-tests-dns-dtq7k/dns-test-375f57aa-fc03-11e8-a4b0-6e57f5092bbd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:39:02.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-dtq7k" for this suite.
Dec  9 22:39:08.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:39:08.478: INFO: namespace: e2e-tests-dns-dtq7k, resource: bindings, ignored listing per whitelist
Dec  9 22:39:08.540: INFO: namespace e2e-tests-dns-dtq7k deletion completed in 6.169914103s

• [SLOW TEST:10.883 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:39:08.540: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-3dddfcff-fc03-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 22:39:08.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-tcmnj" to be "success or failure"
Dec  9 22:39:08.933: INFO: Pod "pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 77.512518ms
Dec  9 22:39:10.939: INFO: Pod "pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082890642s
Dec  9 22:39:12.945: INFO: Pod "pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089290862s
STEP: Saw pod success
Dec  9 22:39:12.945: INFO: Pod "pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:39:12.950: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 22:39:13.086: INFO: Waiting for pod pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:39:13.091: INFO: Pod pod-configmaps-3de4e2d3-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:39:13.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tcmnj" for this suite.
Dec  9 22:39:19.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:39:19.154: INFO: namespace: e2e-tests-configmap-tcmnj, resource: bindings, ignored listing per whitelist
Dec  9 22:39:19.264: INFO: namespace e2e-tests-configmap-tcmnj deletion completed in 6.167658981s

• [SLOW TEST:10.723 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:39:19.264: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Dec  9 22:39:19.549: INFO: Waiting up to 5m0s for pod "client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-containers-55x9d" to be "success or failure"
Dec  9 22:39:19.585: INFO: Pod "client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.377533ms
Dec  9 22:39:21.591: INFO: Pod "client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042422722s
Dec  9 22:39:23.597: INFO: Pod "client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048401708s
STEP: Saw pod success
Dec  9 22:39:23.597: INFO: Pod "client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:39:23.602: INFO: Trying to get logs from node k8s-g1 pod client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:39:23.699: INFO: Waiting for pod client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:39:23.721: INFO: Pod client-containers-443db694-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:39:23.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-55x9d" for this suite.
Dec  9 22:39:29.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:39:29.985: INFO: namespace: e2e-tests-containers-55x9d, resource: bindings, ignored listing per whitelist
Dec  9 22:39:30.012: INFO: namespace e2e-tests-containers-55x9d deletion completed in 6.285553828s

• [SLOW TEST:10.748 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:39:30.012: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-xp5z
STEP: Creating a pod to test atomic-volume-subpath
Dec  9 22:39:30.405: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xp5z" in namespace "e2e-tests-subpath-q566d" to be "success or failure"
Dec  9 22:39:30.412: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286102ms
Dec  9 22:39:32.417: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012853678s
Dec  9 22:39:34.424: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019488359s
Dec  9 22:39:36.430: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 6.025572448s
Dec  9 22:39:38.436: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 8.031311443s
Dec  9 22:39:40.441: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 10.036684106s
Dec  9 22:39:42.448: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 12.043053893s
Dec  9 22:39:44.456: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 14.050968916s
Dec  9 22:39:46.462: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 16.056987073s
Dec  9 22:39:48.467: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 18.062688177s
Dec  9 22:39:50.473: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 20.067997525s
Dec  9 22:39:52.478: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.073653988s
Dec  9 22:39:54.483: INFO: Pod "pod-subpath-test-secret-xp5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07886207s
STEP: Saw pod success
Dec  9 22:39:54.483: INFO: Pod "pod-subpath-test-secret-xp5z" satisfied condition "success or failure"
Dec  9 22:39:54.488: INFO: Trying to get logs from node k8s-g2 pod pod-subpath-test-secret-xp5z container test-container-subpath-secret-xp5z: <nil>
STEP: delete the pod
Dec  9 22:39:54.638: INFO: Waiting for pod pod-subpath-test-secret-xp5z to disappear
Dec  9 22:39:54.663: INFO: Pod pod-subpath-test-secret-xp5z no longer exists
STEP: Deleting pod pod-subpath-test-secret-xp5z
Dec  9 22:39:54.663: INFO: Deleting pod "pod-subpath-test-secret-xp5z" in namespace "e2e-tests-subpath-q566d"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:39:54.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-q566d" for this suite.
Dec  9 22:40:00.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:40:00.781: INFO: namespace: e2e-tests-subpath-q566d, resource: bindings, ignored listing per whitelist
Dec  9 22:40:00.829: INFO: namespace e2e-tests-subpath-q566d deletion completed in 6.155875377s

• [SLOW TEST:30.817 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:40:00.829: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:40:21.188: INFO: Container started at 2018-12-09 22:40:03 +0000 UTC, pod became ready at 2018-12-09 22:40:20 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:40:21.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vf5wh" for this suite.
Dec  9 22:40:43.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:40:43.286: INFO: namespace: e2e-tests-container-probe-vf5wh, resource: bindings, ignored listing per whitelist
Dec  9 22:40:43.428: INFO: namespace e2e-tests-container-probe-vf5wh deletion completed in 22.235424223s

• [SLOW TEST:42.599 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:40:43.429: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 22:40:43.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-5xmjf'
Dec  9 22:40:46.409: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  9 22:40:46.409: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Dec  9 22:40:48.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-5xmjf'
Dec  9 22:40:48.549: INFO: stderr: ""
Dec  9 22:40:48.549: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:40:48.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5xmjf" for this suite.
Dec  9 22:42:52.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:42:52.628: INFO: namespace: e2e-tests-kubectl-5xmjf, resource: bindings, ignored listing per whitelist
Dec  9 22:42:52.737: INFO: namespace e2e-tests-kubectl-5xmjf deletion completed in 2m4.183564082s

• [SLOW TEST:129.309 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:42:52.737: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  9 22:42:53.034: INFO: Waiting up to 5m0s for pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-tshms" to be "success or failure"
Dec  9 22:42:53.075: INFO: Pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.367346ms
Dec  9 22:42:55.081: INFO: Pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046311594s
Dec  9 22:42:57.086: INFO: Pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051853186s
Dec  9 22:42:59.092: INFO: Pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057564736s
STEP: Saw pod success
Dec  9 22:42:59.092: INFO: Pod "pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:42:59.097: INFO: Trying to get logs from node k8s-g1 pod pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:42:59.186: INFO: Waiting for pod pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:42:59.191: INFO: Pod pod-c3816d96-fc03-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:42:59.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tshms" for this suite.
Dec  9 22:43:05.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:43:05.296: INFO: namespace: e2e-tests-emptydir-tshms, resource: bindings, ignored listing per whitelist
Dec  9 22:43:05.354: INFO: namespace e2e-tests-emptydir-tshms deletion completed in 6.156802818s

• [SLOW TEST:12.616 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:43:05.354: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  9 22:43:05.587: INFO: PodSpec: initContainers in spec.initContainers
Dec  9 22:43:53.885: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cb03118f-fc03-11e8-a4b0-6e57f5092bbd", GenerateName:"", Namespace:"e2e-tests-init-container-c9rqh", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-c9rqh/pods/pod-init-cb03118f-fc03-11e8-a4b0-6e57f5092bbd", UID:"cb03f308-fc03-11e8-9ee5-54a05085d523", ResourceVersion:"45563", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679992185, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"587756039", "name":"foo"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.4.88/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kv8bp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001a27f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kv8bp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kv8bp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kv8bp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000e65828), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-g2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001352b40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000e659d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000e659f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000e659f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000e659fc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992185, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992185, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992185, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992185, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.22.132.14", PodIP:"10.244.4.88", StartTime:(*v1.Time)(0xc0012483a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000133650)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000133730)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:2a03a6059f21e150ae84b0973863609494aad70f0a80eaeb64bddd8d92465812", ContainerID:"docker://4a70ec2d6f3c46397938d06e0c350a2e0b930ce48eedd7aaa2571c4da13adfcb"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001248420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0012483e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:43:53.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-c9rqh" for this suite.
Dec  9 22:44:15.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:44:16.058: INFO: namespace: e2e-tests-init-container-c9rqh, resource: bindings, ignored listing per whitelist
Dec  9 22:44:16.299: INFO: namespace e2e-tests-init-container-c9rqh deletion completed in 22.408238524s

• [SLOW TEST:70.946 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:44:16.300: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:44:16.619: INFO: Creating deployment "nginx-deployment"
Dec  9 22:44:16.725: INFO: Waiting for observed generation 1
Dec  9 22:44:18.737: INFO: Waiting for all required pods to come up
Dec  9 22:44:18.746: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  9 22:44:24.762: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  9 22:44:24.769: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  9 22:44:24.794: INFO: Updating deployment nginx-deployment
Dec  9 22:44:24.794: INFO: Waiting for observed generation 2
Dec  9 22:44:26.802: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  9 22:44:26.806: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  9 22:44:26.809: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  9 22:44:26.820: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  9 22:44:26.820: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  9 22:44:26.824: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  9 22:44:26.830: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  9 22:44:26.830: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  9 22:44:26.872: INFO: Updating deployment nginx-deployment
Dec  9 22:44:26.872: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  9 22:44:26.880: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  9 22:44:28.905: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  9 22:44:28.912: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-h96rq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h96rq/deployments/nginx-deployment,UID:f559df89-fc03-11e8-9ee5-54a05085d523,ResourceVersion:45898,Generation:3,CreationTimestamp:2018-12-09 22:44:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2018-12-09 22:44:26 +0000 UTC 2018-12-09 22:44:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-09 22:44:28 +0000 UTC 2018-12-09 22:44:16 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  9 22:44:28.917: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-h96rq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h96rq/replicasets/nginx-deployment-65bbdb5f8,UID:fa3d2600-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45892,Generation:3,CreationTimestamp:2018-12-09 22:44:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f559df89-fc03-11e8-9ee5-54a05085d523 0xc001b317c7 0xc001b317c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 22:44:28.917: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  9 22:44:28.917: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-h96rq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h96rq/replicasets/nginx-deployment-555b55d965,UID:f56bbbf6-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45885,Generation:3,CreationTimestamp:2018-12-09 22:44:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f559df89-fc03-11e8-9ee5-54a05085d523 0xc001b315f7 0xc001b315f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-8lzh8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8lzh8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-8lzh8,UID:f57eec8f-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45734,Generation:0,CreationTimestamp:2018-12-09 22:44:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.140/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023ca4f7 0xc0023ca4f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023ca570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023ca590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:16 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:10.244.3.140,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://33f68c06663f81470c58954f2e232af6babfbe63c3cbda059e4422698e79d272}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-967xf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-967xf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-967xf,UID:f595ea71-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45740,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.91/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023ca667 0xc0023ca668}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023ca6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023ca700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.91,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://a8722a92f35d53fd0075f0fc8d030840b6a5bf381b8d35b1d2e5c99aacfd96da}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-9bm92" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9bm92,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-9bm92,UID:fba90fb1-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45920,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023ca7c0 0xc0023ca7c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023ca830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023ca850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-9jdsj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9jdsj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-9jdsj,UID:fba91d47-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45926,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023ca907 0xc0023ca908}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023ca980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023ca9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-9ppx2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9ppx2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-9ppx2,UID:f595d15a-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45739,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.141/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023caa67 0xc0023caa68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023caae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cab00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:10.244.3.141,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://550b2780a75a5d71c620db49c5d746ea80915781a719d44a3f9cb4ea0449bf32}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-cz8n8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cz8n8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-cz8n8,UID:fba8c09c-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45903,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cadd7 0xc0023cadd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cae50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cae70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.052: INFO: Pod "nginx-deployment-555b55d965-dd8qh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dd8qh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-dd8qh,UID:fbce3e75-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45873,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cafb7 0xc0023cafb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cb030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cb050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-f72qh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f72qh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-f72qh,UID:fbce4715-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45877,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cb0c0 0xc0023cb0c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cb130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cb410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-hbq5m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hbq5m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-hbq5m,UID:fba923c0-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45867,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cb480 0xc0023cb481}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cb4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cb510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-lllnk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lllnk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-lllnk,UID:f595bff3-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45752,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.90/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cb590 0xc0023cb591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cb8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cb8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.90,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://a22bcc780f6e837ba9de5ce34f3e14630c2fb6f7afe385deebb2071a3f78e926}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-nhggm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nhggm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-nhggm,UID:fb9575ed-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45894,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cb990 0xc0023cb991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cba10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cba90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-qbjfx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qbjfx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-qbjfx,UID:f5b62f2f-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45743,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.92/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cbb57 0xc0023cbb58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cbbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cbbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.92,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://cd392dcb44f71104824f6fdf75597aeeae19e83e0e93573281fce5e9658a4f8a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-rv8st" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rv8st,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-rv8st,UID:fbce4bb0-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45879,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cbd00 0xc0023cbd01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cbd70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cbd90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-sdmt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sdmt7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-sdmt7,UID:fb95553e-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45902,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cbe00 0xc0023cbe01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023cbeb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023cbed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-tq89v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tq89v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-tq89v,UID:fbce31ad-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45874,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0023cbf87 0xc0023cbf88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021aa6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021aa700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-w9n22" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w9n22,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-w9n22,UID:f5b6339b-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45744,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.143/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0021aa830 0xc0021aa831}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021aa8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021aa8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:10.244.3.143,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://091590a69a78d44ce430b281d9d064e1ca2dfc1cb2ab22edcc6de3d1d966f881}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-wlq2z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-wlq2z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-wlq2z,UID:f595d557-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45748,Generation:0,CreationTimestamp:2018-12-09 22:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.93/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0021aab07 0xc0021aab08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021aab80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021aaba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.93,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://9d1ea57d4ed1e363d683f8246cc66790e02f15e69e52a17f3664f27d47b365a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-xcdxd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xcdxd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-xcdxd,UID:fb8644af-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45881,Generation:0,CreationTimestamp:2018-12-09 22:44:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0021ab090 0xc0021ab091}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ab170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ab190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-z4k56" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z4k56,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-z4k56,UID:fbce52b6-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45875,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0021ab247 0xc0021ab248}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ab2c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ab8a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-555b55d965-zvz5z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zvz5z,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-555b55d965-zvz5z,UID:f58a91ee-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45721,Generation:0,CreationTimestamp:2018-12-09 22:44:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.89/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 f56bbbf6-fc03-11e8-9811-448a5b81d79a 0xc0021ab930 0xc0021ab931}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ab9a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ab9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:17 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.89,StartTime:2018-12-09 22:44:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-09 22:44:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://4b91abf640d5d7fe4fbb57ad79c9699dd1347a9270a8b065a52d7866680cb9ee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.053: INFO: Pod "nginx-deployment-65bbdb5f8-27lcc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-27lcc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-27lcc,UID:fa9b5fd7-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45909,Generation:0,CreationTimestamp:2018-12-09 22:44:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.96/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0021abc30 0xc0021abc31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021abcb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021abcd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-6k9t4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6k9t4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-6k9t4,UID:fba9087a-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45860,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0021abdd0 0xc0021abdd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021abe50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021abe70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-8h9mf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8h9mf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-8h9mf,UID:fb9573c5-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45908,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0021abee0 0xc0021abee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021abf90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021abfb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-96lbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-96lbv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-96lbv,UID:fa40b857-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45882,Generation:0,CreationTimestamp:2018-12-09 22:44:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.94/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b4080 0xc0008b4081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b42a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b42c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:24 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-bnxgq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bnxgq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-bnxgq,UID:fb86484e-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45893,Generation:0,CreationTimestamp:2018-12-09 22:44:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b4520 0xc0008b4521}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b45a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b45d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-fr6c4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-fr6c4,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-fr6c4,UID:fba90020-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45857,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b4730 0xc0008b4731}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b48e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b4900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-j6b8m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-j6b8m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-j6b8m,UID:fa487766-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45890,Generation:0,CreationTimestamp:2018-12-09 22:44:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.95/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b4aa0 0xc0008b4aa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b4b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b4b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:24 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-kll8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kll8d,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-kll8d,UID:fb95727f-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45880,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b4ea0 0xc0008b4ea1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b5270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b52a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-pwcvs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-pwcvs,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-pwcvs,UID:fa486ff7-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45911,Generation:0,CreationTimestamp:2018-12-09 22:44:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.145/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b5680 0xc0008b5681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b5960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b5980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:24 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-tv9bn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-tv9bn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-tv9bn,UID:fba91836-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45907,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b5a90 0xc0008b5a91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b5b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b5ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-vsrq2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vsrq2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-vsrq2,UID:fba90c16-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45919,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b5cb0 0xc0008b5cb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b5d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b5d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-wr2f8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-wr2f8,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-wr2f8,UID:fa872dd1-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45809,Generation:0,CreationTimestamp:2018-12-09 22:44:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc0008b5ef0 0xc0008b5ef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008b5f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008b5f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:25 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:,StartTime:2018-12-09 22:44:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  9 22:44:29.054: INFO: Pod "nginx-deployment-65bbdb5f8-zzpkp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zzpkp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h96rq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h96rq/pods/nginx-deployment-65bbdb5f8-zzpkp,UID:fbc4cd98-fc03-11e8-9811-448a5b81d79a,ResourceVersion:45871,Generation:0,CreationTimestamp:2018-12-09 22:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 fa3d2600-fc03-11e8-9811-448a5b81d79a 0xc001ec40b0 0xc001ec40b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-kwn9g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwn9g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kwn9g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ec4140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ec4160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:44:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:44:29.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-h96rq" for this suite.
Dec  9 22:44:41.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:44:41.383: INFO: namespace: e2e-tests-deployment-h96rq, resource: bindings, ignored listing per whitelist
Dec  9 22:44:41.441: INFO: namespace e2e-tests-deployment-h96rq deletion completed in 12.341566291s

• [SLOW TEST:25.142 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:44:41.441: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-g5n42
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-g5n42
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-g5n42
Dec  9 22:44:41.769: INFO: Found 0 stateful pods, waiting for 1
Dec  9 22:44:51.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  9 22:44:51.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:44:52.011: INFO: stderr: ""
Dec  9 22:44:52.011: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:44:52.011: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:44:52.036: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  9 22:45:02.043: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:45:02.044: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:45:02.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999784s
Dec  9 22:45:03.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983885163s
Dec  9 22:45:04.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977523706s
Dec  9 22:45:05.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.971541282s
Dec  9 22:45:06.121: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964042614s
Dec  9 22:45:07.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.957362001s
Dec  9 22:45:08.135: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948800484s
Dec  9 22:45:09.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943221913s
Dec  9 22:45:10.147: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.937176538s
Dec  9 22:45:11.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 930.566025ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-g5n42
Dec  9 22:45:12.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:45:12.278: INFO: stderr: ""
Dec  9 22:45:12.278: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:45:12.278: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:45:12.283: INFO: Found 1 stateful pods, waiting for 3
Dec  9 22:45:22.289: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:45:22.289: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:45:22.289: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  9 22:45:22.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:45:22.413: INFO: stderr: ""
Dec  9 22:45:22.413: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:45:22.413: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:45:22.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:45:22.591: INFO: stderr: ""
Dec  9 22:45:22.591: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:45:22.591: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:45:22.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  9 22:45:22.789: INFO: stderr: ""
Dec  9 22:45:22.789: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  9 22:45:22.789: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  9 22:45:22.789: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:45:22.841: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec  9 22:45:32.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:45:32.851: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:45:32.851: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 22:45:32.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999652s
Dec  9 22:45:33.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980719527s
Dec  9 22:45:34.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974894492s
Dec  9 22:45:35.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9679715s
Dec  9 22:45:36.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956176051s
Dec  9 22:45:37.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.949948962s
Dec  9 22:45:38.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.93581657s
Dec  9 22:45:40.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930353615s
Dec  9 22:45:41.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.879849722s
Dec  9 22:45:42.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 873.396972ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-g5n42
Dec  9 22:45:43.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:45:43.198: INFO: stderr: ""
Dec  9 22:45:43.198: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:45:43.198: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:45:43.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:45:43.326: INFO: stderr: ""
Dec  9 22:45:43.326: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:45:43.326: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:45:43.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 exec --namespace=e2e-tests-statefulset-g5n42 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  9 22:45:43.449: INFO: stderr: ""
Dec  9 22:45:43.449: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  9 22:45:43.449: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  9 22:45:43.449: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  9 22:46:13.470: INFO: Deleting all statefulset in ns e2e-tests-statefulset-g5n42
Dec  9 22:46:13.475: INFO: Scaling statefulset ss to 0
Dec  9 22:46:13.492: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:46:13.496: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:46:13.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-g5n42" for this suite.
Dec  9 22:46:21.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:46:21.713: INFO: namespace: e2e-tests-statefulset-g5n42, resource: bindings, ignored listing per whitelist
Dec  9 22:46:21.760: INFO: namespace e2e-tests-statefulset-g5n42 deletion completed in 8.196195416s

• [SLOW TEST:100.319 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:46:21.760: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  9 22:46:26.678: INFO: Successfully updated pod "pod-update-activedeadlineseconds-40146712-fc04-11e8-a4b0-6e57f5092bbd"
Dec  9 22:46:26.678: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-40146712-fc04-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-pods-sgqdh" to be "terminated due to deadline exceeded"
Dec  9 22:46:26.683: INFO: Pod "pod-update-activedeadlineseconds-40146712-fc04-11e8-a4b0-6e57f5092bbd": Phase="Running", Reason="", readiness=true. Elapsed: 5.749866ms
Dec  9 22:46:28.706: INFO: Pod "pod-update-activedeadlineseconds-40146712-fc04-11e8-a4b0-6e57f5092bbd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.02809343s
Dec  9 22:46:28.706: INFO: Pod "pod-update-activedeadlineseconds-40146712-fc04-11e8-a4b0-6e57f5092bbd" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:46:28.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-sgqdh" for this suite.
Dec  9 22:46:34.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:46:34.769: INFO: namespace: e2e-tests-pods-sgqdh, resource: bindings, ignored listing per whitelist
Dec  9 22:46:34.876: INFO: namespace e2e-tests-pods-sgqdh deletion completed in 6.165537982s

• [SLOW TEST:13.116 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:46:34.876: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  9 22:46:43.441: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.441: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.544: INFO: Exec stderr: ""
Dec  9 22:46:43.544: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.544: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.676: INFO: Exec stderr: ""
Dec  9 22:46:43.676: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.749: INFO: Exec stderr: ""
Dec  9 22:46:43.749: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.749: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.809: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  9 22:46:43.809: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.876: INFO: Exec stderr: ""
Dec  9 22:46:43.876: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:43.941: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  9 22:46:43.941: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:43.941: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:44.009: INFO: Exec stderr: ""
Dec  9 22:46:44.009: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:44.073: INFO: Exec stderr: ""
Dec  9 22:46:44.073: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:44.073: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:44.128: INFO: Exec stderr: ""
Dec  9 22:46:44.128: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-bcp4q PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:46:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:46:44.197: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:46:44.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-bcp4q" for this suite.
Dec  9 22:47:32.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:47:32.368: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-bcp4q, resource: bindings, ignored listing per whitelist
Dec  9 22:47:32.388: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-bcp4q deletion completed in 48.186746476s

• [SLOW TEST:57.512 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:47:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd
Dec  9 22:47:32.652: INFO: Pod name my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd: Found 0 pods out of 1
Dec  9 22:47:37.691: INFO: Pod name my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd: Found 1 pods out of 1
Dec  9 22:47:37.691: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd" are running
Dec  9 22:47:37.697: INFO: Pod "my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd-fdbjr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 22:47:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 22:47:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 22:47:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 22:47:32 +0000 UTC Reason: Message:}])
Dec  9 22:47:37.698: INFO: Trying to dial the pod
Dec  9 22:47:42.736: INFO: Controller my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd: Got expected result from replica 1 [my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd-fdbjr]: "my-hostname-basic-6a29aff5-fc04-11e8-a4b0-6e57f5092bbd-fdbjr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:47:42.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-rd66v" for this suite.
Dec  9 22:47:48.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:47:48.976: INFO: namespace: e2e-tests-replication-controller-rd66v, resource: bindings, ignored listing per whitelist
Dec  9 22:47:49.034: INFO: namespace e2e-tests-replication-controller-rd66v deletion completed in 6.294176021s

• [SLOW TEST:16.646 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:47:49.034: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 22:47:49.255: INFO: Creating deployment "test-recreate-deployment"
Dec  9 22:47:49.357: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  9 22:47:49.377: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  9 22:47:51.385: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  9 22:47:51.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992469, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992469, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992469, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992469, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 22:47:53.393: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  9 22:47:53.415: INFO: Updating deployment test-recreate-deployment
Dec  9 22:47:53.415: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  9 22:47:54.155: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-d4w9x,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d4w9x/deployments/test-recreate-deployment,UID:74178106-fc04-11e8-9ee5-54a05085d523,ResourceVersion:46872,Generation:2,CreationTimestamp:2018-12-09 22:47:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2018-12-09 22:47:53 +0000 UTC 2018-12-09 22:47:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-09 22:47:54 +0000 UTC 2018-12-09 22:47:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  9 22:47:54.159: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-d4w9x,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d4w9x/replicasets/test-recreate-deployment-697fbf54bf,UID:76c38e4e-fc04-11e8-9811-448a5b81d79a,ResourceVersion:46870,Generation:1,CreationTimestamp:2018-12-09 22:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74178106-fc04-11e8-9ee5-54a05085d523 0xc001237537 0xc001237538}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 22:47:54.159: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  9 22:47:54.159: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-d4w9x,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d4w9x/replicasets/test-recreate-deployment-5dfdcc846d,UID:742610cd-fc04-11e8-9811-448a5b81d79a,ResourceVersion:46859,Generation:2,CreationTimestamp:2018-12-09 22:47:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74178106-fc04-11e8-9ee5-54a05085d523 0xc001237407 0xc001237408}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 22:47:54.165: INFO: Pod "test-recreate-deployment-697fbf54bf-99pc8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-99pc8,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-d4w9x,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d4w9x/pods/test-recreate-deployment-697fbf54bf-99pc8,UID:76cba2fc-fc04-11e8-9811-448a5b81d79a,ResourceVersion:46871,Generation:0,CreationTimestamp:2018-12-09 22:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 76c38e4e-fc04-11e8-9811-448a5b81d79a 0xc000962a87 0xc000962a88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rtmvs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rtmvs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rtmvs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000962b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000962b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:47:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:47:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:47:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 22:47:53 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:,StartTime:2018-12-09 22:47:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:47:54.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-d4w9x" for this suite.
Dec  9 22:48:02.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:48:02.249: INFO: namespace: e2e-tests-deployment-d4w9x, resource: bindings, ignored listing per whitelist
Dec  9 22:48:02.343: INFO: namespace e2e-tests-deployment-d4w9x deletion completed in 8.172756084s

• [SLOW TEST:13.308 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:48:02.343: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1209 22:48:42.815083      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 22:48:42.815: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:48:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xpjqv" for this suite.
Dec  9 22:48:52.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:48:52.927: INFO: namespace: e2e-tests-gc-xpjqv, resource: bindings, ignored listing per whitelist
Dec  9 22:48:53.025: INFO: namespace e2e-tests-gc-xpjqv deletion completed in 10.206296474s

• [SLOW TEST:50.682 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:48:53.025: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 22:48:53.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-smcsz" to be "success or failure"
Dec  9 22:48:53.284: INFO: Pod "downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.36658ms
Dec  9 22:48:55.289: INFO: Pod "downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035571929s
Dec  9 22:48:57.295: INFO: Pod "downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041309973s
STEP: Saw pod success
Dec  9 22:48:57.295: INFO: Pod "downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:48:57.300: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 22:48:57.384: INFO: Waiting for pod downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:48:57.389: INFO: Pod downwardapi-volume-9a399afd-fc04-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:48:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-smcsz" for this suite.
Dec  9 22:49:03.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:49:03.487: INFO: namespace: e2e-tests-projected-smcsz, resource: bindings, ignored listing per whitelist
Dec  9 22:49:03.551: INFO: namespace e2e-tests-projected-smcsz deletion completed in 6.157018052s

• [SLOW TEST:10.526 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:49:03.551: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-zbz9
STEP: Creating a pod to test atomic-volume-subpath
Dec  9 22:49:03.886: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zbz9" in namespace "e2e-tests-subpath-fdbgm" to be "success or failure"
Dec  9 22:49:03.891: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361903ms
Dec  9 22:49:05.896: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009858153s
Dec  9 22:49:07.902: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015479807s
Dec  9 22:49:09.907: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 6.020595448s
Dec  9 22:49:11.912: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 8.026124173s
Dec  9 22:49:13.918: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 10.031875479s
Dec  9 22:49:15.924: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 12.03814561s
Dec  9 22:49:17.931: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 14.044785476s
Dec  9 22:49:19.937: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 16.05045296s
Dec  9 22:49:21.942: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 18.055890508s
Dec  9 22:49:23.947: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 20.061021507s
Dec  9 22:49:25.953: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 22.067133806s
Dec  9 22:49:27.959: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Running", Reason="", readiness=false. Elapsed: 24.07264945s
Dec  9 22:49:29.965: INFO: Pod "pod-subpath-test-projected-zbz9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.078441753s
STEP: Saw pod success
Dec  9 22:49:29.965: INFO: Pod "pod-subpath-test-projected-zbz9" satisfied condition "success or failure"
Dec  9 22:49:29.970: INFO: Trying to get logs from node k8s-g2 pod pod-subpath-test-projected-zbz9 container test-container-subpath-projected-zbz9: <nil>
STEP: delete the pod
Dec  9 22:49:30.070: INFO: Waiting for pod pod-subpath-test-projected-zbz9 to disappear
Dec  9 22:49:30.075: INFO: Pod pod-subpath-test-projected-zbz9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-zbz9
Dec  9 22:49:30.075: INFO: Deleting pod "pod-subpath-test-projected-zbz9" in namespace "e2e-tests-subpath-fdbgm"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:49:30.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fdbgm" for this suite.
Dec  9 22:49:36.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:49:36.334: INFO: namespace: e2e-tests-subpath-fdbgm, resource: bindings, ignored listing per whitelist
Dec  9 22:49:36.338: INFO: namespace e2e-tests-subpath-fdbgm deletion completed in 6.244250776s

• [SLOW TEST:32.787 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:49:36.338: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  9 22:49:44.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 22:49:44.753: INFO: Pod pod-with-poststart-http-hook still exists
Dec  9 22:49:46.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 22:49:46.759: INFO: Pod pod-with-poststart-http-hook still exists
Dec  9 22:49:48.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 22:49:48.758: INFO: Pod pod-with-poststart-http-hook still exists
Dec  9 22:49:50.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 22:49:50.759: INFO: Pod pod-with-poststart-http-hook still exists
Dec  9 22:49:52.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 22:49:52.760: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:49:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-kzdvx" for this suite.
Dec  9 22:50:16.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:50:16.880: INFO: namespace: e2e-tests-container-lifecycle-hook-kzdvx, resource: bindings, ignored listing per whitelist
Dec  9 22:50:16.961: INFO: namespace e2e-tests-container-lifecycle-hook-kzdvx deletion completed in 24.19500852s

• [SLOW TEST:40.622 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:50:16.961: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  9 22:50:21.873: INFO: Successfully updated pod "annotationupdatecc4445ff-fc04-11e8-a4b0-6e57f5092bbd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:50:23.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sxc9c" for this suite.
Dec  9 22:50:46.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:50:46.101: INFO: namespace: e2e-tests-projected-sxc9c, resource: bindings, ignored listing per whitelist
Dec  9 22:50:46.146: INFO: namespace e2e-tests-projected-sxc9c deletion completed in 22.186803528s

• [SLOW TEST:29.185 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:50:46.146: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  9 22:50:46.377: INFO: namespace e2e-tests-kubectl-lxcbm
Dec  9 22:50:46.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-lxcbm'
Dec  9 22:50:49.463: INFO: stderr: ""
Dec  9 22:50:49.463: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  9 22:50:50.470: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:50:50.470: INFO: Found 0 / 1
Dec  9 22:50:51.480: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:50:51.480: INFO: Found 0 / 1
Dec  9 22:50:52.473: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:50:52.473: INFO: Found 0 / 1
Dec  9 22:50:53.469: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:50:53.470: INFO: Found 1 / 1
Dec  9 22:50:53.470: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  9 22:50:53.475: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:50:53.475: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 22:50:53.475: INFO: wait on redis-master startup in e2e-tests-kubectl-lxcbm 
Dec  9 22:50:53.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 logs redis-master-rcml5 redis-master --namespace=e2e-tests-kubectl-lxcbm'
Dec  9 22:50:53.596: INFO: stderr: ""
Dec  9 22:50:53.596: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Dec 22:50:51.696 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Dec 22:50:51.696 # Server started, Redis version 3.2.12\n1:M 09 Dec 22:50:51.696 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Dec 22:50:51.696 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  9 22:50:53.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-lxcbm'
Dec  9 22:50:53.752: INFO: stderr: ""
Dec  9 22:50:53.752: INFO: stdout: "service/rm2 exposed\n"
Dec  9 22:50:53.764: INFO: Service rm2 in namespace e2e-tests-kubectl-lxcbm found.
STEP: exposing service
Dec  9 22:50:55.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-lxcbm'
Dec  9 22:50:55.939: INFO: stderr: ""
Dec  9 22:50:55.939: INFO: stdout: "service/rm3 exposed\n"
Dec  9 22:50:55.948: INFO: Service rm3 in namespace e2e-tests-kubectl-lxcbm found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:50:57.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lxcbm" for this suite.
Dec  9 22:51:21.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:51:22.086: INFO: namespace: e2e-tests-kubectl-lxcbm, resource: bindings, ignored listing per whitelist
Dec  9 22:51:22.130: INFO: namespace e2e-tests-kubectl-lxcbm deletion completed in 24.16848152s

• [SLOW TEST:35.984 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:51:22.130: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:51:22.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zgl8g" for this suite.
Dec  9 22:51:44.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:51:44.578: INFO: namespace: e2e-tests-pods-zgl8g, resource: bindings, ignored listing per whitelist
Dec  9 22:51:44.647: INFO: namespace e2e-tests-pods-zgl8g deletion completed in 22.161102824s

• [SLOW TEST:22.517 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:51:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-ftb6j
Dec  9 22:51:49.011: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-ftb6j
STEP: checking the pod's current state and verifying that restartCount is present
Dec  9 22:51:49.016: INFO: Initial restart count of pod liveness-exec is 0
Dec  9 22:52:41.291: INFO: Restart count of pod e2e-tests-container-probe-ftb6j/liveness-exec is now 1 (52.275620815s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:52:41.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ftb6j" for this suite.
Dec  9 22:52:47.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:52:47.548: INFO: namespace: e2e-tests-container-probe-ftb6j, resource: bindings, ignored listing per whitelist
Dec  9 22:52:47.570: INFO: namespace e2e-tests-container-probe-ftb6j deletion completed in 6.189252221s

• [SLOW TEST:62.923 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:52:47.570: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 22:52:47.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:52:47.844: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  9 22:52:47.844: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec  9 22:52:47.849: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec  9 22:52:47.903: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec  9 22:52:48.043: INFO: scanned /root for discovery docs: <nil>
Dec  9 22:52:48.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:53:03.973: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  9 22:53:03.973: INFO: stdout: "Created e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1\nScaling up e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  9 22:53:03.973: INFO: stdout: "Created e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1\nScaling up e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  9 22:53:03.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:53:04.124: INFO: stderr: ""
Dec  9 22:53:04.124: INFO: stdout: "e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1-rzx6l "
Dec  9 22:53:04.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1-rzx6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:53:04.203: INFO: stderr: ""
Dec  9 22:53:04.203: INFO: stdout: "true"
Dec  9 22:53:04.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1-rzx6l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:53:04.268: INFO: stderr: ""
Dec  9 22:53:04.268: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  9 22:53:04.268: INFO: e2e-test-nginx-rc-b07a34a5218068848a6beb4ed7e811b1-rzx6l is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Dec  9 22:53:04.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-49m6j'
Dec  9 22:53:04.350: INFO: stderr: ""
Dec  9 22:53:04.350: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:53:04.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-49m6j" for this suite.
Dec  9 22:53:28.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:53:28.543: INFO: namespace: e2e-tests-kubectl-49m6j, resource: bindings, ignored listing per whitelist
Dec  9 22:53:28.594: INFO: namespace e2e-tests-kubectl-49m6j deletion completed in 24.239147064s

• [SLOW TEST:41.024 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:53:28.594: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  9 22:53:28.840: INFO: Waiting up to 5m0s for pod "pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-nxjch" to be "success or failure"
Dec  9 22:53:28.901: INFO: Pod "pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 60.380249ms
Dec  9 22:53:30.907: INFO: Pod "pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066613693s
Dec  9 22:53:32.913: INFO: Pod "pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072197667s
STEP: Saw pod success
Dec  9 22:53:32.913: INFO: Pod "pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:53:32.917: INFO: Trying to get logs from node k8s-g1 pod pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:53:33.002: INFO: Waiting for pod pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:53:33.007: INFO: Pod pod-3e7827c9-fc05-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:53:33.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nxjch" for this suite.
Dec  9 22:53:39.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:53:39.066: INFO: namespace: e2e-tests-emptydir-nxjch, resource: bindings, ignored listing per whitelist
Dec  9 22:53:39.191: INFO: namespace e2e-tests-emptydir-nxjch deletion completed in 6.179188916s

• [SLOW TEST:10.597 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:53:39.191: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  9 22:53:39.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:39.680: INFO: stderr: ""
Dec  9 22:53:39.680: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 22:53:39.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:39.760: INFO: stderr: ""
Dec  9 22:53:39.760: INFO: stdout: "update-demo-nautilus-bn99s "
STEP: Replicas for name=update-demo: expected=2 actual=1
Dec  9 22:53:44.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:44.832: INFO: stderr: ""
Dec  9 22:53:44.832: INFO: stdout: "update-demo-nautilus-bn99s update-demo-nautilus-rmdjl "
Dec  9 22:53:44.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-bn99s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:44.886: INFO: stderr: ""
Dec  9 22:53:44.886: INFO: stdout: "true"
Dec  9 22:53:44.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-bn99s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:44.940: INFO: stderr: ""
Dec  9 22:53:44.940: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:53:44.940: INFO: validating pod update-demo-nautilus-bn99s
Dec  9 22:53:44.947: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:53:44.947: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:53:44.947: INFO: update-demo-nautilus-bn99s is verified up and running
Dec  9 22:53:44.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:45.002: INFO: stderr: ""
Dec  9 22:53:45.002: INFO: stdout: "true"
Dec  9 22:53:45.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:45.056: INFO: stderr: ""
Dec  9 22:53:45.056: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:53:45.056: INFO: validating pod update-demo-nautilus-rmdjl
Dec  9 22:53:45.063: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:53:45.063: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:53:45.063: INFO: update-demo-nautilus-rmdjl is verified up and running
STEP: scaling down the replication controller
Dec  9 22:53:45.064: INFO: scanned /root for discovery docs: <nil>
Dec  9 22:53:45.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:46.187: INFO: stderr: ""
Dec  9 22:53:46.187: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 22:53:46.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:46.297: INFO: stderr: ""
Dec  9 22:53:46.297: INFO: stdout: "update-demo-nautilus-bn99s update-demo-nautilus-rmdjl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  9 22:53:51.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:51.369: INFO: stderr: ""
Dec  9 22:53:51.369: INFO: stdout: "update-demo-nautilus-bn99s update-demo-nautilus-rmdjl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  9 22:53:56.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:56.431: INFO: stderr: ""
Dec  9 22:53:56.431: INFO: stdout: "update-demo-nautilus-rmdjl "
Dec  9 22:53:56.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:56.486: INFO: stderr: ""
Dec  9 22:53:56.486: INFO: stdout: "true"
Dec  9 22:53:56.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:56.539: INFO: stderr: ""
Dec  9 22:53:56.539: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:53:56.539: INFO: validating pod update-demo-nautilus-rmdjl
Dec  9 22:53:56.545: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:53:56.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:53:56.545: INFO: update-demo-nautilus-rmdjl is verified up and running
STEP: scaling up the replication controller
Dec  9 22:53:56.546: INFO: scanned /root for discovery docs: <nil>
Dec  9 22:53:56.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:57.640: INFO: stderr: ""
Dec  9 22:53:57.640: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 22:53:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:57.711: INFO: stderr: ""
Dec  9 22:53:57.711: INFO: stdout: "update-demo-nautilus-4svhb update-demo-nautilus-rmdjl "
Dec  9 22:53:57.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-4svhb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:53:57.766: INFO: stderr: ""
Dec  9 22:53:57.766: INFO: stdout: ""
Dec  9 22:53:57.766: INFO: update-demo-nautilus-4svhb is created but not running
Dec  9 22:54:02.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:02.886: INFO: stderr: ""
Dec  9 22:54:02.886: INFO: stdout: "update-demo-nautilus-4svhb update-demo-nautilus-rmdjl "
Dec  9 22:54:02.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-4svhb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:02.939: INFO: stderr: ""
Dec  9 22:54:02.939: INFO: stdout: "true"
Dec  9 22:54:02.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-4svhb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:02.992: INFO: stderr: ""
Dec  9 22:54:02.992: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:54:02.992: INFO: validating pod update-demo-nautilus-4svhb
Dec  9 22:54:02.999: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:54:02.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:54:02.999: INFO: update-demo-nautilus-4svhb is verified up and running
Dec  9 22:54:02.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:03.053: INFO: stderr: ""
Dec  9 22:54:03.053: INFO: stdout: "true"
Dec  9 22:54:03.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-rmdjl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:03.108: INFO: stderr: ""
Dec  9 22:54:03.108: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 22:54:03.108: INFO: validating pod update-demo-nautilus-rmdjl
Dec  9 22:54:03.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 22:54:03.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 22:54:03.114: INFO: update-demo-nautilus-rmdjl is verified up and running
STEP: using delete to clean up resources
Dec  9 22:54:03.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:03.191: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 22:54:03.191: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  9 22:54:03.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-bhpgf'
Dec  9 22:54:03.264: INFO: stderr: "No resources found.\n"
Dec  9 22:54:03.264: INFO: stdout: ""
Dec  9 22:54:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -l name=update-demo --namespace=e2e-tests-kubectl-bhpgf -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 22:54:03.326: INFO: stderr: ""
Dec  9 22:54:03.326: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:54:03.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bhpgf" for this suite.
Dec  9 22:54:27.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:54:27.523: INFO: namespace: e2e-tests-kubectl-bhpgf, resource: bindings, ignored listing per whitelist
Dec  9 22:54:27.557: INFO: namespace e2e-tests-kubectl-bhpgf deletion completed in 24.225029333s

• [SLOW TEST:48.366 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:54:27.557: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Dec  9 22:54:31.963: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-61a160f9-fc05-11e8-a4b0-6e57f5092bbd", GenerateName:"", Namespace:"e2e-tests-pods-lqtr4", SelfLink:"/api/v1/namespaces/e2e-tests-pods-lqtr4/pods/pod-submit-remove-61a160f9-fc05-11e8-a4b0-6e57f5092bbd", UID:"61a37dcd-fc05-11e8-9ee5-54a05085d523", ResourceVersion:"48246", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679992867, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"780226190"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.4.124/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-fkzw6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000c4bd40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-fkzw6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001ec4228), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-g2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00267a0c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ec4270)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ec4290)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001ec4298), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001ec429c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992867, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992870, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992870, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679992867, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.22.132.14", PodIP:"10.244.4.124", StartTime:(*v1.Time)(0xc00254e7c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00254e880), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6", ContainerID:"docker://781828aa534fed33b766270f7900aa832fff7f1cdf72391e08e50b008804d2d9"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:54:41.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lqtr4" for this suite.
Dec  9 22:54:47.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:54:47.552: INFO: namespace: e2e-tests-pods-lqtr4, resource: bindings, ignored listing per whitelist
Dec  9 22:54:47.585: INFO: namespace e2e-tests-pods-lqtr4 deletion completed in 6.238801133s

• [SLOW TEST:20.028 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:54:47.585: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 22:54:47.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-l4gcz'
Dec  9 22:54:47.884: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  9 22:54:47.884: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Dec  9 22:54:47.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-l4gcz'
Dec  9 22:54:48.211: INFO: stderr: ""
Dec  9 22:54:48.211: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:54:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l4gcz" for this suite.
Dec  9 22:54:54.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:54:54.439: INFO: namespace: e2e-tests-kubectl-l4gcz, resource: bindings, ignored listing per whitelist
Dec  9 22:54:54.528: INFO: namespace e2e-tests-kubectl-l4gcz deletion completed in 6.290733558s

• [SLOW TEST:6.943 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:54:54.528: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Dec  9 22:54:54.762: INFO: Waiting up to 5m0s for pod "client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-containers-7bsqv" to be "success or failure"
Dec  9 22:54:54.772: INFO: Pod "client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.669679ms
Dec  9 22:54:56.778: INFO: Pod "client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015594978s
Dec  9 22:54:58.784: INFO: Pod "client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02121539s
STEP: Saw pod success
Dec  9 22:54:58.784: INFO: Pod "client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 22:54:58.789: INFO: Trying to get logs from node k8s-g2 pod client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 22:54:58.898: INFO: Waiting for pod client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 22:54:58.912: INFO: Pod client-containers-71b13d19-fc05-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:54:58.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7bsqv" for this suite.
Dec  9 22:55:04.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:55:05.043: INFO: namespace: e2e-tests-containers-7bsqv, resource: bindings, ignored listing per whitelist
Dec  9 22:55:05.095: INFO: namespace e2e-tests-containers-7bsqv deletion completed in 6.177339094s

• [SLOW TEST:10.567 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:55:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Dec  9 22:55:05.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-h4wjh'
Dec  9 22:55:05.547: INFO: stderr: ""
Dec  9 22:55:05.547: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Dec  9 22:55:06.553: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:55:06.553: INFO: Found 0 / 1
Dec  9 22:55:07.597: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:55:07.597: INFO: Found 0 / 1
Dec  9 22:55:08.553: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:55:08.553: INFO: Found 1 / 1
Dec  9 22:55:08.553: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  9 22:55:08.559: INFO: Selector matched 1 pods for map[app:redis]
Dec  9 22:55:08.559: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  9 22:55:08.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 logs redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh'
Dec  9 22:55:08.668: INFO: stderr: ""
Dec  9 22:55:08.668: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Dec 22:55:07.437 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Dec 22:55:07.437 # Server started, Redis version 3.2.12\n1:M 09 Dec 22:55:07.437 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Dec 22:55:07.437 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  9 22:55:08.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 log redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh --tail=1'
Dec  9 22:55:08.735: INFO: stderr: ""
Dec  9 22:55:08.735: INFO: stdout: "1:M 09 Dec 22:55:07.437 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  9 22:55:08.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 log redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh --limit-bytes=1'
Dec  9 22:55:08.804: INFO: stderr: ""
Dec  9 22:55:08.804: INFO: stdout: " "
STEP: exposing timestamps
Dec  9 22:55:08.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 log redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh --tail=1 --timestamps'
Dec  9 22:55:08.873: INFO: stderr: ""
Dec  9 22:55:08.873: INFO: stdout: "2018-12-09T22:55:07.437442906Z 1:M 09 Dec 22:55:07.437 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  9 22:55:11.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 log redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh --since=1s'
Dec  9 22:55:11.448: INFO: stderr: ""
Dec  9 22:55:11.448: INFO: stdout: ""
Dec  9 22:55:11.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 log redis-master-9hwtf redis-master --namespace=e2e-tests-kubectl-h4wjh --since=24h'
Dec  9 22:55:11.516: INFO: stderr: ""
Dec  9 22:55:11.516: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Dec 22:55:07.437 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Dec 22:55:07.437 # Server started, Redis version 3.2.12\n1:M 09 Dec 22:55:07.437 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Dec 22:55:07.437 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Dec  9 22:55:11.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-h4wjh'
Dec  9 22:55:11.591: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 22:55:11.591: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  9 22:55:11.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-h4wjh'
Dec  9 22:55:11.650: INFO: stderr: "No resources found.\n"
Dec  9 22:55:11.650: INFO: stdout: ""
Dec  9 22:55:11.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -l name=nginx --namespace=e2e-tests-kubectl-h4wjh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 22:55:11.703: INFO: stderr: ""
Dec  9 22:55:11.703: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:55:11.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h4wjh" for this suite.
Dec  9 22:55:17.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:55:17.867: INFO: namespace: e2e-tests-kubectl-h4wjh, resource: bindings, ignored listing per whitelist
Dec  9 22:55:18.014: INFO: namespace e2e-tests-kubectl-h4wjh deletion completed in 6.305014826s

• [SLOW TEST:12.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:55:18.014: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  9 22:55:22.888: INFO: Successfully updated pod "labelsupdate7fb61e2d-fc05-11e8-a4b0-6e57f5092bbd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:55:24.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-plnx7" for this suite.
Dec  9 22:55:48.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:55:49.054: INFO: namespace: e2e-tests-downward-api-plnx7, resource: bindings, ignored listing per whitelist
Dec  9 22:55:49.086: INFO: namespace e2e-tests-downward-api-plnx7 deletion completed in 24.172047909s

• [SLOW TEST:31.072 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:55:49.086: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wxxwc
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Dec  9 22:55:49.314: INFO: Found 0 stateful pods, waiting for 3
Dec  9 22:55:59.321: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:55:59.321: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:55:59.321: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  9 22:55:59.395: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  9 22:56:09.448: INFO: Updating stateful set ss2
Dec  9 22:56:09.483: INFO: Waiting for Pod e2e-tests-statefulset-wxxwc/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Dec  9 22:56:19.790: INFO: Found 2 stateful pods, waiting for 3
Dec  9 22:56:29.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:56:29.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 22:56:29.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  9 22:56:29.844: INFO: Updating stateful set ss2
Dec  9 22:56:29.852: INFO: Waiting for Pod e2e-tests-statefulset-wxxwc/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  9 22:56:39.885: INFO: Waiting for Pod e2e-tests-statefulset-wxxwc/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  9 22:56:49.893: INFO: Updating stateful set ss2
Dec  9 22:56:49.906: INFO: Waiting for StatefulSet e2e-tests-statefulset-wxxwc/ss2 to complete update
Dec  9 22:56:49.906: INFO: Waiting for Pod e2e-tests-statefulset-wxxwc/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  9 22:56:59.948: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wxxwc
Dec  9 22:56:59.952: INFO: Scaling statefulset ss2 to 0
Dec  9 22:57:20.007: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 22:57:20.010: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:57:20.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wxxwc" for this suite.
Dec  9 22:57:28.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:57:28.215: INFO: namespace: e2e-tests-statefulset-wxxwc, resource: bindings, ignored listing per whitelist
Dec  9 22:57:28.280: INFO: namespace e2e-tests-statefulset-wxxwc deletion completed in 8.186684454s

• [SLOW TEST:99.194 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:57:28.280: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-wtx6f
Dec  9 22:57:34.603: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-wtx6f
STEP: checking the pod's current state and verifying that restartCount is present
Dec  9 22:57:34.608: INFO: Initial restart count of pod liveness-http is 0
Dec  9 22:57:54.688: INFO: Restart count of pod e2e-tests-container-probe-wtx6f/liveness-http is now 1 (20.080291935s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:57:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wtx6f" for this suite.
Dec  9 22:58:00.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:58:00.883: INFO: namespace: e2e-tests-container-probe-wtx6f, resource: bindings, ignored listing per whitelist
Dec  9 22:58:00.981: INFO: namespace e2e-tests-container-probe-wtx6f deletion completed in 6.200678028s

• [SLOW TEST:32.701 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:58:00.982: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-k449z
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  9 22:58:01.254: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  9 22:58:24.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.175:8080/dial?request=hostName&protocol=http&host=10.244.3.174&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-k449z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:58:24.231: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:58:24.379: INFO: Waiting for endpoints: map[]
Dec  9 22:58:24.384: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.175:8080/dial?request=hostName&protocol=http&host=10.244.4.132&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-k449z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 22:58:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 22:58:24.457: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 22:58:24.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-k449z" for this suite.
Dec  9 22:58:48.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 22:58:48.652: INFO: namespace: e2e-tests-pod-network-test-k449z, resource: bindings, ignored listing per whitelist
Dec  9 22:58:48.764: INFO: namespace e2e-tests-pod-network-test-k449z deletion completed in 24.301836123s

• [SLOW TEST:47.783 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 22:58:48.765: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fd4f992e-fc05-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fd4f992e-fc05-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:00:18.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kvctp" for this suite.
Dec  9 23:00:40.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:00:40.174: INFO: namespace: e2e-tests-projected-kvctp, resource: bindings, ignored listing per whitelist
Dec  9 23:00:40.282: INFO: namespace e2e-tests-projected-kvctp deletion completed in 22.201500139s

• [SLOW TEST:111.517 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:00:40.282: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Dec  9 23:00:40.581: INFO: Waiting up to 5m0s for pod "var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-var-expansion-jhrnq" to be "success or failure"
Dec  9 23:00:40.595: INFO: Pod "var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.771469ms
Dec  9 23:00:42.600: INFO: Pod "var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019170593s
Dec  9 23:00:44.607: INFO: Pod "var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025900556s
STEP: Saw pod success
Dec  9 23:00:44.607: INFO: Pod "var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:00:44.613: INFO: Trying to get logs from node k8s-g2 pod var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 23:00:44.734: INFO: Waiting for pod var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:00:44.739: INFO: Pod var-expansion-3fd14df6-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:00:44.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jhrnq" for this suite.
Dec  9 23:00:50.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:00:50.845: INFO: namespace: e2e-tests-var-expansion-jhrnq, resource: bindings, ignored listing per whitelist
Dec  9 23:00:50.940: INFO: namespace e2e-tests-var-expansion-jhrnq deletion completed in 6.195691426s

• [SLOW TEST:10.658 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:00:50.940: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  9 23:00:59.411: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:00:59.435: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:01.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:01.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:03.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:03.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:05.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:05.441: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:07.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:07.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:09.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:09.441: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:11.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:11.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:13.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:13.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:15.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:15.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:17.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:17.441: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:19.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:19.440: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 23:01:21.435: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 23:01:21.440: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:01:21.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-cx8sb" for this suite.
Dec  9 23:01:45.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:01:45.559: INFO: namespace: e2e-tests-container-lifecycle-hook-cx8sb, resource: bindings, ignored listing per whitelist
Dec  9 23:01:45.665: INFO: namespace e2e-tests-container-lifecycle-hook-cx8sb deletion completed in 24.219010464s

• [SLOW TEST:54.725 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:01:45.665: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:01:45.878: INFO: (0) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 7.993189ms)
Dec  9 23:01:45.884: INFO: (1) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 6.228105ms)
Dec  9 23:01:45.890: INFO: (2) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.957553ms)
Dec  9 23:01:45.896: INFO: (3) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.874261ms)
Dec  9 23:01:45.902: INFO: (4) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.679846ms)
Dec  9 23:01:45.908: INFO: (5) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.60417ms)
Dec  9 23:01:45.913: INFO: (6) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.76374ms)
Dec  9 23:01:45.919: INFO: (7) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.609169ms)
Dec  9 23:01:45.925: INFO: (8) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.464641ms)
Dec  9 23:01:45.930: INFO: (9) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.721524ms)
Dec  9 23:01:45.956: INFO: (10) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 25.533067ms)
Dec  9 23:01:45.961: INFO: (11) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.569744ms)
Dec  9 23:01:45.968: INFO: (12) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 6.249425ms)
Dec  9 23:01:45.973: INFO: (13) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.457801ms)
Dec  9 23:01:45.979: INFO: (14) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.483934ms)
Dec  9 23:01:45.984: INFO: (15) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.37594ms)
Dec  9 23:01:45.989: INFO: (16) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.354686ms)
Dec  9 23:01:45.995: INFO: (17) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.536209ms)
Dec  9 23:01:46.000: INFO: (18) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.330451ms)
Dec  9 23:01:46.006: INFO: (19) /api/v1/nodes/k8s-g1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="al... (200; 5.484537ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:01:46.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-5599r" for this suite.
Dec  9 23:01:52.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:01:52.094: INFO: namespace: e2e-tests-proxy-5599r, resource: bindings, ignored listing per whitelist
Dec  9 23:01:52.242: INFO: namespace e2e-tests-proxy-5599r deletion completed in 6.233040882s

• [SLOW TEST:6.578 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:01:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:01:52.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-kkv46" to be "success or failure"
Dec  9 23:01:52.575: INFO: Pod "downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 86.90594ms
Dec  9 23:01:54.581: INFO: Pod "downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093330945s
Dec  9 23:01:56.591: INFO: Pod "downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102511742s
STEP: Saw pod success
Dec  9 23:01:56.591: INFO: Pod "downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:01:56.596: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:01:56.716: INFO: Waiting for pod downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:01:56.721: INFO: Pod downwardapi-volume-6aacaee6-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:01:56.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kkv46" for this suite.
Dec  9 23:02:02.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:02:02.906: INFO: namespace: e2e-tests-downward-api-kkv46, resource: bindings, ignored listing per whitelist
Dec  9 23:02:03.011: INFO: namespace e2e-tests-downward-api-kkv46 deletion completed in 6.284520442s

• [SLOW TEST:10.769 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:02:03.011: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  9 23:02:03.287: INFO: Waiting up to 5m0s for pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-rz6ql" to be "success or failure"
Dec  9 23:02:03.344: INFO: Pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 57.06555ms
Dec  9 23:02:05.349: INFO: Pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062545611s
Dec  9 23:02:07.355: INFO: Pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068263508s
Dec  9 23:02:09.360: INFO: Pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073857225s
STEP: Saw pod success
Dec  9 23:02:09.360: INFO: Pod "pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:02:09.365: INFO: Trying to get logs from node k8s-g2 pod pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:02:09.402: INFO: Waiting for pod pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:02:09.407: INFO: Pod pod-711967f4-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:02:09.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rz6ql" for this suite.
Dec  9 23:02:15.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:02:15.518: INFO: namespace: e2e-tests-emptydir-rz6ql, resource: bindings, ignored listing per whitelist
Dec  9 23:02:15.596: INFO: namespace e2e-tests-emptydir-rz6ql deletion completed in 6.183496525s

• [SLOW TEST:12.584 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:02:15.596: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-789d4a6c-fc06-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:02:15.904: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-sfx4q" to be "success or failure"
Dec  9 23:02:15.939: INFO: Pod "pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 35.095263ms
Dec  9 23:02:17.945: INFO: Pod "pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040937973s
Dec  9 23:02:19.952: INFO: Pod "pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048294074s
STEP: Saw pod success
Dec  9 23:02:19.952: INFO: Pod "pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:02:19.958: INFO: Trying to get logs from node k8s-g1 pod pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:02:20.046: INFO: Waiting for pod pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:02:20.052: INFO: Pod pod-projected-configmaps-78a2e150-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:02:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sfx4q" for this suite.
Dec  9 23:02:26.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:02:26.183: INFO: namespace: e2e-tests-projected-sfx4q, resource: bindings, ignored listing per whitelist
Dec  9 23:02:26.258: INFO: namespace e2e-tests-projected-sfx4q deletion completed in 6.183251764s

• [SLOW TEST:10.663 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:02:26.259: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-7ef36146-fc06-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:02:26.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-m679r" to be "success or failure"
Dec  9 23:02:26.669: INFO: Pod "pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 27.624173ms
Dec  9 23:02:28.675: INFO: Pod "pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033353668s
Dec  9 23:02:30.680: INFO: Pod "pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038742291s
STEP: Saw pod success
Dec  9 23:02:30.680: INFO: Pod "pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:02:30.685: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:02:30.778: INFO: Waiting for pod pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:02:30.783: INFO: Pod pod-configmaps-7f018931-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:02:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-m679r" for this suite.
Dec  9 23:02:36.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:02:36.948: INFO: namespace: e2e-tests-configmap-m679r, resource: bindings, ignored listing per whitelist
Dec  9 23:02:36.959: INFO: namespace e2e-tests-configmap-m679r deletion completed in 6.156796802s

• [SLOW TEST:10.700 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:02:36.959: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  9 23:02:37.182: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 23:02:37.191: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 23:02:37.195: INFO: 
Logging pods the kubelet thinks is on node k8s-g1 before test
Dec  9 23:02:37.208: INFO: ssh-server-75fc845b9d-h6g6b from default started at 2018-12-09 17:11:36 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container ssh-server ready: true, restart count 1
Dec  9 23:02:37.208: INFO: coredns-5569467fdf-x57ml from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container coredns ready: true, restart count 1
Dec  9 23:02:37.208: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-8kkg6 from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  9 23:02:37.208: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  9 23:02:37.208: INFO: kube-proxy-vv5ch from kube-system started at 2018-12-09 16:53:42 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 23:02:37.208: INFO: calico-node-xzzvt from kube-system started at 2018-12-09 16:52:48 +0000 UTC (2 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 23:02:37.208: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 23:02:37.208: INFO: sonobuoy-e2e-job-1185eb09e2a5466d from heptio-sonobuoy started at 2018-12-09 21:58:15 +0000 UTC (2 container statuses recorded)
Dec  9 23:02:37.208: INFO: 	Container e2e ready: true, restart count 0
Dec  9 23:02:37.208: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 23:02:37.208: INFO: 
Logging pods the kubelet thinks is on node k8s-g2 before test
Dec  9 23:02:37.214: INFO: kube-proxy-lw2nk from kube-system started at 2018-12-09 16:53:48 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.214: INFO: 	Container kube-proxy ready: true, restart count 1
Dec  9 23:02:37.214: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-09 21:58:11 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.214: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 23:02:37.214: INFO: calico-node-29r2w from kube-system started at 2018-12-09 16:52:49 +0000 UTC (2 container statuses recorded)
Dec  9 23:02:37.214: INFO: 	Container calico-node ready: true, restart count 1
Dec  9 23:02:37.214: INFO: 	Container install-cni ready: true, restart count 1
Dec  9 23:02:37.214: INFO: sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-r6m2t from heptio-sonobuoy started at 2018-12-09 21:58:16 +0000 UTC (2 container statuses recorded)
Dec  9 23:02:37.214: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  9 23:02:37.214: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  9 23:02:37.214: INFO: coredns-5569467fdf-cj6br from kube-system started at 2018-12-09 17:22:30 +0000 UTC (1 container statuses recorded)
Dec  9 23:02:37.214: INFO: 	Container coredns ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node k8s-g1
STEP: verifying the node has the label node k8s-g2
Dec  9 23:02:37.424: INFO: Pod ssh-server-75fc845b9d-h6g6b requesting resource cpu=0m on Node k8s-g1
Dec  9 23:02:37.424: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-g2
Dec  9 23:02:37.424: INFO: Pod sonobuoy-e2e-job-1185eb09e2a5466d requesting resource cpu=0m on Node k8s-g1
Dec  9 23:02:37.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-8kkg6 requesting resource cpu=0m on Node k8s-g1
Dec  9 23:02:37.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-071fc14130704ee8-r6m2t requesting resource cpu=0m on Node k8s-g2
Dec  9 23:02:37.424: INFO: Pod calico-node-29r2w requesting resource cpu=250m on Node k8s-g2
Dec  9 23:02:37.424: INFO: Pod calico-node-xzzvt requesting resource cpu=250m on Node k8s-g1
Dec  9 23:02:37.424: INFO: Pod coredns-5569467fdf-cj6br requesting resource cpu=100m on Node k8s-g2
Dec  9 23:02:37.424: INFO: Pod coredns-5569467fdf-x57ml requesting resource cpu=100m on Node k8s-g1
Dec  9 23:02:37.424: INFO: Pod kube-proxy-lw2nk requesting resource cpu=0m on Node k8s-g2
Dec  9 23:02:37.424: INFO: Pod kube-proxy-vv5ch requesting resource cpu=0m on Node k8s-g1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857b1fb0-fc06-11e8-a4b0-6e57f5092bbd.156ecc7c8a3f2b64], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-lxfsp/filler-pod-857b1fb0-fc06-11e8-a4b0-6e57f5092bbd to k8s-g1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857b1fb0-fc06-11e8-a4b0-6e57f5092bbd.156ecc7ce5b6a0b4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857b1fb0-fc06-11e8-a4b0-6e57f5092bbd.156ecc7cf467f645], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857b1fb0-fc06-11e8-a4b0-6e57f5092bbd.156ecc7d12d828e3], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8581e714-fc06-11e8-a4b0-6e57f5092bbd.156ecc7c903e06d8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-lxfsp/filler-pod-8581e714-fc06-11e8-a4b0-6e57f5092bbd to k8s-g2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8581e714-fc06-11e8-a4b0-6e57f5092bbd.156ecc7cea1dd7a9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8581e714-fc06-11e8-a4b0-6e57f5092bbd.156ecc7cf6f7aabb], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8581e714-fc06-11e8-a4b0-6e57f5092bbd.156ecc7d0af26113], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.156ecc7d86a403c3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node k8s-g1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-g2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:02:42.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-lxfsp" for this suite.
Dec  9 23:02:48.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:02:49.091: INFO: namespace: e2e-tests-sched-pred-lxfsp, resource: bindings, ignored listing per whitelist
Dec  9 23:02:49.107: INFO: namespace e2e-tests-sched-pred-lxfsp deletion completed in 6.185718597s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.148 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:02:49.107: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Dec  9 23:02:49.873: INFO: created pod pod-service-account-defaultsa
Dec  9 23:02:49.873: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  9 23:02:49.946: INFO: created pod pod-service-account-mountsa
Dec  9 23:02:49.946: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  9 23:02:49.979: INFO: created pod pod-service-account-nomountsa
Dec  9 23:02:49.979: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  9 23:02:50.016: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  9 23:02:50.016: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  9 23:02:50.080: INFO: created pod pod-service-account-mountsa-mountspec
Dec  9 23:02:50.080: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  9 23:02:50.230: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  9 23:02:50.230: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  9 23:02:50.298: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  9 23:02:50.299: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  9 23:02:50.395: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  9 23:02:50.395: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  9 23:02:50.463: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  9 23:02:50.463: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:02:50.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-htpjj" for this suite.
Dec  9 23:03:14.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:03:14.869: INFO: namespace: e2e-tests-svcaccounts-htpjj, resource: bindings, ignored listing per whitelist
Dec  9 23:03:15.020: INFO: namespace e2e-tests-svcaccounts-htpjj deletion completed in 24.526529127s

• [SLOW TEST:25.912 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:03:15.020: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:03:19.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hwwr5" for this suite.
Dec  9 23:04:03.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:04:03.701: INFO: namespace: e2e-tests-kubelet-test-hwwr5, resource: bindings, ignored listing per whitelist
Dec  9 23:04:03.715: INFO: namespace e2e-tests-kubelet-test-hwwr5 deletion completed in 44.263999376s

• [SLOW TEST:48.695 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:04:03.715: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b90e7605-fc06-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:04:04.027: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-7q5v4" to be "success or failure"
Dec  9 23:04:04.084: INFO: Pod "pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.731941ms
Dec  9 23:04:06.089: INFO: Pod "pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06189331s
Dec  9 23:04:08.094: INFO: Pod "pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067203886s
STEP: Saw pod success
Dec  9 23:04:08.094: INFO: Pod "pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:04:08.099: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:04:08.218: INFO: Waiting for pod pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:04:08.223: INFO: Pod pod-configmaps-b9171d85-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:04:08.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7q5v4" for this suite.
Dec  9 23:04:14.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:04:14.356: INFO: namespace: e2e-tests-configmap-7q5v4, resource: bindings, ignored listing per whitelist
Dec  9 23:04:14.488: INFO: namespace e2e-tests-configmap-7q5v4 deletion completed in 6.250715198s

• [SLOW TEST:10.773 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:04:14.488: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-6gtg
STEP: Creating a pod to test atomic-volume-subpath
Dec  9 23:04:14.775: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6gtg" in namespace "e2e-tests-subpath-qr5bf" to be "success or failure"
Dec  9 23:04:14.852: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Pending", Reason="", readiness=false. Elapsed: 77.280071ms
Dec  9 23:04:16.859: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08336484s
Dec  9 23:04:18.864: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089219445s
Dec  9 23:04:20.870: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 6.094800559s
Dec  9 23:04:22.876: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 8.100745648s
Dec  9 23:04:24.882: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 10.106319829s
Dec  9 23:04:26.888: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 12.113121143s
Dec  9 23:04:28.894: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 14.119167391s
Dec  9 23:04:30.900: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 16.124418596s
Dec  9 23:04:32.905: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 18.130149834s
Dec  9 23:04:34.911: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 20.135986059s
Dec  9 23:04:36.918: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Running", Reason="", readiness=false. Elapsed: 22.142857612s
Dec  9 23:04:38.925: INFO: Pod "pod-subpath-test-configmap-6gtg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.149578685s
STEP: Saw pod success
Dec  9 23:04:38.925: INFO: Pod "pod-subpath-test-configmap-6gtg" satisfied condition "success or failure"
Dec  9 23:04:38.932: INFO: Trying to get logs from node k8s-g2 pod pod-subpath-test-configmap-6gtg container test-container-subpath-configmap-6gtg: <nil>
STEP: delete the pod
Dec  9 23:04:39.028: INFO: Waiting for pod pod-subpath-test-configmap-6gtg to disappear
Dec  9 23:04:39.033: INFO: Pod pod-subpath-test-configmap-6gtg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6gtg
Dec  9 23:04:39.033: INFO: Deleting pod "pod-subpath-test-configmap-6gtg" in namespace "e2e-tests-subpath-qr5bf"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:04:39.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qr5bf" for this suite.
Dec  9 23:04:45.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:04:45.171: INFO: namespace: e2e-tests-subpath-qr5bf, resource: bindings, ignored listing per whitelist
Dec  9 23:04:45.293: INFO: namespace e2e-tests-subpath-qr5bf deletion completed in 6.24481336s

• [SLOW TEST:30.805 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:04:45.293: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  9 23:04:45.512: INFO: Waiting up to 5m0s for pod "downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-r5v8d" to be "success or failure"
Dec  9 23:04:45.609: INFO: Pod "downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 96.782756ms
Dec  9 23:04:47.615: INFO: Pod "downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10252271s
Dec  9 23:04:49.620: INFO: Pod "downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107870218s
STEP: Saw pod success
Dec  9 23:04:49.620: INFO: Pod "downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:04:49.625: INFO: Trying to get logs from node k8s-g1 pod downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 23:04:49.690: INFO: Waiting for pod downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:04:49.696: INFO: Pod downward-api-d1d0b17a-fc06-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:04:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r5v8d" for this suite.
Dec  9 23:04:55.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:04:55.888: INFO: namespace: e2e-tests-downward-api-r5v8d, resource: bindings, ignored listing per whitelist
Dec  9 23:04:55.888: INFO: namespace e2e-tests-downward-api-r5v8d deletion completed in 6.186219477s

• [SLOW TEST:10.594 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:04:55.888: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  9 23:05:01.087: INFO: Successfully updated pod "annotationupdated8421c2b-fc06-11e8-a4b0-6e57f5092bbd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:05:03.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5s26f" for this suite.
Dec  9 23:05:27.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:05:27.283: INFO: namespace: e2e-tests-downward-api-5s26f, resource: bindings, ignored listing per whitelist
Dec  9 23:05:27.322: INFO: namespace e2e-tests-downward-api-5s26f deletion completed in 24.193400867s

• [SLOW TEST:31.434 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:05:27.322: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2q7gc
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-2q7gc
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-2q7gc
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-2q7gc
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-2q7gc
Dec  9 23:05:31.732: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-2q7gc, name: ss-0, uid: eb5370f3-fc06-11e8-9811-448a5b81d79a, status phase: Pending. Waiting for statefulset controller to delete.
Dec  9 23:05:31.846: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-2q7gc, name: ss-0, uid: eb5370f3-fc06-11e8-9811-448a5b81d79a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  9 23:05:31.888: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-2q7gc, name: ss-0, uid: eb5370f3-fc06-11e8-9811-448a5b81d79a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  9 23:05:31.978: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-2q7gc
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-2q7gc
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-2q7gc and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  9 23:05:42.253: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2q7gc
Dec  9 23:05:42.257: INFO: Scaling statefulset ss to 0
Dec  9 23:05:52.290: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 23:05:52.294: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:05:52.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2q7gc" for this suite.
Dec  9 23:06:00.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:06:00.537: INFO: namespace: e2e-tests-statefulset-2q7gc, resource: bindings, ignored listing per whitelist
Dec  9 23:06:00.551: INFO: namespace e2e-tests-statefulset-2q7gc deletion completed in 8.163136108s

• [SLOW TEST:33.229 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:06:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:06:05.067: INFO: Waiting up to 5m0s for pod "client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-pods-t86x4" to be "success or failure"
Dec  9 23:06:05.078: INFO: Pod "client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.98189ms
Dec  9 23:06:07.083: INFO: Pod "client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016478702s
Dec  9 23:06:09.089: INFO: Pod "client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021686779s
STEP: Saw pod success
Dec  9 23:06:09.089: INFO: Pod "client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:06:09.093: INFO: Trying to get logs from node k8s-g2 pod client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd container env3cont: <nil>
STEP: delete the pod
Dec  9 23:06:09.281: INFO: Waiting for pod client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:06:09.285: INFO: Pod client-envvars-013baf3e-fc07-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:06:09.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-t86x4" for this suite.
Dec  9 23:06:53.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:06:53.404: INFO: namespace: e2e-tests-pods-t86x4, resource: bindings, ignored listing per whitelist
Dec  9 23:06:53.463: INFO: namespace e2e-tests-pods-t86x4 deletion completed in 44.172538542s

• [SLOW TEST:52.912 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:06:53.463: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  9 23:06:53.689: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:06:58.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-lrwlm" for this suite.
Dec  9 23:07:06.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:07:07.016: INFO: namespace: e2e-tests-init-container-lrwlm, resource: bindings, ignored listing per whitelist
Dec  9 23:07:07.076: INFO: namespace e2e-tests-init-container-lrwlm deletion completed in 8.315977423s

• [SLOW TEST:13.612 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:07:07.076: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-56pm2
Dec  9 23:07:11.405: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-56pm2
STEP: checking the pod's current state and verifying that restartCount is present
Dec  9 23:07:11.411: INFO: Initial restart count of pod liveness-http is 0
Dec  9 23:07:21.477: INFO: Restart count of pod e2e-tests-container-probe-56pm2/liveness-http is now 1 (10.065847162s elapsed)
Dec  9 23:07:41.568: INFO: Restart count of pod e2e-tests-container-probe-56pm2/liveness-http is now 2 (30.157190549s elapsed)
Dec  9 23:08:01.628: INFO: Restart count of pod e2e-tests-container-probe-56pm2/liveness-http is now 3 (50.217180002s elapsed)
Dec  9 23:08:21.721: INFO: Restart count of pod e2e-tests-container-probe-56pm2/liveness-http is now 4 (1m10.310060593s elapsed)
Dec  9 23:09:22.076: INFO: Restart count of pod e2e-tests-container-probe-56pm2/liveness-http is now 5 (2m10.665434952s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:09:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-56pm2" for this suite.
Dec  9 23:09:28.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:09:28.354: INFO: namespace: e2e-tests-container-probe-56pm2, resource: bindings, ignored listing per whitelist
Dec  9 23:09:28.373: INFO: namespace e2e-tests-container-probe-56pm2 deletion completed in 6.169312093s

• [SLOW TEST:141.297 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:09:28.373: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-7a9506a9-fc07-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:09:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bx4nr" for this suite.
Dec  9 23:09:54.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:09:54.957: INFO: namespace: e2e-tests-configmap-bx4nr, resource: bindings, ignored listing per whitelist
Dec  9 23:09:54.986: INFO: namespace e2e-tests-configmap-bx4nr deletion completed in 22.166529532s

• [SLOW TEST:26.613 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:09:54.986: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-pjzs7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-pjzs7 to expose endpoints map[]
Dec  9 23:09:55.305: INFO: Get endpoints failed (26.588776ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec  9 23:09:56.311: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-pjzs7 exposes endpoints map[] (1.032830945s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-pjzs7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-pjzs7 to expose endpoints map[pod1:[100]]
Dec  9 23:09:59.429: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-pjzs7 exposes endpoints map[pod1:[100]] (3.06492035s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-pjzs7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-pjzs7 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  9 23:10:02.763: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-pjzs7 exposes endpoints map[pod1:[100] pod2:[101]] (3.232912205s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-pjzs7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-pjzs7 to expose endpoints map[pod2:[101]]
Dec  9 23:10:02.885: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-pjzs7 exposes endpoints map[pod2:[101]] (78.463446ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-pjzs7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-pjzs7 to expose endpoints map[]
Dec  9 23:10:02.922: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-pjzs7 exposes endpoints map[] (8.883226ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:10:03.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-pjzs7" for this suite.
Dec  9 23:10:27.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:10:27.306: INFO: namespace: e2e-tests-services-pjzs7, resource: bindings, ignored listing per whitelist
Dec  9 23:10:27.324: INFO: namespace e2e-tests-services-pjzs7 deletion completed in 24.191525917s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:32.338 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:10:27.325: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-lqggv
I1209 23:10:27.734925      17 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-lqggv, replica count: 1
I1209 23:10:28.785239      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:10:29.785456      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:10:30.785623      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 23:10:31.044: INFO: Created: latency-svc-9wdgv
Dec  9 23:10:31.062: INFO: Got endpoints: latency-svc-9wdgv [176.392153ms]
Dec  9 23:10:31.259: INFO: Created: latency-svc-7lngn
Dec  9 23:10:31.320: INFO: Got endpoints: latency-svc-7lngn [258.671226ms]
Dec  9 23:10:31.395: INFO: Created: latency-svc-g2dfp
Dec  9 23:10:31.445: INFO: Got endpoints: latency-svc-g2dfp [383.498417ms]
Dec  9 23:10:31.553: INFO: Created: latency-svc-bcvc5
Dec  9 23:10:31.605: INFO: Got endpoints: latency-svc-bcvc5 [543.123166ms]
Dec  9 23:10:31.749: INFO: Created: latency-svc-p254d
Dec  9 23:10:31.749: INFO: Got endpoints: latency-svc-p254d [687.118302ms]
Dec  9 23:10:31.804: INFO: Created: latency-svc-p96r6
Dec  9 23:10:31.862: INFO: Got endpoints: latency-svc-p96r6 [800.066894ms]
Dec  9 23:10:31.960: INFO: Created: latency-svc-5fhs8
Dec  9 23:10:32.020: INFO: Got endpoints: latency-svc-5fhs8 [958.297746ms]
Dec  9 23:10:32.177: INFO: Created: latency-svc-b552s
Dec  9 23:10:32.177: INFO: Got endpoints: latency-svc-b552s [1.114760113s]
Dec  9 23:10:32.255: INFO: Created: latency-svc-ws725
Dec  9 23:10:32.370: INFO: Got endpoints: latency-svc-ws725 [1.308309056s]
Dec  9 23:10:32.440: INFO: Created: latency-svc-7wght
Dec  9 23:10:32.449: INFO: Got endpoints: latency-svc-7wght [1.38662864s]
Dec  9 23:10:32.602: INFO: Created: latency-svc-zlm6m
Dec  9 23:10:32.626: INFO: Got endpoints: latency-svc-zlm6m [1.563475232s]
Dec  9 23:10:32.828: INFO: Created: latency-svc-ghnlj
Dec  9 23:10:32.945: INFO: Got endpoints: latency-svc-ghnlj [1.883522266s]
Dec  9 23:10:33.027: INFO: Created: latency-svc-b67jn
Dec  9 23:10:33.180: INFO: Got endpoints: latency-svc-b67jn [2.118252132s]
Dec  9 23:10:33.260: INFO: Created: latency-svc-bfttg
Dec  9 23:10:33.276: INFO: Got endpoints: latency-svc-bfttg [2.214372593s]
Dec  9 23:10:33.510: INFO: Created: latency-svc-mc98d
Dec  9 23:10:33.557: INFO: Got endpoints: latency-svc-mc98d [2.495420076s]
Dec  9 23:10:33.695: INFO: Created: latency-svc-w98pj
Dec  9 23:10:33.710: INFO: Got endpoints: latency-svc-w98pj [2.647844365s]
Dec  9 23:10:33.761: INFO: Created: latency-svc-pcksg
Dec  9 23:10:33.829: INFO: Got endpoints: latency-svc-pcksg [2.50822559s]
Dec  9 23:10:33.894: INFO: Created: latency-svc-bpcvn
Dec  9 23:10:33.902: INFO: Got endpoints: latency-svc-bpcvn [2.456875723s]
Dec  9 23:10:34.027: INFO: Created: latency-svc-qdknk
Dec  9 23:10:34.045: INFO: Got endpoints: latency-svc-qdknk [2.439605394s]
Dec  9 23:10:34.211: INFO: Created: latency-svc-vj2jg
Dec  9 23:10:34.220: INFO: Got endpoints: latency-svc-vj2jg [2.471088736s]
Dec  9 23:10:34.280: INFO: Created: latency-svc-wpm68
Dec  9 23:10:34.618: INFO: Got endpoints: latency-svc-wpm68 [2.755983286s]
Dec  9 23:10:34.659: INFO: Created: latency-svc-sk6qq
Dec  9 23:10:34.700: INFO: Got endpoints: latency-svc-sk6qq [2.679912406s]
Dec  9 23:10:34.789: INFO: Created: latency-svc-wwftm
Dec  9 23:10:34.806: INFO: Got endpoints: latency-svc-wwftm [2.629195265s]
Dec  9 23:10:34.865: INFO: Created: latency-svc-bbqnj
Dec  9 23:10:34.979: INFO: Got endpoints: latency-svc-bbqnj [2.60821037s]
Dec  9 23:10:35.023: INFO: Created: latency-svc-4q7w8
Dec  9 23:10:35.033: INFO: Got endpoints: latency-svc-4q7w8 [2.584071746s]
Dec  9 23:10:35.164: INFO: Created: latency-svc-2nlht
Dec  9 23:10:35.201: INFO: Got endpoints: latency-svc-2nlht [2.575458354s]
Dec  9 23:10:35.364: INFO: Created: latency-svc-fwm75
Dec  9 23:10:35.364: INFO: Got endpoints: latency-svc-fwm75 [2.418844715s]
Dec  9 23:10:35.433: INFO: Created: latency-svc-5cwpx
Dec  9 23:10:35.444: INFO: Got endpoints: latency-svc-5cwpx [2.264149889s]
Dec  9 23:10:35.566: INFO: Created: latency-svc-nrz2t
Dec  9 23:10:35.579: INFO: Got endpoints: latency-svc-nrz2t [2.302466766s]
Dec  9 23:10:35.634: INFO: Created: latency-svc-k7dx4
Dec  9 23:10:35.729: INFO: Got endpoints: latency-svc-k7dx4 [2.171184576s]
Dec  9 23:10:35.842: INFO: Created: latency-svc-d5rp8
Dec  9 23:10:35.978: INFO: Got endpoints: latency-svc-d5rp8 [2.268651195s]
Dec  9 23:10:36.090: INFO: Created: latency-svc-th7dv
Dec  9 23:10:36.105: INFO: Got endpoints: latency-svc-th7dv [2.276650364s]
Dec  9 23:10:36.302: INFO: Created: latency-svc-bg947
Dec  9 23:10:36.378: INFO: Got endpoints: latency-svc-bg947 [2.476172801s]
Dec  9 23:10:36.494: INFO: Created: latency-svc-r6vwx
Dec  9 23:10:36.637: INFO: Got endpoints: latency-svc-r6vwx [2.592722325s]
Dec  9 23:10:36.750: INFO: Created: latency-svc-hf2jv
Dec  9 23:10:36.871: INFO: Got endpoints: latency-svc-hf2jv [2.650832888s]
Dec  9 23:10:37.045: INFO: Created: latency-svc-g6h45
Dec  9 23:10:37.059: INFO: Got endpoints: latency-svc-g6h45 [2.441167472s]
Dec  9 23:10:37.210: INFO: Created: latency-svc-ks8c8
Dec  9 23:10:37.222: INFO: Got endpoints: latency-svc-ks8c8 [2.52141597s]
Dec  9 23:10:37.372: INFO: Created: latency-svc-sg447
Dec  9 23:10:37.439: INFO: Got endpoints: latency-svc-sg447 [2.632957704s]
Dec  9 23:10:37.602: INFO: Created: latency-svc-prjs4
Dec  9 23:10:37.623: INFO: Got endpoints: latency-svc-prjs4 [2.644359605s]
Dec  9 23:10:37.787: INFO: Created: latency-svc-k4s5l
Dec  9 23:10:37.799: INFO: Got endpoints: latency-svc-k4s5l [2.766104694s]
Dec  9 23:10:38.049: INFO: Created: latency-svc-krsb2
Dec  9 23:10:38.095: INFO: Got endpoints: latency-svc-krsb2 [2.893993741s]
Dec  9 23:10:38.147: INFO: Created: latency-svc-7wpzh
Dec  9 23:10:38.290: INFO: Got endpoints: latency-svc-7wpzh [2.925930173s]
Dec  9 23:10:38.333: INFO: Created: latency-svc-d9q8s
Dec  9 23:10:38.343: INFO: Got endpoints: latency-svc-d9q8s [2.898641445s]
Dec  9 23:10:38.506: INFO: Created: latency-svc-mmp2g
Dec  9 23:10:38.526: INFO: Got endpoints: latency-svc-mmp2g [2.94733086s]
Dec  9 23:10:38.652: INFO: Created: latency-svc-fm9gr
Dec  9 23:10:38.695: INFO: Got endpoints: latency-svc-fm9gr [2.966345006s]
Dec  9 23:10:38.859: INFO: Created: latency-svc-wgkl5
Dec  9 23:10:38.902: INFO: Got endpoints: latency-svc-wgkl5 [2.923498071s]
Dec  9 23:10:39.076: INFO: Created: latency-svc-clvvg
Dec  9 23:10:39.094: INFO: Got endpoints: latency-svc-clvvg [2.98825338s]
Dec  9 23:10:39.244: INFO: Created: latency-svc-4tzmb
Dec  9 23:10:39.284: INFO: Got endpoints: latency-svc-4tzmb [2.905190766s]
Dec  9 23:10:39.439: INFO: Created: latency-svc-k8gl7
Dec  9 23:10:39.597: INFO: Got endpoints: latency-svc-k8gl7 [2.959772923s]
Dec  9 23:10:39.612: INFO: Created: latency-svc-cbn8b
Dec  9 23:10:39.632: INFO: Got endpoints: latency-svc-cbn8b [2.760779357s]
Dec  9 23:10:39.811: INFO: Created: latency-svc-wwxtp
Dec  9 23:10:39.811: INFO: Got endpoints: latency-svc-wwxtp [2.751796827s]
Dec  9 23:10:39.863: INFO: Created: latency-svc-j68w2
Dec  9 23:10:40.029: INFO: Got endpoints: latency-svc-j68w2 [2.806919987s]
Dec  9 23:10:40.050: INFO: Created: latency-svc-zshhz
Dec  9 23:10:40.064: INFO: Got endpoints: latency-svc-zshhz [2.625456251s]
Dec  9 23:10:40.122: INFO: Created: latency-svc-dspfd
Dec  9 23:10:40.245: INFO: Got endpoints: latency-svc-dspfd [2.622093893s]
Dec  9 23:10:40.269: INFO: Created: latency-svc-5v25d
Dec  9 23:10:40.308: INFO: Got endpoints: latency-svc-5v25d [2.508144166s]
Dec  9 23:10:40.440: INFO: Created: latency-svc-74ldq
Dec  9 23:10:40.455: INFO: Got endpoints: latency-svc-74ldq [2.359997838s]
Dec  9 23:10:40.516: INFO: Created: latency-svc-r22x8
Dec  9 23:10:40.603: INFO: Got endpoints: latency-svc-r22x8 [2.313172122s]
Dec  9 23:10:40.666: INFO: Created: latency-svc-s6gtp
Dec  9 23:10:40.795: INFO: Got endpoints: latency-svc-s6gtp [2.452095477s]
Dec  9 23:10:40.831: INFO: Created: latency-svc-jh2r6
Dec  9 23:10:40.843: INFO: Got endpoints: latency-svc-jh2r6 [2.317143434s]
Dec  9 23:10:41.003: INFO: Created: latency-svc-4677p
Dec  9 23:10:41.123: INFO: Got endpoints: latency-svc-4677p [2.427707423s]
Dec  9 23:10:41.232: INFO: Created: latency-svc-9sbsf
Dec  9 23:10:41.395: INFO: Created: latency-svc-hpbm9
Dec  9 23:10:41.435: INFO: Got endpoints: latency-svc-9sbsf [2.532754066s]
Dec  9 23:10:41.441: INFO: Got endpoints: latency-svc-hpbm9 [2.347710853s]
Dec  9 23:10:41.569: INFO: Created: latency-svc-lhz2c
Dec  9 23:10:41.642: INFO: Got endpoints: latency-svc-lhz2c [2.358372082s]
Dec  9 23:10:41.728: INFO: Created: latency-svc-lzvtp
Dec  9 23:10:41.771: INFO: Got endpoints: latency-svc-lzvtp [2.173551241s]
Dec  9 23:10:41.822: INFO: Created: latency-svc-q7hb7
Dec  9 23:10:41.830: INFO: Got endpoints: latency-svc-q7hb7 [2.198094107s]
Dec  9 23:10:41.977: INFO: Created: latency-svc-wbppm
Dec  9 23:10:42.021: INFO: Got endpoints: latency-svc-wbppm [2.209672437s]
Dec  9 23:10:42.146: INFO: Created: latency-svc-h8vpq
Dec  9 23:10:42.155: INFO: Got endpoints: latency-svc-h8vpq [2.126012239s]
Dec  9 23:10:42.329: INFO: Created: latency-svc-6kpfj
Dec  9 23:10:42.329: INFO: Got endpoints: latency-svc-6kpfj [2.264148192s]
Dec  9 23:10:42.410: INFO: Created: latency-svc-4lnvf
Dec  9 23:10:42.570: INFO: Got endpoints: latency-svc-4lnvf [2.324786888s]
Dec  9 23:10:42.614: INFO: Created: latency-svc-djhzq
Dec  9 23:10:42.633: INFO: Got endpoints: latency-svc-djhzq [2.324648103s]
Dec  9 23:10:42.879: INFO: Created: latency-svc-pwqrq
Dec  9 23:10:42.987: INFO: Got endpoints: latency-svc-pwqrq [2.531797371s]
Dec  9 23:10:43.053: INFO: Created: latency-svc-6vx6j
Dec  9 23:10:43.195: INFO: Got endpoints: latency-svc-6vx6j [2.591592212s]
Dec  9 23:10:43.217: INFO: Created: latency-svc-dhdnj
Dec  9 23:10:43.282: INFO: Got endpoints: latency-svc-dhdnj [2.486731994s]
Dec  9 23:10:43.414: INFO: Created: latency-svc-rd7jh
Dec  9 23:10:43.449: INFO: Got endpoints: latency-svc-rd7jh [2.605831043s]
Dec  9 23:10:43.557: INFO: Created: latency-svc-dzhj7
Dec  9 23:10:43.577: INFO: Got endpoints: latency-svc-dzhj7 [2.454366523s]
Dec  9 23:10:43.704: INFO: Created: latency-svc-fmn27
Dec  9 23:10:43.712: INFO: Got endpoints: latency-svc-fmn27 [2.276894481s]
Dec  9 23:10:43.778: INFO: Created: latency-svc-th25m
Dec  9 23:10:43.795: INFO: Got endpoints: latency-svc-th25m [2.353537647s]
Dec  9 23:10:43.945: INFO: Created: latency-svc-gzd8n
Dec  9 23:10:43.962: INFO: Got endpoints: latency-svc-gzd8n [2.320181491s]
Dec  9 23:10:44.178: INFO: Created: latency-svc-l582v
Dec  9 23:10:44.250: INFO: Got endpoints: latency-svc-l582v [2.479497312s]
Dec  9 23:10:44.418: INFO: Created: latency-svc-g7vcs
Dec  9 23:10:44.430: INFO: Got endpoints: latency-svc-g7vcs [2.599860808s]
Dec  9 23:10:44.570: INFO: Created: latency-svc-th5vx
Dec  9 23:10:44.584: INFO: Got endpoints: latency-svc-th5vx [2.563517278s]
Dec  9 23:10:44.658: INFO: Created: latency-svc-4ssv8
Dec  9 23:10:44.783: INFO: Got endpoints: latency-svc-4ssv8 [2.62851438s]
Dec  9 23:10:44.878: INFO: Created: latency-svc-tqhgm
Dec  9 23:10:44.943: INFO: Got endpoints: latency-svc-tqhgm [2.613979819s]
Dec  9 23:10:45.046: INFO: Created: latency-svc-p7z7b
Dec  9 23:10:45.111: INFO: Got endpoints: latency-svc-p7z7b [2.541214411s]
Dec  9 23:10:45.224: INFO: Created: latency-svc-pjl5w
Dec  9 23:10:45.239: INFO: Got endpoints: latency-svc-pjl5w [2.605909077s]
Dec  9 23:10:45.300: INFO: Created: latency-svc-cc4rf
Dec  9 23:10:45.403: INFO: Got endpoints: latency-svc-cc4rf [2.416460548s]
Dec  9 23:10:45.561: INFO: Created: latency-svc-4lmmh
Dec  9 23:10:45.618: INFO: Got endpoints: latency-svc-4lmmh [2.423269973s]
Dec  9 23:10:45.788: INFO: Created: latency-svc-8pdx2
Dec  9 23:10:45.788: INFO: Got endpoints: latency-svc-8pdx2 [2.506453347s]
Dec  9 23:10:45.860: INFO: Created: latency-svc-rzffk
Dec  9 23:10:46.103: INFO: Got endpoints: latency-svc-rzffk [2.654163029s]
Dec  9 23:10:46.211: INFO: Created: latency-svc-4lp7m
Dec  9 23:10:46.211: INFO: Got endpoints: latency-svc-4lp7m [2.63396906s]
Dec  9 23:10:46.370: INFO: Created: latency-svc-lnl2s
Dec  9 23:10:46.520: INFO: Got endpoints: latency-svc-lnl2s [2.808021798s]
Dec  9 23:10:46.540: INFO: Created: latency-svc-x28c6
Dec  9 23:10:46.701: INFO: Got endpoints: latency-svc-x28c6 [2.905660528s]
Dec  9 23:10:46.747: INFO: Created: latency-svc-ck97n
Dec  9 23:10:46.780: INFO: Got endpoints: latency-svc-ck97n [2.817930991s]
Dec  9 23:10:46.929: INFO: Created: latency-svc-b44gl
Dec  9 23:10:46.939: INFO: Got endpoints: latency-svc-b44gl [2.688472493s]
Dec  9 23:10:47.145: INFO: Created: latency-svc-vrtv9
Dec  9 23:10:47.156: INFO: Got endpoints: latency-svc-vrtv9 [2.726262384s]
Dec  9 23:10:47.332: INFO: Created: latency-svc-zn7qf
Dec  9 23:10:47.350: INFO: Got endpoints: latency-svc-zn7qf [2.765323942s]
Dec  9 23:10:47.520: INFO: Created: latency-svc-svw9r
Dec  9 23:10:47.525: INFO: Got endpoints: latency-svc-svw9r [2.742165951s]
Dec  9 23:10:47.703: INFO: Created: latency-svc-5lsxd
Dec  9 23:10:47.718: INFO: Got endpoints: latency-svc-5lsxd [2.774842054s]
Dec  9 23:10:47.864: INFO: Created: latency-svc-htmcl
Dec  9 23:10:47.889: INFO: Got endpoints: latency-svc-htmcl [2.777447103s]
Dec  9 23:10:48.042: INFO: Created: latency-svc-82bml
Dec  9 23:10:48.056: INFO: Got endpoints: latency-svc-82bml [2.816551228s]
Dec  9 23:10:48.187: INFO: Created: latency-svc-6l4m8
Dec  9 23:10:48.224: INFO: Got endpoints: latency-svc-6l4m8 [2.820943583s]
Dec  9 23:10:48.428: INFO: Created: latency-svc-n57ln
Dec  9 23:10:48.603: INFO: Got endpoints: latency-svc-n57ln [2.984724678s]
Dec  9 23:10:48.625: INFO: Created: latency-svc-ztzxt
Dec  9 23:10:48.665: INFO: Got endpoints: latency-svc-ztzxt [2.877056399s]
Dec  9 23:10:48.791: INFO: Created: latency-svc-zsfh7
Dec  9 23:10:48.804: INFO: Got endpoints: latency-svc-zsfh7 [2.701249397s]
Dec  9 23:10:48.945: INFO: Created: latency-svc-556b5
Dec  9 23:10:48.988: INFO: Got endpoints: latency-svc-556b5 [2.776611932s]
Dec  9 23:10:49.188: INFO: Created: latency-svc-ns6ft
Dec  9 23:10:49.278: INFO: Got endpoints: latency-svc-ns6ft [2.758467361s]
Dec  9 23:10:49.437: INFO: Created: latency-svc-8zrrk
Dec  9 23:10:49.461: INFO: Got endpoints: latency-svc-8zrrk [2.760739034s]
Dec  9 23:10:49.588: INFO: Created: latency-svc-pz5ss
Dec  9 23:10:49.678: INFO: Got endpoints: latency-svc-pz5ss [2.897693537s]
Dec  9 23:10:49.794: INFO: Created: latency-svc-nxbtc
Dec  9 23:10:49.875: INFO: Got endpoints: latency-svc-nxbtc [2.935615823s]
Dec  9 23:10:50.014: INFO: Created: latency-svc-82dnv
Dec  9 23:10:50.087: INFO: Got endpoints: latency-svc-82dnv [2.930380934s]
Dec  9 23:10:50.208: INFO: Created: latency-svc-tgc5v
Dec  9 23:10:50.257: INFO: Got endpoints: latency-svc-tgc5v [2.90729439s]
Dec  9 23:10:50.383: INFO: Created: latency-svc-9fb8t
Dec  9 23:10:50.520: INFO: Got endpoints: latency-svc-9fb8t [2.994408328s]
Dec  9 23:10:50.605: INFO: Created: latency-svc-9brcb
Dec  9 23:10:50.745: INFO: Got endpoints: latency-svc-9brcb [3.027247109s]
Dec  9 23:10:50.797: INFO: Created: latency-svc-zc6jf
Dec  9 23:10:50.838: INFO: Got endpoints: latency-svc-zc6jf [2.949455172s]
Dec  9 23:10:50.995: INFO: Created: latency-svc-vz94d
Dec  9 23:10:51.012: INFO: Got endpoints: latency-svc-vz94d [2.95663372s]
Dec  9 23:10:51.192: INFO: Created: latency-svc-w9j9v
Dec  9 23:10:51.205: INFO: Got endpoints: latency-svc-w9j9v [2.980288436s]
Dec  9 23:10:51.262: INFO: Created: latency-svc-9w4vm
Dec  9 23:10:51.361: INFO: Got endpoints: latency-svc-9w4vm [2.758250638s]
Dec  9 23:10:51.556: INFO: Created: latency-svc-5k5zb
Dec  9 23:10:51.715: INFO: Got endpoints: latency-svc-5k5zb [3.049238237s]
Dec  9 23:10:51.729: INFO: Created: latency-svc-824r4
Dec  9 23:10:51.772: INFO: Got endpoints: latency-svc-824r4 [2.968018506s]
Dec  9 23:10:51.934: INFO: Created: latency-svc-gjdgz
Dec  9 23:10:51.998: INFO: Got endpoints: latency-svc-gjdgz [3.010704815s]
Dec  9 23:10:52.172: INFO: Created: latency-svc-2d2zm
Dec  9 23:10:52.182: INFO: Got endpoints: latency-svc-2d2zm [2.903415437s]
Dec  9 23:10:52.358: INFO: Created: latency-svc-lkbfh
Dec  9 23:10:52.478: INFO: Got endpoints: latency-svc-lkbfh [3.016645052s]
Dec  9 23:10:52.504: INFO: Created: latency-svc-5dnw2
Dec  9 23:10:52.533: INFO: Got endpoints: latency-svc-5dnw2 [2.855180803s]
Dec  9 23:10:52.674: INFO: Created: latency-svc-qf2rl
Dec  9 23:10:52.708: INFO: Got endpoints: latency-svc-qf2rl [2.833426379s]
Dec  9 23:10:52.857: INFO: Created: latency-svc-f2dkl
Dec  9 23:10:52.872: INFO: Got endpoints: latency-svc-f2dkl [2.78497953s]
Dec  9 23:10:52.943: INFO: Created: latency-svc-5vkb9
Dec  9 23:10:53.034: INFO: Got endpoints: latency-svc-5vkb9 [2.777042174s]
Dec  9 23:10:53.145: INFO: Created: latency-svc-hb567
Dec  9 23:10:53.199: INFO: Got endpoints: latency-svc-hb567 [2.679124113s]
Dec  9 23:10:53.342: INFO: Created: latency-svc-s4v6m
Dec  9 23:10:53.354: INFO: Got endpoints: latency-svc-s4v6m [2.608809951s]
Dec  9 23:10:53.521: INFO: Created: latency-svc-zmvgp
Dec  9 23:10:53.521: INFO: Got endpoints: latency-svc-zmvgp [2.68246616s]
Dec  9 23:10:53.578: INFO: Created: latency-svc-h548f
Dec  9 23:10:53.695: INFO: Got endpoints: latency-svc-h548f [2.682651563s]
Dec  9 23:10:53.720: INFO: Created: latency-svc-p6cs2
Dec  9 23:10:53.729: INFO: Got endpoints: latency-svc-p6cs2 [2.523945245s]
Dec  9 23:10:53.781: INFO: Created: latency-svc-dpggt
Dec  9 23:10:53.787: INFO: Got endpoints: latency-svc-dpggt [2.425989059s]
Dec  9 23:10:53.938: INFO: Created: latency-svc-chw9z
Dec  9 23:10:53.953: INFO: Got endpoints: latency-svc-chw9z [2.238654194s]
Dec  9 23:10:54.079: INFO: Created: latency-svc-lvnw7
Dec  9 23:10:54.123: INFO: Got endpoints: latency-svc-lvnw7 [2.350388521s]
Dec  9 23:10:54.174: INFO: Created: latency-svc-6tcff
Dec  9 23:10:54.257: INFO: Got endpoints: latency-svc-6tcff [2.258630749s]
Dec  9 23:10:54.311: INFO: Created: latency-svc-k277s
Dec  9 23:10:54.319: INFO: Got endpoints: latency-svc-k277s [2.137163377s]
Dec  9 23:10:54.505: INFO: Created: latency-svc-698xq
Dec  9 23:10:54.516: INFO: Got endpoints: latency-svc-698xq [2.038198093s]
Dec  9 23:10:54.688: INFO: Created: latency-svc-bz6s9
Dec  9 23:10:54.700: INFO: Got endpoints: latency-svc-bz6s9 [2.167051711s]
Dec  9 23:10:54.883: INFO: Created: latency-svc-x6snw
Dec  9 23:10:54.894: INFO: Got endpoints: latency-svc-x6snw [2.186385325s]
Dec  9 23:10:55.063: INFO: Created: latency-svc-sbbv9
Dec  9 23:10:55.076: INFO: Got endpoints: latency-svc-sbbv9 [2.204601441s]
Dec  9 23:10:55.246: INFO: Created: latency-svc-bdcd9
Dec  9 23:10:55.261: INFO: Got endpoints: latency-svc-bdcd9 [2.226155798s]
Dec  9 23:10:55.463: INFO: Created: latency-svc-lhfxr
Dec  9 23:10:55.570: INFO: Got endpoints: latency-svc-lhfxr [2.370520569s]
Dec  9 23:10:55.656: INFO: Created: latency-svc-nbshp
Dec  9 23:10:55.665: INFO: Got endpoints: latency-svc-nbshp [2.31107245s]
Dec  9 23:10:55.863: INFO: Created: latency-svc-tmvhm
Dec  9 23:10:55.969: INFO: Got endpoints: latency-svc-tmvhm [2.448675253s]
Dec  9 23:10:56.062: INFO: Created: latency-svc-wvb8c
Dec  9 23:10:56.178: INFO: Got endpoints: latency-svc-wvb8c [2.482821416s]
Dec  9 23:10:56.360: INFO: Created: latency-svc-9bjkh
Dec  9 23:10:56.389: INFO: Got endpoints: latency-svc-9bjkh [2.659907744s]
Dec  9 23:10:56.622: INFO: Created: latency-svc-7sf2p
Dec  9 23:10:56.678: INFO: Got endpoints: latency-svc-7sf2p [2.890103774s]
Dec  9 23:10:56.895: INFO: Created: latency-svc-m6w5z
Dec  9 23:10:56.907: INFO: Got endpoints: latency-svc-m6w5z [2.953470354s]
Dec  9 23:10:57.038: INFO: Created: latency-svc-hnwxd
Dec  9 23:10:57.049: INFO: Got endpoints: latency-svc-hnwxd [2.926429314s]
Dec  9 23:10:57.207: INFO: Created: latency-svc-lqnlm
Dec  9 23:10:57.207: INFO: Got endpoints: latency-svc-lqnlm [2.950171623s]
Dec  9 23:10:57.258: INFO: Created: latency-svc-fdltc
Dec  9 23:10:57.266: INFO: Got endpoints: latency-svc-fdltc [2.947087637s]
Dec  9 23:10:57.480: INFO: Created: latency-svc-krwdb
Dec  9 23:10:57.565: INFO: Got endpoints: latency-svc-krwdb [3.048662673s]
Dec  9 23:10:57.713: INFO: Created: latency-svc-8lflf
Dec  9 23:10:57.776: INFO: Got endpoints: latency-svc-8lflf [3.075498548s]
Dec  9 23:10:58.011: INFO: Created: latency-svc-dkq7l
Dec  9 23:10:58.102: INFO: Got endpoints: latency-svc-dkq7l [3.207214528s]
Dec  9 23:10:58.213: INFO: Created: latency-svc-2wzfv
Dec  9 23:10:58.261: INFO: Got endpoints: latency-svc-2wzfv [3.184455912s]
Dec  9 23:10:58.396: INFO: Created: latency-svc-9vd2x
Dec  9 23:10:58.461: INFO: Got endpoints: latency-svc-9vd2x [3.200436803s]
Dec  9 23:10:58.548: INFO: Created: latency-svc-b4cst
Dec  9 23:10:58.670: INFO: Got endpoints: latency-svc-b4cst [3.100035247s]
Dec  9 23:10:58.697: INFO: Created: latency-svc-bjqq7
Dec  9 23:10:58.745: INFO: Got endpoints: latency-svc-bjqq7 [3.080657646s]
Dec  9 23:10:58.765: INFO: Created: latency-svc-b4skj
Dec  9 23:10:58.870: INFO: Got endpoints: latency-svc-b4skj [2.900169215s]
Dec  9 23:10:58.897: INFO: Created: latency-svc-6dchh
Dec  9 23:10:58.956: INFO: Got endpoints: latency-svc-6dchh [2.778616264s]
Dec  9 23:10:59.163: INFO: Created: latency-svc-q868b
Dec  9 23:10:59.261: INFO: Got endpoints: latency-svc-q868b [2.872433307s]
Dec  9 23:10:59.356: INFO: Created: latency-svc-q46js
Dec  9 23:10:59.356: INFO: Got endpoints: latency-svc-q46js [2.678525272s]
Dec  9 23:10:59.555: INFO: Created: latency-svc-pprck
Dec  9 23:10:59.555: INFO: Got endpoints: latency-svc-pprck [2.64765155s]
Dec  9 23:10:59.730: INFO: Created: latency-svc-7lhgk
Dec  9 23:10:59.734: INFO: Got endpoints: latency-svc-7lhgk [2.684501956s]
Dec  9 23:10:59.926: INFO: Created: latency-svc-jqzkk
Dec  9 23:10:59.938: INFO: Got endpoints: latency-svc-jqzkk [2.730494056s]
Dec  9 23:11:00.075: INFO: Created: latency-svc-7c7sb
Dec  9 23:11:00.089: INFO: Got endpoints: latency-svc-7c7sb [2.82273146s]
Dec  9 23:11:00.151: INFO: Created: latency-svc-d6pkv
Dec  9 23:11:00.244: INFO: Got endpoints: latency-svc-d6pkv [2.679218182s]
Dec  9 23:11:00.311: INFO: Created: latency-svc-t7l2l
Dec  9 23:11:00.453: INFO: Got endpoints: latency-svc-t7l2l [2.676997402s]
Dec  9 23:11:00.495: INFO: Created: latency-svc-lffh8
Dec  9 23:11:00.511: INFO: Got endpoints: latency-svc-lffh8 [2.408983665s]
Dec  9 23:11:00.680: INFO: Created: latency-svc-7ltwt
Dec  9 23:11:00.696: INFO: Got endpoints: latency-svc-7ltwt [2.434766993s]
Dec  9 23:11:00.871: INFO: Created: latency-svc-zz524
Dec  9 23:11:00.994: INFO: Got endpoints: latency-svc-zz524 [2.533267624s]
Dec  9 23:11:01.048: INFO: Created: latency-svc-9pvjm
Dec  9 23:11:01.064: INFO: Got endpoints: latency-svc-9pvjm [2.394749533s]
Dec  9 23:11:01.248: INFO: Created: latency-svc-fn4zq
Dec  9 23:11:01.257: INFO: Got endpoints: latency-svc-fn4zq [2.511194218s]
Dec  9 23:11:01.398: INFO: Created: latency-svc-5jbpv
Dec  9 23:11:01.406: INFO: Got endpoints: latency-svc-5jbpv [2.536078527s]
Dec  9 23:11:01.467: INFO: Created: latency-svc-g7227
Dec  9 23:11:01.474: INFO: Got endpoints: latency-svc-g7227 [2.5172952s]
Dec  9 23:11:01.594: INFO: Created: latency-svc-xvntr
Dec  9 23:11:01.757: INFO: Got endpoints: latency-svc-xvntr [2.495844552s]
Dec  9 23:11:01.821: INFO: Created: latency-svc-jzlmt
Dec  9 23:11:01.834: INFO: Got endpoints: latency-svc-jzlmt [2.477935515s]
Dec  9 23:11:02.005: INFO: Created: latency-svc-pr8xc
Dec  9 23:11:02.018: INFO: Got endpoints: latency-svc-pr8xc [2.463447148s]
Dec  9 23:11:02.159: INFO: Created: latency-svc-cprbp
Dec  9 23:11:02.168: INFO: Got endpoints: latency-svc-cprbp [2.434345631s]
Dec  9 23:11:02.322: INFO: Created: latency-svc-5m4lf
Dec  9 23:11:02.336: INFO: Got endpoints: latency-svc-5m4lf [2.397718077s]
Dec  9 23:11:02.455: INFO: Created: latency-svc-k4jrp
Dec  9 23:11:02.469: INFO: Got endpoints: latency-svc-k4jrp [2.380519939s]
Dec  9 23:11:02.528: INFO: Created: latency-svc-llvck
Dec  9 23:11:02.611: INFO: Got endpoints: latency-svc-llvck [2.366885487s]
Dec  9 23:11:02.703: INFO: Created: latency-svc-t5cjd
Dec  9 23:11:02.795: INFO: Got endpoints: latency-svc-t5cjd [2.342452s]
Dec  9 23:11:02.965: INFO: Created: latency-svc-9pxnw
Dec  9 23:11:02.980: INFO: Got endpoints: latency-svc-9pxnw [2.469668331s]
Dec  9 23:11:03.039: INFO: Created: latency-svc-rvg4b
Dec  9 23:11:03.153: INFO: Got endpoints: latency-svc-rvg4b [2.457175322s]
Dec  9 23:11:03.176: INFO: Created: latency-svc-rphcd
Dec  9 23:11:03.190: INFO: Got endpoints: latency-svc-rphcd [2.195206666s]
Dec  9 23:11:03.253: INFO: Created: latency-svc-59cxf
Dec  9 23:11:03.328: INFO: Got endpoints: latency-svc-59cxf [2.26316113s]
Dec  9 23:11:03.511: INFO: Created: latency-svc-6h624
Dec  9 23:11:03.552: INFO: Got endpoints: latency-svc-6h624 [2.294953223s]
Dec  9 23:11:03.687: INFO: Created: latency-svc-qtkkk
Dec  9 23:11:03.725: INFO: Got endpoints: latency-svc-qtkkk [2.319352158s]
Dec  9 23:11:03.883: INFO: Created: latency-svc-g2vs6
Dec  9 23:11:04.061: INFO: Got endpoints: latency-svc-g2vs6 [2.587231924s]
Dec  9 23:11:04.103: INFO: Created: latency-svc-kljm2
Dec  9 23:11:04.152: INFO: Got endpoints: latency-svc-kljm2 [2.395307059s]
Dec  9 23:11:04.431: INFO: Created: latency-svc-jrnd6
Dec  9 23:11:04.629: INFO: Got endpoints: latency-svc-jrnd6 [2.794913134s]
Dec  9 23:11:04.674: INFO: Created: latency-svc-h9n55
Dec  9 23:11:04.702: INFO: Got endpoints: latency-svc-h9n55 [2.684317063s]
Dec  9 23:11:04.831: INFO: Created: latency-svc-j9twm
Dec  9 23:11:04.854: INFO: Got endpoints: latency-svc-j9twm [2.685464947s]
Dec  9 23:11:05.023: INFO: Created: latency-svc-sb47x
Dec  9 23:11:05.161: INFO: Got endpoints: latency-svc-sb47x [2.825237497s]
Dec  9 23:11:05.184: INFO: Created: latency-svc-sd9lp
Dec  9 23:11:05.199: INFO: Got endpoints: latency-svc-sd9lp [2.729456893s]
Dec  9 23:11:05.254: INFO: Created: latency-svc-jsx99
Dec  9 23:11:05.361: INFO: Got endpoints: latency-svc-jsx99 [2.749812551s]
Dec  9 23:11:05.388: INFO: Created: latency-svc-dsb6b
Dec  9 23:11:05.398: INFO: Got endpoints: latency-svc-dsb6b [2.602739153s]
Dec  9 23:11:05.457: INFO: Created: latency-svc-grbjd
Dec  9 23:11:05.561: INFO: Got endpoints: latency-svc-grbjd [2.580339972s]
Dec  9 23:11:05.620: INFO: Created: latency-svc-lf595
Dec  9 23:11:05.736: INFO: Got endpoints: latency-svc-lf595 [2.583074634s]
Dec  9 23:11:05.804: INFO: Created: latency-svc-7fdwr
Dec  9 23:11:05.814: INFO: Got endpoints: latency-svc-7fdwr [2.624798325s]
Dec  9 23:11:05.815: INFO: Latencies: [258.671226ms 383.498417ms 543.123166ms 687.118302ms 800.066894ms 958.297746ms 1.114760113s 1.308309056s 1.38662864s 1.563475232s 1.883522266s 2.038198093s 2.118252132s 2.126012239s 2.137163377s 2.167051711s 2.171184576s 2.173551241s 2.186385325s 2.195206666s 2.198094107s 2.204601441s 2.209672437s 2.214372593s 2.226155798s 2.238654194s 2.258630749s 2.26316113s 2.264148192s 2.264149889s 2.268651195s 2.276650364s 2.276894481s 2.294953223s 2.302466766s 2.31107245s 2.313172122s 2.317143434s 2.319352158s 2.320181491s 2.324648103s 2.324786888s 2.342452s 2.347710853s 2.350388521s 2.353537647s 2.358372082s 2.359997838s 2.366885487s 2.370520569s 2.380519939s 2.394749533s 2.395307059s 2.397718077s 2.408983665s 2.416460548s 2.418844715s 2.423269973s 2.425989059s 2.427707423s 2.434345631s 2.434766993s 2.439605394s 2.441167472s 2.448675253s 2.452095477s 2.454366523s 2.456875723s 2.457175322s 2.463447148s 2.469668331s 2.471088736s 2.476172801s 2.477935515s 2.479497312s 2.482821416s 2.486731994s 2.495420076s 2.495844552s 2.506453347s 2.508144166s 2.50822559s 2.511194218s 2.5172952s 2.52141597s 2.523945245s 2.531797371s 2.532754066s 2.533267624s 2.536078527s 2.541214411s 2.563517278s 2.575458354s 2.580339972s 2.583074634s 2.584071746s 2.587231924s 2.591592212s 2.592722325s 2.599860808s 2.602739153s 2.605831043s 2.605909077s 2.60821037s 2.608809951s 2.613979819s 2.622093893s 2.624798325s 2.625456251s 2.62851438s 2.629195265s 2.632957704s 2.63396906s 2.644359605s 2.64765155s 2.647844365s 2.650832888s 2.654163029s 2.659907744s 2.676997402s 2.678525272s 2.679124113s 2.679218182s 2.679912406s 2.68246616s 2.682651563s 2.684317063s 2.684501956s 2.685464947s 2.688472493s 2.701249397s 2.726262384s 2.729456893s 2.730494056s 2.742165951s 2.749812551s 2.751796827s 2.755983286s 2.758250638s 2.758467361s 2.760739034s 2.760779357s 2.765323942s 2.766104694s 2.774842054s 2.776611932s 2.777042174s 2.777447103s 2.778616264s 2.78497953s 2.794913134s 2.806919987s 2.808021798s 2.816551228s 2.817930991s 2.820943583s 2.82273146s 2.825237497s 2.833426379s 2.855180803s 2.872433307s 2.877056399s 2.890103774s 2.893993741s 2.897693537s 2.898641445s 2.900169215s 2.903415437s 2.905190766s 2.905660528s 2.90729439s 2.923498071s 2.925930173s 2.926429314s 2.930380934s 2.935615823s 2.947087637s 2.94733086s 2.949455172s 2.950171623s 2.953470354s 2.95663372s 2.959772923s 2.966345006s 2.968018506s 2.980288436s 2.984724678s 2.98825338s 2.994408328s 3.010704815s 3.016645052s 3.027247109s 3.048662673s 3.049238237s 3.075498548s 3.080657646s 3.100035247s 3.184455912s 3.200436803s 3.207214528s]
Dec  9 23:11:05.815: INFO: 50 %ile: 2.602739153s
Dec  9 23:11:05.815: INFO: 90 %ile: 2.953470354s
Dec  9 23:11:05.815: INFO: 99 %ile: 3.200436803s
Dec  9 23:11:05.815: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:11:05.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-lqggv" for this suite.
Dec  9 23:11:57.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:11:57.922: INFO: namespace: e2e-tests-svc-latency-lqggv, resource: bindings, ignored listing per whitelist
Dec  9 23:11:58.040: INFO: namespace e2e-tests-svc-latency-lqggv deletion completed in 52.219714037s

• [SLOW TEST:90.716 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:11:58.040: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  9 23:11:58.284: INFO: Waiting up to 5m0s for pod "pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-h9z9s" to be "success or failure"
Dec  9 23:11:58.341: INFO: Pod "pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.720489ms
Dec  9 23:12:00.346: INFO: Pod "pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062284315s
Dec  9 23:12:02.352: INFO: Pod "pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068371333s
STEP: Saw pod success
Dec  9 23:12:02.352: INFO: Pod "pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:12:02.358: INFO: Trying to get logs from node k8s-g1 pod pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:12:02.402: INFO: Waiting for pod pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:12:02.408: INFO: Pod pod-d3c03c79-fc07-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:12:02.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h9z9s" for this suite.
Dec  9 23:12:08.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:12:08.670: INFO: namespace: e2e-tests-emptydir-h9z9s, resource: bindings, ignored listing per whitelist
Dec  9 23:12:08.697: INFO: namespace e2e-tests-emptydir-h9z9s deletion completed in 6.267513551s

• [SLOW TEST:10.656 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:12:08.697: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  9 23:12:09.017: INFO: Waiting up to 5m0s for pod "pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-78cbv" to be "success or failure"
Dec  9 23:12:09.044: INFO: Pod "pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 27.790431ms
Dec  9 23:12:11.050: INFO: Pod "pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033666172s
Dec  9 23:12:13.057: INFO: Pod "pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040446029s
STEP: Saw pod success
Dec  9 23:12:13.057: INFO: Pod "pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:12:13.062: INFO: Trying to get logs from node k8s-g1 pod pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:12:13.112: INFO: Waiting for pod pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:12:13.118: INFO: Pod pod-da20a29b-fc07-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:12:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-78cbv" for this suite.
Dec  9 23:12:19.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:12:19.302: INFO: namespace: e2e-tests-emptydir-78cbv, resource: bindings, ignored listing per whitelist
Dec  9 23:12:19.322: INFO: namespace e2e-tests-emptydir-78cbv deletion completed in 6.199094942s

• [SLOW TEST:10.626 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:12:19.322: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-e06cbe76-fc07-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e06cbe76-fc07-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:13:30.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bnp2x" for this suite.
Dec  9 23:13:52.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:13:52.082: INFO: namespace: e2e-tests-configmap-bnp2x, resource: bindings, ignored listing per whitelist
Dec  9 23:13:52.210: INFO: namespace e2e-tests-configmap-bnp2x deletion completed in 22.199917375s

• [SLOW TEST:92.888 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:13:52.210: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  9 23:13:52.497: INFO: Waiting up to 5m0s for pod "pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-vp6mp" to be "success or failure"
Dec  9 23:13:52.553: INFO: Pod "pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.303519ms
Dec  9 23:13:54.559: INFO: Pod "pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062374247s
Dec  9 23:13:56.564: INFO: Pod "pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067913583s
STEP: Saw pod success
Dec  9 23:13:56.565: INFO: Pod "pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:13:56.569: INFO: Trying to get logs from node k8s-g2 pod pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:13:56.714: INFO: Waiting for pod pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:13:56.720: INFO: Pod pod-17d157b0-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:13:56.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vp6mp" for this suite.
Dec  9 23:14:02.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:14:02.872: INFO: namespace: e2e-tests-emptydir-vp6mp, resource: bindings, ignored listing per whitelist
Dec  9 23:14:02.911: INFO: namespace e2e-tests-emptydir-vp6mp deletion completed in 6.185724334s

• [SLOW TEST:10.701 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:14:02.911: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:14:03.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-v4m8k" to be "success or failure"
Dec  9 23:14:03.214: INFO: Pod "downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 51.30219ms
Dec  9 23:14:05.221: INFO: Pod "downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057862385s
Dec  9 23:14:07.226: INFO: Pod "downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0631342s
STEP: Saw pod success
Dec  9 23:14:07.226: INFO: Pod "downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:14:07.232: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:14:07.290: INFO: Waiting for pod downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:14:07.295: INFO: Pod downwardapi-volume-1e30afe4-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:14:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v4m8k" for this suite.
Dec  9 23:14:13.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:14:13.409: INFO: namespace: e2e-tests-downward-api-v4m8k, resource: bindings, ignored listing per whitelist
Dec  9 23:14:13.521: INFO: namespace e2e-tests-downward-api-v4m8k deletion completed in 6.220866389s

• [SLOW TEST:10.610 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:14:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  9 23:14:13.739: INFO: Waiting up to 5m0s for pod "pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-lqh9r" to be "success or failure"
Dec  9 23:14:13.775: INFO: Pod "pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 35.989884ms
Dec  9 23:14:15.796: INFO: Pod "pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056853249s
Dec  9 23:14:17.848: INFO: Pod "pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.108238895s
STEP: Saw pod success
Dec  9 23:14:17.848: INFO: Pod "pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:14:17.852: INFO: Trying to get logs from node k8s-g2 pod pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:14:17.931: INFO: Waiting for pod pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:14:17.937: INFO: Pod pod-24806c80-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:14:17.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lqh9r" for this suite.
Dec  9 23:14:23.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:14:24.029: INFO: namespace: e2e-tests-emptydir-lqh9r, resource: bindings, ignored listing per whitelist
Dec  9 23:14:24.128: INFO: namespace e2e-tests-emptydir-lqh9r deletion completed in 6.186808228s

• [SLOW TEST:10.607 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:14:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:14:24.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-2zj4s" to be "success or failure"
Dec  9 23:14:24.480: INFO: Pod "downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 76.707731ms
Dec  9 23:14:26.485: INFO: Pod "downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082488398s
Dec  9 23:14:28.491: INFO: Pod "downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087977588s
STEP: Saw pod success
Dec  9 23:14:28.491: INFO: Pod "downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:14:28.496: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:14:28.545: INFO: Waiting for pod downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:14:28.551: INFO: Pod downwardapi-volume-2adb6aec-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:14:28.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2zj4s" for this suite.
Dec  9 23:14:34.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:14:34.805: INFO: namespace: e2e-tests-downward-api-2zj4s, resource: bindings, ignored listing per whitelist
Dec  9 23:14:34.816: INFO: namespace e2e-tests-downward-api-2zj4s deletion completed in 6.25900927s

• [SLOW TEST:10.688 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:14:34.817: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 23:14:35.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-wcs6q'
Dec  9 23:14:37.993: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  9 23:14:37.993: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  9 23:14:38.089: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-jlmcr]
Dec  9 23:14:38.089: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-jlmcr" in namespace "e2e-tests-kubectl-wcs6q" to be "running and ready"
Dec  9 23:14:38.094: INFO: Pod "e2e-test-nginx-rc-jlmcr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.55123ms
Dec  9 23:14:40.112: INFO: Pod "e2e-test-nginx-rc-jlmcr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022711313s
Dec  9 23:14:42.117: INFO: Pod "e2e-test-nginx-rc-jlmcr": Phase="Running", Reason="", readiness=true. Elapsed: 4.028382524s
Dec  9 23:14:42.117: INFO: Pod "e2e-test-nginx-rc-jlmcr" satisfied condition "running and ready"
Dec  9 23:14:42.117: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-jlmcr]
Dec  9 23:14:42.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wcs6q'
Dec  9 23:14:42.197: INFO: stderr: ""
Dec  9 23:14:42.197: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Dec  9 23:14:42.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-wcs6q'
Dec  9 23:14:42.317: INFO: stderr: ""
Dec  9 23:14:42.317: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:14:42.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wcs6q" for this suite.
Dec  9 23:15:04.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:15:04.427: INFO: namespace: e2e-tests-kubectl-wcs6q, resource: bindings, ignored listing per whitelist
Dec  9 23:15:04.538: INFO: namespace e2e-tests-kubectl-wcs6q deletion completed in 22.183458503s

• [SLOW TEST:29.721 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:15:04.538: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:15:04.805: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  9 23:15:09.810: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  9 23:15:09.810: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  9 23:15:11.814: INFO: Creating deployment "test-rollover-deployment"
Dec  9 23:15:11.843: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  9 23:15:13.850: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  9 23:15:13.857: INFO: Ensure that both replica sets have 1 created replica
Dec  9 23:15:13.864: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  9 23:15:13.911: INFO: Updating deployment test-rollover-deployment
Dec  9 23:15:13.911: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  9 23:15:15.939: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  9 23:15:15.946: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  9 23:15:15.953: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:15.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994114, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:17.961: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:17.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:19.960: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:19.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:21.960: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:21.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:23.961: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:23.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:25.961: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 23:15:25.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994112, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994116, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679994111, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 23:15:27.980: INFO: 
Dec  9 23:15:27.980: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  9 23:15:27.989: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-9rk27,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9rk27/deployments/test-rollover-deployment,UID:47221da9-fc08-11e8-9ee5-54a05085d523,ResourceVersion:53586,Generation:2,CreationTimestamp:2018-12-09 23:15:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-09 23:15:12 +0000 UTC 2018-12-09 23:15:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-09 23:15:27 +0000 UTC 2018-12-09 23:15:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 23:15:28.042: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-9rk27,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9rk27/replicasets/test-rollover-deployment-6b7f9d6597,UID:486214ac-fc08-11e8-9811-448a5b81d79a,ResourceVersion:53577,Generation:2,CreationTimestamp:2018-12-09 23:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 47221da9-fc08-11e8-9ee5-54a05085d523 0xc001b31fd7 0xc001b31fd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  9 23:15:28.042: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  9 23:15:28.042: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-9rk27,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9rk27/replicasets/test-rollover-controller,UID:42ed9994-fc08-11e8-9ee5-54a05085d523,ResourceVersion:53585,Generation:2,CreationTimestamp:2018-12-09 23:15:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 47221da9-fc08-11e8-9ee5-54a05085d523 0xc001b31da7 0xc001b31da8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 23:15:28.042: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-9rk27,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9rk27/replicasets/test-rollover-deployment-6586df867b,UID:472ed6c4-fc08-11e8-9811-448a5b81d79a,ResourceVersion:53543,Generation:2,CreationTimestamp:2018-12-09 23:15:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 47221da9-fc08-11e8-9ee5-54a05085d523 0xc001b31e67 0xc001b31e68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  9 23:15:28.069: INFO: Pod "test-rollover-deployment-6b7f9d6597-7lkgg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-7lkgg,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-9rk27,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9rk27/pods/test-rollover-deployment-6b7f9d6597-7lkgg,UID:488aaaab-fc08-11e8-9811-448a5b81d79a,ResourceVersion:53556,Generation:0,CreationTimestamp:2018-12-09 23:15:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.201/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 486214ac-fc08-11e8-9811-448a5b81d79a 0xc0023be647 0xc0023be648}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-x6gt5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-x6gt5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-x6gt5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023be6c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023be6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:15:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:15:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:15:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:15:14 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.13,PodIP:10.244.3.201,StartTime:2018-12-09 23:15:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-09 23:15:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9af028efa5350374916288788422361b943680d58daf2deb52d711de066787ab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:15:28.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9rk27" for this suite.
Dec  9 23:15:36.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:15:36.131: INFO: namespace: e2e-tests-deployment-9rk27, resource: bindings, ignored listing per whitelist
Dec  9 23:15:36.240: INFO: namespace e2e-tests-deployment-9rk27 deletion completed in 8.166131369s

• [SLOW TEST:31.702 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:15:36.240: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Dec  9 23:15:36.593: INFO: Waiting up to 5m0s for pod "var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-var-expansion-dr4b9" to be "success or failure"
Dec  9 23:15:36.616: INFO: Pod "var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.469854ms
Dec  9 23:15:38.621: INFO: Pod "var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028581217s
Dec  9 23:15:40.628: INFO: Pod "var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034711974s
STEP: Saw pod success
Dec  9 23:15:40.628: INFO: Pod "var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:15:40.633: INFO: Trying to get logs from node k8s-g2 pod var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 23:15:40.691: INFO: Waiting for pod var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:15:40.697: INFO: Pod var-expansion-55db98fd-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:15:40.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-dr4b9" for this suite.
Dec  9 23:15:46.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:15:46.897: INFO: namespace: e2e-tests-var-expansion-dr4b9, resource: bindings, ignored listing per whitelist
Dec  9 23:15:46.931: INFO: namespace e2e-tests-var-expansion-dr4b9 deletion completed in 6.228623617s

• [SLOW TEST:10.691 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:15:46.931: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:15:47.161: INFO: Creating ReplicaSet my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd
Dec  9 23:15:47.213: INFO: Pod name my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd: Found 0 pods out of 1
Dec  9 23:15:52.219: INFO: Pod name my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd: Found 1 pods out of 1
Dec  9 23:15:52.219: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd" is running
Dec  9 23:15:52.225: INFO: Pod "my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd-2vvnq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 23:15:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 23:15:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 23:15:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-09 23:15:47 +0000 UTC Reason: Message:}])
Dec  9 23:15:52.225: INFO: Trying to dial the pod
Dec  9 23:15:57.251: INFO: Controller my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd: Got expected result from replica 1 [my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd-2vvnq]: "my-hostname-basic-5c337f7f-fc08-11e8-a4b0-6e57f5092bbd-2vvnq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:15:57.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-wq4tm" for this suite.
Dec  9 23:16:03.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:16:03.370: INFO: namespace: e2e-tests-replicaset-wq4tm, resource: bindings, ignored listing per whitelist
Dec  9 23:16:03.435: INFO: namespace e2e-tests-replicaset-wq4tm deletion completed in 6.177605368s

• [SLOW TEST:16.503 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:16:03.435: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-6602718c-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating secret with name secret-projected-all-test-volume-66027179-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  9 23:16:03.722: INFO: Waiting up to 5m0s for pod "projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-9k6j4" to be "success or failure"
Dec  9 23:16:03.743: INFO: Pod "projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.9475ms
Dec  9 23:16:05.748: INFO: Pod "projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026566535s
Dec  9 23:16:07.776: INFO: Pod "projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05435598s
STEP: Saw pod success
Dec  9 23:16:07.776: INFO: Pod "projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:16:07.783: INFO: Trying to get logs from node k8s-g2 pod projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  9 23:16:07.841: INFO: Waiting for pod projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:16:07.847: INFO: Pod projected-volume-6602714b-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:16:07.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9k6j4" for this suite.
Dec  9 23:16:13.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:16:14.031: INFO: namespace: e2e-tests-projected-9k6j4, resource: bindings, ignored listing per whitelist
Dec  9 23:16:14.122: INFO: namespace e2e-tests-projected-9k6j4 deletion completed in 6.258253722s

• [SLOW TEST:10.687 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:16:14.122: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  9 23:16:14.451: INFO: Waiting up to 5m0s for pod "pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-xrlpp" to be "success or failure"
Dec  9 23:16:14.558: INFO: Pod "pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 107.841212ms
Dec  9 23:16:16.564: INFO: Pod "pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113452312s
Dec  9 23:16:18.575: INFO: Pod "pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.124484248s
STEP: Saw pod success
Dec  9 23:16:18.575: INFO: Pod "pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:16:18.580: INFO: Trying to get logs from node k8s-g1 pod pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:16:18.708: INFO: Waiting for pod pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:16:18.727: INFO: Pod pod-6c744092-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:16:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xrlpp" for this suite.
Dec  9 23:16:24.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:16:24.780: INFO: namespace: e2e-tests-emptydir-xrlpp, resource: bindings, ignored listing per whitelist
Dec  9 23:16:24.893: INFO: namespace e2e-tests-emptydir-xrlpp deletion completed in 6.160932482s

• [SLOW TEST:10.771 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:16:24.893: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1209 23:16:55.712941      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 23:16:55.712: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:16:55.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-csljj" for this suite.
Dec  9 23:17:01.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:17:01.772: INFO: namespace: e2e-tests-gc-csljj, resource: bindings, ignored listing per whitelist
Dec  9 23:17:01.879: INFO: namespace e2e-tests-gc-csljj deletion completed in 6.163375528s

• [SLOW TEST:36.986 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:17:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  9 23:17:06.231: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-88e0bd45-fc08-11e8-a4b0-6e57f5092bbd,GenerateName:,Namespace:e2e-tests-events-6jnlg,SelfLink:/api/v1/namespaces/e2e-tests-events-6jnlg/pods/send-events-88e0bd45-fc08-11e8-a4b0-6e57f5092bbd,UID:88e18fd4-fc08-11e8-9ee5-54a05085d523,ResourceVersion:53982,Generation:0,CreationTimestamp:2018-12-09 23:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 116798232,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.170/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p8pv4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8pv4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-p8pv4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-g2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001143230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001143250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:17:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:17:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:17:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-09 23:17:02 +0000 UTC  }],Message:,Reason:,HostIP:172.22.132.14,PodIP:10.244.4.170,StartTime:2018-12-09 23:17:02 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-12-09 23:17:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://b26729a6237f182b306a952a0af7645a8cd8f4fb17d11729d7eac4d7133f41ad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  9 23:17:08.237: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  9 23:17:10.244: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:17:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-6jnlg" for this suite.
Dec  9 23:17:52.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:17:52.448: INFO: namespace: e2e-tests-events-6jnlg, resource: bindings, ignored listing per whitelist
Dec  9 23:17:52.543: INFO: namespace e2e-tests-events-6jnlg deletion completed in 42.230972394s

• [SLOW TEST:50.663 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:17:52.543: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a70e6333-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:17:52.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-42jd6" to be "success or failure"
Dec  9 23:17:52.880: INFO: Pod "pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.380193ms
Dec  9 23:17:54.885: INFO: Pod "pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037933397s
Dec  9 23:17:56.891: INFO: Pod "pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04331386s
STEP: Saw pod success
Dec  9 23:17:56.891: INFO: Pod "pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:17:56.895: INFO: Trying to get logs from node k8s-g1 pod pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:17:57.017: INFO: Waiting for pod pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:17:57.022: INFO: Pod pod-configmaps-a714ac34-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:17:57.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-42jd6" for this suite.
Dec  9 23:18:03.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:18:03.130: INFO: namespace: e2e-tests-configmap-42jd6, resource: bindings, ignored listing per whitelist
Dec  9 23:18:03.201: INFO: namespace e2e-tests-configmap-42jd6 deletion completed in 6.174048729s

• [SLOW TEST:10.658 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:18:03.202: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-ad6d77ce-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating secret with name s-test-opt-upd-ad6d7812-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ad6d77ce-fc08-11e8-a4b0-6e57f5092bbd
STEP: Updating secret s-test-opt-upd-ad6d7812-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating secret with name s-test-opt-create-ad6d7842-fc08-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:18:11.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k4qmp" for this suite.
Dec  9 23:18:35.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:18:35.832: INFO: namespace: e2e-tests-projected-k4qmp, resource: bindings, ignored listing per whitelist
Dec  9 23:18:35.878: INFO: namespace e2e-tests-projected-k4qmp deletion completed in 24.171875767s

• [SLOW TEST:32.676 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:18:35.878: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:18:36.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-4slnl" to be "success or failure"
Dec  9 23:18:36.211: INFO: Pod "downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 76.589381ms
Dec  9 23:18:38.218: INFO: Pod "downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083147113s
Dec  9 23:18:40.223: INFO: Pod "downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088142639s
STEP: Saw pod success
Dec  9 23:18:40.223: INFO: Pod "downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:18:40.227: INFO: Trying to get logs from node k8s-g1 pod downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:18:40.348: INFO: Waiting for pod downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:18:40.355: INFO: Pod downwardapi-volume-c0e76ef5-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:18:40.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4slnl" for this suite.
Dec  9 23:18:46.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:18:46.463: INFO: namespace: e2e-tests-downward-api-4slnl, resource: bindings, ignored listing per whitelist
Dec  9 23:18:46.549: INFO: namespace e2e-tests-downward-api-4slnl deletion completed in 6.169459957s

• [SLOW TEST:10.671 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:18:46.549: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  9 23:18:46.848: INFO: Waiting up to 5m0s for pod "pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-6dpl6" to be "success or failure"
Dec  9 23:18:46.862: INFO: Pod "pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.388692ms
Dec  9 23:18:48.867: INFO: Pod "pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0197668s
Dec  9 23:18:50.873: INFO: Pod "pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025096364s
STEP: Saw pod success
Dec  9 23:18:50.873: INFO: Pod "pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:18:50.877: INFO: Trying to get logs from node k8s-g2 pod pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:18:50.953: INFO: Waiting for pod pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:18:50.958: INFO: Pod pod-c74379c8-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:18:50.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6dpl6" for this suite.
Dec  9 23:18:57.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:18:57.056: INFO: namespace: e2e-tests-emptydir-6dpl6, resource: bindings, ignored listing per whitelist
Dec  9 23:18:57.166: INFO: namespace e2e-tests-emptydir-6dpl6 deletion completed in 6.20371356s

• [SLOW TEST:10.617 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:18:57.166: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:18:57.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-n5csf" for this suite.
Dec  9 23:19:03.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:19:03.763: INFO: namespace: e2e-tests-kubelet-test-n5csf, resource: bindings, ignored listing per whitelist
Dec  9 23:19:03.803: INFO: namespace e2e-tests-kubelet-test-n5csf deletion completed in 6.304847486s

• [SLOW TEST:6.637 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:19:03.803: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-cp5rd/secret-test-d185535a-fc08-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 23:19:04.057: INFO: Waiting up to 5m0s for pod "pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-cp5rd" to be "success or failure"
Dec  9 23:19:04.110: INFO: Pod "pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.506661ms
Dec  9 23:19:06.116: INFO: Pod "pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059022108s
Dec  9 23:19:08.122: INFO: Pod "pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065027628s
STEP: Saw pod success
Dec  9 23:19:08.122: INFO: Pod "pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:19:08.127: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd container env-test: <nil>
STEP: delete the pod
Dec  9 23:19:08.231: INFO: Waiting for pod pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:19:08.237: INFO: Pod pod-configmaps-d188adf1-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:19:08.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-cp5rd" for this suite.
Dec  9 23:19:14.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:19:14.319: INFO: namespace: e2e-tests-secrets-cp5rd, resource: bindings, ignored listing per whitelist
Dec  9 23:19:14.437: INFO: namespace e2e-tests-secrets-cp5rd deletion completed in 6.179404813s

• [SLOW TEST:10.634 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:19:14.437: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  9 23:19:14.748: INFO: Waiting up to 5m0s for pod "downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-9n4pb" to be "success or failure"
Dec  9 23:19:14.785: INFO: Pod "downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 37.046809ms
Dec  9 23:19:16.791: INFO: Pod "downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043439598s
Dec  9 23:19:18.797: INFO: Pod "downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048855578s
STEP: Saw pod success
Dec  9 23:19:18.797: INFO: Pod "downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:19:18.802: INFO: Trying to get logs from node k8s-g2 pod downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd container dapi-container: <nil>
STEP: delete the pod
Dec  9 23:19:18.966: INFO: Waiting for pod downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:19:18.971: INFO: Pod downward-api-d7ea0e6f-fc08-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:19:18.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9n4pb" for this suite.
Dec  9 23:19:25.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:19:25.069: INFO: namespace: e2e-tests-downward-api-9n4pb, resource: bindings, ignored listing per whitelist
Dec  9 23:19:25.168: INFO: namespace e2e-tests-downward-api-9n4pb deletion completed in 6.183897421s

• [SLOW TEST:10.731 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:19:25.169: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1209 23:19:36.077408      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 23:19:36.077: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:19:36.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6gqt4" for this suite.
Dec  9 23:19:44.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:19:44.227: INFO: namespace: e2e-tests-gc-6gqt4, resource: bindings, ignored listing per whitelist
Dec  9 23:19:44.308: INFO: namespace e2e-tests-gc-6gqt4 deletion completed in 8.227921786s

• [SLOW TEST:19.140 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:19:44.309: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Dec  9 23:19:45.122: INFO: Waiting up to 5m0s for pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz" in namespace "e2e-tests-svcaccounts-lncd5" to be "success or failure"
Dec  9 23:19:45.168: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz": Phase="Pending", Reason="", readiness=false. Elapsed: 46.359738ms
Dec  9 23:19:47.195: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073058121s
Dec  9 23:19:49.200: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078711029s
Dec  9 23:19:51.206: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084046438s
STEP: Saw pod success
Dec  9 23:19:51.206: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz" satisfied condition "success or failure"
Dec  9 23:19:51.210: INFO: Trying to get logs from node k8s-g2 pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz container token-test: <nil>
STEP: delete the pod
Dec  9 23:19:51.304: INFO: Waiting for pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz to disappear
Dec  9 23:19:51.309: INFO: Pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-gt8zz no longer exists
STEP: Creating a pod to test consume service account root CA
Dec  9 23:19:51.383: INFO: Waiting up to 5m0s for pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw" in namespace "e2e-tests-svcaccounts-lncd5" to be "success or failure"
Dec  9 23:19:51.413: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw": Phase="Pending", Reason="", readiness=false. Elapsed: 29.320464ms
Dec  9 23:19:53.418: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034909841s
Dec  9 23:19:55.424: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040424566s
Dec  9 23:19:57.429: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045720996s
STEP: Saw pod success
Dec  9 23:19:57.429: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw" satisfied condition "success or failure"
Dec  9 23:19:57.434: INFO: Trying to get logs from node k8s-g1 pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw container root-ca-test: <nil>
STEP: delete the pod
Dec  9 23:19:57.497: INFO: Waiting for pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw to disappear
Dec  9 23:19:57.541: INFO: Pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-fs2fw no longer exists
STEP: Creating a pod to test consume service account namespace
Dec  9 23:19:57.570: INFO: Waiting up to 5m0s for pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd" in namespace "e2e-tests-svcaccounts-lncd5" to be "success or failure"
Dec  9 23:19:57.582: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.6016ms
Dec  9 23:19:59.588: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017177879s
Dec  9 23:20:01.594: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0232957s
Dec  9 23:20:03.600: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029100513s
STEP: Saw pod success
Dec  9 23:20:03.600: INFO: Pod "pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd" satisfied condition "success or failure"
Dec  9 23:20:03.604: INFO: Trying to get logs from node k8s-g2 pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd container namespace-test: <nil>
STEP: delete the pod
Dec  9 23:20:03.717: INFO: Waiting for pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd to disappear
Dec  9 23:20:03.722: INFO: Pod pod-service-account-ea00232c-fc08-11e8-a4b0-6e57f5092bbd-vnxqd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:20:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-lncd5" for this suite.
Dec  9 23:20:11.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:20:11.919: INFO: namespace: e2e-tests-svcaccounts-lncd5, resource: bindings, ignored listing per whitelist
Dec  9 23:20:11.926: INFO: namespace e2e-tests-svcaccounts-lncd5 deletion completed in 8.187368131s

• [SLOW TEST:27.618 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:20:11.926: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  9 23:20:12.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54826,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  9 23:20:12.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54826,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  9 23:20:22.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54845,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  9 23:20:22.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54845,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  9 23:20:32.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54864,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  9 23:20:32.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54864,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  9 23:20:42.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54883,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  9 23:20:42.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-a,UID:fa2ddda0-fc08-11e8-9ee5-54a05085d523,ResourceVersion:54883,Generation:0,CreationTimestamp:2018-12-09 23:20:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  9 23:20:52.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-b,UID:121976e1-fc09-11e8-9ee5-54a05085d523,ResourceVersion:54902,Generation:0,CreationTimestamp:2018-12-09 23:20:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  9 23:20:52.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-b,UID:121976e1-fc09-11e8-9ee5-54a05085d523,ResourceVersion:54902,Generation:0,CreationTimestamp:2018-12-09 23:20:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  9 23:21:02.382: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-b,UID:121976e1-fc09-11e8-9ee5-54a05085d523,ResourceVersion:54921,Generation:0,CreationTimestamp:2018-12-09 23:20:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  9 23:21:02.382: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-mr7n8,SelfLink:/api/v1/namespaces/e2e-tests-watch-mr7n8/configmaps/e2e-watch-test-configmap-b,UID:121976e1-fc09-11e8-9ee5-54a05085d523,ResourceVersion:54921,Generation:0,CreationTimestamp:2018-12-09 23:20:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:21:12.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-mr7n8" for this suite.
Dec  9 23:21:18.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:21:18.551: INFO: namespace: e2e-tests-watch-mr7n8, resource: bindings, ignored listing per whitelist
Dec  9 23:21:18.619: INFO: namespace e2e-tests-watch-mr7n8 deletion completed in 6.230887265s

• [SLOW TEST:66.693 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:21:18.619: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  9 23:21:18.856: INFO: Waiting up to 5m0s for pod "pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-6gvkl" to be "success or failure"
Dec  9 23:21:18.892: INFO: Pod "pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 35.971578ms
Dec  9 23:21:20.898: INFO: Pod "pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041463597s
Dec  9 23:21:22.905: INFO: Pod "pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048963349s
STEP: Saw pod success
Dec  9 23:21:22.905: INFO: Pod "pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:21:22.910: INFO: Trying to get logs from node k8s-g1 pod pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:21:23.000: INFO: Waiting for pod pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:21:23.038: INFO: Pod pod-21e49b69-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:21:23.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6gvkl" for this suite.
Dec  9 23:21:29.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:21:29.119: INFO: namespace: e2e-tests-emptydir-6gvkl, resource: bindings, ignored listing per whitelist
Dec  9 23:21:29.217: INFO: namespace e2e-tests-emptydir-6gvkl deletion completed in 6.173665563s

• [SLOW TEST:10.598 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:21:29.217: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  9 23:21:29.432: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:21:34.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-h77bl" for this suite.
Dec  9 23:21:58.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:21:58.331: INFO: namespace: e2e-tests-init-container-h77bl, resource: bindings, ignored listing per whitelist
Dec  9 23:21:58.473: INFO: namespace e2e-tests-init-container-h77bl deletion completed in 24.257612044s

• [SLOW TEST:29.257 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:21:58.474: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Dec  9 23:21:58.761: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-vm8xd" to be "success or failure"
Dec  9 23:21:58.799: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 37.453085ms
Dec  9 23:22:00.805: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04341128s
Dec  9 23:22:02.810: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048684048s
STEP: Saw pod success
Dec  9 23:22:02.810: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  9 23:22:02.814: INFO: Trying to get logs from node k8s-g1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  9 23:22:02.898: INFO: Waiting for pod pod-host-path-test to disappear
Dec  9 23:22:02.904: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:22:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-vm8xd" for this suite.
Dec  9 23:22:08.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:22:09.013: INFO: namespace: e2e-tests-hostpath-vm8xd, resource: bindings, ignored listing per whitelist
Dec  9 23:22:09.102: INFO: namespace e2e-tests-hostpath-vm8xd deletion completed in 6.183751026s

• [SLOW TEST:10.628 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:22:09.102: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-2cjn4/configmap-test-3ffdebb7-fc09-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:22:09.417: INFO: Waiting up to 5m0s for pod "pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-2cjn4" to be "success or failure"
Dec  9 23:22:09.486: INFO: Pod "pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 69.471933ms
Dec  9 23:22:11.549: INFO: Pod "pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131840718s
Dec  9 23:22:13.554: INFO: Pod "pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.137353029s
STEP: Saw pod success
Dec  9 23:22:13.554: INFO: Pod "pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:22:13.559: INFO: Trying to get logs from node k8s-g2 pod pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd container env-test: <nil>
STEP: delete the pod
Dec  9 23:22:13.644: INFO: Waiting for pod pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:22:13.649: INFO: Pod pod-configmaps-400460dd-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:22:13.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2cjn4" for this suite.
Dec  9 23:22:19.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:22:19.763: INFO: namespace: e2e-tests-configmap-2cjn4, resource: bindings, ignored listing per whitelist
Dec  9 23:22:19.868: INFO: namespace e2e-tests-configmap-2cjn4 deletion completed in 6.199106682s

• [SLOW TEST:10.766 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:22:19.868: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-46689453-fc09-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:22:20.170: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-r5c7m" to be "success or failure"
Dec  9 23:22:20.198: INFO: Pod "pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.065195ms
Dec  9 23:22:22.205: INFO: Pod "pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035127641s
Dec  9 23:22:24.211: INFO: Pod "pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041242062s
STEP: Saw pod success
Dec  9 23:22:24.211: INFO: Pod "pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:22:24.216: INFO: Trying to get logs from node k8s-g1 pod pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:22:24.298: INFO: Waiting for pod pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:22:24.304: INFO: Pod pod-projected-configmaps-466ca795-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:22:24.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r5c7m" for this suite.
Dec  9 23:22:30.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:22:30.413: INFO: namespace: e2e-tests-projected-r5c7m, resource: bindings, ignored listing per whitelist
Dec  9 23:22:30.562: INFO: namespace e2e-tests-projected-r5c7m deletion completed in 6.252981779s

• [SLOW TEST:10.693 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:22:30.562: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:22:37.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-n9bfb" for this suite.
Dec  9 23:22:43.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:22:43.606: INFO: namespace: e2e-tests-namespaces-n9bfb, resource: bindings, ignored listing per whitelist
Dec  9 23:22:43.673: INFO: namespace e2e-tests-namespaces-n9bfb deletion completed in 6.18183925s
STEP: Destroying namespace "e2e-tests-nsdeletetest-w48xn" for this suite.
Dec  9 23:22:43.678: INFO: Namespace e2e-tests-nsdeletetest-w48xn was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-h2t6w" for this suite.
Dec  9 23:22:49.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:22:49.736: INFO: namespace: e2e-tests-nsdeletetest-h2t6w, resource: bindings, ignored listing per whitelist
Dec  9 23:22:49.875: INFO: namespace e2e-tests-nsdeletetest-h2t6w deletion completed in 6.197249269s

• [SLOW TEST:19.313 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:22:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  9 23:22:50.099: INFO: Waiting up to 5m0s for pod "pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-cclcx" to be "success or failure"
Dec  9 23:22:50.134: INFO: Pod "pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 35.203737ms
Dec  9 23:22:52.139: INFO: Pod "pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040625216s
Dec  9 23:22:54.145: INFO: Pod "pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046448032s
STEP: Saw pod success
Dec  9 23:22:54.145: INFO: Pod "pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:22:54.150: INFO: Trying to get logs from node k8s-g2 pod pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:22:54.275: INFO: Waiting for pod pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:22:54.280: INFO: Pod pod-5843baee-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:22:54.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cclcx" for this suite.
Dec  9 23:23:00.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:23:00.516: INFO: namespace: e2e-tests-emptydir-cclcx, resource: bindings, ignored listing per whitelist
Dec  9 23:23:00.527: INFO: namespace e2e-tests-emptydir-cclcx deletion completed in 6.24210123s

• [SLOW TEST:10.652 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:23:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-5eb1d14c-fc09-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 23:23:00.891: INFO: Waiting up to 5m0s for pod "pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-n2xng" to be "success or failure"
Dec  9 23:23:00.929: INFO: Pod "pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 37.70621ms
Dec  9 23:23:02.934: INFO: Pod "pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043279313s
Dec  9 23:23:04.941: INFO: Pod "pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049892527s
STEP: Saw pod success
Dec  9 23:23:04.941: INFO: Pod "pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:23:04.947: INFO: Trying to get logs from node k8s-g2 pod pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 23:23:04.995: INFO: Waiting for pod pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:23:05.000: INFO: Pod pod-secrets-5eb6e77c-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:23:05.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n2xng" for this suite.
Dec  9 23:23:11.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:23:11.146: INFO: namespace: e2e-tests-secrets-n2xng, resource: bindings, ignored listing per whitelist
Dec  9 23:23:11.170: INFO: namespace e2e-tests-secrets-n2xng deletion completed in 6.164515468s

• [SLOW TEST:10.642 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:23:11.170: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:23:11.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-j58sg" to be "success or failure"
Dec  9 23:23:11.543: INFO: Pod "downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 45.306319ms
Dec  9 23:23:13.549: INFO: Pod "downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051254103s
Dec  9 23:23:15.555: INFO: Pod "downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057530062s
STEP: Saw pod success
Dec  9 23:23:15.555: INFO: Pod "downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:23:15.560: INFO: Trying to get logs from node k8s-g2 pod downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:23:15.677: INFO: Waiting for pod downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:23:15.681: INFO: Pod downwardapi-volume-6505f6a0-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:23:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j58sg" for this suite.
Dec  9 23:23:21.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:23:21.840: INFO: namespace: e2e-tests-projected-j58sg, resource: bindings, ignored listing per whitelist
Dec  9 23:23:21.857: INFO: namespace e2e-tests-projected-j58sg deletion completed in 6.16433816s

• [SLOW TEST:10.687 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:23:21.857: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:23:22.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-lx7qd" to be "success or failure"
Dec  9 23:23:22.360: INFO: Pod "downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.58589ms
Dec  9 23:23:24.368: INFO: Pod "downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058648323s
Dec  9 23:23:26.374: INFO: Pod "downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065173789s
STEP: Saw pod success
Dec  9 23:23:26.374: INFO: Pod "downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:23:26.379: INFO: Trying to get logs from node k8s-g1 pod downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:23:26.467: INFO: Waiting for pod downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:23:26.475: INFO: Pod downwardapi-volume-6b6ebc77-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:23:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lx7qd" for this suite.
Dec  9 23:23:32.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:23:32.650: INFO: namespace: e2e-tests-projected-lx7qd, resource: bindings, ignored listing per whitelist
Dec  9 23:23:32.669: INFO: namespace e2e-tests-projected-lx7qd deletion completed in 6.184216109s

• [SLOW TEST:10.812 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:23:32.669: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:23:32.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 version'
Dec  9 23:23:33.001: INFO: stderr: ""
Dec  9 23:23:33.001: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T20:56:12Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:23:33.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z7c7g" for this suite.
Dec  9 23:23:39.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:23:39.162: INFO: namespace: e2e-tests-kubectl-z7c7g, resource: bindings, ignored listing per whitelist
Dec  9 23:23:39.194: INFO: namespace e2e-tests-kubectl-z7c7g deletion completed in 6.188585488s

• [SLOW TEST:6.525 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:23:39.194: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Dec  9 23:23:39.415: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  9 23:23:39.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:39.782: INFO: stderr: ""
Dec  9 23:23:39.782: INFO: stdout: "service/redis-slave created\n"
Dec  9 23:23:39.782: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  9 23:23:39.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:40.154: INFO: stderr: ""
Dec  9 23:23:40.154: INFO: stdout: "service/redis-master created\n"
Dec  9 23:23:40.154: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  9 23:23:40.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:40.616: INFO: stderr: ""
Dec  9 23:23:40.616: INFO: stdout: "service/frontend created\n"
Dec  9 23:23:40.616: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  9 23:23:40.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:40.928: INFO: stderr: ""
Dec  9 23:23:40.928: INFO: stdout: "deployment.extensions/frontend created\n"
Dec  9 23:23:40.928: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  9 23:23:40.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:41.231: INFO: stderr: ""
Dec  9 23:23:41.231: INFO: stdout: "deployment.extensions/redis-master created\n"
Dec  9 23:23:41.231: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  9 23:23:41.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:23:41.613: INFO: stderr: ""
Dec  9 23:23:41.613: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Dec  9 23:23:41.613: INFO: Waiting for all frontend pods to be Running.
Dec  9 23:24:21.664: INFO: Waiting for frontend to serve content.
Dec  9 23:24:22.726: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  9 23:24:28.742: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  9 23:24:33.758: INFO: Trying to add a new entry to the guestbook.
Dec  9 23:24:33.771: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  9 23:24:33.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.011: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  9 23:24:34.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.242: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.242: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  9 23:24:34.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.492: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.492: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  9 23:24:34.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.598: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.598: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  9 23:24:34.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.724: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.724: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  9 23:24:34.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qd2xl'
Dec  9 23:24:34.854: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:24:34.854: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:24:34.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qd2xl" for this suite.
Dec  9 23:25:14.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:25:14.994: INFO: namespace: e2e-tests-kubectl-qd2xl, resource: bindings, ignored listing per whitelist
Dec  9 23:25:15.054: INFO: namespace e2e-tests-kubectl-qd2xl deletion completed in 40.194717737s

• [SLOW TEST:95.860 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:25:15.054: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-aed3d111-fc09-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:25:15.382: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-858hw" to be "success or failure"
Dec  9 23:25:15.419: INFO: Pod "pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 37.100042ms
Dec  9 23:25:17.425: INFO: Pod "pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04326097s
Dec  9 23:25:19.455: INFO: Pod "pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073927699s
STEP: Saw pod success
Dec  9 23:25:19.456: INFO: Pod "pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:25:19.461: INFO: Trying to get logs from node k8s-g2 pod pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:25:19.603: INFO: Waiting for pod pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:25:19.608: INFO: Pod pod-projected-configmaps-aed64312-fc09-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:25:19.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-858hw" for this suite.
Dec  9 23:25:25.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:25:25.690: INFO: namespace: e2e-tests-projected-858hw, resource: bindings, ignored listing per whitelist
Dec  9 23:25:25.803: INFO: namespace e2e-tests-projected-858hw deletion completed in 6.189465705s

• [SLOW TEST:10.749 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:25:25.803: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-wbvkg
Dec  9 23:25:30.144: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-wbvkg
STEP: checking the pod's current state and verifying that restartCount is present
Dec  9 23:25:30.149: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:29:31.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wbvkg" for this suite.
Dec  9 23:29:37.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:29:37.654: INFO: namespace: e2e-tests-container-probe-wbvkg, resource: bindings, ignored listing per whitelist
Dec  9 23:29:37.849: INFO: namespace e2e-tests-container-probe-wbvkg deletion completed in 6.248072378s

• [SLOW TEST:252.046 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:29:37.849: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-xbhc
STEP: Creating a pod to test atomic-volume-subpath
Dec  9 23:29:38.205: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xbhc" in namespace "e2e-tests-subpath-jn8hs" to be "success or failure"
Dec  9 23:29:38.289: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Pending", Reason="", readiness=false. Elapsed: 83.614229ms
Dec  9 23:29:40.294: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089322238s
Dec  9 23:29:42.300: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095377656s
Dec  9 23:29:44.305: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 6.100525493s
Dec  9 23:29:46.311: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 8.106351003s
Dec  9 23:29:48.317: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 10.111920995s
Dec  9 23:29:50.323: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 12.118522932s
Dec  9 23:29:52.330: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 14.125165889s
Dec  9 23:29:54.336: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 16.131313532s
Dec  9 23:29:56.342: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 18.137174719s
Dec  9 23:29:58.348: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 20.142618833s
Dec  9 23:30:00.353: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Running", Reason="", readiness=false. Elapsed: 22.148244982s
Dec  9 23:30:02.370: INFO: Pod "pod-subpath-test-configmap-xbhc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.164991258s
STEP: Saw pod success
Dec  9 23:30:02.370: INFO: Pod "pod-subpath-test-configmap-xbhc" satisfied condition "success or failure"
Dec  9 23:30:02.375: INFO: Trying to get logs from node k8s-g2 pod pod-subpath-test-configmap-xbhc container test-container-subpath-configmap-xbhc: <nil>
STEP: delete the pod
Dec  9 23:30:02.504: INFO: Waiting for pod pod-subpath-test-configmap-xbhc to disappear
Dec  9 23:30:02.560: INFO: Pod pod-subpath-test-configmap-xbhc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xbhc
Dec  9 23:30:02.560: INFO: Deleting pod "pod-subpath-test-configmap-xbhc" in namespace "e2e-tests-subpath-jn8hs"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:30:02.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-jn8hs" for this suite.
Dec  9 23:30:08.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:30:08.704: INFO: namespace: e2e-tests-subpath-jn8hs, resource: bindings, ignored listing per whitelist
Dec  9 23:30:08.822: INFO: namespace e2e-tests-subpath-jn8hs deletion completed in 6.219235235s

• [SLOW TEST:30.973 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:30:08.822: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-5deac516-fc0a-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 23:30:09.109: INFO: Waiting up to 5m0s for pod "pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-secrets-lszmt" to be "success or failure"
Dec  9 23:30:09.178: INFO: Pod "pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 69.095689ms
Dec  9 23:30:11.210: INFO: Pod "pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100534298s
Dec  9 23:30:13.215: INFO: Pod "pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.106274548s
STEP: Saw pod success
Dec  9 23:30:13.215: INFO: Pod "pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:30:13.274: INFO: Trying to get logs from node k8s-g1 pod pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  9 23:30:13.427: INFO: Waiting for pod pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:30:13.433: INFO: Pod pod-secrets-5df2603c-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:30:13.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lszmt" for this suite.
Dec  9 23:30:19.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:30:19.533: INFO: namespace: e2e-tests-secrets-lszmt, resource: bindings, ignored listing per whitelist
Dec  9 23:30:19.632: INFO: namespace e2e-tests-secrets-lszmt deletion completed in 6.193396594s

• [SLOW TEST:10.810 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:30:19.632: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-6462cf3a-fc0a-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:30:20.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-jqm94" to be "success or failure"
Dec  9 23:30:20.061: INFO: Pod "pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.144785ms
Dec  9 23:30:22.067: INFO: Pod "pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031502796s
Dec  9 23:30:24.072: INFO: Pod "pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036943318s
STEP: Saw pod success
Dec  9 23:30:24.072: INFO: Pod "pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:30:24.077: INFO: Trying to get logs from node k8s-g2 pod pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:30:24.211: INFO: Waiting for pod pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:30:24.217: INFO: Pod pod-projected-configmaps-646c95f9-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:30:24.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jqm94" for this suite.
Dec  9 23:30:30.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:30:30.390: INFO: namespace: e2e-tests-projected-jqm94, resource: bindings, ignored listing per whitelist
Dec  9 23:30:30.430: INFO: namespace e2e-tests-projected-jqm94 deletion completed in 6.207924458s

• [SLOW TEST:10.798 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:30:30.430: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:30:34.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-z2ztk" for this suite.
Dec  9 23:30:40.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:30:41.010: INFO: namespace: e2e-tests-kubelet-test-z2ztk, resource: bindings, ignored listing per whitelist
Dec  9 23:30:41.123: INFO: namespace e2e-tests-kubelet-test-z2ztk deletion completed in 6.311180361s

• [SLOW TEST:10.694 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:30:41.124: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-692zh in namespace e2e-tests-proxy-97x6g
I1209 23:30:41.569732      17 runners.go:184] Created replication controller with name: proxy-service-692zh, namespace: e2e-tests-proxy-97x6g, replica count: 1
I1209 23:30:42.620018      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:30:43.620156      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:30:44.620295      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:30:45.620446      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:30:46.620593      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 23:30:47.620720      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1209 23:30:48.620868      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1209 23:30:49.621069      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1209 23:30:50.621224      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1209 23:30:51.621374      17 runners.go:184] proxy-service-692zh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 23:30:51.634: INFO: setup took 10.268502689s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  9 23:30:51.646: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 12.325207ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 12.464154ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.479834ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 12.609997ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 12.613273ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 12.950987ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 13.056635ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 13.002352ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 13.045562ms)
Dec  9 23:30:51.647: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.973394ms)
Dec  9 23:30:51.648: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 13.372058ms)
Dec  9 23:30:51.648: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 14.16211ms)
Dec  9 23:30:51.648: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 14.166005ms)
Dec  9 23:30:51.651: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 17.174948ms)
Dec  9 23:30:51.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 22.889364ms)
Dec  9 23:30:51.657: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 22.712951ms)
Dec  9 23:30:51.666: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 9.220283ms)
Dec  9 23:30:51.666: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 9.290756ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 10.580071ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 10.507719ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.529608ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 10.677842ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 11.172645ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 11.128131ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 11.167094ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 11.061642ms)
Dec  9 23:30:51.668: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 11.117324ms)
Dec  9 23:30:51.670: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.774154ms)
Dec  9 23:30:51.670: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.770436ms)
Dec  9 23:30:51.670: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.831667ms)
Dec  9 23:30:51.670: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.976731ms)
Dec  9 23:30:51.670: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 13.014235ms)
Dec  9 23:30:51.675: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 5.120623ms)
Dec  9 23:30:51.677: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 6.7354ms)
Dec  9 23:30:51.677: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 6.779151ms)
Dec  9 23:30:51.678: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.349044ms)
Dec  9 23:30:51.678: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.279948ms)
Dec  9 23:30:51.678: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 7.314768ms)
Dec  9 23:30:51.678: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.427087ms)
Dec  9 23:30:51.679: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 9.051832ms)
Dec  9 23:30:51.679: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 9.085834ms)
Dec  9 23:30:51.679: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 9.020015ms)
Dec  9 23:30:51.681: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 10.687416ms)
Dec  9 23:30:51.681: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 10.690907ms)
Dec  9 23:30:51.681: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.78142ms)
Dec  9 23:30:51.681: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 10.825952ms)
Dec  9 23:30:51.681: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 10.695053ms)
Dec  9 23:30:51.682: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.719029ms)
Dec  9 23:30:51.689: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 6.764425ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.223199ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.20324ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.18242ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 8.271783ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 8.295626ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.28608ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.361662ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.342685ms)
Dec  9 23:30:51.690: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.21621ms)
Dec  9 23:30:51.694: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.862676ms)
Dec  9 23:30:51.695: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.732301ms)
Dec  9 23:30:51.695: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.944368ms)
Dec  9 23:30:51.695: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.982294ms)
Dec  9 23:30:51.695: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 13.027052ms)
Dec  9 23:30:51.695: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.941087ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.239299ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.279312ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.348346ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.827048ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.180336ms)
Dec  9 23:30:51.703: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.19997ms)
Dec  9 23:30:51.704: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.27813ms)
Dec  9 23:30:51.704: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.307198ms)
Dec  9 23:30:51.704: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.1538ms)
Dec  9 23:30:51.704: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.337154ms)
Dec  9 23:30:51.704: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 9.073963ms)
Dec  9 23:30:51.706: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 10.417458ms)
Dec  9 23:30:51.707: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 11.992027ms)
Dec  9 23:30:51.707: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 12.135577ms)
Dec  9 23:30:51.707: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.007202ms)
Dec  9 23:30:51.707: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.083563ms)
Dec  9 23:30:51.714: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 6.821063ms)
Dec  9 23:30:51.716: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.379907ms)
Dec  9 23:30:51.716: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.307876ms)
Dec  9 23:30:51.716: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.365589ms)
Dec  9 23:30:51.716: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.519425ms)
Dec  9 23:30:51.717: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 9.336154ms)
Dec  9 23:30:51.717: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 9.187328ms)
Dec  9 23:30:51.717: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 9.263914ms)
Dec  9 23:30:51.717: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 9.20945ms)
Dec  9 23:30:51.717: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.292734ms)
Dec  9 23:30:51.718: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 10.222318ms)
Dec  9 23:30:51.720: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.631511ms)
Dec  9 23:30:51.720: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.58963ms)
Dec  9 23:30:51.720: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.735505ms)
Dec  9 23:30:51.720: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 12.718821ms)
Dec  9 23:30:51.720: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 12.833487ms)
Dec  9 23:30:51.726: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 5.314222ms)
Dec  9 23:30:51.728: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.199455ms)
Dec  9 23:30:51.728: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.721883ms)
Dec  9 23:30:51.728: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 7.901803ms)
Dec  9 23:30:51.728: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.867509ms)
Dec  9 23:30:51.729: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.776758ms)
Dec  9 23:30:51.729: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.026146ms)
Dec  9 23:30:51.729: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 8.517913ms)
Dec  9 23:30:51.729: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.355275ms)
Dec  9 23:30:51.729: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.505796ms)
Dec  9 23:30:51.730: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 9.299617ms)
Dec  9 23:30:51.730: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 9.690119ms)
Dec  9 23:30:51.732: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 10.923809ms)
Dec  9 23:30:51.732: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.224566ms)
Dec  9 23:30:51.732: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 11.261393ms)
Dec  9 23:30:51.732: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.306911ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.076661ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.152221ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.893078ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.950347ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.221007ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 8.287228ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 7.993719ms)
Dec  9 23:30:51.740: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.27675ms)
Dec  9 23:30:51.741: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.532391ms)
Dec  9 23:30:51.741: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.590686ms)
Dec  9 23:30:51.741: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 9.517786ms)
Dec  9 23:30:51.744: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.489285ms)
Dec  9 23:30:51.745: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.887757ms)
Dec  9 23:30:51.745: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 13.081954ms)
Dec  9 23:30:51.745: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 13.084509ms)
Dec  9 23:30:51.746: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 13.285402ms)
Dec  9 23:30:51.751: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 5.623066ms)
Dec  9 23:30:51.754: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.047178ms)
Dec  9 23:30:51.754: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.999393ms)
Dec  9 23:30:51.754: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.65646ms)
Dec  9 23:30:51.755: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.165169ms)
Dec  9 23:30:51.755: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 9.048027ms)
Dec  9 23:30:51.755: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 9.198258ms)
Dec  9 23:30:51.755: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.250746ms)
Dec  9 23:30:51.756: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 9.975841ms)
Dec  9 23:30:51.756: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 9.949456ms)
Dec  9 23:30:51.757: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 10.90348ms)
Dec  9 23:30:51.758: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.865049ms)
Dec  9 23:30:51.758: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.913662ms)
Dec  9 23:30:51.758: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 11.91756ms)
Dec  9 23:30:51.758: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 12.006763ms)
Dec  9 23:30:51.758: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 11.870347ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 6.917338ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.137971ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 6.988338ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.397235ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 7.476618ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.5444ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 7.781819ms)
Dec  9 23:30:51.765: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.766863ms)
Dec  9 23:30:51.766: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 7.761979ms)
Dec  9 23:30:51.767: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 9.060216ms)
Dec  9 23:30:51.767: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.098905ms)
Dec  9 23:30:51.767: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 9.576303ms)
Dec  9 23:30:51.768: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 10.349177ms)
Dec  9 23:30:51.769: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.039804ms)
Dec  9 23:30:51.770: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.234274ms)
Dec  9 23:30:51.770: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 12.179582ms)
Dec  9 23:30:51.778: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.965477ms)
Dec  9 23:30:51.778: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.12155ms)
Dec  9 23:30:51.778: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.080952ms)
Dec  9 23:30:51.778: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.15918ms)
Dec  9 23:30:51.779: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 9.184934ms)
Dec  9 23:30:51.779: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 9.151574ms)
Dec  9 23:30:51.779: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.174218ms)
Dec  9 23:30:51.779: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 9.162451ms)
Dec  9 23:30:51.780: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 9.912862ms)
Dec  9 23:30:51.780: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 9.960332ms)
Dec  9 23:30:51.780: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.132794ms)
Dec  9 23:30:51.780: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 10.33702ms)
Dec  9 23:30:51.782: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 11.625025ms)
Dec  9 23:30:51.783: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 13.141898ms)
Dec  9 23:30:51.783: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 13.03803ms)
Dec  9 23:30:51.783: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 13.145139ms)
Dec  9 23:30:51.788: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 5.081526ms)
Dec  9 23:30:51.790: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 7.035003ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.146193ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.141029ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.120444ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.206071ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 8.484532ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.555439ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.507151ms)
Dec  9 23:30:51.792: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 8.576881ms)
Dec  9 23:30:51.795: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 11.914199ms)
Dec  9 23:30:51.795: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.991461ms)
Dec  9 23:30:51.795: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.048951ms)
Dec  9 23:30:51.795: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.006383ms)
Dec  9 23:30:51.796: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.10762ms)
Dec  9 23:30:51.797: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 13.069794ms)
Dec  9 23:30:51.802: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 5.060458ms)
Dec  9 23:30:51.803: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 5.985701ms)
Dec  9 23:30:51.803: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 5.959542ms)
Dec  9 23:30:51.803: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 6.564432ms)
Dec  9 23:30:51.804: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 6.838348ms)
Dec  9 23:30:51.805: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.826588ms)
Dec  9 23:30:51.805: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.917661ms)
Dec  9 23:30:51.805: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.090331ms)
Dec  9 23:30:51.805: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.034333ms)
Dec  9 23:30:51.806: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 8.772508ms)
Dec  9 23:30:51.806: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 9.522581ms)
Dec  9 23:30:51.808: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 11.343693ms)
Dec  9 23:30:51.808: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.536798ms)
Dec  9 23:30:51.808: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.510679ms)
Dec  9 23:30:51.808: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 11.487685ms)
Dec  9 23:30:51.808: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.736163ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 8.262497ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 8.123163ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.258405ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.327843ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.705529ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.645461ms)
Dec  9 23:30:51.817: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.074413ms)
Dec  9 23:30:51.818: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 9.008776ms)
Dec  9 23:30:51.818: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.248508ms)
Dec  9 23:30:51.818: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 9.414273ms)
Dec  9 23:30:51.818: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 9.957255ms)
Dec  9 23:30:51.819: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.592062ms)
Dec  9 23:30:51.819: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 10.614591ms)
Dec  9 23:30:51.820: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 11.778228ms)
Dec  9 23:30:51.820: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.82421ms)
Dec  9 23:30:51.821: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.887243ms)
Dec  9 23:30:51.827: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 5.757881ms)
Dec  9 23:30:51.829: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.051647ms)
Dec  9 23:30:51.829: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.025438ms)
Dec  9 23:30:51.829: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.191078ms)
Dec  9 23:30:51.829: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.902648ms)
Dec  9 23:30:51.830: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.596055ms)
Dec  9 23:30:51.830: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.684409ms)
Dec  9 23:30:51.830: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.673203ms)
Dec  9 23:30:51.830: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.861838ms)
Dec  9 23:30:51.830: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.843846ms)
Dec  9 23:30:51.832: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 10.385744ms)
Dec  9 23:30:51.834: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.053948ms)
Dec  9 23:30:51.834: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 12.402112ms)
Dec  9 23:30:51.834: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.400887ms)
Dec  9 23:30:51.834: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 12.507515ms)
Dec  9 23:30:51.834: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.458041ms)
Dec  9 23:30:51.841: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.465225ms)
Dec  9 23:30:51.842: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 7.82632ms)
Dec  9 23:30:51.842: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.977641ms)
Dec  9 23:30:51.842: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.141327ms)
Dec  9 23:30:51.842: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.884211ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 9.543097ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 9.510424ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.557849ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 9.641476ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 9.59832ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 9.843994ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 9.928322ms)
Dec  9 23:30:51.844: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 10.036053ms)
Dec  9 23:30:51.845: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 11.317061ms)
Dec  9 23:30:51.845: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.449822ms)
Dec  9 23:30:51.846: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.40106ms)
Dec  9 23:30:51.852: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 5.205855ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.247815ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.227454ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 7.088644ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 7.139578ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.462343ms)
Dec  9 23:30:51.854: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.711947ms)
Dec  9 23:30:51.856: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.646067ms)
Dec  9 23:30:51.856: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 7.941888ms)
Dec  9 23:30:51.856: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 9.038034ms)
Dec  9 23:30:51.856: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 8.251041ms)
Dec  9 23:30:51.858: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.036277ms)
Dec  9 23:30:51.860: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.377916ms)
Dec  9 23:30:51.860: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.654882ms)
Dec  9 23:30:51.860: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 12.600705ms)
Dec  9 23:30:51.860: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.534706ms)
Dec  9 23:30:51.866: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 6.297687ms)
Dec  9 23:30:51.867: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 7.094994ms)
Dec  9 23:30:51.867: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 7.298769ms)
Dec  9 23:30:51.867: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 7.25305ms)
Dec  9 23:30:51.868: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 7.777398ms)
Dec  9 23:30:51.868: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.860367ms)
Dec  9 23:30:51.868: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.77782ms)
Dec  9 23:30:51.869: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.629433ms)
Dec  9 23:30:51.869: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 8.55859ms)
Dec  9 23:30:51.869: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.588639ms)
Dec  9 23:30:51.871: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 10.575426ms)
Dec  9 23:30:51.872: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 11.899532ms)
Dec  9 23:30:51.872: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 11.950649ms)
Dec  9 23:30:51.872: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 11.854996ms)
Dec  9 23:30:51.872: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 11.90412ms)
Dec  9 23:30:51.872: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.889988ms)
Dec  9 23:30:51.878: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 5.774283ms)
Dec  9 23:30:51.878: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 5.836526ms)
Dec  9 23:30:51.878: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 6.186216ms)
Dec  9 23:30:51.880: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.791173ms)
Dec  9 23:30:51.880: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 7.870096ms)
Dec  9 23:30:51.880: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 7.851953ms)
Dec  9 23:30:51.881: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 8.572202ms)
Dec  9 23:30:51.881: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 8.514973ms)
Dec  9 23:30:51.881: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.582454ms)
Dec  9 23:30:51.881: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 8.563922ms)
Dec  9 23:30:51.883: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.270151ms)
Dec  9 23:30:51.884: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 12.046044ms)
Dec  9 23:30:51.884: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 12.00221ms)
Dec  9 23:30:51.884: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 12.033675ms)
Dec  9 23:30:51.884: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 11.994483ms)
Dec  9 23:30:51.884: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.056953ms)
Dec  9 23:30:51.890: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:1080/proxy/rewri... (200; 5.723201ms)
Dec  9 23:30:51.891: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:462/proxy/: tls qux (200; 6.513282ms)
Dec  9 23:30:51.891: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:443/proxy/... (200; 6.451934ms)
Dec  9 23:30:51.891: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5/proxy/rewriteme"... (200; 6.820634ms)
Dec  9 23:30:51.891: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:1080/proxy/... (200; 6.792233ms)
Dec  9 23:30:51.892: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:160/proxy/: foo (200; 7.638549ms)
Dec  9 23:30:51.893: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/proxy-service-692zh-9z7k5:162/proxy/: bar (200; 8.038558ms)
Dec  9 23:30:51.894: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:160/proxy/: foo (200; 9.679742ms)
Dec  9 23:30:51.894: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/http:proxy-service-692zh-9z7k5:162/proxy/: bar (200; 9.610417ms)
Dec  9 23:30:51.894: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/pods/https:proxy-service-692zh-9z7k5:460/proxy/: tls baz (200; 9.594465ms)
Dec  9 23:30:51.895: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname1/proxy/: foo (200; 10.619239ms)
Dec  9 23:30:51.896: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/http:proxy-service-692zh:portname2/proxy/: bar (200; 11.608251ms)
Dec  9 23:30:51.897: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname2/proxy/: bar (200; 11.676908ms)
Dec  9 23:30:51.898: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/proxy-service-692zh:portname1/proxy/: foo (200; 12.792894ms)
Dec  9 23:30:51.898: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname2/proxy/: tls qux (200; 12.652077ms)
Dec  9 23:30:51.898: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97x6g/services/https:proxy-service-692zh:tlsportname1/proxy/: tls baz (200; 13.09697ms)
STEP: deleting ReplicationController proxy-service-692zh in namespace e2e-tests-proxy-97x6g, will wait for the garbage collector to delete the pods
Dec  9 23:30:51.977: INFO: Deleting ReplicationController proxy-service-692zh took: 24.922294ms
Dec  9 23:30:52.077: INFO: Terminating ReplicationController proxy-service-692zh pods took: 100.093058ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:31:01.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-97x6g" for this suite.
Dec  9 23:31:07.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:31:07.587: INFO: namespace: e2e-tests-proxy-97x6g, resource: bindings, ignored listing per whitelist
Dec  9 23:31:07.670: INFO: namespace e2e-tests-proxy-97x6g deletion completed in 6.187041301s

• [SLOW TEST:26.546 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:31:07.670: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:31:07.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-downward-api-n49mf" to be "success or failure"
Dec  9 23:31:08.000: INFO: Pod "downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.196834ms
Dec  9 23:31:10.006: INFO: Pod "downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037427216s
Dec  9 23:31:12.011: INFO: Pod "downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042966964s
STEP: Saw pod success
Dec  9 23:31:12.011: INFO: Pod "downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:31:12.016: INFO: Trying to get logs from node k8s-g1 pod downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:31:12.060: INFO: Waiting for pod downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:31:12.066: INFO: Pod downwardapi-volume-8107fce4-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:31:12.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n49mf" for this suite.
Dec  9 23:31:18.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:31:18.213: INFO: namespace: e2e-tests-downward-api-n49mf, resource: bindings, ignored listing per whitelist
Dec  9 23:31:18.363: INFO: namespace e2e-tests-downward-api-n49mf deletion completed in 6.292918213s

• [SLOW TEST:10.693 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:31:18.363: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-87557c0a-fc0a-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume configMaps
Dec  9 23:31:18.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-configmap-v5lbz" to be "success or failure"
Dec  9 23:31:18.598: INFO: Pod "pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380778ms
Dec  9 23:31:20.604: INFO: Pod "pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011171654s
Dec  9 23:31:22.609: INFO: Pod "pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016330158s
STEP: Saw pod success
Dec  9 23:31:22.609: INFO: Pod "pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:31:22.614: INFO: Trying to get logs from node k8s-g1 pod pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd container configmap-volume-test: <nil>
STEP: delete the pod
Dec  9 23:31:22.662: INFO: Waiting for pod pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:31:22.668: INFO: Pod pod-configmaps-875e4e82-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:31:22.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-v5lbz" for this suite.
Dec  9 23:31:28.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:31:28.745: INFO: namespace: e2e-tests-configmap-v5lbz, resource: bindings, ignored listing per whitelist
Dec  9 23:31:28.865: INFO: namespace e2e-tests-configmap-v5lbz deletion completed in 6.191726299s

• [SLOW TEST:10.502 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:31:28.865: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  9 23:31:29.277: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-6jfp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-6jfp2/configmaps/e2e-watch-test-resource-version,UID:8da10ccb-fc0a-11e8-9ee5-54a05085d523,ResourceVersion:56814,Generation:0,CreationTimestamp:2018-12-09 23:31:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  9 23:31:29.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-6jfp2,SelfLink:/api/v1/namespaces/e2e-tests-watch-6jfp2/configmaps/e2e-watch-test-resource-version,UID:8da10ccb-fc0a-11e8-9ee5-54a05085d523,ResourceVersion:56815,Generation:0,CreationTimestamp:2018-12-09 23:31:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:31:29.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6jfp2" for this suite.
Dec  9 23:31:35.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:31:35.446: INFO: namespace: e2e-tests-watch-6jfp2, resource: bindings, ignored listing per whitelist
Dec  9 23:31:35.543: INFO: namespace e2e-tests-watch-6jfp2 deletion completed in 6.260835354s

• [SLOW TEST:6.678 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:31:35.544: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-919cd4ee-fc0a-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 23:31:35.866: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-76hrn" to be "success or failure"
Dec  9 23:31:35.969: INFO: Pod "pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 102.411603ms
Dec  9 23:31:37.974: INFO: Pod "pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108011777s
Dec  9 23:31:39.980: INFO: Pod "pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113819209s
STEP: Saw pod success
Dec  9 23:31:39.980: INFO: Pod "pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:31:39.985: INFO: Trying to get logs from node k8s-g2 pod pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  9 23:31:40.077: INFO: Waiting for pod pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:31:40.082: INFO: Pod pod-projected-secrets-91a66161-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:31:40.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-76hrn" for this suite.
Dec  9 23:31:46.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:31:46.278: INFO: namespace: e2e-tests-projected-76hrn, resource: bindings, ignored listing per whitelist
Dec  9 23:31:46.298: INFO: namespace e2e-tests-projected-76hrn deletion completed in 6.211469184s

• [SLOW TEST:10.755 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:31:46.298: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:32:11.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-r82mw" for this suite.
Dec  9 23:32:17.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:32:17.635: INFO: namespace: e2e-tests-container-runtime-r82mw, resource: bindings, ignored listing per whitelist
Dec  9 23:32:17.646: INFO: namespace e2e-tests-container-runtime-r82mw deletion completed in 6.204625776s

• [SLOW TEST:31.348 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:32:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  9 23:32:17.950: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  9 23:32:22.956: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:32:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lxm52" for this suite.
Dec  9 23:32:29.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:32:29.335: INFO: namespace: e2e-tests-replication-controller-lxm52, resource: bindings, ignored listing per whitelist
Dec  9 23:32:29.353: INFO: namespace e2e-tests-replication-controller-lxm52 deletion completed in 6.317546943s

• [SLOW TEST:11.707 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:32:29.353: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1209 23:32:30.687072      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  9 23:32:30.687: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:32:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-s5qz8" for this suite.
Dec  9 23:32:36.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:32:36.740: INFO: namespace: e2e-tests-gc-s5qz8, resource: bindings, ignored listing per whitelist
Dec  9 23:32:36.882: INFO: namespace e2e-tests-gc-s5qz8 deletion completed in 6.191511759s

• [SLOW TEST:7.529 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:32:36.882: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  9 23:32:37.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-8tzpb" to be "success or failure"
Dec  9 23:32:37.129: INFO: Pod "downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.971565ms
Dec  9 23:32:39.134: INFO: Pod "downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038405661s
Dec  9 23:32:41.140: INFO: Pod "downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043960231s
STEP: Saw pod success
Dec  9 23:32:41.140: INFO: Pod "downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:32:41.145: INFO: Trying to get logs from node k8s-g1 pod downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd container client-container: <nil>
STEP: delete the pod
Dec  9 23:32:41.202: INFO: Waiting for pod downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:32:41.210: INFO: Pod downwardapi-volume-b6278dbf-fc0a-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:32:41.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8tzpb" for this suite.
Dec  9 23:32:47.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:32:47.399: INFO: namespace: e2e-tests-projected-8tzpb, resource: bindings, ignored listing per whitelist
Dec  9 23:32:47.463: INFO: namespace e2e-tests-projected-8tzpb deletion completed in 6.247667932s

• [SLOW TEST:10.581 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:32:47.463: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-gmfd6
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  9 23:32:47.670: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  9 23:33:14.095: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.4.198 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-gmfd6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 23:33:14.096: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 23:33:15.183: INFO: Found all expected endpoints: [netserver-0]
Dec  9 23:33:15.188: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.3.231 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-gmfd6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  9 23:33:15.188: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
Dec  9 23:33:16.254: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:33:16.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-gmfd6" for this suite.
Dec  9 23:33:40.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:33:40.450: INFO: namespace: e2e-tests-pod-network-test-gmfd6, resource: bindings, ignored listing per whitelist
Dec  9 23:33:40.478: INFO: namespace e2e-tests-pod-network-test-gmfd6 deletion completed in 24.218232812s

• [SLOW TEST:53.015 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:33:40.478: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:33:40.798: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  9 23:33:40.981: INFO: Number of nodes with available pods: 0
Dec  9 23:33:40.981: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  9 23:33:41.161: INFO: Number of nodes with available pods: 0
Dec  9 23:33:41.161: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:42.167: INFO: Number of nodes with available pods: 0
Dec  9 23:33:42.167: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:43.205: INFO: Number of nodes with available pods: 0
Dec  9 23:33:43.205: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:44.168: INFO: Number of nodes with available pods: 1
Dec  9 23:33:44.168: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  9 23:33:44.387: INFO: Number of nodes with available pods: 0
Dec  9 23:33:44.387: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  9 23:33:44.421: INFO: Number of nodes with available pods: 0
Dec  9 23:33:44.421: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:45.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:45.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:46.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:46.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:47.428: INFO: Number of nodes with available pods: 0
Dec  9 23:33:47.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:48.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:48.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:49.437: INFO: Number of nodes with available pods: 0
Dec  9 23:33:49.437: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:50.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:50.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:51.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:51.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:52.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:52.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:53.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:53.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:54.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:54.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:55.428: INFO: Number of nodes with available pods: 0
Dec  9 23:33:55.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:56.426: INFO: Number of nodes with available pods: 0
Dec  9 23:33:56.426: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:57.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:57.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:58.427: INFO: Number of nodes with available pods: 0
Dec  9 23:33:58.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:33:59.428: INFO: Number of nodes with available pods: 0
Dec  9 23:33:59.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:00.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:00.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:01.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:01.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:02.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:02.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:03.428: INFO: Number of nodes with available pods: 0
Dec  9 23:34:03.429: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:04.428: INFO: Number of nodes with available pods: 0
Dec  9 23:34:04.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:05.428: INFO: Number of nodes with available pods: 0
Dec  9 23:34:05.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:06.426: INFO: Number of nodes with available pods: 0
Dec  9 23:34:06.426: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:07.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:07.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:08.429: INFO: Number of nodes with available pods: 0
Dec  9 23:34:08.429: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:09.444: INFO: Number of nodes with available pods: 0
Dec  9 23:34:09.445: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:10.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:10.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:11.428: INFO: Number of nodes with available pods: 0
Dec  9 23:34:11.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:12.429: INFO: Number of nodes with available pods: 0
Dec  9 23:34:12.429: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:13.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:13.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:14.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:14.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:15.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:15.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:16.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:16.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:17.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:17.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:18.428: INFO: Number of nodes with available pods: 0
Dec  9 23:34:18.428: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:19.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:19.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:20.427: INFO: Number of nodes with available pods: 0
Dec  9 23:34:20.427: INFO: Node k8s-g1 is running more than one daemon pod
Dec  9 23:34:21.427: INFO: Number of nodes with available pods: 1
Dec  9 23:34:21.427: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tnc8r, will wait for the garbage collector to delete the pods
Dec  9 23:34:21.519: INFO: Deleting DaemonSet.extensions daemon-set took: 30.34207ms
Dec  9 23:34:21.720: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.171023ms
Dec  9 23:35:01.941: INFO: Number of nodes with available pods: 0
Dec  9 23:35:01.941: INFO: Number of running nodes: 0, number of available pods: 0
Dec  9 23:35:01.945: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tnc8r/daemonsets","resourceVersion":"57526"},"items":null}

Dec  9 23:35:01.949: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tnc8r/pods","resourceVersion":"57526"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:35:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tnc8r" for this suite.
Dec  9 23:35:10.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:35:10.218: INFO: namespace: e2e-tests-daemonsets-tnc8r, resource: bindings, ignored listing per whitelist
Dec  9 23:35:10.237: INFO: namespace e2e-tests-daemonsets-tnc8r deletion completed in 8.212742479s

• [SLOW TEST:89.759 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:35:10.237: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  9 23:35:10.467: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:35:14.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-l4lfc" for this suite.
Dec  9 23:35:23.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:35:23.104: INFO: namespace: e2e-tests-init-container-l4lfc, resource: bindings, ignored listing per whitelist
Dec  9 23:35:23.246: INFO: namespace e2e-tests-init-container-l4lfc deletion completed in 8.345502555s

• [SLOW TEST:13.009 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:35:23.246: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  9 23:35:23.569: INFO: Waiting up to 5m0s for pod "pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-emptydir-8zq9t" to be "success or failure"
Dec  9 23:35:23.585: INFO: Pod "pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.470429ms
Dec  9 23:35:25.616: INFO: Pod "pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047674114s
Dec  9 23:35:27.621: INFO: Pod "pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052821472s
STEP: Saw pod success
Dec  9 23:35:27.621: INFO: Pod "pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:35:27.626: INFO: Trying to get logs from node k8s-g1 pod pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd container test-container: <nil>
STEP: delete the pod
Dec  9 23:35:27.719: INFO: Waiting for pod pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:35:27.725: INFO: Pod pod-195fc3f5-fc0b-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:35:27.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8zq9t" for this suite.
Dec  9 23:35:33.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:35:33.859: INFO: namespace: e2e-tests-emptydir-8zq9t, resource: bindings, ignored listing per whitelist
Dec  9 23:35:33.954: INFO: namespace e2e-tests-emptydir-8zq9t deletion completed in 6.193708821s

• [SLOW TEST:10.708 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:35:33.954: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 23:35:34.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-7d9cs'
Dec  9 23:35:37.366: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  9 23:35:37.366: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Dec  9 23:35:41.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-7d9cs'
Dec  9 23:35:41.540: INFO: stderr: ""
Dec  9 23:35:41.540: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:35:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7d9cs" for this suite.
Dec  9 23:36:03.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:36:03.806: INFO: namespace: e2e-tests-kubectl-7d9cs, resource: bindings, ignored listing per whitelist
Dec  9 23:36:03.823: INFO: namespace e2e-tests-kubectl-7d9cs deletion completed in 22.277895087s

• [SLOW TEST:29.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:36:03.823: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  9 23:36:04.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 create -f - --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:04.308: INFO: stderr: ""
Dec  9 23:36:04.308: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  9 23:36:04.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:04.405: INFO: stderr: ""
Dec  9 23:36:04.405: INFO: stdout: "update-demo-nautilus-v554r "
STEP: Replicas for name=update-demo: expected=2 actual=1
Dec  9 23:36:09.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.469: INFO: stderr: ""
Dec  9 23:36:09.469: INFO: stdout: "update-demo-nautilus-jpzcz update-demo-nautilus-v554r "
Dec  9 23:36:09.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-jpzcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.524: INFO: stderr: ""
Dec  9 23:36:09.524: INFO: stdout: "true"
Dec  9 23:36:09.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-jpzcz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.578: INFO: stderr: ""
Dec  9 23:36:09.578: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 23:36:09.578: INFO: validating pod update-demo-nautilus-jpzcz
Dec  9 23:36:09.585: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 23:36:09.585: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 23:36:09.585: INFO: update-demo-nautilus-jpzcz is verified up and running
Dec  9 23:36:09.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-v554r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.639: INFO: stderr: ""
Dec  9 23:36:09.639: INFO: stdout: "true"
Dec  9 23:36:09.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods update-demo-nautilus-v554r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.693: INFO: stderr: ""
Dec  9 23:36:09.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  9 23:36:09.693: INFO: validating pod update-demo-nautilus-v554r
Dec  9 23:36:09.700: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 23:36:09.700: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 23:36:09.700: INFO: update-demo-nautilus-v554r is verified up and running
STEP: using delete to clean up resources
Dec  9 23:36:09.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.780: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 23:36:09.780: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  9 23:36:09.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-b884d'
Dec  9 23:36:09.965: INFO: stderr: "No resources found.\n"
Dec  9 23:36:09.965: INFO: stdout: ""
Dec  9 23:36:09.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 get pods -l name=update-demo --namespace=e2e-tests-kubectl-b884d -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 23:36:10.048: INFO: stderr: ""
Dec  9 23:36:10.048: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:36:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b884d" for this suite.
Dec  9 23:36:34.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:36:34.141: INFO: namespace: e2e-tests-kubectl-b884d, resource: bindings, ignored listing per whitelist
Dec  9 23:36:34.261: INFO: namespace e2e-tests-kubectl-b884d deletion completed in 24.20717206s

• [SLOW TEST:30.438 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:36:34.261: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-qjl7
STEP: Creating a pod to test atomic-volume-subpath
Dec  9 23:36:34.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qjl7" in namespace "e2e-tests-subpath-9gksr" to be "success or failure"
Dec  9 23:36:34.642: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457097ms
Dec  9 23:36:36.647: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012000923s
Dec  9 23:36:38.652: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017171564s
Dec  9 23:36:40.659: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 6.024245495s
Dec  9 23:36:42.666: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 8.030400796s
Dec  9 23:36:44.672: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 10.036544552s
Dec  9 23:36:46.678: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 12.042772064s
Dec  9 23:36:48.683: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 14.048157682s
Dec  9 23:36:50.689: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 16.05375679s
Dec  9 23:36:52.695: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 18.0595131s
Dec  9 23:36:54.700: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 20.065022609s
Dec  9 23:36:56.706: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Running", Reason="", readiness=false. Elapsed: 22.070946597s
Dec  9 23:36:58.712: INFO: Pod "pod-subpath-test-downwardapi-qjl7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.076571467s
STEP: Saw pod success
Dec  9 23:36:58.712: INFO: Pod "pod-subpath-test-downwardapi-qjl7" satisfied condition "success or failure"
Dec  9 23:36:58.717: INFO: Trying to get logs from node k8s-g1 pod pod-subpath-test-downwardapi-qjl7 container test-container-subpath-downwardapi-qjl7: <nil>
STEP: delete the pod
Dec  9 23:36:58.838: INFO: Waiting for pod pod-subpath-test-downwardapi-qjl7 to disappear
Dec  9 23:36:58.843: INFO: Pod pod-subpath-test-downwardapi-qjl7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qjl7
Dec  9 23:36:58.843: INFO: Deleting pod "pod-subpath-test-downwardapi-qjl7" in namespace "e2e-tests-subpath-9gksr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:36:58.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-9gksr" for this suite.
Dec  9 23:37:04.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:37:04.991: INFO: namespace: e2e-tests-subpath-9gksr, resource: bindings, ignored listing per whitelist
Dec  9 23:37:05.054: INFO: namespace e2e-tests-subpath-9gksr deletion completed in 6.200661334s

• [SLOW TEST:30.793 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:37:05.054: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  9 23:37:05.391: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Dec  9 23:37:05.397: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-m76k4/daemonsets","resourceVersion":"57971"},"items":null}

Dec  9 23:37:05.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-m76k4/pods","resourceVersion":"57971"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:37:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-m76k4" for this suite.
Dec  9 23:37:11.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:37:11.612: INFO: namespace: e2e-tests-daemonsets-m76k4, resource: bindings, ignored listing per whitelist
Dec  9 23:37:11.661: INFO: namespace e2e-tests-daemonsets-m76k4 deletion completed in 6.246636109s

S [SKIPPING] [6.607 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Dec  9 23:37:05.391: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:37:11.661: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:37:11.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-bfq4j" for this suite.
Dec  9 23:37:17.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:37:17.960: INFO: namespace: e2e-tests-services-bfq4j, resource: bindings, ignored listing per whitelist
Dec  9 23:37:18.108: INFO: namespace e2e-tests-services-bfq4j deletion completed in 6.262484291s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.447 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:37:18.109: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  9 23:37:18.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-b54st'
Dec  9 23:37:18.424: INFO: stderr: ""
Dec  9 23:37:18.424: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Dec  9 23:37:18.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348219877 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-b54st'
Dec  9 23:37:21.324: INFO: stderr: ""
Dec  9 23:37:21.324: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:37:21.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b54st" for this suite.
Dec  9 23:37:27.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:37:27.383: INFO: namespace: e2e-tests-kubectl-b54st, resource: bindings, ignored listing per whitelist
Dec  9 23:37:27.485: INFO: namespace e2e-tests-kubectl-b54st deletion completed in 6.155676441s

• [SLOW TEST:9.377 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:37:27.485: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Dec  9 23:37:32.117: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:37:56.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-9nbzn" for this suite.
Dec  9 23:38:02.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:38:02.594: INFO: namespace: e2e-tests-namespaces-9nbzn, resource: bindings, ignored listing per whitelist
Dec  9 23:38:02.721: INFO: namespace e2e-tests-namespaces-9nbzn deletion completed in 6.203985552s
STEP: Destroying namespace "e2e-tests-nsdeletetest-hdmm2" for this suite.
Dec  9 23:38:02.726: INFO: Namespace e2e-tests-nsdeletetest-hdmm2 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-n2khw" for this suite.
Dec  9 23:38:08.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:38:08.855: INFO: namespace: e2e-tests-nsdeletetest-n2khw, resource: bindings, ignored listing per whitelist
Dec  9 23:38:08.916: INFO: namespace e2e-tests-nsdeletetest-n2khw deletion completed in 6.19040144s

• [SLOW TEST:41.431 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:38:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-7c16b3da-fc0b-11e8-a4b0-6e57f5092bbd
STEP: Creating secret with name s-test-opt-upd-7c16b404-fc0b-11e8-a4b0-6e57f5092bbd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7c16b3da-fc0b-11e8-a4b0-6e57f5092bbd
STEP: Updating secret s-test-opt-upd-7c16b404-fc0b-11e8-a4b0-6e57f5092bbd
STEP: Creating secret with name s-test-opt-create-7c16b416-fc0b-11e8-a4b0-6e57f5092bbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:38:15.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7wv6h" for this suite.
Dec  9 23:38:39.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:38:39.645: INFO: namespace: e2e-tests-secrets-7wv6h, resource: bindings, ignored listing per whitelist
Dec  9 23:38:39.765: INFO: namespace e2e-tests-secrets-7wv6h deletion completed in 24.218481475s

• [SLOW TEST:30.848 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:38:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  9 23:38:40.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58285,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  9 23:38:40.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58286,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  9 23:38:40.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58287,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  9 23:38:50.342: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58307,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  9 23:38:50.342: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58308,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  9 23:38:50.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-ltm2s,SelfLink:/api/v1/namespaces/e2e-tests-watch-ltm2s/configmaps/e2e-watch-test-label-changed,UID:8e767c7c-fc0b-11e8-9ee5-54a05085d523,ResourceVersion:58310,Generation:0,CreationTimestamp:2018-12-09 23:38:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:38:50.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-ltm2s" for this suite.
Dec  9 23:38:56.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:38:56.672: INFO: namespace: e2e-tests-watch-ltm2s, resource: bindings, ignored listing per whitelist
Dec  9 23:38:56.685: INFO: namespace e2e-tests-watch-ltm2s deletion completed in 6.317617147s

• [SLOW TEST:16.920 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:38:56.685: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Dec  9 23:38:56.978: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-348219877 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:38:57.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jg28x" for this suite.
Dec  9 23:39:03.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:39:03.217: INFO: namespace: e2e-tests-kubectl-jg28x, resource: bindings, ignored listing per whitelist
Dec  9 23:39:03.258: INFO: namespace e2e-tests-kubectl-jg28x deletion completed in 6.227764309s

• [SLOW TEST:6.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:39:03.258: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  9 23:39:08.156: INFO: Successfully updated pod "labelsupdate9c721a14-fc0b-11e8-a4b0-6e57f5092bbd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:39:10.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8cvjq" for this suite.
Dec  9 23:39:32.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:39:32.271: INFO: namespace: e2e-tests-projected-8cvjq, resource: bindings, ignored listing per whitelist
Dec  9 23:39:32.430: INFO: namespace e2e-tests-projected-8cvjq deletion completed in 22.218095638s

• [SLOW TEST:29.171 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:39:32.430: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Dec  9 23:39:32.666: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-348219877 proxy --unix-socket=/tmp/kubectl-proxy-unix522703872/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:39:32.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qz44w" for this suite.
Dec  9 23:39:38.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:39:39.023: INFO: namespace: e2e-tests-kubectl-qz44w, resource: bindings, ignored listing per whitelist
Dec  9 23:39:39.033: INFO: namespace e2e-tests-kubectl-qz44w deletion completed in 6.310766225s

• [SLOW TEST:6.603 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  9 23:39:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-348219877
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-b1dba61f-fc0b-11e8-a4b0-6e57f5092bbd
STEP: Creating a pod to test consume secrets
Dec  9 23:39:39.506: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd" in namespace "e2e-tests-projected-g29cd" to be "success or failure"
Dec  9 23:39:39.535: INFO: Pod "pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.607097ms
Dec  9 23:39:41.540: INFO: Pod "pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034155205s
Dec  9 23:39:43.557: INFO: Pod "pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050511835s
STEP: Saw pod success
Dec  9 23:39:43.557: INFO: Pod "pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd" satisfied condition "success or failure"
Dec  9 23:39:43.562: INFO: Trying to get logs from node k8s-g1 pod pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  9 23:39:43.707: INFO: Waiting for pod pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd to disappear
Dec  9 23:39:43.711: INFO: Pod pod-projected-secrets-b1e1ac75-fc0b-11e8-a4b0-6e57f5092bbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  9 23:39:43.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g29cd" for this suite.
Dec  9 23:39:49.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  9 23:39:49.833: INFO: namespace: e2e-tests-projected-g29cd, resource: bindings, ignored listing per whitelist
Dec  9 23:39:49.890: INFO: namespace e2e-tests-projected-g29cd deletion completed in 6.173213678s

• [SLOW TEST:10.857 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec  9 23:39:49.891: INFO: Running AfterSuite actions on all nodes
Dec  9 23:39:49.891: INFO: Running AfterSuite actions on node 1
Dec  9 23:39:49.891: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6068.718 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h41m9.269267952s
Test Suite Passed
