I1205 21:41:00.536147      17 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-502647789
I1205 21:41:00.536269      17 e2e.go:224] Starting e2e run "74d54637-f8d6-11e8-8a71-d2079f97accb" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1544046060 - Will randomize all specs
Will run 201 of 1946 specs

Dec  5 21:41:00.690: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 21:41:00.691: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  5 21:41:00.697: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  5 21:41:00.716: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  5 21:41:00.716: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Dec  5 21:41:00.716: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  5 21:41:00.720: INFO: e2e test version: v1.13.0
Dec  5 21:41:00.720: INFO: kube-apiserver version: v1.13.0
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:41:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
Dec  5 21:41:00.758: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 21:41:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:41:06.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-smbrl" for this suite.
Dec  5 21:41:46.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:41:46.890: INFO: namespace: e2e-tests-pods-smbrl, resource: bindings, ignored listing per whitelist
Dec  5 21:41:46.897: INFO: namespace e2e-tests-pods-smbrl deletion completed in 40.051335123s

â€¢ [SLOW TEST:46.177 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:41:46.897: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  5 21:41:46.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:47.561: INFO: stderr: ""
Dec  5 21:41:47.561: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 21:41:47.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:47.641: INFO: stderr: ""
Dec  5 21:41:47.641: INFO: stdout: "update-demo-nautilus-5tp48 update-demo-nautilus-tcnk5 "
Dec  5 21:41:47.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5tp48 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:47.707: INFO: stderr: ""
Dec  5 21:41:47.707: INFO: stdout: ""
Dec  5 21:41:47.707: INFO: update-demo-nautilus-5tp48 is created but not running
Dec  5 21:41:52.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:52.776: INFO: stderr: ""
Dec  5 21:41:52.776: INFO: stdout: "update-demo-nautilus-5tp48 update-demo-nautilus-tcnk5 "
Dec  5 21:41:52.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5tp48 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:52.839: INFO: stderr: ""
Dec  5 21:41:52.839: INFO: stdout: "true"
Dec  5 21:41:52.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5tp48 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:52.893: INFO: stderr: ""
Dec  5 21:41:52.893: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 21:41:52.893: INFO: validating pod update-demo-nautilus-5tp48
Dec  5 21:41:52.895: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 21:41:52.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 21:41:52.895: INFO: update-demo-nautilus-5tp48 is verified up and running
Dec  5 21:41:52.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-tcnk5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:52.944: INFO: stderr: ""
Dec  5 21:41:52.944: INFO: stdout: "true"
Dec  5 21:41:52.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-tcnk5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:52.997: INFO: stderr: ""
Dec  5 21:41:52.997: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 21:41:52.997: INFO: validating pod update-demo-nautilus-tcnk5
Dec  5 21:41:53.000: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 21:41:53.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 21:41:53.000: INFO: update-demo-nautilus-tcnk5 is verified up and running
STEP: using delete to clean up resources
Dec  5 21:41:53.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:53.070: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 21:41:53.070: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 21:41:53.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-w2qgv'
Dec  5 21:41:53.137: INFO: stderr: "No resources found.\n"
Dec  5 21:41:53.137: INFO: stdout: ""
Dec  5 21:41:53.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -l name=update-demo --namespace=e2e-tests-kubectl-w2qgv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 21:41:53.200: INFO: stderr: ""
Dec  5 21:41:53.200: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:41:53.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w2qgv" for this suite.
Dec  5 21:42:15.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:42:15.245: INFO: namespace: e2e-tests-kubectl-w2qgv, resource: bindings, ignored listing per whitelist
Dec  5 21:42:15.260: INFO: namespace e2e-tests-kubectl-w2qgv deletion completed in 22.058883497s

â€¢ [SLOW TEST:28.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:42:15.260: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qbvsg
Dec  5 21:42:17.328: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qbvsg
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 21:42:17.332: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:46:17.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qbvsg" for this suite.
Dec  5 21:46:23.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:46:23.811: INFO: namespace: e2e-tests-container-probe-qbvsg, resource: bindings, ignored listing per whitelist
Dec  5 21:46:23.849: INFO: namespace e2e-tests-container-probe-qbvsg deletion completed in 6.06123996s

â€¢ [SLOW TEST:248.589 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:46:23.849: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 21:46:23.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-2xz68'
Dec  5 21:46:23.966: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 21:46:23.966: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Dec  5 21:46:23.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-2xz68'
Dec  5 21:46:24.038: INFO: stderr: ""
Dec  5 21:46:24.038: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:46:24.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2xz68" for this suite.
Dec  5 21:46:46.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:46:46.070: INFO: namespace: e2e-tests-kubectl-2xz68, resource: bindings, ignored listing per whitelist
Dec  5 21:46:46.087: INFO: namespace e2e-tests-kubectl-2xz68 deletion completed in 22.045649888s

â€¢ [SLOW TEST:22.238 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:46:46.087: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2kj6g
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Dec  5 21:46:46.133: INFO: Found 0 stateful pods, waiting for 3
Dec  5 21:46:56.137: INFO: Found 1 stateful pods, waiting for 3
Dec  5 21:47:06.136: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 21:47:06.137: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 21:47:06.137: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 21:47:06.160: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  5 21:47:16.192: INFO: Updating stateful set ss2
Dec  5 21:47:16.198: INFO: Waiting for Pod e2e-tests-statefulset-2kj6g/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Dec  5 21:47:26.265: INFO: Found 2 stateful pods, waiting for 3
Dec  5 21:47:36.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 21:47:36.268: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 21:47:36.268: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  5 21:47:36.292: INFO: Updating stateful set ss2
Dec  5 21:47:36.302: INFO: Waiting for Pod e2e-tests-statefulset-2kj6g/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  5 21:47:46.324: INFO: Updating stateful set ss2
Dec  5 21:47:46.329: INFO: Waiting for StatefulSet e2e-tests-statefulset-2kj6g/ss2 to complete update
Dec  5 21:47:46.329: INFO: Waiting for Pod e2e-tests-statefulset-2kj6g/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 21:47:56.333: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2kj6g
Dec  5 21:47:56.335: INFO: Scaling statefulset ss2 to 0
Dec  5 21:48:06.347: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 21:48:06.351: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:48:06.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2kj6g" for this suite.
Dec  5 21:48:12.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:48:12.396: INFO: namespace: e2e-tests-statefulset-2kj6g, resource: bindings, ignored listing per whitelist
Dec  5 21:48:12.413: INFO: namespace e2e-tests-statefulset-2kj6g deletion completed in 6.050334078s

â€¢ [SLOW TEST:86.326 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:48:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 21:48:12.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-9lp7r" to be "success or failure"
Dec  5 21:48:12.464: INFO: Pod "downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.477729ms
Dec  5 21:48:14.467: INFO: Pod "downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432229s
Dec  5 21:48:16.470: INFO: Pod "downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007328545s
STEP: Saw pod success
Dec  5 21:48:16.470: INFO: Pod "downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:48:16.473: INFO: Trying to get logs from node single-node pod downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 21:48:16.506: INFO: Waiting for pod downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb to disappear
Dec  5 21:48:16.510: INFO: Pod downwardapi-volume-767f9239-f8d7-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:48:16.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9lp7r" for this suite.
Dec  5 21:48:22.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:48:22.550: INFO: namespace: e2e-tests-downward-api-9lp7r, resource: bindings, ignored listing per whitelist
Dec  5 21:48:22.570: INFO: namespace e2e-tests-downward-api-9lp7r deletion completed in 6.058997733s

â€¢ [SLOW TEST:10.157 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:48:22.571: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-gqbg6
I1205 21:48:22.608909      17 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-gqbg6, replica count: 1
I1205 21:48:23.663820      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 21:48:23.775: INFO: Created: latency-svc-zzbmc
Dec  5 21:48:23.780: INFO: Got endpoints: latency-svc-zzbmc [15.305985ms]
Dec  5 21:48:23.792: INFO: Created: latency-svc-ccr4n
Dec  5 21:48:23.795: INFO: Got endpoints: latency-svc-ccr4n [13.890967ms]
Dec  5 21:48:23.797: INFO: Created: latency-svc-7ktrq
Dec  5 21:48:23.801: INFO: Got endpoints: latency-svc-7ktrq [19.884853ms]
Dec  5 21:48:23.802: INFO: Created: latency-svc-xpc92
Dec  5 21:48:23.805: INFO: Got endpoints: latency-svc-xpc92 [23.269815ms]
Dec  5 21:48:23.810: INFO: Created: latency-svc-s7t56
Dec  5 21:48:23.810: INFO: Created: latency-svc-4x6qg
Dec  5 21:48:23.816: INFO: Created: latency-svc-4szdr
Dec  5 21:48:23.817: INFO: Got endpoints: latency-svc-s7t56 [35.201068ms]
Dec  5 21:48:23.818: INFO: Got endpoints: latency-svc-4x6qg [36.880275ms]
Dec  5 21:48:23.821: INFO: Got endpoints: latency-svc-4szdr [39.273234ms]
Dec  5 21:48:23.824: INFO: Created: latency-svc-h596q
Dec  5 21:48:23.827: INFO: Created: latency-svc-t6bc9
Dec  5 21:48:23.828: INFO: Got endpoints: latency-svc-h596q [46.594089ms]
Dec  5 21:48:23.832: INFO: Got endpoints: latency-svc-t6bc9 [50.384415ms]
Dec  5 21:48:23.836: INFO: Created: latency-svc-9xx8g
Dec  5 21:48:23.839: INFO: Got endpoints: latency-svc-9xx8g [57.930681ms]
Dec  5 21:48:23.842: INFO: Created: latency-svc-b5j95
Dec  5 21:48:23.844: INFO: Got endpoints: latency-svc-b5j95 [62.909049ms]
Dec  5 21:48:23.847: INFO: Created: latency-svc-gwdxm
Dec  5 21:48:23.850: INFO: Got endpoints: latency-svc-gwdxm [68.111364ms]
Dec  5 21:48:23.853: INFO: Created: latency-svc-ktmm9
Dec  5 21:48:23.855: INFO: Created: latency-svc-c5fqn
Dec  5 21:48:23.857: INFO: Got endpoints: latency-svc-ktmm9 [75.965984ms]
Dec  5 21:48:23.861: INFO: Got endpoints: latency-svc-c5fqn [79.50813ms]
Dec  5 21:48:23.870: INFO: Created: latency-svc-zdhv9
Dec  5 21:48:23.873: INFO: Got endpoints: latency-svc-zdhv9 [91.078768ms]
Dec  5 21:48:23.875: INFO: Created: latency-svc-sft7h
Dec  5 21:48:23.878: INFO: Got endpoints: latency-svc-sft7h [96.420857ms]
Dec  5 21:48:23.883: INFO: Created: latency-svc-d62qq
Dec  5 21:48:23.886: INFO: Got endpoints: latency-svc-d62qq [91.005079ms]
Dec  5 21:48:23.890: INFO: Created: latency-svc-k9mhh
Dec  5 21:48:23.891: INFO: Created: latency-svc-hj667
Dec  5 21:48:23.894: INFO: Got endpoints: latency-svc-hj667 [89.158594ms]
Dec  5 21:48:23.895: INFO: Got endpoints: latency-svc-k9mhh [93.400122ms]
Dec  5 21:48:23.898: INFO: Created: latency-svc-phcn4
Dec  5 21:48:23.902: INFO: Created: latency-svc-rmgwz
Dec  5 21:48:23.904: INFO: Got endpoints: latency-svc-phcn4 [87.194686ms]
Dec  5 21:48:23.905: INFO: Got endpoints: latency-svc-rmgwz [86.712589ms]
Dec  5 21:48:23.910: INFO: Created: latency-svc-cwnsc
Dec  5 21:48:23.910: INFO: Got endpoints: latency-svc-cwnsc [89.220726ms]
Dec  5 21:48:23.916: INFO: Created: latency-svc-8kkfs
Dec  5 21:48:23.919: INFO: Got endpoints: latency-svc-8kkfs [90.690156ms]
Dec  5 21:48:23.922: INFO: Created: latency-svc-wnglm
Dec  5 21:48:23.923: INFO: Created: latency-svc-6mv4j
Dec  5 21:48:23.925: INFO: Got endpoints: latency-svc-wnglm [93.183775ms]
Dec  5 21:48:23.930: INFO: Got endpoints: latency-svc-6mv4j [90.789929ms]
Dec  5 21:48:23.932: INFO: Created: latency-svc-848b7
Dec  5 21:48:23.940: INFO: Created: latency-svc-w6q24
Dec  5 21:48:23.940: INFO: Got endpoints: latency-svc-848b7 [95.709823ms]
Dec  5 21:48:23.943: INFO: Created: latency-svc-7942t
Dec  5 21:48:23.948: INFO: Got endpoints: latency-svc-7942t [90.715578ms]
Dec  5 21:48:23.949: INFO: Got endpoints: latency-svc-w6q24 [99.420599ms]
Dec  5 21:48:23.952: INFO: Created: latency-svc-qmmkg
Dec  5 21:48:23.953: INFO: Got endpoints: latency-svc-qmmkg [91.589795ms]
Dec  5 21:48:23.957: INFO: Created: latency-svc-f8626
Dec  5 21:48:23.959: INFO: Got endpoints: latency-svc-f8626 [86.042105ms]
Dec  5 21:48:23.960: INFO: Created: latency-svc-hh8gp
Dec  5 21:48:23.963: INFO: Created: latency-svc-dw8tn
Dec  5 21:48:23.964: INFO: Got endpoints: latency-svc-hh8gp [86.275539ms]
Dec  5 21:48:23.967: INFO: Created: latency-svc-vhvc2
Dec  5 21:48:23.969: INFO: Got endpoints: latency-svc-dw8tn [82.730658ms]
Dec  5 21:48:23.971: INFO: Got endpoints: latency-svc-vhvc2 [76.93947ms]
Dec  5 21:48:23.973: INFO: Created: latency-svc-ndgqm
Dec  5 21:48:23.976: INFO: Got endpoints: latency-svc-ndgqm [81.390657ms]
Dec  5 21:48:23.978: INFO: Created: latency-svc-568ln
Dec  5 21:48:23.980: INFO: Created: latency-svc-8vz9f
Dec  5 21:48:23.983: INFO: Created: latency-svc-wckrb
Dec  5 21:48:23.988: INFO: Created: latency-svc-6lxss
Dec  5 21:48:23.991: INFO: Created: latency-svc-mpkz5
Dec  5 21:48:23.996: INFO: Created: latency-svc-fc2xh
Dec  5 21:48:23.998: INFO: Created: latency-svc-zb2j9
Dec  5 21:48:24.005: INFO: Created: latency-svc-7v5gc
Dec  5 21:48:24.012: INFO: Created: latency-svc-sddh8
Dec  5 21:48:24.012: INFO: Created: latency-svc-psfx8
Dec  5 21:48:24.012: INFO: Created: latency-svc-c4bx7
Dec  5 21:48:24.016: INFO: Created: latency-svc-gtpqz
Dec  5 21:48:24.018: INFO: Created: latency-svc-v6cd2
Dec  5 21:48:24.020: INFO: Created: latency-svc-2tjnc
Dec  5 21:48:24.026: INFO: Created: latency-svc-vf2dg
Dec  5 21:48:24.027: INFO: Got endpoints: latency-svc-568ln [120.489622ms]
Dec  5 21:48:24.033: INFO: Created: latency-svc-spp8m
Dec  5 21:48:24.076: INFO: Got endpoints: latency-svc-8vz9f [168.158996ms]
Dec  5 21:48:24.084: INFO: Created: latency-svc-7k6n9
Dec  5 21:48:24.127: INFO: Got endpoints: latency-svc-wckrb [215.916917ms]
Dec  5 21:48:24.136: INFO: Created: latency-svc-lg5bz
Dec  5 21:48:24.177: INFO: Got endpoints: latency-svc-6lxss [257.888543ms]
Dec  5 21:48:24.186: INFO: Created: latency-svc-4d6x6
Dec  5 21:48:24.227: INFO: Got endpoints: latency-svc-mpkz5 [301.750981ms]
Dec  5 21:48:24.233: INFO: Created: latency-svc-6r2kn
Dec  5 21:48:24.279: INFO: Got endpoints: latency-svc-fc2xh [348.880543ms]
Dec  5 21:48:24.293: INFO: Created: latency-svc-zfw6j
Dec  5 21:48:24.326: INFO: Got endpoints: latency-svc-zb2j9 [386.456661ms]
Dec  5 21:48:24.335: INFO: Created: latency-svc-xkdvl
Dec  5 21:48:24.375: INFO: Got endpoints: latency-svc-c4bx7 [427.306345ms]
Dec  5 21:48:24.379: INFO: Created: latency-svc-zgkpt
Dec  5 21:48:24.427: INFO: Got endpoints: latency-svc-7v5gc [477.753542ms]
Dec  5 21:48:24.434: INFO: Created: latency-svc-jbzdf
Dec  5 21:48:24.477: INFO: Got endpoints: latency-svc-psfx8 [523.636552ms]
Dec  5 21:48:24.484: INFO: Created: latency-svc-r96q7
Dec  5 21:48:24.528: INFO: Got endpoints: latency-svc-sddh8 [569.270404ms]
Dec  5 21:48:24.537: INFO: Created: latency-svc-4b5sp
Dec  5 21:48:24.577: INFO: Got endpoints: latency-svc-v6cd2 [612.631982ms]
Dec  5 21:48:24.584: INFO: Created: latency-svc-rdlh9
Dec  5 21:48:24.626: INFO: Got endpoints: latency-svc-gtpqz [657.214282ms]
Dec  5 21:48:24.633: INFO: Created: latency-svc-jxvwp
Dec  5 21:48:24.675: INFO: Got endpoints: latency-svc-2tjnc [704.276198ms]
Dec  5 21:48:24.685: INFO: Created: latency-svc-656m7
Dec  5 21:48:24.726: INFO: Got endpoints: latency-svc-vf2dg [749.931905ms]
Dec  5 21:48:24.732: INFO: Created: latency-svc-vlcjj
Dec  5 21:48:24.776: INFO: Got endpoints: latency-svc-spp8m [748.928397ms]
Dec  5 21:48:24.784: INFO: Created: latency-svc-qb2fp
Dec  5 21:48:24.825: INFO: Got endpoints: latency-svc-7k6n9 [747.71406ms]
Dec  5 21:48:24.833: INFO: Created: latency-svc-5gmjj
Dec  5 21:48:24.877: INFO: Got endpoints: latency-svc-lg5bz [750.214727ms]
Dec  5 21:48:24.887: INFO: Created: latency-svc-v6dlz
Dec  5 21:48:24.927: INFO: Got endpoints: latency-svc-4d6x6 [749.84528ms]
Dec  5 21:48:24.933: INFO: Created: latency-svc-q2r8v
Dec  5 21:48:24.976: INFO: Got endpoints: latency-svc-6r2kn [748.933849ms]
Dec  5 21:48:24.985: INFO: Created: latency-svc-p9cgb
Dec  5 21:48:25.027: INFO: Got endpoints: latency-svc-zfw6j [747.810833ms]
Dec  5 21:48:25.034: INFO: Created: latency-svc-xq7t8
Dec  5 21:48:25.075: INFO: Got endpoints: latency-svc-xkdvl [748.785092ms]
Dec  5 21:48:25.086: INFO: Created: latency-svc-dbpnl
Dec  5 21:48:25.127: INFO: Got endpoints: latency-svc-zgkpt [751.878115ms]
Dec  5 21:48:25.134: INFO: Created: latency-svc-nwb9h
Dec  5 21:48:25.178: INFO: Got endpoints: latency-svc-jbzdf [750.547937ms]
Dec  5 21:48:25.184: INFO: Created: latency-svc-ct2lm
Dec  5 21:48:25.227: INFO: Got endpoints: latency-svc-r96q7 [749.587772ms]
Dec  5 21:48:25.236: INFO: Created: latency-svc-8r2dp
Dec  5 21:48:25.277: INFO: Got endpoints: latency-svc-4b5sp [748.749049ms]
Dec  5 21:48:25.289: INFO: Created: latency-svc-8hwbw
Dec  5 21:48:25.326: INFO: Got endpoints: latency-svc-rdlh9 [749.150708ms]
Dec  5 21:48:25.333: INFO: Created: latency-svc-hxdd5
Dec  5 21:48:25.376: INFO: Got endpoints: latency-svc-jxvwp [749.731335ms]
Dec  5 21:48:25.382: INFO: Created: latency-svc-5l2fm
Dec  5 21:48:25.436: INFO: Got endpoints: latency-svc-656m7 [761.211866ms]
Dec  5 21:48:25.454: INFO: Created: latency-svc-tqptd
Dec  5 21:48:25.476: INFO: Got endpoints: latency-svc-vlcjj [750.216258ms]
Dec  5 21:48:25.484: INFO: Created: latency-svc-cp77t
Dec  5 21:48:25.526: INFO: Got endpoints: latency-svc-qb2fp [750.075594ms]
Dec  5 21:48:25.532: INFO: Created: latency-svc-mrcxb
Dec  5 21:48:25.575: INFO: Got endpoints: latency-svc-5gmjj [750.1861ms]
Dec  5 21:48:25.584: INFO: Created: latency-svc-dm2nk
Dec  5 21:48:25.626: INFO: Got endpoints: latency-svc-v6dlz [748.276505ms]
Dec  5 21:48:25.633: INFO: Created: latency-svc-np9p4
Dec  5 21:48:25.676: INFO: Got endpoints: latency-svc-q2r8v [749.683802ms]
Dec  5 21:48:25.691: INFO: Created: latency-svc-wzb7b
Dec  5 21:48:25.726: INFO: Got endpoints: latency-svc-p9cgb [750.67963ms]
Dec  5 21:48:25.733: INFO: Created: latency-svc-rnxtr
Dec  5 21:48:25.776: INFO: Got endpoints: latency-svc-xq7t8 [748.48194ms]
Dec  5 21:48:25.784: INFO: Created: latency-svc-wgckp
Dec  5 21:48:25.826: INFO: Got endpoints: latency-svc-dbpnl [748.855343ms]
Dec  5 21:48:25.833: INFO: Created: latency-svc-5b7j8
Dec  5 21:48:25.875: INFO: Got endpoints: latency-svc-nwb9h [747.874822ms]
Dec  5 21:48:25.882: INFO: Created: latency-svc-8kzjh
Dec  5 21:48:25.926: INFO: Got endpoints: latency-svc-ct2lm [748.919116ms]
Dec  5 21:48:25.932: INFO: Created: latency-svc-sr2w8
Dec  5 21:48:25.976: INFO: Got endpoints: latency-svc-8r2dp [749.224089ms]
Dec  5 21:48:25.983: INFO: Created: latency-svc-kbmwz
Dec  5 21:48:26.026: INFO: Got endpoints: latency-svc-8hwbw [748.765248ms]
Dec  5 21:48:26.033: INFO: Created: latency-svc-4hkqz
Dec  5 21:48:26.076: INFO: Got endpoints: latency-svc-hxdd5 [749.837421ms]
Dec  5 21:48:26.081: INFO: Created: latency-svc-dg9zb
Dec  5 21:48:26.126: INFO: Got endpoints: latency-svc-5l2fm [749.957229ms]
Dec  5 21:48:26.133: INFO: Created: latency-svc-p7zsc
Dec  5 21:48:26.176: INFO: Got endpoints: latency-svc-tqptd [739.862183ms]
Dec  5 21:48:26.195: INFO: Created: latency-svc-2fn9d
Dec  5 21:48:26.226: INFO: Got endpoints: latency-svc-cp77t [749.317942ms]
Dec  5 21:48:26.233: INFO: Created: latency-svc-wgs6r
Dec  5 21:48:26.276: INFO: Got endpoints: latency-svc-mrcxb [749.673356ms]
Dec  5 21:48:26.282: INFO: Created: latency-svc-tqm9k
Dec  5 21:48:26.326: INFO: Got endpoints: latency-svc-dm2nk [750.609306ms]
Dec  5 21:48:26.333: INFO: Created: latency-svc-56hnh
Dec  5 21:48:26.377: INFO: Got endpoints: latency-svc-np9p4 [750.852917ms]
Dec  5 21:48:26.382: INFO: Created: latency-svc-bvdpc
Dec  5 21:48:26.428: INFO: Got endpoints: latency-svc-wzb7b [751.063263ms]
Dec  5 21:48:26.434: INFO: Created: latency-svc-glv8k
Dec  5 21:48:26.475: INFO: Got endpoints: latency-svc-rnxtr [748.541387ms]
Dec  5 21:48:26.481: INFO: Created: latency-svc-fsjxq
Dec  5 21:48:26.527: INFO: Got endpoints: latency-svc-wgckp [751.442037ms]
Dec  5 21:48:26.536: INFO: Created: latency-svc-bmzlf
Dec  5 21:48:26.576: INFO: Got endpoints: latency-svc-5b7j8 [749.963027ms]
Dec  5 21:48:26.582: INFO: Created: latency-svc-4phlz
Dec  5 21:48:26.627: INFO: Got endpoints: latency-svc-8kzjh [751.374035ms]
Dec  5 21:48:26.633: INFO: Created: latency-svc-44k78
Dec  5 21:48:26.676: INFO: Got endpoints: latency-svc-sr2w8 [749.288895ms]
Dec  5 21:48:26.684: INFO: Created: latency-svc-csvrb
Dec  5 21:48:26.726: INFO: Got endpoints: latency-svc-kbmwz [749.758127ms]
Dec  5 21:48:26.731: INFO: Created: latency-svc-9zg86
Dec  5 21:48:26.776: INFO: Got endpoints: latency-svc-4hkqz [749.591461ms]
Dec  5 21:48:26.783: INFO: Created: latency-svc-s9tzc
Dec  5 21:48:26.825: INFO: Got endpoints: latency-svc-dg9zb [749.009953ms]
Dec  5 21:48:26.832: INFO: Created: latency-svc-9vc57
Dec  5 21:48:26.875: INFO: Got endpoints: latency-svc-p7zsc [748.996882ms]
Dec  5 21:48:26.883: INFO: Created: latency-svc-5nqjc
Dec  5 21:48:26.926: INFO: Got endpoints: latency-svc-2fn9d [749.443851ms]
Dec  5 21:48:26.933: INFO: Created: latency-svc-lbbv9
Dec  5 21:48:26.978: INFO: Got endpoints: latency-svc-wgs6r [752.219447ms]
Dec  5 21:48:26.986: INFO: Created: latency-svc-bt2rx
Dec  5 21:48:27.027: INFO: Got endpoints: latency-svc-tqm9k [750.217039ms]
Dec  5 21:48:27.035: INFO: Created: latency-svc-x2gwn
Dec  5 21:48:27.076: INFO: Got endpoints: latency-svc-56hnh [749.977445ms]
Dec  5 21:48:27.081: INFO: Created: latency-svc-mf2gp
Dec  5 21:48:27.126: INFO: Got endpoints: latency-svc-bvdpc [749.289766ms]
Dec  5 21:48:27.132: INFO: Created: latency-svc-rkbbw
Dec  5 21:48:27.176: INFO: Got endpoints: latency-svc-glv8k [748.704176ms]
Dec  5 21:48:27.181: INFO: Created: latency-svc-dsqzj
Dec  5 21:48:27.227: INFO: Got endpoints: latency-svc-fsjxq [751.648748ms]
Dec  5 21:48:27.232: INFO: Created: latency-svc-bdzmf
Dec  5 21:48:27.275: INFO: Got endpoints: latency-svc-bmzlf [748.003997ms]
Dec  5 21:48:27.281: INFO: Created: latency-svc-wxw9p
Dec  5 21:48:27.326: INFO: Got endpoints: latency-svc-4phlz [750.687748ms]
Dec  5 21:48:27.332: INFO: Created: latency-svc-cv5rz
Dec  5 21:48:27.377: INFO: Got endpoints: latency-svc-44k78 [750.305508ms]
Dec  5 21:48:27.382: INFO: Created: latency-svc-k5vw7
Dec  5 21:48:27.427: INFO: Got endpoints: latency-svc-csvrb [751.274185ms]
Dec  5 21:48:27.436: INFO: Created: latency-svc-tccp8
Dec  5 21:48:27.476: INFO: Got endpoints: latency-svc-9zg86 [749.594431ms]
Dec  5 21:48:27.483: INFO: Created: latency-svc-xpc2q
Dec  5 21:48:27.527: INFO: Got endpoints: latency-svc-s9tzc [750.129045ms]
Dec  5 21:48:27.534: INFO: Created: latency-svc-z4fnb
Dec  5 21:48:27.577: INFO: Got endpoints: latency-svc-9vc57 [752.055477ms]
Dec  5 21:48:27.586: INFO: Created: latency-svc-rrddt
Dec  5 21:48:27.625: INFO: Got endpoints: latency-svc-5nqjc [750.068873ms]
Dec  5 21:48:27.632: INFO: Created: latency-svc-flv58
Dec  5 21:48:27.682: INFO: Got endpoints: latency-svc-lbbv9 [755.952841ms]
Dec  5 21:48:27.698: INFO: Created: latency-svc-pc4bp
Dec  5 21:48:27.727: INFO: Got endpoints: latency-svc-bt2rx [748.259287ms]
Dec  5 21:48:27.731: INFO: Created: latency-svc-qck5d
Dec  5 21:48:27.776: INFO: Got endpoints: latency-svc-x2gwn [748.795284ms]
Dec  5 21:48:27.781: INFO: Created: latency-svc-fbx9h
Dec  5 21:48:27.827: INFO: Got endpoints: latency-svc-mf2gp [750.848969ms]
Dec  5 21:48:27.834: INFO: Created: latency-svc-fvvnc
Dec  5 21:48:27.875: INFO: Got endpoints: latency-svc-rkbbw [749.437666ms]
Dec  5 21:48:27.880: INFO: Created: latency-svc-8wrb4
Dec  5 21:48:27.925: INFO: Got endpoints: latency-svc-dsqzj [749.125485ms]
Dec  5 21:48:27.931: INFO: Created: latency-svc-6nsqq
Dec  5 21:48:27.978: INFO: Got endpoints: latency-svc-bdzmf [750.754904ms]
Dec  5 21:48:27.982: INFO: Created: latency-svc-b9cxf
Dec  5 21:48:28.025: INFO: Got endpoints: latency-svc-wxw9p [749.842047ms]
Dec  5 21:48:28.031: INFO: Created: latency-svc-5svwm
Dec  5 21:48:28.077: INFO: Got endpoints: latency-svc-cv5rz [750.202539ms]
Dec  5 21:48:28.082: INFO: Created: latency-svc-n74fl
Dec  5 21:48:28.127: INFO: Got endpoints: latency-svc-k5vw7 [750.08303ms]
Dec  5 21:48:28.132: INFO: Created: latency-svc-4rthg
Dec  5 21:48:28.176: INFO: Got endpoints: latency-svc-tccp8 [749.09334ms]
Dec  5 21:48:28.182: INFO: Created: latency-svc-bgdpm
Dec  5 21:48:28.225: INFO: Got endpoints: latency-svc-xpc2q [749.629741ms]
Dec  5 21:48:28.233: INFO: Created: latency-svc-bbztv
Dec  5 21:48:28.276: INFO: Got endpoints: latency-svc-z4fnb [748.72184ms]
Dec  5 21:48:28.282: INFO: Created: latency-svc-5rmj2
Dec  5 21:48:28.326: INFO: Got endpoints: latency-svc-rrddt [748.847491ms]
Dec  5 21:48:28.332: INFO: Created: latency-svc-8b627
Dec  5 21:48:28.376: INFO: Got endpoints: latency-svc-flv58 [750.734238ms]
Dec  5 21:48:28.385: INFO: Created: latency-svc-7l47k
Dec  5 21:48:28.426: INFO: Got endpoints: latency-svc-pc4bp [743.981359ms]
Dec  5 21:48:28.432: INFO: Created: latency-svc-4ghgh
Dec  5 21:48:28.476: INFO: Got endpoints: latency-svc-qck5d [749.557594ms]
Dec  5 21:48:28.483: INFO: Created: latency-svc-2vdzj
Dec  5 21:48:28.525: INFO: Got endpoints: latency-svc-fbx9h [749.295619ms]
Dec  5 21:48:28.530: INFO: Created: latency-svc-bqrlw
Dec  5 21:48:28.577: INFO: Got endpoints: latency-svc-fvvnc [750.37968ms]
Dec  5 21:48:28.586: INFO: Created: latency-svc-d2m89
Dec  5 21:48:28.627: INFO: Got endpoints: latency-svc-8wrb4 [750.645121ms]
Dec  5 21:48:28.631: INFO: Created: latency-svc-pfwj6
Dec  5 21:48:28.677: INFO: Got endpoints: latency-svc-6nsqq [751.620639ms]
Dec  5 21:48:28.682: INFO: Created: latency-svc-g6l5h
Dec  5 21:48:28.727: INFO: Got endpoints: latency-svc-b9cxf [748.588467ms]
Dec  5 21:48:28.733: INFO: Created: latency-svc-swjr7
Dec  5 21:48:28.776: INFO: Got endpoints: latency-svc-5svwm [750.725296ms]
Dec  5 21:48:28.780: INFO: Created: latency-svc-n47pv
Dec  5 21:48:28.826: INFO: Got endpoints: latency-svc-n74fl [749.33606ms]
Dec  5 21:48:28.834: INFO: Created: latency-svc-86dpq
Dec  5 21:48:28.875: INFO: Got endpoints: latency-svc-4rthg [748.395264ms]
Dec  5 21:48:28.880: INFO: Created: latency-svc-2sx95
Dec  5 21:48:28.926: INFO: Got endpoints: latency-svc-bgdpm [749.583531ms]
Dec  5 21:48:28.932: INFO: Created: latency-svc-9hjbq
Dec  5 21:48:28.978: INFO: Got endpoints: latency-svc-bbztv [752.948411ms]
Dec  5 21:48:28.986: INFO: Created: latency-svc-zzp7h
Dec  5 21:48:29.026: INFO: Got endpoints: latency-svc-5rmj2 [750.14939ms]
Dec  5 21:48:29.032: INFO: Created: latency-svc-vj4gt
Dec  5 21:48:29.075: INFO: Got endpoints: latency-svc-8b627 [748.126024ms]
Dec  5 21:48:29.080: INFO: Created: latency-svc-pkcks
Dec  5 21:48:29.127: INFO: Got endpoints: latency-svc-7l47k [748.860353ms]
Dec  5 21:48:29.131: INFO: Created: latency-svc-xlwwc
Dec  5 21:48:29.175: INFO: Got endpoints: latency-svc-4ghgh [749.191251ms]
Dec  5 21:48:29.180: INFO: Created: latency-svc-9hjl4
Dec  5 21:48:29.229: INFO: Got endpoints: latency-svc-2vdzj [752.740853ms]
Dec  5 21:48:29.234: INFO: Created: latency-svc-rv857
Dec  5 21:48:29.276: INFO: Got endpoints: latency-svc-bqrlw [749.631539ms]
Dec  5 21:48:29.280: INFO: Created: latency-svc-6j85r
Dec  5 21:48:29.326: INFO: Got endpoints: latency-svc-d2m89 [748.195073ms]
Dec  5 21:48:29.339: INFO: Created: latency-svc-nhq8v
Dec  5 21:48:29.377: INFO: Got endpoints: latency-svc-pfwj6 [750.676925ms]
Dec  5 21:48:29.388: INFO: Created: latency-svc-hr9zp
Dec  5 21:48:29.426: INFO: Got endpoints: latency-svc-g6l5h [748.907461ms]
Dec  5 21:48:29.432: INFO: Created: latency-svc-rgj4r
Dec  5 21:48:29.476: INFO: Got endpoints: latency-svc-swjr7 [748.671178ms]
Dec  5 21:48:29.482: INFO: Created: latency-svc-4d8nw
Dec  5 21:48:29.526: INFO: Got endpoints: latency-svc-n47pv [749.537738ms]
Dec  5 21:48:29.532: INFO: Created: latency-svc-k99zp
Dec  5 21:48:29.576: INFO: Got endpoints: latency-svc-86dpq [749.733237ms]
Dec  5 21:48:29.582: INFO: Created: latency-svc-tvbk9
Dec  5 21:48:29.631: INFO: Got endpoints: latency-svc-2sx95 [754.893702ms]
Dec  5 21:48:29.636: INFO: Created: latency-svc-zkpdf
Dec  5 21:48:29.676: INFO: Got endpoints: latency-svc-9hjbq [750.03926ms]
Dec  5 21:48:29.681: INFO: Created: latency-svc-wtxlt
Dec  5 21:48:29.728: INFO: Got endpoints: latency-svc-zzp7h [749.475869ms]
Dec  5 21:48:29.734: INFO: Created: latency-svc-glr4v
Dec  5 21:48:29.775: INFO: Got endpoints: latency-svc-vj4gt [749.281668ms]
Dec  5 21:48:29.780: INFO: Created: latency-svc-tnjwr
Dec  5 21:48:29.826: INFO: Got endpoints: latency-svc-pkcks [750.523093ms]
Dec  5 21:48:29.830: INFO: Created: latency-svc-8vchp
Dec  5 21:48:29.876: INFO: Got endpoints: latency-svc-xlwwc [749.005346ms]
Dec  5 21:48:29.885: INFO: Created: latency-svc-wsksv
Dec  5 21:48:29.930: INFO: Got endpoints: latency-svc-9hjl4 [754.313987ms]
Dec  5 21:48:29.938: INFO: Created: latency-svc-7fn8t
Dec  5 21:48:29.977: INFO: Got endpoints: latency-svc-rv857 [747.942369ms]
Dec  5 21:48:29.986: INFO: Created: latency-svc-rv5hj
Dec  5 21:48:30.026: INFO: Got endpoints: latency-svc-6j85r [750.115957ms]
Dec  5 21:48:30.034: INFO: Created: latency-svc-th684
Dec  5 21:48:30.081: INFO: Got endpoints: latency-svc-nhq8v [755.166125ms]
Dec  5 21:48:30.086: INFO: Created: latency-svc-rlkqh
Dec  5 21:48:30.129: INFO: Got endpoints: latency-svc-hr9zp [751.603406ms]
Dec  5 21:48:30.137: INFO: Created: latency-svc-jsqgx
Dec  5 21:48:30.176: INFO: Got endpoints: latency-svc-rgj4r [750.257971ms]
Dec  5 21:48:30.198: INFO: Created: latency-svc-kwzdj
Dec  5 21:48:30.226: INFO: Got endpoints: latency-svc-4d8nw [750.140576ms]
Dec  5 21:48:30.231: INFO: Created: latency-svc-klgbq
Dec  5 21:48:30.275: INFO: Got endpoints: latency-svc-k99zp [749.676131ms]
Dec  5 21:48:30.282: INFO: Created: latency-svc-8kxq5
Dec  5 21:48:30.326: INFO: Got endpoints: latency-svc-tvbk9 [749.827876ms]
Dec  5 21:48:30.333: INFO: Created: latency-svc-8nmkh
Dec  5 21:48:30.377: INFO: Got endpoints: latency-svc-zkpdf [746.046856ms]
Dec  5 21:48:30.381: INFO: Created: latency-svc-b85pn
Dec  5 21:48:30.426: INFO: Got endpoints: latency-svc-wtxlt [750.004746ms]
Dec  5 21:48:30.435: INFO: Created: latency-svc-w4jrg
Dec  5 21:48:30.476: INFO: Got endpoints: latency-svc-glr4v [747.750656ms]
Dec  5 21:48:30.481: INFO: Created: latency-svc-snsg7
Dec  5 21:48:30.526: INFO: Got endpoints: latency-svc-tnjwr [750.549465ms]
Dec  5 21:48:30.533: INFO: Created: latency-svc-pbdw7
Dec  5 21:48:30.577: INFO: Got endpoints: latency-svc-8vchp [750.625575ms]
Dec  5 21:48:30.586: INFO: Created: latency-svc-ztjq2
Dec  5 21:48:30.626: INFO: Got endpoints: latency-svc-wsksv [750.202711ms]
Dec  5 21:48:30.631: INFO: Created: latency-svc-x7gdt
Dec  5 21:48:30.675: INFO: Got endpoints: latency-svc-7fn8t [745.256422ms]
Dec  5 21:48:30.680: INFO: Created: latency-svc-5knb5
Dec  5 21:48:30.725: INFO: Got endpoints: latency-svc-rv5hj [747.973796ms]
Dec  5 21:48:30.731: INFO: Created: latency-svc-kwghh
Dec  5 21:48:30.778: INFO: Got endpoints: latency-svc-th684 [751.9391ms]
Dec  5 21:48:30.783: INFO: Created: latency-svc-p42zz
Dec  5 21:48:30.826: INFO: Got endpoints: latency-svc-rlkqh [744.70506ms]
Dec  5 21:48:30.831: INFO: Created: latency-svc-4mmn4
Dec  5 21:48:30.876: INFO: Got endpoints: latency-svc-jsqgx [746.926338ms]
Dec  5 21:48:30.881: INFO: Created: latency-svc-gszmz
Dec  5 21:48:30.926: INFO: Got endpoints: latency-svc-kwzdj [750.037628ms]
Dec  5 21:48:30.931: INFO: Created: latency-svc-kqtmf
Dec  5 21:48:30.975: INFO: Got endpoints: latency-svc-klgbq [749.098391ms]
Dec  5 21:48:30.980: INFO: Created: latency-svc-fh4rq
Dec  5 21:48:31.026: INFO: Got endpoints: latency-svc-8kxq5 [750.715653ms]
Dec  5 21:48:31.033: INFO: Created: latency-svc-2s9np
Dec  5 21:48:31.077: INFO: Got endpoints: latency-svc-8nmkh [750.847881ms]
Dec  5 21:48:31.082: INFO: Created: latency-svc-5gmfj
Dec  5 21:48:31.127: INFO: Got endpoints: latency-svc-b85pn [749.929197ms]
Dec  5 21:48:31.134: INFO: Created: latency-svc-d9xwk
Dec  5 21:48:31.175: INFO: Got endpoints: latency-svc-w4jrg [749.348351ms]
Dec  5 21:48:31.180: INFO: Created: latency-svc-q8xpr
Dec  5 21:48:31.226: INFO: Got endpoints: latency-svc-snsg7 [749.787588ms]
Dec  5 21:48:31.257: INFO: Created: latency-svc-n4mkm
Dec  5 21:48:31.276: INFO: Got endpoints: latency-svc-pbdw7 [750.413672ms]
Dec  5 21:48:31.281: INFO: Created: latency-svc-rnb4t
Dec  5 21:48:31.327: INFO: Got endpoints: latency-svc-ztjq2 [749.882945ms]
Dec  5 21:48:31.334: INFO: Created: latency-svc-9bsnd
Dec  5 21:48:31.376: INFO: Got endpoints: latency-svc-x7gdt [749.699659ms]
Dec  5 21:48:31.382: INFO: Created: latency-svc-x2b9s
Dec  5 21:48:31.426: INFO: Got endpoints: latency-svc-5knb5 [750.964699ms]
Dec  5 21:48:31.430: INFO: Created: latency-svc-lkpgp
Dec  5 21:48:31.477: INFO: Got endpoints: latency-svc-kwghh [752.183022ms]
Dec  5 21:48:31.489: INFO: Created: latency-svc-9px7n
Dec  5 21:48:31.527: INFO: Got endpoints: latency-svc-p42zz [748.465757ms]
Dec  5 21:48:31.533: INFO: Created: latency-svc-77jqx
Dec  5 21:48:31.578: INFO: Got endpoints: latency-svc-4mmn4 [751.987952ms]
Dec  5 21:48:31.584: INFO: Created: latency-svc-d7rj6
Dec  5 21:48:31.625: INFO: Got endpoints: latency-svc-gszmz [749.388689ms]
Dec  5 21:48:31.675: INFO: Got endpoints: latency-svc-kqtmf [748.522291ms]
Dec  5 21:48:31.726: INFO: Got endpoints: latency-svc-fh4rq [750.780031ms]
Dec  5 21:48:31.776: INFO: Got endpoints: latency-svc-2s9np [749.758769ms]
Dec  5 21:48:31.826: INFO: Got endpoints: latency-svc-5gmfj [749.501914ms]
Dec  5 21:48:31.875: INFO: Got endpoints: latency-svc-d9xwk [748.741104ms]
Dec  5 21:48:31.926: INFO: Got endpoints: latency-svc-q8xpr [750.136703ms]
Dec  5 21:48:31.975: INFO: Got endpoints: latency-svc-n4mkm [749.642354ms]
Dec  5 21:48:32.026: INFO: Got endpoints: latency-svc-rnb4t [749.983908ms]
Dec  5 21:48:32.075: INFO: Got endpoints: latency-svc-9bsnd [748.835296ms]
Dec  5 21:48:32.125: INFO: Got endpoints: latency-svc-x2b9s [749.788444ms]
Dec  5 21:48:32.175: INFO: Got endpoints: latency-svc-lkpgp [749.337299ms]
Dec  5 21:48:32.226: INFO: Got endpoints: latency-svc-9px7n [748.233757ms]
Dec  5 21:48:32.276: INFO: Got endpoints: latency-svc-77jqx [749.346448ms]
Dec  5 21:48:32.327: INFO: Got endpoints: latency-svc-d7rj6 [749.197673ms]
Dec  5 21:48:32.328: INFO: Latencies: [13.890967ms 19.884853ms 23.269815ms 35.201068ms 36.880275ms 39.273234ms 46.594089ms 50.384415ms 57.930681ms 62.909049ms 68.111364ms 75.965984ms 76.93947ms 79.50813ms 81.390657ms 82.730658ms 86.042105ms 86.275539ms 86.712589ms 87.194686ms 89.158594ms 89.220726ms 90.690156ms 90.715578ms 90.789929ms 91.005079ms 91.078768ms 91.589795ms 93.183775ms 93.400122ms 95.709823ms 96.420857ms 99.420599ms 120.489622ms 168.158996ms 215.916917ms 257.888543ms 301.750981ms 348.880543ms 386.456661ms 427.306345ms 477.753542ms 523.636552ms 569.270404ms 612.631982ms 657.214282ms 704.276198ms 739.862183ms 743.981359ms 744.70506ms 745.256422ms 746.046856ms 746.926338ms 747.71406ms 747.750656ms 747.810833ms 747.874822ms 747.942369ms 747.973796ms 748.003997ms 748.126024ms 748.195073ms 748.233757ms 748.259287ms 748.276505ms 748.395264ms 748.465757ms 748.48194ms 748.522291ms 748.541387ms 748.588467ms 748.671178ms 748.704176ms 748.72184ms 748.741104ms 748.749049ms 748.765248ms 748.785092ms 748.795284ms 748.835296ms 748.847491ms 748.855343ms 748.860353ms 748.907461ms 748.919116ms 748.928397ms 748.933849ms 748.996882ms 749.005346ms 749.009953ms 749.09334ms 749.098391ms 749.125485ms 749.150708ms 749.191251ms 749.197673ms 749.224089ms 749.281668ms 749.288895ms 749.289766ms 749.295619ms 749.317942ms 749.33606ms 749.337299ms 749.346448ms 749.348351ms 749.388689ms 749.437666ms 749.443851ms 749.475869ms 749.501914ms 749.537738ms 749.557594ms 749.583531ms 749.587772ms 749.591461ms 749.594431ms 749.629741ms 749.631539ms 749.642354ms 749.673356ms 749.676131ms 749.683802ms 749.699659ms 749.731335ms 749.733237ms 749.758127ms 749.758769ms 749.787588ms 749.788444ms 749.827876ms 749.837421ms 749.842047ms 749.84528ms 749.882945ms 749.929197ms 749.931905ms 749.957229ms 749.963027ms 749.977445ms 749.983908ms 750.004746ms 750.037628ms 750.03926ms 750.068873ms 750.075594ms 750.08303ms 750.115957ms 750.129045ms 750.136703ms 750.140576ms 750.14939ms 750.1861ms 750.202539ms 750.202711ms 750.214727ms 750.216258ms 750.217039ms 750.257971ms 750.305508ms 750.37968ms 750.413672ms 750.523093ms 750.547937ms 750.549465ms 750.609306ms 750.625575ms 750.645121ms 750.676925ms 750.67963ms 750.687748ms 750.715653ms 750.725296ms 750.734238ms 750.754904ms 750.780031ms 750.847881ms 750.848969ms 750.852917ms 750.964699ms 751.063263ms 751.274185ms 751.374035ms 751.442037ms 751.603406ms 751.620639ms 751.648748ms 751.878115ms 751.9391ms 751.987952ms 752.055477ms 752.183022ms 752.219447ms 752.740853ms 752.948411ms 754.313987ms 754.893702ms 755.166125ms 755.952841ms 761.211866ms]
Dec  5 21:48:32.328: INFO: 50 %ile: 749.295619ms
Dec  5 21:48:32.328: INFO: 90 %ile: 751.063263ms
Dec  5 21:48:32.328: INFO: 99 %ile: 755.952841ms
Dec  5 21:48:32.328: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:48:32.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-gqbg6" for this suite.
Dec  5 21:48:46.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:48:46.380: INFO: namespace: e2e-tests-svc-latency-gqbg6, resource: bindings, ignored listing per whitelist
Dec  5 21:48:46.407: INFO: namespace e2e-tests-svc-latency-gqbg6 deletion completed in 14.073297858s

â€¢ [SLOW TEST:23.836 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:48:46.408: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-8ac377b2-f8d7-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 21:48:46.463: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-jc6g2" to be "success or failure"
Dec  5 21:48:46.467: INFO: Pod "pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389207ms
Dec  5 21:48:48.471: INFO: Pod "pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007553726s
STEP: Saw pod success
Dec  5 21:48:48.471: INFO: Pod "pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:48:48.476: INFO: Trying to get logs from node single-node pod pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 21:48:48.490: INFO: Waiting for pod pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb to disappear
Dec  5 21:48:48.494: INFO: Pod pod-projected-secrets-8ac42074-f8d7-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:48:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jc6g2" for this suite.
Dec  5 21:48:54.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:48:54.544: INFO: namespace: e2e-tests-projected-jc6g2, resource: bindings, ignored listing per whitelist
Dec  5 21:48:54.552: INFO: namespace e2e-tests-projected-jc6g2 deletion completed in 6.055536188s

â€¢ [SLOW TEST:8.145 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:48:54.552: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 21:48:54.592: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Dec  5 21:48:54.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mxxxn/daemonsets","resourceVersion":"3562"},"items":null}

Dec  5 21:48:54.602: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mxxxn/pods","resourceVersion":"3562"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:48:54.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mxxxn" for this suite.
Dec  5 21:49:00.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:49:00.646: INFO: namespace: e2e-tests-daemonsets-mxxxn, resource: bindings, ignored listing per whitelist
Dec  5 21:49:00.660: INFO: namespace e2e-tests-daemonsets-mxxxn deletion completed in 6.054265594s

S [SKIPPING] [6.108 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Dec  5 21:48:54.592: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:49:00.661: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 21:49:00.698: INFO: Creating deployment "nginx-deployment"
Dec  5 21:49:00.702: INFO: Waiting for observed generation 1
Dec  5 21:49:02.708: INFO: Waiting for all required pods to come up
Dec  5 21:49:02.712: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  5 21:49:08.719: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  5 21:49:08.725: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  5 21:49:08.730: INFO: Updating deployment nginx-deployment
Dec  5 21:49:08.730: INFO: Waiting for observed generation 2
Dec  5 21:49:10.735: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  5 21:49:10.736: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  5 21:49:10.738: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 21:49:10.743: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  5 21:49:10.743: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  5 21:49:10.744: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 21:49:10.746: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  5 21:49:10.746: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  5 21:49:10.751: INFO: Updating deployment nginx-deployment
Dec  5 21:49:10.751: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  5 21:49:10.755: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  5 21:49:10.758: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 21:49:12.837: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8jlx/deployments/nginx-deployment,UID:93417f75-f8d7-11e8-aca6-08002781145e,ResourceVersion:3861,Generation:3,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2018-12-05 21:49:10 +0000 UTC 2018-12-05 21:49:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-05 21:49:10 +0000 UTC 2018-12-05 21:49:00 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  5 21:49:12.839: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8jlx/replicasets/nginx-deployment-65bbdb5f8,UID:980aedf8-f8d7-11e8-aca6-08002781145e,ResourceVersion:3858,Generation:3,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 93417f75-f8d7-11e8-aca6-08002781145e 0xc000919477 0xc000919478}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 21:49:12.839: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  5 21:49:12.839: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h8jlx/replicasets/nginx-deployment-555b55d965,UID:9342701e-f8d7-11e8-aca6-08002781145e,ResourceVersion:3860,Generation:3,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 93417f75-f8d7-11e8-aca6-08002781145e 0xc0009193b7 0xc0009193b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  5 21:49:12.843: INFO: Pod "nginx-deployment-555b55d965-5bnvk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5bnvk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-5bnvk,UID:93486d65-f8d7-11e8-aca6-08002781145e,ResourceVersion:3714,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.34/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403830 0xc001403831}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0014038a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014038c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.34,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:02 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://ab393f74286232b87cbade1d68e7653c3c9a4129156170843965981f2069840e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.843: INFO: Pod "nginx-deployment-555b55d965-6gfc8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6gfc8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-6gfc8,UID:9343c447-f8d7-11e8-aca6-08002781145e,ResourceVersion:3693,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403990 0xc001403991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001403a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001403a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.25,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://34e665c857d339d9f30b720072fdb79e1c3d2d2f4bc833e86408d73a84e87e3f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.843: INFO: Pod "nginx-deployment-555b55d965-85mwp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-85mwp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-85mwp,UID:9941f3d2-f8d7-11e8-aca6-08002781145e,ResourceVersion:3886,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.41/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403af0 0xc001403af1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001403b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001403b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.843: INFO: Pod "nginx-deployment-555b55d965-887q2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-887q2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-887q2,UID:994601eb-f8d7-11e8-aca6-08002781145e,ResourceVersion:3917,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.59/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403c47 0xc001403c48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001403cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001403ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.843: INFO: Pod "nginx-deployment-555b55d965-9p8lv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9p8lv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-9p8lv,UID:93487580-f8d7-11e8-aca6-08002781145e,ResourceVersion:3677,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.29/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403d60 0xc001403d61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001403dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001403df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.29,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://6b0967dadaa2838a6852db6ee9f52439a8c74d2480d9bbd278f43505560581d5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-b46hg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b46hg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-b46hg,UID:9941ffdb-f8d7-11e8-aca6-08002781145e,ResourceVersion:3890,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.43/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc001403ec0 0xc001403ec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001403f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001403f50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-b95tn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b95tn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-b95tn,UID:9944a6ba-f8d7-11e8-aca6-08002781145e,ResourceVersion:3900,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.46/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66057 0xc000c66058}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c660d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c660f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-cb42j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cb42j,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-cb42j,UID:9946047e-f8d7-11e8-aca6-08002781145e,ResourceVersion:3909,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.52/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c661d7 0xc000c661d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c663b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-fxp5v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-fxp5v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-fxp5v,UID:99461081-f8d7-11e8-aca6-08002781145e,ResourceVersion:3912,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66430 0xc000c66431}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c664a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c664c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-h8x7n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-h8x7n,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-h8x7n,UID:934597cf-f8d7-11e8-aca6-08002781145e,ResourceVersion:3699,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.27/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66540 0xc000c66541}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c665b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c665d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.27,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://3aec099f66ca4e19884fc74008d98737bbfeb6c64350297a44149f0772735111}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-jftfc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jftfc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-jftfc,UID:93458b5d-f8d7-11e8-aca6-08002781145e,ResourceVersion:3721,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.32/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c666a0 0xc000c666a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.32,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:02 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://b0e16d8d8fc63c37b6f053d5f1d761a4f1f86e57925e0a5e1ab3aa956ebf1e5d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-llns6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-llns6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-llns6,UID:93459892-f8d7-11e8-aca6-08002781145e,ResourceVersion:3683,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.28/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66800 0xc000c66801}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.28,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:02 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://9235f0f71c6d1dc471d833ca98b7f30202c2ba2cde768e1273093281cb197ff6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-ppxwm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ppxwm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-ppxwm,UID:9944bc4c-f8d7-11e8-aca6-08002781145e,ResourceVersion:3927,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.44/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66960 0xc000c66961}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c669d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c669f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-pqwvn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-pqwvn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-pqwvn,UID:9944b69a-f8d7-11e8-aca6-08002781145e,ResourceVersion:3933,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.45/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66ab7 0xc000c66ab8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-rsvcw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rsvcw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-rsvcw,UID:9940f407-f8d7-11e8-aca6-08002781145e,ResourceVersion:3882,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.40/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66c17 0xc000c66c18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-s92w6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-s92w6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-s92w6,UID:9944b97b-f8d7-11e8-aca6-08002781145e,ResourceVersion:3914,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.47/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66d77 0xc000c66d78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-t9tjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t9tjr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-t9tjr,UID:9946105f-f8d7-11e8-aca6-08002781145e,ResourceVersion:3915,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66ed7 0xc000c66ed8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c66f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c66f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.844: INFO: Pod "nginx-deployment-555b55d965-vkwvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vkwvc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-vkwvc,UID:99460927-f8d7-11e8-aca6-08002781145e,ResourceVersion:3916,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c66ff0 0xc000c66ff1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-555b55d965-vr829" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vr829,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-vr829,UID:93449104-f8d7-11e8-aca6-08002781145e,ResourceVersion:3688,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c67100 0xc000c67101}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.26,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://554119ee8cc77d25d4c8d4c2afa9813882372cd251ab5fe70da75360ac36512d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-555b55d965-zbbtx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zbbtx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-555b55d965-zbbtx,UID:93485909-f8d7-11e8-aca6-08002781145e,ResourceVersion:3711,Generation:0,CreationTimestamp:2018-12-05 21:49:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.33/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9342701e-f8d7-11e8-aca6-08002781145e 0xc000c67260 0xc000c67261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c672d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c672f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.33,StartTime:2018-12-05 21:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 21:49:02 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://6b17499a448f0d2c9c3d7f31f06494beb8df969b9c0a39fa30a564cb5b946ae2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-2vx25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2vx25,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-2vx25,UID:9946d9dc-f8d7-11e8-aca6-08002781145e,ResourceVersion:3908,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.51/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c673c0 0xc000c673c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-4l69m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4l69m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-4l69m,UID:9814a85c-f8d7-11e8-aca6-08002781145e,ResourceVersion:3785,Generation:0,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.39/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c674e0 0xc000c674e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-6dpfb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6dpfb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-6dpfb,UID:9946e232-f8d7-11e8-aca6-08002781145e,ResourceVersion:3911,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.53/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67650 0xc000c67651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c676d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c676f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-72kvn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-72kvn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-72kvn,UID:9946e13c-f8d7-11e8-aca6-08002781145e,ResourceVersion:3905,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.50/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67770 0xc000c67771}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c677f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-7vql6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7vql6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-7vql6,UID:980ce1e2-f8d7-11e8-aca6-08002781145e,ResourceVersion:3781,Generation:0,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.36/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67890 0xc000c67891}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-bb8pm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bb8pm,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-bb8pm,UID:981652a0-f8d7-11e8-aca6-08002781145e,ResourceVersion:3787,Generation:0,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.38/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67a00 0xc000c67a01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-c8b8h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-c8b8h,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-c8b8h,UID:99428168-f8d7-11e8-aca6-08002781145e,ResourceVersion:3888,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.42/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67b70 0xc000c67b71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-kxt2z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kxt2z,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-kxt2z,UID:980d0ebe-f8d7-11e8-aca6-08002781145e,ResourceVersion:3782,Generation:0,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.37/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67ce0 0xc000c67ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-lmhrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lmhrv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-lmhrv,UID:9946e28d-f8d7-11e8-aca6-08002781145e,ResourceVersion:3910,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.54/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67e50 0xc000c67e51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c67ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-nwkcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nwkcq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-nwkcq,UID:9948c1c0-f8d7-11e8-aca6-08002781145e,ResourceVersion:3913,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000c67f70 0xc000c67f71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c67ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e92010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-p7ngt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-p7ngt,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-p7ngt,UID:980b7cf8-f8d7-11e8-aca6-08002781145e,ResourceVersion:3778,Generation:0,CreationTimestamp:2018-12-05 21:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.35/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000e92090 0xc000e92091}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000e92110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e92130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:08 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:49:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-w5dqt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-w5dqt,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-w5dqt,UID:99458a0e-f8d7-11e8-aca6-08002781145e,ResourceVersion:3903,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.49/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000e92280 0xc000e92281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000e92300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e92320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 21:49:12.845: INFO: Pod "nginx-deployment-65bbdb5f8-xmgmn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xmgmn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-h8jlx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h8jlx/pods/nginx-deployment-65bbdb5f8-xmgmn,UID:99458c2a-f8d7-11e8-aca6-08002781145e,ResourceVersion:3904,Generation:0,CreationTimestamp:2018-12-05 21:49:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.48/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 980aedf8-f8d7-11e8-aca6-08002781145e 0xc000e923a0 0xc000e923a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fpw4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fpw4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fpw4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000e924e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e92500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:49:10 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:49:12.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-h8jlx" for this suite.
Dec  5 21:49:18.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:49:18.896: INFO: namespace: e2e-tests-deployment-h8jlx, resource: bindings, ignored listing per whitelist
Dec  5 21:49:18.942: INFO: namespace e2e-tests-deployment-h8jlx deletion completed in 6.089786614s

â€¢ [SLOW TEST:18.281 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:49:18.942: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Dec  5 21:49:18.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 api-versions'
Dec  5 21:49:19.055: INFO: stderr: ""
Dec  5 21:49:19.055: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:49:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-25wzp" for this suite.
Dec  5 21:49:25.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:49:25.082: INFO: namespace: e2e-tests-kubectl-25wzp, resource: bindings, ignored listing per whitelist
Dec  5 21:49:25.109: INFO: namespace e2e-tests-kubectl-25wzp deletion completed in 6.052544626s

â€¢ [SLOW TEST:6.168 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:49:25.110: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 21:49:25.146: INFO: Waiting up to 5m0s for pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-7l7h8" to be "success or failure"
Dec  5 21:49:25.151: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.076545ms
Dec  5 21:49:27.153: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007530795s
Dec  5 21:49:29.156: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009901392s
Dec  5 21:49:31.158: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012618213s
Dec  5 21:49:33.163: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017607892s
STEP: Saw pod success
Dec  5 21:49:33.163: INFO: Pod "pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:49:33.166: INFO: Trying to get logs from node single-node pod pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 21:49:33.181: INFO: Waiting for pod pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb to disappear
Dec  5 21:49:33.186: INFO: Pod pod-a1d328bb-f8d7-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:49:33.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7l7h8" for this suite.
Dec  5 21:49:39.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:49:39.248: INFO: namespace: e2e-tests-emptydir-7l7h8, resource: bindings, ignored listing per whitelist
Dec  5 21:49:39.251: INFO: namespace e2e-tests-emptydir-7l7h8 deletion completed in 6.062781792s

â€¢ [SLOW TEST:14.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:49:39.252: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Dec  5 21:49:41.310: INFO: Pod pod-hostip-aa415d91-f8d7-11e8-8a71-d2079f97accb has hostIP: 192.168.100.50
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:49:41.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bjl96" for this suite.
Dec  5 21:50:03.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:50:03.363: INFO: namespace: e2e-tests-pods-bjl96, resource: bindings, ignored listing per whitelist
Dec  5 21:50:03.368: INFO: namespace e2e-tests-pods-bjl96 deletion completed in 22.05551448s

â€¢ [SLOW TEST:24.116 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:50:03.369: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b8a1a7d7-f8d7-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 21:50:03.432: INFO: Waiting up to 5m0s for pod "pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-qp26g" to be "success or failure"
Dec  5 21:50:03.435: INFO: Pod "pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456635ms
Dec  5 21:50:05.437: INFO: Pod "pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005376215s
STEP: Saw pod success
Dec  5 21:50:05.437: INFO: Pod "pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:50:05.439: INFO: Trying to get logs from node single-node pod pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 21:50:05.453: INFO: Waiting for pod pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb to disappear
Dec  5 21:50:05.464: INFO: Pod pod-secrets-b8a4b4ab-f8d7-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:50:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qp26g" for this suite.
Dec  5 21:50:11.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:50:11.496: INFO: namespace: e2e-tests-secrets-qp26g, resource: bindings, ignored listing per whitelist
Dec  5 21:50:11.527: INFO: namespace e2e-tests-secrets-qp26g deletion completed in 6.059177534s
STEP: Destroying namespace "e2e-tests-secret-namespace-zgm7x" for this suite.
Dec  5 21:50:17.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:50:17.579: INFO: namespace: e2e-tests-secret-namespace-zgm7x, resource: bindings, ignored listing per whitelist
Dec  5 21:50:17.580: INFO: namespace e2e-tests-secret-namespace-zgm7x deletion completed in 6.053372685s

â€¢ [SLOW TEST:14.211 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:50:17.580: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 21:50:17.618: INFO: Creating deployment "test-recreate-deployment"
Dec  5 21:50:17.621: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  5 21:50:17.625: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Dec  5 21:50:19.630: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  5 21:50:19.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679643417, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679643417, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679643417, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679643417, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 21:50:21.635: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  5 21:50:21.643: INFO: Updating deployment test-recreate-deployment
Dec  5 21:50:21.643: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 21:50:21.700: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-8gnpl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8gnpl/deployments/test-recreate-deployment,UID:c11a8be5-f8d7-11e8-aca6-08002781145e,ResourceVersion:4413,Generation:2,CreationTimestamp:2018-12-05 21:50:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2018-12-05 21:50:21 +0000 UTC 2018-12-05 21:50:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-05 21:50:21 +0000 UTC 2018-12-05 21:50:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  5 21:50:21.703: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-8gnpl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8gnpl/replicasets/test-recreate-deployment-697fbf54bf,UID:c38503b1-f8d7-11e8-aca6-08002781145e,ResourceVersion:4409,Generation:1,CreationTimestamp:2018-12-05 21:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment c11a8be5-f8d7-11e8-aca6-08002781145e 0xc001cc6257 0xc001cc6258}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 21:50:21.703: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  5 21:50:21.703: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-8gnpl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8gnpl/replicasets/test-recreate-deployment-5dfdcc846d,UID:c11b85a9-f8d7-11e8-aca6-08002781145e,ResourceVersion:4401,Generation:2,CreationTimestamp:2018-12-05 21:50:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment c11a8be5-f8d7-11e8-aca6-08002781145e 0xc001cc60b7 0xc001cc60b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 21:50:21.706: INFO: Pod "test-recreate-deployment-697fbf54bf-ckwqg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-ckwqg,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-8gnpl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8gnpl/pods/test-recreate-deployment-697fbf54bf-ckwqg,UID:c3854ec6-f8d7-11e8-aca6-08002781145e,ResourceVersion:4412,Generation:0,CreationTimestamp:2018-12-05 21:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf c38503b1-f8d7-11e8-aca6-08002781145e 0xc001cc7517 0xc001cc7518}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-n82fj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n82fj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n82fj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cc7590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cc75b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:50:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:50:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:50:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:,StartTime:2018-12-05 21:50:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:50:21.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-8gnpl" for this suite.
Dec  5 21:50:27.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:50:27.746: INFO: namespace: e2e-tests-deployment-8gnpl, resource: bindings, ignored listing per whitelist
Dec  5 21:50:27.783: INFO: namespace e2e-tests-deployment-8gnpl deletion completed in 6.073688548s

â€¢ [SLOW TEST:10.203 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:50:27.785: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-4zlw9/configmap-test-c72f3f9f-f8d7-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 21:50:27.829: INFO: Waiting up to 5m0s for pod "pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-4zlw9" to be "success or failure"
Dec  5 21:50:27.831: INFO: Pod "pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410256ms
Dec  5 21:50:29.888: INFO: Pod "pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059722961s
STEP: Saw pod success
Dec  5 21:50:29.888: INFO: Pod "pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:50:29.891: INFO: Trying to get logs from node single-node pod pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb container env-test: <nil>
STEP: delete the pod
Dec  5 21:50:29.906: INFO: Waiting for pod pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb to disappear
Dec  5 21:50:29.912: INFO: Pod pod-configmaps-c72faabc-f8d7-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:50:29.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4zlw9" for this suite.
Dec  5 21:50:35.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:50:35.949: INFO: namespace: e2e-tests-configmap-4zlw9, resource: bindings, ignored listing per whitelist
Dec  5 21:50:35.974: INFO: namespace e2e-tests-configmap-4zlw9 deletion completed in 6.057024125s

â€¢ [SLOW TEST:8.189 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:50:35.976: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-ql7hb
Dec  5 21:50:38.035: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-ql7hb
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 21:50:38.039: INFO: Initial restart count of pod liveness-exec is 0
Dec  5 21:51:28.134: INFO: Restart count of pod e2e-tests-container-probe-ql7hb/liveness-exec is now 1 (50.09450525s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:51:28.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ql7hb" for this suite.
Dec  5 21:51:34.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:51:34.173: INFO: namespace: e2e-tests-container-probe-ql7hb, resource: bindings, ignored listing per whitelist
Dec  5 21:51:34.211: INFO: namespace e2e-tests-container-probe-ql7hb deletion completed in 6.057142622s

â€¢ [SLOW TEST:58.236 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:51:34.215: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-rz522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rz522 to expose endpoints map[]
Dec  5 21:51:34.260: INFO: Get endpoints failed (3.045345ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec  5 21:51:35.263: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rz522 exposes endpoints map[] (1.00607104s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-rz522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rz522 to expose endpoints map[pod1:[80]]
Dec  5 21:51:37.292: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rz522 exposes endpoints map[pod1:[80]] (2.020662835s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-rz522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rz522 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  5 21:51:39.315: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rz522 exposes endpoints map[pod1:[80] pod2:[80]] (2.018796991s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-rz522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rz522 to expose endpoints map[pod2:[80]]
Dec  5 21:51:39.335: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rz522 exposes endpoints map[pod2:[80]] (10.605592ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-rz522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rz522 to expose endpoints map[]
Dec  5 21:51:39.348: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rz522 exposes endpoints map[] (1.400178ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:51:39.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-rz522" for this suite.
Dec  5 21:52:01.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:01.414: INFO: namespace: e2e-tests-services-rz522, resource: bindings, ignored listing per whitelist
Dec  5 21:52:01.416: INFO: namespace e2e-tests-services-rz522 deletion completed in 22.053127982s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:27.202 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:01.416: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1205 21:52:11.519014      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 21:52:11.519: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:11.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5zbnh" for this suite.
Dec  5 21:52:17.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:17.552: INFO: namespace: e2e-tests-gc-5zbnh, resource: bindings, ignored listing per whitelist
Dec  5 21:52:17.579: INFO: namespace e2e-tests-gc-5zbnh deletion completed in 6.055740248s

â€¢ [SLOW TEST:16.163 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:17.580: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 21:52:17.621: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  5 21:52:17.630: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  5 21:52:22.634: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 21:52:22.634: INFO: Creating deployment "test-rolling-update-deployment"
Dec  5 21:52:22.640: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  5 21:52:22.643: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  5 21:52:24.648: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  5 21:52:24.650: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 21:52:24.654: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-hk7z7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk7z7/deployments/test-rolling-update-deployment,UID:0b9eb92d-f8d8-11e8-aca6-08002781145e,ResourceVersion:5050,Generation:1,CreationTimestamp:2018-12-05 21:52:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-05 21:52:22 +0000 UTC 2018-12-05 21:52:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-05 21:52:23 +0000 UTC 2018-12-05 21:52:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 21:52:24.656: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-hk7z7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk7z7/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:0ba05e5f-f8d8-11e8-aca6-08002781145e,ResourceVersion:5041,Generation:1,CreationTimestamp:2018-12-05 21:52:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0b9eb92d-f8d8-11e8-aca6-08002781145e 0xc002b621c7 0xc002b621c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 21:52:24.656: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  5 21:52:24.656: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-hk7z7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hk7z7/replicasets/test-rolling-update-controller,UID:08a19dcf-f8d8-11e8-aca6-08002781145e,ResourceVersion:5049,Generation:2,CreationTimestamp:2018-12-05 21:52:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0b9eb92d-f8d8-11e8-aca6-08002781145e 0xc002b62117 0xc002b62118}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 21:52:24.660: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-29bz7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-29bz7,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-hk7z7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hk7z7/pods/test-rolling-update-deployment-68b55d7bc6-29bz7,UID:0ba09f41-f8d8-11e8-aca6-08002781145e,ResourceVersion:5040,Generation:0,CreationTimestamp:2018-12-05 21:52:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.80/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 0ba05e5f-f8d8-11e8-aca6-08002781145e 0xc002b62ba7 0xc002b62ba8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dh5wf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dh5wf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dh5wf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b62c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b62c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:52:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:52:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:52:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 21:52:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.80,StartTime:2018-12-05 21:52:22 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-05 21:52:23 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://4369b8a781d965be069fbeddf1b754c8c9551de7433ae6f4905ea4cc6866613c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hk7z7" for this suite.
Dec  5 21:52:30.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:30.696: INFO: namespace: e2e-tests-deployment-hk7z7, resource: bindings, ignored listing per whitelist
Dec  5 21:52:30.718: INFO: namespace e2e-tests-deployment-hk7z7 deletion completed in 6.056573582s

â€¢ [SLOW TEST:13.138 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:30.721: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-10755e57-f8d8-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 21:52:30.762: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-n6gm9" to be "success or failure"
Dec  5 21:52:30.765: INFO: Pod "pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882603ms
Dec  5 21:52:32.766: INFO: Pod "pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004481096s
STEP: Saw pod success
Dec  5 21:52:32.767: INFO: Pod "pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:52:32.769: INFO: Trying to get logs from node single-node pod pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 21:52:32.783: INFO: Waiting for pod pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:52:32.787: INFO: Pod pod-projected-secrets-1075d5dc-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:32.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n6gm9" for this suite.
Dec  5 21:52:38.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:38.818: INFO: namespace: e2e-tests-projected-n6gm9, resource: bindings, ignored listing per whitelist
Dec  5 21:52:38.849: INFO: namespace e2e-tests-projected-n6gm9 deletion completed in 6.058550298s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:38.849: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-154d9076-f8d8-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 21:52:38.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-fmml8" to be "success or failure"
Dec  5 21:52:38.892: INFO: Pod "pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419627ms
Dec  5 21:52:40.900: INFO: Pod "pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010850806s
STEP: Saw pod success
Dec  5 21:52:40.900: INFO: Pod "pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:52:40.904: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 21:52:40.924: INFO: Waiting for pod pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:52:40.926: INFO: Pod pod-projected-configmaps-154deef4-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:40.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fmml8" for this suite.
Dec  5 21:52:46.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:46.977: INFO: namespace: e2e-tests-projected-fmml8, resource: bindings, ignored listing per whitelist
Dec  5 21:52:46.979: INFO: namespace e2e-tests-projected-fmml8 deletion completed in 6.047403416s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:46.979: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 21:52:47.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-4rhsb" to be "success or failure"
Dec  5 21:52:47.022: INFO: Pod "downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682996ms
Dec  5 21:52:49.024: INFO: Pod "downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004641762s
STEP: Saw pod success
Dec  5 21:52:49.024: INFO: Pod "downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:52:49.026: INFO: Trying to get logs from node single-node pod downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 21:52:49.052: INFO: Waiting for pod downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:52:49.065: INFO: Pod downwardapi-volume-1a266931-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:49.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4rhsb" for this suite.
Dec  5 21:52:55.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:52:55.102: INFO: namespace: e2e-tests-projected-4rhsb, resource: bindings, ignored listing per whitelist
Dec  5 21:52:55.129: INFO: namespace e2e-tests-projected-4rhsb deletion completed in 6.061040621s

â€¢ [SLOW TEST:8.150 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:52:55.131: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:52:57.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-k9mhw" for this suite.
Dec  5 21:53:35.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:53:35.216: INFO: namespace: e2e-tests-kubelet-test-k9mhw, resource: bindings, ignored listing per whitelist
Dec  5 21:53:35.239: INFO: namespace e2e-tests-kubelet-test-k9mhw deletion completed in 38.054554886s

â€¢ [SLOW TEST:40.108 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:53:35.239: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  5 21:53:37.804: INFO: Successfully updated pod "annotationupdate36eadc89-f8d8-11e8-8a71-d2079f97accb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:53:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-469z7" for this suite.
Dec  5 21:54:01.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:54:01.866: INFO: namespace: e2e-tests-projected-469z7, resource: bindings, ignored listing per whitelist
Dec  5 21:54:01.880: INFO: namespace e2e-tests-projected-469z7 deletion completed in 22.055193429s

â€¢ [SLOW TEST:26.641 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:54:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1205 21:54:02.943763      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 21:54:02.943: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:54:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mcbk2" for this suite.
Dec  5 21:54:08.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:54:08.984: INFO: namespace: e2e-tests-gc-mcbk2, resource: bindings, ignored listing per whitelist
Dec  5 21:54:08.992: INFO: namespace e2e-tests-gc-mcbk2 deletion completed in 6.046888203s

â€¢ [SLOW TEST:7.112 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:54:08.993: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-krmhb/secret-test-4b086edf-f8d8-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 21:54:09.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-krmhb" to be "success or failure"
Dec  5 21:54:09.049: INFO: Pod "pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.629176ms
Dec  5 21:54:11.052: INFO: Pod "pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019385693s
STEP: Saw pod success
Dec  5 21:54:11.052: INFO: Pod "pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:54:11.054: INFO: Trying to get logs from node single-node pod pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb container env-test: <nil>
STEP: delete the pod
Dec  5 21:54:11.068: INFO: Waiting for pod pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:54:11.069: INFO: Pod pod-configmaps-4b08da92-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:54:11.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-krmhb" for this suite.
Dec  5 21:54:17.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:54:17.108: INFO: namespace: e2e-tests-secrets-krmhb, resource: bindings, ignored listing per whitelist
Dec  5 21:54:17.127: INFO: namespace e2e-tests-secrets-krmhb deletion completed in 6.056509459s

â€¢ [SLOW TEST:8.135 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:54:17.129: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 21:54:17.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-cmn42" to be "success or failure"
Dec  5 21:54:17.173: INFO: Pod "downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26668ms
Dec  5 21:54:19.176: INFO: Pod "downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006141645s
STEP: Saw pod success
Dec  5 21:54:19.176: INFO: Pod "downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:54:19.178: INFO: Trying to get logs from node single-node pod downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 21:54:19.193: INFO: Waiting for pod downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:54:19.197: INFO: Pod downwardapi-volume-4fe24129-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:54:19.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cmn42" for this suite.
Dec  5 21:54:25.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:54:25.242: INFO: namespace: e2e-tests-projected-cmn42, resource: bindings, ignored listing per whitelist
Dec  5 21:54:25.252: INFO: namespace e2e-tests-projected-cmn42 deletion completed in 6.046967566s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:54:25.252: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-54b95aef-f8d8-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 21:54:25.292: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-2xk8w" to be "success or failure"
Dec  5 21:54:25.296: INFO: Pod "pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052199ms
Dec  5 21:54:27.304: INFO: Pod "pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011203635s
STEP: Saw pod success
Dec  5 21:54:27.304: INFO: Pod "pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 21:54:27.306: INFO: Trying to get logs from node single-node pod pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 21:54:27.328: INFO: Waiting for pod pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb to disappear
Dec  5 21:54:27.330: INFO: Pod pod-projected-secrets-54b9bf64-f8d8-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:54:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2xk8w" for this suite.
Dec  5 21:54:33.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:54:33.359: INFO: namespace: e2e-tests-projected-2xk8w, resource: bindings, ignored listing per whitelist
Dec  5 21:54:33.387: INFO: namespace e2e-tests-projected-2xk8w deletion completed in 6.05394368s

â€¢ [SLOW TEST:8.136 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:54:33.389: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-56cxn.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-56cxn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-56cxn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-56cxn.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-56cxn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-56cxn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 21:55:07.469: INFO: DNS probes using e2e-tests-dns-56cxn/dns-test-59933483-f8d8-11e8-8a71-d2079f97accb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:55:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-56cxn" for this suite.
Dec  5 21:55:13.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:55:13.504: INFO: namespace: e2e-tests-dns-56cxn, resource: bindings, ignored listing per whitelist
Dec  5 21:55:13.541: INFO: namespace e2e-tests-dns-56cxn deletion completed in 6.056959442s

â€¢ [SLOW TEST:40.152 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:55:13.541: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 21:55:21.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:21.627: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:23.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:23.633: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:25.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:25.632: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:27.627: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:27.629: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:29.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:29.633: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:31.630: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:31.634: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:33.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:33.631: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:35.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:35.634: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:37.627: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:37.629: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:39.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:39.629: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:41.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:41.632: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:43.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:43.632: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:45.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:45.632: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 21:55:47.628: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 21:55:47.630: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:55:47.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-nfmrh" for this suite.
Dec  5 21:56:09.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:56:09.675: INFO: namespace: e2e-tests-container-lifecycle-hook-nfmrh, resource: bindings, ignored listing per whitelist
Dec  5 21:56:09.695: INFO: namespace e2e-tests-container-lifecycle-hook-nfmrh deletion completed in 22.058745924s

â€¢ [SLOW TEST:56.153 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:56:09.695: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-v4lcv
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 21:56:09.733: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 21:56:27.776: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.200.0.94 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v4lcv PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 21:56:27.776: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 21:56:28.849: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:56:28.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-v4lcv" for this suite.
Dec  5 21:56:50.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:56:50.865: INFO: namespace: e2e-tests-pod-network-test-v4lcv, resource: bindings, ignored listing per whitelist
Dec  5 21:56:50.899: INFO: namespace e2e-tests-pod-network-test-v4lcv deletion completed in 22.048537022s

â€¢ [SLOW TEST:41.205 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:56:50.903: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Dec  5 21:56:50.938: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-x8cb7" to be "success or failure"
Dec  5 21:56:50.944: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.12133ms
Dec  5 21:56:52.947: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008617167s
STEP: Saw pod success
Dec  5 21:56:52.947: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  5 21:56:52.948: INFO: Trying to get logs from node single-node pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  5 21:56:52.960: INFO: Waiting for pod pod-host-path-test to disappear
Dec  5 21:56:52.966: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 21:56:52.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-x8cb7" for this suite.
Dec  5 21:56:58.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 21:56:59.027: INFO: namespace: e2e-tests-hostpath-x8cb7, resource: bindings, ignored listing per whitelist
Dec  5 21:56:59.031: INFO: namespace e2e-tests-hostpath-x8cb7 deletion completed in 6.052681526s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 21:56:59.032: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-px2fn
Dec  5 21:57:01.072: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-px2fn
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 21:57:01.074: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:01:01.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-px2fn" for this suite.
Dec  5 22:01:07.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:01:07.552: INFO: namespace: e2e-tests-container-probe-px2fn, resource: bindings, ignored listing per whitelist
Dec  5 22:01:07.591: INFO: namespace e2e-tests-container-probe-px2fn deletion completed in 6.058066811s

â€¢ [SLOW TEST:248.559 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:01:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:01:07.630: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:01:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bgj5p" for this suite.
Dec  5 22:01:59.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:01:59.682: INFO: namespace: e2e-tests-pods-bgj5p, resource: bindings, ignored listing per whitelist
Dec  5 22:01:59.721: INFO: namespace e2e-tests-pods-bgj5p deletion completed in 50.058299832s

â€¢ [SLOW TEST:52.129 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:01:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-639d0175-f8d9-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:02:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n7f9c" for this suite.
Dec  5 22:02:23.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:02:23.845: INFO: namespace: e2e-tests-configmap-n7f9c, resource: bindings, ignored listing per whitelist
Dec  5 22:02:23.845: INFO: namespace e2e-tests-configmap-n7f9c deletion completed in 22.053373758s

â€¢ [SLOW TEST:24.124 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:02:23.845: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-71fd9700-f8d9-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:02:23.892: INFO: Waiting up to 5m0s for pod "pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-rwrwv" to be "success or failure"
Dec  5 22:02:23.907: INFO: Pod "pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.965702ms
Dec  5 22:02:25.917: INFO: Pod "pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025367145s
STEP: Saw pod success
Dec  5 22:02:25.917: INFO: Pod "pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:02:25.920: INFO: Trying to get logs from node single-node pod pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:02:25.933: INFO: Waiting for pod pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb to disappear
Dec  5 22:02:25.941: INFO: Pod pod-secrets-71fe0ec6-f8d9-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:02:25.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rwrwv" for this suite.
Dec  5 22:02:31.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:02:31.969: INFO: namespace: e2e-tests-secrets-rwrwv, resource: bindings, ignored listing per whitelist
Dec  5 22:02:31.993: INFO: namespace e2e-tests-secrets-rwrwv deletion completed in 6.049615773s

â€¢ [SLOW TEST:8.148 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:02:31.993: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Dec  5 22:02:32.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:32.629: INFO: stderr: ""
Dec  5 22:02:32.629: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 22:02:32.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:32.705: INFO: stderr: ""
Dec  5 22:02:32.705: INFO: stdout: "update-demo-nautilus-m9ps9 update-demo-nautilus-z2ftq "
Dec  5 22:02:32.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-m9ps9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:32.754: INFO: stderr: ""
Dec  5 22:02:32.754: INFO: stdout: ""
Dec  5 22:02:32.754: INFO: update-demo-nautilus-m9ps9 is created but not running
Dec  5 22:02:37.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:37.817: INFO: stderr: ""
Dec  5 22:02:37.817: INFO: stdout: "update-demo-nautilus-m9ps9 update-demo-nautilus-z2ftq "
Dec  5 22:02:37.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-m9ps9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:37.874: INFO: stderr: ""
Dec  5 22:02:37.874: INFO: stdout: "true"
Dec  5 22:02:37.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-m9ps9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:37.938: INFO: stderr: ""
Dec  5 22:02:37.938: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 22:02:37.938: INFO: validating pod update-demo-nautilus-m9ps9
Dec  5 22:02:37.940: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 22:02:37.940: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 22:02:37.940: INFO: update-demo-nautilus-m9ps9 is verified up and running
Dec  5 22:02:37.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-z2ftq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:37.988: INFO: stderr: ""
Dec  5 22:02:37.988: INFO: stdout: "true"
Dec  5 22:02:37.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-z2ftq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:02:38.041: INFO: stderr: ""
Dec  5 22:02:38.041: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 22:02:38.041: INFO: validating pod update-demo-nautilus-z2ftq
Dec  5 22:02:38.045: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 22:02:38.045: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 22:02:38.045: INFO: update-demo-nautilus-z2ftq is verified up and running
STEP: rolling-update to new replication controller
Dec  5 22:02:38.046: INFO: scanned /root for discovery docs: <nil>
Dec  5 22:02:38.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.377: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 22:03:00.377: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 22:03:00.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.431: INFO: stderr: ""
Dec  5 22:03:00.431: INFO: stdout: "update-demo-kitten-bf7mb update-demo-kitten-qrtcg "
Dec  5 22:03:00.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-kitten-bf7mb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.486: INFO: stderr: ""
Dec  5 22:03:00.486: INFO: stdout: "true"
Dec  5 22:03:00.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-kitten-bf7mb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.536: INFO: stderr: ""
Dec  5 22:03:00.536: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 22:03:00.536: INFO: validating pod update-demo-kitten-bf7mb
Dec  5 22:03:00.539: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 22:03:00.539: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 22:03:00.539: INFO: update-demo-kitten-bf7mb is verified up and running
Dec  5 22:03:00.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-kitten-qrtcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.595: INFO: stderr: ""
Dec  5 22:03:00.595: INFO: stdout: "true"
Dec  5 22:03:00.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-kitten-qrtcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-h86p6'
Dec  5 22:03:00.651: INFO: stderr: ""
Dec  5 22:03:00.651: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 22:03:00.651: INFO: validating pod update-demo-kitten-qrtcg
Dec  5 22:03:00.653: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 22:03:00.653: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 22:03:00.653: INFO: update-demo-kitten-qrtcg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:03:00.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h86p6" for this suite.
Dec  5 22:03:22.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:03:22.698: INFO: namespace: e2e-tests-kubectl-h86p6, resource: bindings, ignored listing per whitelist
Dec  5 22:03:22.709: INFO: namespace e2e-tests-kubectl-h86p6 deletion completed in 22.053569028s

â€¢ [SLOW TEST:50.716 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:03:22.713: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-vkvdm
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-vkvdm
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-vkvdm
Dec  5 22:03:22.758: INFO: Found 0 stateful pods, waiting for 1
Dec  5 22:03:32.762: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  5 22:03:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:03:32.907: INFO: stderr: ""
Dec  5 22:03:32.907: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:03:32.907: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:03:32.909: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 22:03:42.914: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:03:42.914: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:03:42.924: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:03:42.924: INFO: ss-0  single-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:03:42.924: INFO: 
Dec  5 22:03:42.924: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  5 22:03:43.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997240727s
Dec  5 22:03:44.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992135219s
Dec  5 22:03:45.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986506432s
Dec  5 22:03:46.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983653058s
Dec  5 22:03:47.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980299315s
Dec  5 22:03:48.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977337612s
Dec  5 22:03:49.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97180275s
Dec  5 22:03:50.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965017881s
Dec  5 22:03:51.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.380407ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-vkvdm
Dec  5 22:03:52.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:03:53.143: INFO: stderr: ""
Dec  5 22:03:53.143: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:03:53.143: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:03:53.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:03:53.265: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  5 22:03:53.265: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:03:53.265: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:03:53.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:03:53.388: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  5 22:03:53.388: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:03:53.388: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:03:53.389: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  5 22:04:03.393: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 22:04:03.393: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 22:04:03.393: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  5 22:04:03.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:04:03.517: INFO: stderr: ""
Dec  5 22:04:03.517: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:04:03.517: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:04:03.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:04:03.647: INFO: stderr: ""
Dec  5 22:04:03.647: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:04:03.647: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:04:03.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:04:03.798: INFO: stderr: ""
Dec  5 22:04:03.798: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:04:03.798: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:04:03.798: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:04:03.800: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  5 22:04:13.808: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:04:13.808: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:04:13.808: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:04:13.818: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:13.818: INFO: ss-0  single-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:13.818: INFO: ss-1  single-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:13.818: INFO: ss-2  single-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:13.818: INFO: 
Dec  5 22:04:13.818: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 22:04:14.822: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:14.822: INFO: ss-0  single-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:14.822: INFO: ss-1  single-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:14.822: INFO: ss-2  single-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:14.822: INFO: 
Dec  5 22:04:14.822: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 22:04:15.829: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:15.829: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:15.830: INFO: ss-1  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:15.830: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:15.830: INFO: 
Dec  5 22:04:15.830: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 22:04:16.832: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:16.832: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:16.832: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:16.832: INFO: 
Dec  5 22:04:16.832: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:17.836: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:17.836: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:17.836: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:17.836: INFO: 
Dec  5 22:04:17.836: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:18.841: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:18.841: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:18.841: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:18.841: INFO: 
Dec  5 22:04:18.841: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:19.843: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:19.843: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:19.843: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:19.843: INFO: 
Dec  5 22:04:19.843: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:20.846: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:20.846: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:20.847: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:20.847: INFO: 
Dec  5 22:04:20.847: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:21.850: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:21.850: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:21.850: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:21.850: INFO: 
Dec  5 22:04:21.850: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 22:04:22.855: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Dec  5 22:04:22.855: INFO: ss-0  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:22 +0000 UTC  }]
Dec  5 22:04:22.855: INFO: ss-2  single-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:03:42 +0000 UTC  }]
Dec  5 22:04:22.855: INFO: 
Dec  5 22:04:22.855: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-vkvdm
Dec  5 22:04:23.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:04:23.928: INFO: rc: 1
Dec  5 22:04:23.928: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00172ed20 exit status 1 <nil> <nil> true [0xc001d091e0 0xc001d09218 0xc001d09230] [0xc001d091e0 0xc001d09218 0xc001d09230] [0xc001d09200 0xc001d09228] [0x92f8e0 0x92f8e0] 0xc0014d8120 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Dec  5 22:04:33.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:04:33.981: INFO: rc: 1
Dec  5 22:04:33.981: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d487b0 exit status 1 <nil> <nil> true [0xc000acad88 0xc000acadd8 0xc000acae28] [0xc000acad88 0xc000acadd8 0xc000acae28] [0xc000acadc0 0xc000acae18] [0x92f8e0 0x92f8e0] 0xc000e5b6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:04:43.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:04:44.038: INFO: rc: 1
Dec  5 22:04:44.038: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001037020 exit status 1 <nil> <nil> true [0xc001914670 0xc001914688 0xc0019146a0] [0xc001914670 0xc001914688 0xc0019146a0] [0xc001914680 0xc001914698] [0x92f8e0 0x92f8e0] 0xc000a60900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:04:54.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:04:54.092: INFO: rc: 1
Dec  5 22:04:54.092: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00172f350 exit status 1 <nil> <nil> true [0xc001d09238 0xc001d09250 0xc001d09268] [0xc001d09238 0xc001d09250 0xc001d09268] [0xc001d09248 0xc001d09260] [0x92f8e0 0x92f8e0] 0xc0014d8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:04.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:04.142: INFO: rc: 1
Dec  5 22:05:04.142: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001037440 exit status 1 <nil> <nil> true [0xc0019146a8 0xc0019146c0 0xc0019146d8] [0xc0019146a8 0xc0019146c0 0xc0019146d8] [0xc0019146b8 0xc0019146d0] [0x92f8e0 0x92f8e0] 0xc000a60c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:14.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:14.192: INFO: rc: 1
Dec  5 22:05:14.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00172f920 exit status 1 <nil> <nil> true [0xc001d09278 0xc001d092b8 0xc001d09308] [0xc001d09278 0xc001d092b8 0xc001d09308] [0xc001d092b0 0xc001d092f0] [0x92f8e0 0x92f8e0] 0xc0014d8840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:24.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:24.251: INFO: rc: 1
Dec  5 22:05:24.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f70450 exit status 1 <nil> <nil> true [0xc00000e740 0xc00000eb20 0xc00000ef58] [0xc00000e740 0xc00000eb20 0xc00000ef58] [0xc00000ea10 0xc00000ee78] [0x92f8e0 0x92f8e0] 0xc0006e41e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:34.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:34.306: INFO: rc: 1
Dec  5 22:05:34.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f709c0 exit status 1 <nil> <nil> true [0xc00000efa0 0xc00000f130 0xc00000f218] [0xc00000efa0 0xc00000f130 0xc00000f218] [0xc00000f0a8 0xc00000f1e8] [0x92f8e0 0x92f8e0] 0xc0006e4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:44.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:44.360: INFO: rc: 1
Dec  5 22:05:44.361: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f5e3c0 exit status 1 <nil> <nil> true [0xc00047dab8 0xc00047dc40 0xc00047dce8] [0xc00047dab8 0xc00047dc40 0xc00047dce8] [0xc00047dc20 0xc00047dcb0] [0x92f8e0 0x92f8e0] 0xc000f784e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:05:54.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:05:54.407: INFO: rc: 1
Dec  5 22:05:54.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b23c0 exit status 1 <nil> <nil> true [0xc0009c00f8 0xc0009c0d98 0xc0009c0f10] [0xc0009c00f8 0xc0009c0d98 0xc0009c0f10] [0xc0009c0cb0 0xc0009c0ee0] [0x92f8e0 0x92f8e0] 0xc000b10240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:04.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:04.468: INFO: rc: 1
Dec  5 22:06:04.468: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f70e10 exit status 1 <nil> <nil> true [0xc00000f278 0xc00000f3e0 0xc00000f7d0] [0xc00000f278 0xc00000f3e0 0xc00000f7d0] [0xc00000f398 0xc00000f528] [0x92f8e0 0x92f8e0] 0xc0006e4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:14.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:14.518: INFO: rc: 1
Dec  5 22:06:14.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b2840 exit status 1 <nil> <nil> true [0xc0009c0f78 0xc0009c11b8 0xc0009c13b0] [0xc0009c0f78 0xc0009c11b8 0xc0009c13b0] [0xc0009c1168 0xc0009c1350] [0x92f8e0 0x92f8e0] 0xc000b105a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:24.582: INFO: rc: 1
Dec  5 22:06:24.583: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474510 exit status 1 <nil> <nil> true [0xc000153878 0xc000153ce0 0xc000153e40] [0xc000153878 0xc000153ce0 0xc000153e40] [0xc000153bf0 0xc000153db8] [0x92f8e0 0x92f8e0] 0xc001930480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:34.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:34.643: INFO: rc: 1
Dec  5 22:06:34.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474a50 exit status 1 <nil> <nil> true [0xc000153f88 0xc000aca030 0xc000aca080] [0xc000153f88 0xc000aca030 0xc000aca080] [0xc000aca020 0xc000aca060] [0x92f8e0 0x92f8e0] 0xc001930ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:44.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:44.692: INFO: rc: 1
Dec  5 22:06:44.692: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474ea0 exit status 1 <nil> <nil> true [0xc000aca090 0xc000aca0d8 0xc000aca110] [0xc000aca090 0xc000aca0d8 0xc000aca110] [0xc000aca0c0 0xc000aca100] [0x92f8e0 0x92f8e0] 0xc001931020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:06:54.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:06:54.741: INFO: rc: 1
Dec  5 22:06:54.741: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b2c00 exit status 1 <nil> <nil> true [0xc0009c13b8 0xc0009c1438 0xc0009c1518] [0xc0009c13b8 0xc0009c1438 0xc0009c1518] [0xc0009c1408 0xc0009c14e0] [0x92f8e0 0x92f8e0] 0xc000b108a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:04.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:04.791: INFO: rc: 1
Dec  5 22:07:04.791: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f71230 exit status 1 <nil> <nil> true [0xc00000f800 0xc00000fb58 0xc00000fd98] [0xc00000f800 0xc00000fb58 0xc00000fd98] [0xc00000faf0 0xc00000fd08] [0x92f8e0 0x92f8e0] 0xc0006e4c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:14.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:14.841: INFO: rc: 1
Dec  5 22:07:14.842: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f71620 exit status 1 <nil> <nil> true [0xc00000fe48 0xc00000ff88 0xc00000fff0] [0xc00000fe48 0xc00000ff88 0xc00000fff0] [0xc00000ff48 0xc00000ffe0] [0x92f8e0 0x92f8e0] 0xc0006e4f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:24.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:24.897: INFO: rc: 1
Dec  5 22:07:24.897: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474630 exit status 1 <nil> <nil> true [0xc000153a90 0xc000153d70 0xc000153f88] [0xc000153a90 0xc000153d70 0xc000153f88] [0xc000153ce0 0xc000153e40] [0x92f8e0 0x92f8e0] 0xc001930300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:34.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:34.947: INFO: rc: 1
Dec  5 22:07:34.947: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474ae0 exit status 1 <nil> <nil> true [0xc00000e698 0xc00000ea10 0xc00000ee78] [0xc00000e698 0xc00000ea10 0xc00000ee78] [0xc00000e870 0xc00000ec88] [0x92f8e0 0x92f8e0] 0xc001930900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:44.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:45.023: INFO: rc: 1
Dec  5 22:07:45.023: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b23f0 exit status 1 <nil> <nil> true [0xc000aca000 0xc000aca050 0xc000aca090] [0xc000aca000 0xc000aca050 0xc000aca090] [0xc000aca030 0xc000aca080] [0x92f8e0 0x92f8e0] 0xc0006e4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:07:55.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:07:55.085: INFO: rc: 1
Dec  5 22:07:55.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001474f30 exit status 1 <nil> <nil> true [0xc00000ef58 0xc00000f0a8 0xc00000f1e8] [0xc00000ef58 0xc00000f0a8 0xc00000f1e8] [0xc00000f038 0xc00000f1b0] [0x92f8e0 0x92f8e0] 0xc001930f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:05.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:05.147: INFO: rc: 1
Dec  5 22:08:05.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001475320 exit status 1 <nil> <nil> true [0xc00000f218 0xc00000f398 0xc00000f528] [0xc00000f218 0xc00000f398 0xc00000f528] [0xc00000f350 0xc00000f4d0] [0x92f8e0 0x92f8e0] 0xc001931620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:15.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:15.205: INFO: rc: 1
Dec  5 22:08:15.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b2870 exit status 1 <nil> <nil> true [0xc000aca0b0 0xc000aca0e8 0xc000aca130] [0xc000aca0b0 0xc000aca0e8 0xc000aca130] [0xc000aca0d8 0xc000aca110] [0x92f8e0 0x92f8e0] 0xc0006e45a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:25.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:25.265: INFO: rc: 1
Dec  5 22:08:25.265: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b2c90 exit status 1 <nil> <nil> true [0xc000aca140 0xc000aca198 0xc000aca1e8] [0xc000aca140 0xc000aca198 0xc000aca1e8] [0xc000aca188 0xc000aca1c8] [0x92f8e0 0x92f8e0] 0xc0006e48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:35.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:35.318: INFO: rc: 1
Dec  5 22:08:35.318: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001475980 exit status 1 <nil> <nil> true [0xc00000f7d0 0xc00000faf0 0xc00000fd08] [0xc00000f7d0 0xc00000faf0 0xc00000fd08] [0xc00000fa78 0xc00000fc20] [0x92f8e0 0x92f8e0] 0xc001931c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:45.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:45.393: INFO: rc: 1
Dec  5 22:08:45.393: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001475dd0 exit status 1 <nil> <nil> true [0xc00000fd98 0xc00000ff48 0xc00000ffe0] [0xc00000fd98 0xc00000ff48 0xc00000ffe0] [0xc00000ff10 0xc00000ffb8] [0x92f8e0 0x92f8e0] 0xc000b100c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:08:55.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:08:55.462: INFO: rc: 1
Dec  5 22:08:55.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f5e1e0 exit status 1 <nil> <nil> true [0xc00000fff0 0xc0009c0cb0 0xc0009c0ee0] [0xc00000fff0 0xc0009c0cb0 0xc0009c0ee0] [0xc0009c01b0 0xc0009c0e40] [0x92f8e0 0x92f8e0] 0xc000b103c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:09:05.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:09:05.518: INFO: rc: 1
Dec  5 22:09:05.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012b3230 exit status 1 <nil> <nil> true [0xc000aca1f8 0xc000aca248 0xc000aca288] [0xc000aca1f8 0xc000aca248 0xc000aca288] [0xc000aca228 0xc000aca278] [0x92f8e0 0x92f8e0] 0xc0006e4c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:09:15.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:09:15.581: INFO: rc: 1
Dec  5 22:09:15.581: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f5e600 exit status 1 <nil> <nil> true [0xc0009c0f10 0xc0009c1168 0xc0009c1350] [0xc0009c0f10 0xc0009c1168 0xc0009c1350] [0xc0009c0ff0 0xc0009c1278] [0x92f8e0 0x92f8e0] 0xc000b10720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 22:09:25.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-vkvdm ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:09:25.638: INFO: rc: 1
Dec  5 22:09:25.638: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec  5 22:09:25.638: INFO: Scaling statefulset ss to 0
Dec  5 22:09:25.643: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 22:09:25.645: INFO: Deleting all statefulset in ns e2e-tests-statefulset-vkvdm
Dec  5 22:09:25.647: INFO: Scaling statefulset ss to 0
Dec  5 22:09:25.651: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:09:25.653: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:09:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-vkvdm" for this suite.
Dec  5 22:09:31.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:09:31.716: INFO: namespace: e2e-tests-statefulset-vkvdm, resource: bindings, ignored listing per whitelist
Dec  5 22:09:31.723: INFO: namespace e2e-tests-statefulset-vkvdm deletion completed in 6.05976383s

â€¢ [SLOW TEST:369.010 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:09:31.730: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  5 22:09:31.764: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:09:34.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-jcnv7" for this suite.
Dec  5 22:09:40.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:09:40.112: INFO: namespace: e2e-tests-init-container-jcnv7, resource: bindings, ignored listing per whitelist
Dec  5 22:09:40.136: INFO: namespace e2e-tests-init-container-jcnv7 deletion completed in 6.052263566s

â€¢ [SLOW TEST:8.408 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:09:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7609b9e2-f8da-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:09:40.179: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-dndcp" to be "success or failure"
Dec  5 22:09:40.182: INFO: Pod "pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904524ms
Dec  5 22:09:42.186: INFO: Pod "pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007235681s
STEP: Saw pod success
Dec  5 22:09:42.186: INFO: Pod "pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:09:42.190: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:09:42.208: INFO: Waiting for pod pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:09:42.214: INFO: Pod pod-projected-configmaps-760a3a0d-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:09:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dndcp" for this suite.
Dec  5 22:09:48.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:09:48.241: INFO: namespace: e2e-tests-projected-dndcp, resource: bindings, ignored listing per whitelist
Dec  5 22:09:48.279: INFO: namespace e2e-tests-projected-dndcp deletion completed in 6.060264074s

â€¢ [SLOW TEST:8.141 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:09:48.279: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  5 22:09:50.856: INFO: Successfully updated pod "labelsupdate7ae52050-f8da-11e8-8a71-d2079f97accb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:09:54.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-brnfk" for this suite.
Dec  5 22:10:16.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:10:16.905: INFO: namespace: e2e-tests-downward-api-brnfk, resource: bindings, ignored listing per whitelist
Dec  5 22:10:16.940: INFO: namespace e2e-tests-downward-api-brnfk deletion completed in 22.06289734s

â€¢ [SLOW TEST:28.661 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:10:16.940: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-8bfb7f7e-f8da-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:10:16.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-s8bw6" to be "success or failure"
Dec  5 22:10:17.009: INFO: Pod "pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.524027ms
Dec  5 22:10:19.013: INFO: Pod "pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016938166s
STEP: Saw pod success
Dec  5 22:10:19.013: INFO: Pod "pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:10:19.017: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:10:19.026: INFO: Waiting for pod pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:10:19.034: INFO: Pod pod-projected-configmaps-8bfbed27-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:10:19.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s8bw6" for this suite.
Dec  5 22:10:25.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:10:25.079: INFO: namespace: e2e-tests-projected-s8bw6, resource: bindings, ignored listing per whitelist
Dec  5 22:10:25.101: INFO: namespace e2e-tests-projected-s8bw6 deletion completed in 6.062205554s

â€¢ [SLOW TEST:8.161 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:10:25.101: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:10:25.139: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-fhjsr" to be "success or failure"
Dec  5 22:10:25.149: INFO: Pod "downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629971ms
Dec  5 22:10:27.153: INFO: Pod "downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013789517s
STEP: Saw pod success
Dec  5 22:10:27.153: INFO: Pod "downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:10:27.156: INFO: Trying to get logs from node single-node pod downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:10:27.175: INFO: Waiting for pod downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:10:27.180: INFO: Pod downwardapi-volume-90d6a6fe-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:10:27.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fhjsr" for this suite.
Dec  5 22:10:33.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:10:33.226: INFO: namespace: e2e-tests-projected-fhjsr, resource: bindings, ignored listing per whitelist
Dec  5 22:10:33.248: INFO: namespace e2e-tests-projected-fhjsr deletion completed in 6.064797926s

â€¢ [SLOW TEST:8.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:10:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  5 22:10:33.289: INFO: Waiting up to 5m0s for pod "downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-xwfmp" to be "success or failure"
Dec  5 22:10:33.301: INFO: Pod "downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.548784ms
Dec  5 22:10:35.305: INFO: Pod "downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016412342s
STEP: Saw pod success
Dec  5 22:10:35.305: INFO: Pod "downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:10:35.307: INFO: Trying to get logs from node single-node pod downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:10:35.321: INFO: Waiting for pod downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:10:35.324: INFO: Pod downward-api-95b268f8-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:10:35.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xwfmp" for this suite.
Dec  5 22:10:41.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:10:41.367: INFO: namespace: e2e-tests-downward-api-xwfmp, resource: bindings, ignored listing per whitelist
Dec  5 22:10:41.392: INFO: namespace e2e-tests-downward-api-xwfmp deletion completed in 6.065844254s

â€¢ [SLOW TEST:8.143 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:10:41.392: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:10:41.430: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-zsklc" to be "success or failure"
Dec  5 22:10:41.439: INFO: Pod "downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.655708ms
Dec  5 22:10:43.444: INFO: Pod "downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013546501s
STEP: Saw pod success
Dec  5 22:10:43.444: INFO: Pod "downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:10:43.446: INFO: Trying to get logs from node single-node pod downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:10:43.464: INFO: Waiting for pod downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:10:43.466: INFO: Pod downwardapi-volume-9a8c7e12-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:10:43.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zsklc" for this suite.
Dec  5 22:10:49.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:10:49.521: INFO: namespace: e2e-tests-projected-zsklc, resource: bindings, ignored listing per whitelist
Dec  5 22:10:49.538: INFO: namespace e2e-tests-projected-zsklc deletion completed in 6.068882263s

â€¢ [SLOW TEST:8.146 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:10:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:11:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-6mfqj" for this suite.
Dec  5 22:11:32.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:11:32.654: INFO: namespace: e2e-tests-replication-controller-6mfqj, resource: bindings, ignored listing per whitelist
Dec  5 22:11:32.669: INFO: namespace e2e-tests-replication-controller-6mfqj deletion completed in 22.063519472s

â€¢ [SLOW TEST:43.129 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:11:32.669: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 22:11:32.708: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 22:11:32.711: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 22:11:32.712: INFO: 
Logging pods the kubelet thinks is on node single-node before test
Dec  5 22:11:32.718: INFO: etcd-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: gobetween-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: sonobuoy-e2e-job-d548272ad7774311 from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container e2e ready: true, restart count 0
Dec  5 22:11:32.718: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 22:11:32.718: INFO: coredns-74c648b76d-nr9vj from kube-system started at 2018-12-05 21:38:14 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container coredns ready: true, restart count 0
Dec  5 22:11:32.718: INFO: kube-proxy-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: tiller-deploy-6cf89f5895-gqbh8 from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container tiller ready: true, restart count 0
Dec  5 22:11:32.718: INFO: cert-manager-58898b5d55-hhsrf from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container cert-manager ready: true, restart count 0
Dec  5 22:11:32.718: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-05 21:38:34 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 22:11:32.718: INFO: kube-controller-manager-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: kube-apiserver-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: kubernetes-dashboard-7b8b594b7b-g5tmb from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  5 22:11:32.718: INFO: nginx-ingress-default-backend-7b84c755cc-d6fmh from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  5 22:11:32.718: INFO: kube-scheduler-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:11:32.718: INFO: nginx-ingress-controller-4v5js from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  5 22:11:32.718: INFO: coredns-74c648b76d-mn6rk from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container coredns ready: true, restart count 0
Dec  5 22:11:32.718: INFO: sonobuoy-systemd-logs-daemon-set-5b8b8e26ffb94402-m4fwd from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  5 22:11:32.718: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 22:11:32.718: INFO: calico-node-4fdbb from networking started at 2018-12-05 21:37:46 +0000 UTC (2 container statuses recorded)
Dec  5 22:11:32.718: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 22:11:32.718: INFO: 	Container install-cni ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node single-node
Dec  5 22:11:32.734: INFO: Pod sonobuoy requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod sonobuoy-e2e-job-d548272ad7774311 requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod sonobuoy-systemd-logs-daemon-set-5b8b8e26ffb94402-m4fwd requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod coredns-74c648b76d-mn6rk requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod coredns-74c648b76d-nr9vj requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod etcd-single-node requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod gobetween-single-node requesting resource cpu=100m on Node single-node
Dec  5 22:11:32.734: INFO: Pod kube-apiserver-single-node requesting resource cpu=250m on Node single-node
Dec  5 22:11:32.734: INFO: Pod kube-controller-manager-single-node requesting resource cpu=200m on Node single-node
Dec  5 22:11:32.734: INFO: Pod kube-proxy-single-node requesting resource cpu=200m on Node single-node
Dec  5 22:11:32.734: INFO: Pod kube-scheduler-single-node requesting resource cpu=100m on Node single-node
Dec  5 22:11:32.734: INFO: Pod kubernetes-dashboard-7b8b594b7b-g5tmb requesting resource cpu=100m on Node single-node
Dec  5 22:11:32.734: INFO: Pod tiller-deploy-6cf89f5895-gqbh8 requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod calico-node-4fdbb requesting resource cpu=250m on Node single-node
Dec  5 22:11:32.734: INFO: Pod cert-manager-58898b5d55-hhsrf requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod nginx-ingress-controller-4v5js requesting resource cpu=0m on Node single-node
Dec  5 22:11:32.734: INFO: Pod nginx-ingress-default-backend-7b84c755cc-d6fmh requesting resource cpu=0m on Node single-node
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9218c40-f8da-11e8-8a71-d2079f97accb.156d8f60b3104d2b], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-lznz5/filler-pod-b9218c40-f8da-11e8-8a71-d2079f97accb to single-node]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9218c40-f8da-11e8-8a71-d2079f97accb.156d8f60d59ff523], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9218c40-f8da-11e8-8a71-d2079f97accb.156d8f60d9afd8b8], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9218c40-f8da-11e8-8a71-d2079f97accb.156d8f60e1d7076f], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.156d8f612b3df48e], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node single-node
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:11:35.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-lznz5" for this suite.
Dec  5 22:11:41.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:11:41.800: INFO: namespace: e2e-tests-sched-pred-lznz5, resource: bindings, ignored listing per whitelist
Dec  5 22:11:41.832: INFO: namespace e2e-tests-sched-pred-lznz5 deletion completed in 6.052806308s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.163 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:11:41.832: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-wzgj
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 22:11:41.883: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wzgj" in namespace "e2e-tests-subpath-2qvks" to be "success or failure"
Dec  5 22:11:41.886: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499445ms
Dec  5 22:11:43.889: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006353111s
Dec  5 22:11:45.892: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 4.009298508s
Dec  5 22:11:47.896: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 6.013325929s
Dec  5 22:11:49.904: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 8.021016918s
Dec  5 22:11:51.907: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 10.023937961s
Dec  5 22:11:53.910: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 12.027222584s
Dec  5 22:11:55.913: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 14.029444724s
Dec  5 22:11:57.918: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 16.035105492s
Dec  5 22:11:59.923: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 18.03958511s
Dec  5 22:12:01.928: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 20.044826323s
Dec  5 22:12:03.938: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Running", Reason="", readiness=false. Elapsed: 22.054731902s
Dec  5 22:12:05.941: INFO: Pod "pod-subpath-test-secret-wzgj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05824333s
STEP: Saw pod success
Dec  5 22:12:05.941: INFO: Pod "pod-subpath-test-secret-wzgj" satisfied condition "success or failure"
Dec  5 22:12:05.945: INFO: Trying to get logs from node single-node pod pod-subpath-test-secret-wzgj container test-container-subpath-secret-wzgj: <nil>
STEP: delete the pod
Dec  5 22:12:05.962: INFO: Waiting for pod pod-subpath-test-secret-wzgj to disappear
Dec  5 22:12:05.969: INFO: Pod pod-subpath-test-secret-wzgj no longer exists
STEP: Deleting pod pod-subpath-test-secret-wzgj
Dec  5 22:12:05.970: INFO: Deleting pod "pod-subpath-test-secret-wzgj" in namespace "e2e-tests-subpath-2qvks"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:12:05.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-2qvks" for this suite.
Dec  5 22:12:11.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:12:12.006: INFO: namespace: e2e-tests-subpath-2qvks, resource: bindings, ignored listing per whitelist
Dec  5 22:12:12.024: INFO: namespace e2e-tests-subpath-2qvks deletion completed in 6.049900406s

â€¢ [SLOW TEST:30.192 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:12:12.027: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Dec  5 22:12:12.062: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-502647789 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:12:12.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c26w5" for this suite.
Dec  5 22:12:18.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:12:18.147: INFO: namespace: e2e-tests-kubectl-c26w5, resource: bindings, ignored listing per whitelist
Dec  5 22:12:18.173: INFO: namespace e2e-tests-kubectl-c26w5 deletion completed in 6.05221586s

â€¢ [SLOW TEST:6.146 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:12:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8vd9x;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8vd9x.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 87.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.87_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8vd9x;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8vd9x.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-8vd9x.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8vd9x.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 87.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.87_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 22:12:20.235: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.238: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.243: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.245: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.254: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.256: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.259: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.261: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.263: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.265: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.267: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.269: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:20.280: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:25.288: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.294: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.301: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.319: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.320: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.322: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.324: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.325: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.327: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.329: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.330: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:25.338: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:30.288: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.294: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.302: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.318: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.322: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.324: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.327: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.329: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.332: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.334: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.336: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:30.354: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:35.288: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.294: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.301: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.310: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.311: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.313: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.315: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.317: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.318: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.322: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:35.332: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:40.291: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.294: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.302: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.312: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.314: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.317: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.319: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.321: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.323: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.325: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.327: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:40.336: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:45.284: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.289: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.294: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.296: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.306: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.308: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.313: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.315: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.316: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.318: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.320: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc from pod e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb: the server could not find the requested resource (get pods dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb)
Dec  5 22:12:45.331: INFO: Lookups using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-8vd9x wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8vd9x jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x jessie_udp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@dns-test-service.e2e-tests-dns-8vd9x.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8vd9x.svc]

Dec  5 22:12:50.328: INFO: DNS probes using e2e-tests-dns-8vd9x/dns-test-d43d559f-f8da-11e8-8a71-d2079f97accb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:12:50.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8vd9x" for this suite.
Dec  5 22:12:56.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:12:56.450: INFO: namespace: e2e-tests-dns-8vd9x, resource: bindings, ignored listing per whitelist
Dec  5 22:12:56.456: INFO: namespace e2e-tests-dns-8vd9x deletion completed in 6.05468398s

â€¢ [SLOW TEST:38.283 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:12:56.456: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-eb0d4e98-f8da-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:12:56.494: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-p7k9n" to be "success or failure"
Dec  5 22:12:56.497: INFO: Pod "pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135015ms
Dec  5 22:12:58.501: INFO: Pod "pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006783379s
STEP: Saw pod success
Dec  5 22:12:58.501: INFO: Pod "pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:12:58.503: INFO: Trying to get logs from node single-node pod pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:12:58.524: INFO: Waiting for pod pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb to disappear
Dec  5 22:12:58.531: INFO: Pod pod-configmaps-eb0d9d9b-f8da-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:12:58.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p7k9n" for this suite.
Dec  5 22:13:04.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:13:04.554: INFO: namespace: e2e-tests-configmap-p7k9n, resource: bindings, ignored listing per whitelist
Dec  5 22:13:04.579: INFO: namespace e2e-tests-configmap-p7k9n deletion completed in 6.046212949s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:13:04.581: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 22:13:04.624: INFO: Number of nodes with available pods: 0
Dec  5 22:13:04.624: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:05.628: INFO: Number of nodes with available pods: 0
Dec  5 22:13:05.628: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:06.631: INFO: Number of nodes with available pods: 0
Dec  5 22:13:06.631: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:07.627: INFO: Number of nodes with available pods: 0
Dec  5 22:13:07.627: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:08.631: INFO: Number of nodes with available pods: 0
Dec  5 22:13:08.631: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:09.630: INFO: Number of nodes with available pods: 1
Dec  5 22:13:09.630: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  5 22:13:09.647: INFO: Number of nodes with available pods: 0
Dec  5 22:13:09.647: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:10.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:10.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:11.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:11.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:12.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:12.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:13.652: INFO: Number of nodes with available pods: 0
Dec  5 22:13:13.652: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:14.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:14.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:15.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:15.657: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:16.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:16.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:17.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:17.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:18.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:18.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:19.652: INFO: Number of nodes with available pods: 0
Dec  5 22:13:19.652: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:20.652: INFO: Number of nodes with available pods: 0
Dec  5 22:13:20.652: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:21.652: INFO: Number of nodes with available pods: 0
Dec  5 22:13:21.652: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:22.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:22.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:23.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:23.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:24.655: INFO: Number of nodes with available pods: 0
Dec  5 22:13:24.655: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:25.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:25.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:26.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:26.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:27.655: INFO: Number of nodes with available pods: 0
Dec  5 22:13:27.655: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:28.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:28.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:29.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:29.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:30.656: INFO: Number of nodes with available pods: 0
Dec  5 22:13:30.657: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:31.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:31.656: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:32.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:32.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:33.652: INFO: Number of nodes with available pods: 0
Dec  5 22:13:33.652: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:34.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:34.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:35.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:35.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:36.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:36.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:37.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:37.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:38.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:38.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:39.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:39.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:40.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:40.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:41.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:41.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:42.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:42.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:43.651: INFO: Number of nodes with available pods: 0
Dec  5 22:13:43.651: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:44.654: INFO: Number of nodes with available pods: 0
Dec  5 22:13:44.654: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:45.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:45.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:46.653: INFO: Number of nodes with available pods: 0
Dec  5 22:13:46.653: INFO: Node single-node is running more than one daemon pod
Dec  5 22:13:47.653: INFO: Number of nodes with available pods: 1
Dec  5 22:13:47.653: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-l2rlq, will wait for the garbage collector to delete the pods
Dec  5 22:13:47.712: INFO: Deleting DaemonSet.extensions daemon-set took: 5.075607ms
Dec  5 22:13:47.812: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.236991ms
Dec  5 22:14:25.815: INFO: Number of nodes with available pods: 0
Dec  5 22:14:25.815: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 22:14:25.819: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-l2rlq/daemonsets","resourceVersion":"8781"},"items":null}

Dec  5 22:14:25.821: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-l2rlq/pods","resourceVersion":"8781"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:14:25.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-l2rlq" for this suite.
Dec  5 22:14:31.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:14:31.844: INFO: namespace: e2e-tests-daemonsets-l2rlq, resource: bindings, ignored listing per whitelist
Dec  5 22:14:31.875: INFO: namespace e2e-tests-daemonsets-l2rlq deletion completed in 6.048569613s

â€¢ [SLOW TEST:87.294 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:14:31.875: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Dec  5 22:14:31.910: INFO: Waiting up to 5m0s for pod "var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-var-expansion-849hq" to be "success or failure"
Dec  5 22:14:31.912: INFO: Pod "var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971149ms
Dec  5 22:14:33.920: INFO: Pod "var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00973351s
STEP: Saw pod success
Dec  5 22:14:33.920: INFO: Pod "var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:14:33.923: INFO: Trying to get logs from node single-node pod var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:14:33.944: INFO: Waiting for pod var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:14:33.952: INFO: Pod var-expansion-23ed2489-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:14:33.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-849hq" for this suite.
Dec  5 22:14:39.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:14:39.991: INFO: namespace: e2e-tests-var-expansion-849hq, resource: bindings, ignored listing per whitelist
Dec  5 22:14:40.024: INFO: namespace e2e-tests-var-expansion-849hq deletion completed in 6.069849267s

â€¢ [SLOW TEST:8.149 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:14:40.025: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  5 22:14:40.062: INFO: Waiting up to 5m0s for pod "downward-api-28c90046-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-dpbw2" to be "success or failure"
Dec  5 22:14:40.067: INFO: Pod "downward-api-28c90046-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.124288ms
Dec  5 22:14:42.070: INFO: Pod "downward-api-28c90046-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007335184s
STEP: Saw pod success
Dec  5 22:14:42.070: INFO: Pod "downward-api-28c90046-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:14:42.072: INFO: Trying to get logs from node single-node pod downward-api-28c90046-f8db-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:14:42.087: INFO: Waiting for pod downward-api-28c90046-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:14:42.095: INFO: Pod downward-api-28c90046-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:14:42.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dpbw2" for this suite.
Dec  5 22:14:48.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:14:48.132: INFO: namespace: e2e-tests-downward-api-dpbw2, resource: bindings, ignored listing per whitelist
Dec  5 22:14:48.150: INFO: namespace e2e-tests-downward-api-dpbw2 deletion completed in 6.052002696s

â€¢ [SLOW TEST:8.125 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:14:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-2da1241e-f8db-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:14:48.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-77kjw" to be "success or failure"
Dec  5 22:14:48.197: INFO: Pod "pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981515ms
Dec  5 22:14:50.199: INFO: Pod "pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006127335s
STEP: Saw pod success
Dec  5 22:14:50.199: INFO: Pod "pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:14:50.202: INFO: Trying to get logs from node single-node pod pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:14:50.219: INFO: Waiting for pod pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:14:50.221: INFO: Pod pod-configmaps-2da17d56-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:14:50.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-77kjw" for this suite.
Dec  5 22:14:56.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:14:56.245: INFO: namespace: e2e-tests-configmap-77kjw, resource: bindings, ignored listing per whitelist
Dec  5 22:14:56.288: INFO: namespace e2e-tests-configmap-77kjw deletion completed in 6.056739713s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:14:56.289: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  5 22:14:56.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-q644f,SelfLink:/api/v1/namespaces/e2e-tests-watch-q644f/configmaps/e2e-watch-test-resource-version,UID:327ab7ca-f8db-11e8-aca6-08002781145e,ResourceVersion:8936,Generation:0,CreationTimestamp:2018-12-05 22:14:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 22:14:56.337: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-q644f,SelfLink:/api/v1/namespaces/e2e-tests-watch-q644f/configmaps/e2e-watch-test-resource-version,UID:327ab7ca-f8db-11e8-aca6-08002781145e,ResourceVersion:8937,Generation:0,CreationTimestamp:2018-12-05 22:14:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:14:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-q644f" for this suite.
Dec  5 22:15:02.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:15:02.399: INFO: namespace: e2e-tests-watch-q644f, resource: bindings, ignored listing per whitelist
Dec  5 22:15:02.404: INFO: namespace e2e-tests-watch-q644f deletion completed in 6.06489122s

â€¢ [SLOW TEST:6.116 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:15:02.406: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 22:15:06.497: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:06.499: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 22:15:08.500: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:08.504: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 22:15:10.499: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:10.504: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 22:15:12.500: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:12.502: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 22:15:14.499: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:14.502: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 22:15:16.499: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 22:15:16.503: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:15:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-zqqpq" for this suite.
Dec  5 22:15:38.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:15:38.549: INFO: namespace: e2e-tests-container-lifecycle-hook-zqqpq, resource: bindings, ignored listing per whitelist
Dec  5 22:15:38.558: INFO: namespace e2e-tests-container-lifecycle-hook-zqqpq deletion completed in 22.04793618s

â€¢ [SLOW TEST:36.152 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:15:38.559: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:15:38.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-xwdmw" to be "success or failure"
Dec  5 22:15:38.606: INFO: Pod "downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313682ms
Dec  5 22:15:40.609: INFO: Pod "downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005458236s
STEP: Saw pod success
Dec  5 22:15:40.609: INFO: Pod "downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:15:40.611: INFO: Trying to get logs from node single-node pod downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:15:40.632: INFO: Waiting for pod downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:15:40.637: INFO: Pod downwardapi-volume-4bada171-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:15:40.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xwdmw" for this suite.
Dec  5 22:15:46.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:15:46.691: INFO: namespace: e2e-tests-downward-api-xwdmw, resource: bindings, ignored listing per whitelist
Dec  5 22:15:46.700: INFO: namespace e2e-tests-downward-api-xwdmw deletion completed in 6.058700649s

â€¢ [SLOW TEST:8.141 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:15:46.700: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 22:15:46.738: INFO: Waiting up to 5m0s for pod "pod-5086c04f-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-4l499" to be "success or failure"
Dec  5 22:15:46.742: INFO: Pod "pod-5086c04f-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319071ms
Dec  5 22:15:48.744: INFO: Pod "pod-5086c04f-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005227404s
STEP: Saw pod success
Dec  5 22:15:48.744: INFO: Pod "pod-5086c04f-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:15:48.746: INFO: Trying to get logs from node single-node pod pod-5086c04f-f8db-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:15:48.762: INFO: Waiting for pod pod-5086c04f-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:15:48.769: INFO: Pod pod-5086c04f-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:15:48.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4l499" for this suite.
Dec  5 22:15:54.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:15:54.814: INFO: namespace: e2e-tests-emptydir-4l499, resource: bindings, ignored listing per whitelist
Dec  5 22:15:54.824: INFO: namespace e2e-tests-emptydir-4l499 deletion completed in 6.054132667s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:15:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-rqtqs
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-rqtqs
STEP: Deleting pre-stop pod
Dec  5 22:16:09.908: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:16:09.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-rqtqs" for this suite.
Dec  5 22:16:47.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:16:47.970: INFO: namespace: e2e-tests-prestop-rqtqs, resource: bindings, ignored listing per whitelist
Dec  5 22:16:47.974: INFO: namespace e2e-tests-prestop-rqtqs deletion completed in 38.053502235s

â€¢ [SLOW TEST:53.147 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:16:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:16:48.011: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:16:49.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-x2lvd" for this suite.
Dec  5 22:16:55.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:16:55.085: INFO: namespace: e2e-tests-custom-resource-definition-x2lvd, resource: bindings, ignored listing per whitelist
Dec  5 22:16:55.105: INFO: namespace e2e-tests-custom-resource-definition-x2lvd deletion completed in 6.056424381s

â€¢ [SLOW TEST:7.131 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:16:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Dec  5 22:16:55.135: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  5 22:16:55.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:56.082: INFO: stderr: ""
Dec  5 22:16:56.082: INFO: stdout: "service/redis-slave created\n"
Dec  5 22:16:56.082: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  5 22:16:56.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:56.311: INFO: stderr: ""
Dec  5 22:16:56.312: INFO: stdout: "service/redis-master created\n"
Dec  5 22:16:56.312: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  5 22:16:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:56.557: INFO: stderr: ""
Dec  5 22:16:56.557: INFO: stdout: "service/frontend created\n"
Dec  5 22:16:56.558: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  5 22:16:56.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:56.851: INFO: stderr: ""
Dec  5 22:16:56.851: INFO: stdout: "deployment.extensions/frontend created\n"
Dec  5 22:16:56.851: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  5 22:16:56.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:57.171: INFO: stderr: ""
Dec  5 22:16:57.171: INFO: stdout: "deployment.extensions/redis-master created\n"
Dec  5 22:16:57.176: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  5 22:16:57.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:16:57.668: INFO: stderr: ""
Dec  5 22:16:57.668: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Dec  5 22:16:57.668: INFO: Waiting for all frontend pods to be Running.
Dec  5 22:17:52.771: INFO: Waiting for frontend to serve content.
Dec  5 22:17:57.808: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 22:18:08.327: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 22:18:13.337: INFO: Trying to add a new entry to the guestbook.
Dec  5 22:18:13.345: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  5 22:18:13.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.414: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.414: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 22:18:13.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.493: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 22:18:13.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.566: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.566: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 22:18:13.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.635: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.635: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 22:18:13.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.733: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.733: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 22:18:13.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tzskr'
Dec  5 22:18:13.871: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:18:13.871: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:18:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tzskr" for this suite.
Dec  5 22:18:57.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:18:57.929: INFO: namespace: e2e-tests-kubectl-tzskr, resource: bindings, ignored listing per whitelist
Dec  5 22:18:57.929: INFO: namespace e2e-tests-kubectl-tzskr deletion completed in 44.055327143s

â€¢ [SLOW TEST:122.824 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:18:57.930: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 22:18:57.974: INFO: Waiting up to 5m0s for pod "pod-c28308a3-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-vh2cn" to be "success or failure"
Dec  5 22:18:57.980: INFO: Pod "pod-c28308a3-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.470593ms
Dec  5 22:18:59.982: INFO: Pod "pod-c28308a3-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008372136s
STEP: Saw pod success
Dec  5 22:18:59.982: INFO: Pod "pod-c28308a3-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:18:59.983: INFO: Trying to get logs from node single-node pod pod-c28308a3-f8db-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:19:00.001: INFO: Waiting for pod pod-c28308a3-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:19:00.003: INFO: Pod pod-c28308a3-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:19:00.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vh2cn" for this suite.
Dec  5 22:19:06.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:19:06.039: INFO: namespace: e2e-tests-emptydir-vh2cn, resource: bindings, ignored listing per whitelist
Dec  5 22:19:06.081: INFO: namespace e2e-tests-emptydir-vh2cn deletion completed in 6.071391379s

â€¢ [SLOW TEST:8.152 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:19:06.082: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c75ee562-f8db-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:19:06.129: INFO: Waiting up to 5m0s for pod "pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-bdn5w" to be "success or failure"
Dec  5 22:19:06.133: INFO: Pod "pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756352ms
Dec  5 22:19:08.136: INFO: Pod "pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006749735s
STEP: Saw pod success
Dec  5 22:19:08.136: INFO: Pod "pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:19:08.139: INFO: Trying to get logs from node single-node pod pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:19:08.156: INFO: Waiting for pod pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:19:08.164: INFO: Pod pod-secrets-c75f8e2c-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:19:08.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bdn5w" for this suite.
Dec  5 22:19:14.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:19:14.209: INFO: namespace: e2e-tests-secrets-bdn5w, resource: bindings, ignored listing per whitelist
Dec  5 22:19:14.217: INFO: namespace e2e-tests-secrets-bdn5w deletion completed in 6.050836208s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:19:14.217: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  5 22:19:14.253: INFO: Waiting up to 5m0s for pod "pod-cc3724fc-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-77xtn" to be "success or failure"
Dec  5 22:19:14.259: INFO: Pod "pod-cc3724fc-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74042ms
Dec  5 22:19:16.262: INFO: Pod "pod-cc3724fc-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008890264s
STEP: Saw pod success
Dec  5 22:19:16.262: INFO: Pod "pod-cc3724fc-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:19:16.265: INFO: Trying to get logs from node single-node pod pod-cc3724fc-f8db-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:19:16.279: INFO: Waiting for pod pod-cc3724fc-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:19:16.282: INFO: Pod pod-cc3724fc-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:19:16.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-77xtn" for this suite.
Dec  5 22:19:22.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:19:22.342: INFO: namespace: e2e-tests-emptydir-77xtn, resource: bindings, ignored listing per whitelist
Dec  5 22:19:22.349: INFO: namespace e2e-tests-emptydir-77xtn deletion completed in 6.063438392s

â€¢ [SLOW TEST:8.132 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:19:22.349: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Dec  5 22:19:22.436: INFO: Waiting up to 5m0s for pod "client-containers-d117f309-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-containers-7jwkn" to be "success or failure"
Dec  5 22:19:22.439: INFO: Pod "client-containers-d117f309-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.23674ms
Dec  5 22:19:24.442: INFO: Pod "client-containers-d117f309-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005094032s
Dec  5 22:19:26.445: INFO: Pod "client-containers-d117f309-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008751247s
STEP: Saw pod success
Dec  5 22:19:26.445: INFO: Pod "client-containers-d117f309-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:19:26.447: INFO: Trying to get logs from node single-node pod client-containers-d117f309-f8db-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:19:26.473: INFO: Waiting for pod client-containers-d117f309-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:19:26.476: INFO: Pod client-containers-d117f309-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:19:26.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7jwkn" for this suite.
Dec  5 22:19:32.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:19:32.495: INFO: namespace: e2e-tests-containers-7jwkn, resource: bindings, ignored listing per whitelist
Dec  5 22:19:32.528: INFO: namespace e2e-tests-containers-7jwkn deletion completed in 6.050636817s

â€¢ [SLOW TEST:10.179 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:19:32.529: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  5 22:19:32.570: INFO: Waiting up to 5m0s for pod "pod-d721eac2-f8db-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-ttrs2" to be "success or failure"
Dec  5 22:19:32.575: INFO: Pod "pod-d721eac2-f8db-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.682073ms
Dec  5 22:19:34.577: INFO: Pod "pod-d721eac2-f8db-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00714596s
STEP: Saw pod success
Dec  5 22:19:34.577: INFO: Pod "pod-d721eac2-f8db-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:19:34.579: INFO: Trying to get logs from node single-node pod pod-d721eac2-f8db-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:19:34.592: INFO: Waiting for pod pod-d721eac2-f8db-11e8-8a71-d2079f97accb to disappear
Dec  5 22:19:34.597: INFO: Pod pod-d721eac2-f8db-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:19:34.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ttrs2" for this suite.
Dec  5 22:19:40.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:19:40.628: INFO: namespace: e2e-tests-emptydir-ttrs2, resource: bindings, ignored listing per whitelist
Dec  5 22:19:40.652: INFO: namespace e2e-tests-emptydir-ttrs2 deletion completed in 6.051855489s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:19:40.652: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:20:00.696: INFO: Container started at 2018-12-05 22:19:41 +0000 UTC, pod became ready at 2018-12-05 22:19:59 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:20:00.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-l86qz" for this suite.
Dec  5 22:20:22.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:20:22.754: INFO: namespace: e2e-tests-container-probe-l86qz, resource: bindings, ignored listing per whitelist
Dec  5 22:20:22.788: INFO: namespace e2e-tests-container-probe-l86qz deletion completed in 22.088550251s

â€¢ [SLOW TEST:42.137 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:20:22.788: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Dec  5 22:20:22.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 cluster-info'
Dec  5 22:20:22.911: INFO: stderr: ""
Dec  5 22:20:22.911: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:20:22.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nw2lk" for this suite.
Dec  5 22:20:28.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:20:28.946: INFO: namespace: e2e-tests-kubectl-nw2lk, resource: bindings, ignored listing per whitelist
Dec  5 22:20:28.973: INFO: namespace e2e-tests-kubectl-nw2lk deletion completed in 6.058859181s

â€¢ [SLOW TEST:6.185 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:20:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-fz5n
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 22:20:29.019: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fz5n" in namespace "e2e-tests-subpath-np5dp" to be "success or failure"
Dec  5 22:20:29.035: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Pending", Reason="", readiness=false. Elapsed: 16.076935ms
Dec  5 22:20:31.037: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018153514s
Dec  5 22:20:33.039: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 4.020078309s
Dec  5 22:20:35.045: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 6.026430458s
Dec  5 22:20:37.048: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 8.029210076s
Dec  5 22:20:39.051: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 10.032120593s
Dec  5 22:20:41.055: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 12.035486219s
Dec  5 22:20:43.059: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 14.039529855s
Dec  5 22:20:45.062: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 16.042926799s
Dec  5 22:20:47.066: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 18.047284469s
Dec  5 22:20:49.070: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 20.050880903s
Dec  5 22:20:51.072: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Running", Reason="", readiness=false. Elapsed: 22.053169171s
Dec  5 22:20:53.076: INFO: Pod "pod-subpath-test-configmap-fz5n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056810679s
STEP: Saw pod success
Dec  5 22:20:53.076: INFO: Pod "pod-subpath-test-configmap-fz5n" satisfied condition "success or failure"
Dec  5 22:20:53.082: INFO: Trying to get logs from node single-node pod pod-subpath-test-configmap-fz5n container test-container-subpath-configmap-fz5n: <nil>
STEP: delete the pod
Dec  5 22:20:53.118: INFO: Waiting for pod pod-subpath-test-configmap-fz5n to disappear
Dec  5 22:20:53.120: INFO: Pod pod-subpath-test-configmap-fz5n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fz5n
Dec  5 22:20:53.120: INFO: Deleting pod "pod-subpath-test-configmap-fz5n" in namespace "e2e-tests-subpath-np5dp"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:20:53.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-np5dp" for this suite.
Dec  5 22:20:59.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:20:59.168: INFO: namespace: e2e-tests-subpath-np5dp, resource: bindings, ignored listing per whitelist
Dec  5 22:20:59.188: INFO: namespace e2e-tests-subpath-np5dp deletion completed in 6.063365179s

â€¢ [SLOW TEST:30.215 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:20:59.193: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-sqwmc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 22:20:59.236: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 22:21:15.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.0.145:8080/dial?request=hostName&protocol=http&host=10.200.0.144&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-sqwmc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 22:21:15.284: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 22:21:15.371: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:21:15.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-sqwmc" for this suite.
Dec  5 22:21:37.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:21:37.411: INFO: namespace: e2e-tests-pod-network-test-sqwmc, resource: bindings, ignored listing per whitelist
Dec  5 22:21:37.423: INFO: namespace e2e-tests-pod-network-test-sqwmc deletion completed in 22.049728681s

â€¢ [SLOW TEST:38.230 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:21:37.423: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:21:37.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-ggfc5" to be "success or failure"
Dec  5 22:21:37.466: INFO: Pod "downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06315ms
Dec  5 22:21:39.469: INFO: Pod "downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00691325s
STEP: Saw pod success
Dec  5 22:21:39.469: INFO: Pod "downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:21:39.473: INFO: Trying to get logs from node single-node pod downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:21:39.503: INFO: Waiting for pod downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:21:39.504: INFO: Pod downwardapi-volume-2192fd6f-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:21:39.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ggfc5" for this suite.
Dec  5 22:21:45.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:21:45.547: INFO: namespace: e2e-tests-downward-api-ggfc5, resource: bindings, ignored listing per whitelist
Dec  5 22:21:45.584: INFO: namespace e2e-tests-downward-api-ggfc5 deletion completed in 6.076710014s

â€¢ [SLOW TEST:8.161 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:21:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-2678c4b4-f8dc-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:21:45.695: INFO: Waiting up to 5m0s for pod "pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-knwvs" to be "success or failure"
Dec  5 22:21:45.700: INFO: Pod "pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.333415ms
Dec  5 22:21:47.703: INFO: Pod "pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007987304s
STEP: Saw pod success
Dec  5 22:21:47.703: INFO: Pod "pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:21:47.705: INFO: Trying to get logs from node single-node pod pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:21:47.723: INFO: Waiting for pod pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:21:47.729: INFO: Pod pod-secrets-2679602a-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:21:47.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-knwvs" for this suite.
Dec  5 22:21:53.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:21:53.751: INFO: namespace: e2e-tests-secrets-knwvs, resource: bindings, ignored listing per whitelist
Dec  5 22:21:53.786: INFO: namespace e2e-tests-secrets-knwvs deletion completed in 6.054218718s

â€¢ [SLOW TEST:8.202 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:21:53.787: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:21:57.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-d2fnz" for this suite.
Dec  5 22:22:47.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:22:47.860: INFO: namespace: e2e-tests-kubelet-test-d2fnz, resource: bindings, ignored listing per whitelist
Dec  5 22:22:47.896: INFO: namespace e2e-tests-kubelet-test-d2fnz deletion completed in 50.055595775s

â€¢ [SLOW TEST:54.109 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:22:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:22:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 version --client'
Dec  5 22:22:47.987: INFO: stderr: ""
Dec  5 22:22:47.987: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Dec  5 22:22:47.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-sswvs'
Dec  5 22:22:48.134: INFO: stderr: ""
Dec  5 22:22:48.134: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  5 22:22:48.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-sswvs'
Dec  5 22:22:48.324: INFO: stderr: ""
Dec  5 22:22:48.324: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 22:22:49.332: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:22:49.332: INFO: Found 1 / 1
Dec  5 22:22:49.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 22:22:49.336: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:22:49.336: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 22:22:49.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 describe pod redis-master-wzsx4 --namespace=e2e-tests-kubectl-sswvs'
Dec  5 22:22:49.394: INFO: stderr: ""
Dec  5 22:22:49.394: INFO: stdout: "Name:               redis-master-wzsx4\nNamespace:          e2e-tests-kubectl-sswvs\nPriority:           0\nPriorityClassName:  <none>\nNode:               single-node/192.168.100.50\nStart Time:         Wed, 05 Dec 2018 22:22:48 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 10.200.0.149/32\nStatus:             Running\nIP:                 10.200.0.149\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://e2c0951efb3d90169314572694590869ab9c3dd2c16b96a89800ca1acd2db745\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 05 Dec 2018 22:22:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-s2bxk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-s2bxk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-s2bxk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                  Message\n  ----    ------     ----  ----                  -------\n  Normal  Scheduled  1s    default-scheduler     Successfully assigned e2e-tests-kubectl-sswvs/redis-master-wzsx4 to single-node\n  Normal  Pulled     1s    kubelet, single-node  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, single-node  Created container\n  Normal  Started    1s    kubelet, single-node  Started container\n"
Dec  5 22:22:49.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 describe rc redis-master --namespace=e2e-tests-kubectl-sswvs'
Dec  5 22:22:49.462: INFO: stderr: ""
Dec  5 22:22:49.462: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-sswvs\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: redis-master-wzsx4\n"
Dec  5 22:22:49.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 describe service redis-master --namespace=e2e-tests-kubectl-sswvs'
Dec  5 22:22:49.527: INFO: stderr: ""
Dec  5 22:22:49.527: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-sswvs\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.32.0.180\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.200.0.149:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  5 22:22:49.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 describe node single-node'
Dec  5 22:22:49.620: INFO: stderr: ""
Dec  5 22:22:49.620: INFO: stdout: "Name:               single-node\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=single-node\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.100.50/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 05 Dec 2018 21:37:06 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 05 Dec 2018 22:22:40 +0000   Wed, 05 Dec 2018 21:36:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 05 Dec 2018 22:22:40 +0000   Wed, 05 Dec 2018 21:36:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 05 Dec 2018 22:22:40 +0000   Wed, 05 Dec 2018 21:36:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 05 Dec 2018 22:22:40 +0000   Wed, 05 Dec 2018 21:38:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.100.50\n  Hostname:    single-node\nCapacity:\n cpu:                4\n ephemeral-storage:  64800356Ki\n hugepages-2Mi:      0\n memory:             8167960Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  59720007991\n hugepages-2Mi:      0\n memory:             8065560Ki\n pods:               110\nSystem Info:\n Machine ID:                 14f20a272fb04b49b7890606ac745f3a\n System UUID:                C8FB6001-1EF3-4495-B30B-A3055C3D52AC\n Boot ID:                    103dcd50-277a-4cd4-a1f8-e7a7a9f6e645\n Kernel Version:             4.15.0-29-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.0\n Kubelet Version:            v1.13.0\n Kube-Proxy Version:         v1.13.0\nPodCIDR:                     10.200.0.0/24\nNon-terminated Pods:         (18 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-sswvs    redis-master-wzsx4                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  heptio-sonobuoy            sonobuoy-e2e-job-d548272ad7774311                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-5b8b8e26ffb94402-m4fwd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                coredns-74c648b76d-mn6rk                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                coredns-74c648b76d-nr9vj                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                etcd-single-node                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                gobetween-single-node                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                kube-apiserver-single-node                                 250m (6%)     0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                kube-controller-manager-single-node                        200m (5%)     0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                kube-proxy-single-node                                     200m (5%)     0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                kube-scheduler-single-node                                 100m (2%)     0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                kubernetes-dashboard-7b8b594b7b-g5tmb                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      45m\n  kube-system                tiller-deploy-6cf89f5895-gqbh8                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  networking                 calico-node-4fdbb                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         45m\n  networking                 cert-manager-58898b5d55-hhsrf                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  networking                 nginx-ingress-controller-4v5js                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  networking                 nginx-ingress-default-backend-7b84c755cc-d6fmh             0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1200m (30%)  100m (2%)\n  memory             50Mi (0%)    50Mi (0%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                  Message\n  ----    ------                   ----               ----                  -------\n  Normal  NodeHasSufficientMemory  48m (x8 over 48m)  kubelet, single-node  Node single-node status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    48m (x7 over 48m)  kubelet, single-node  Node single-node status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     48m (x7 over 48m)  kubelet, single-node  Node single-node status is now: NodeHasSufficientPID\n"
Dec  5 22:22:49.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 describe namespace e2e-tests-kubectl-sswvs'
Dec  5 22:22:49.686: INFO: stderr: ""
Dec  5 22:22:49.686: INFO: stdout: "Name:         e2e-tests-kubectl-sswvs\nLabels:       e2e-framework=kubectl\n              e2e-run=74d54637-f8d6-11e8-8a71-d2079f97accb\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:22:49.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sswvs" for this suite.
Dec  5 22:23:11.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:23:11.747: INFO: namespace: e2e-tests-kubectl-sswvs, resource: bindings, ignored listing per whitelist
Dec  5 22:23:11.747: INFO: namespace e2e-tests-kubectl-sswvs deletion completed in 22.058788423s

â€¢ [SLOW TEST:23.851 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:23:11.751: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:23:11.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-2n7bj" to be "success or failure"
Dec  5 22:23:11.795: INFO: Pod "downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.037315ms
Dec  5 22:23:13.799: INFO: Pod "downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006698156s
STEP: Saw pod success
Dec  5 22:23:13.800: INFO: Pod "downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:23:13.803: INFO: Trying to get logs from node single-node pod downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:23:13.822: INFO: Waiting for pod downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:23:13.825: INFO: Pod downwardapi-volume-59cca937-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:23:13.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2n7bj" for this suite.
Dec  5 22:23:19.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:23:19.872: INFO: namespace: e2e-tests-projected-2n7bj, resource: bindings, ignored listing per whitelist
Dec  5 22:23:19.881: INFO: namespace e2e-tests-projected-2n7bj deletion completed in 6.050896411s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:23:19.881: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-5ea5a728-f8dc-11e8-8a71-d2079f97accb
STEP: Creating secret with name s-test-opt-upd-5ea5a751-f8dc-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5ea5a728-f8dc-11e8-8a71-d2079f97accb
STEP: Updating secret s-test-opt-upd-5ea5a751-f8dc-11e8-8a71-d2079f97accb
STEP: Creating secret with name s-test-opt-create-5ea5a75d-f8dc-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:23:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rmrcw" for this suite.
Dec  5 22:23:45.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:23:46.012: INFO: namespace: e2e-tests-projected-rmrcw, resource: bindings, ignored listing per whitelist
Dec  5 22:23:46.031: INFO: namespace e2e-tests-projected-rmrcw deletion completed in 22.059636275s

â€¢ [SLOW TEST:26.150 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:23:46.033: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:23:46.080: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-gwmk9" to be "success or failure"
Dec  5 22:23:46.083: INFO: Pod "downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901695ms
Dec  5 22:23:48.086: INFO: Pod "downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005343435s
STEP: Saw pod success
Dec  5 22:23:48.086: INFO: Pod "downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:23:48.089: INFO: Trying to get logs from node single-node pod downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:23:48.109: INFO: Waiting for pod downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:23:48.118: INFO: Pod downwardapi-volume-6e3cb967-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:23:48.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gwmk9" for this suite.
Dec  5 22:23:54.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:23:54.163: INFO: namespace: e2e-tests-projected-gwmk9, resource: bindings, ignored listing per whitelist
Dec  5 22:23:54.179: INFO: namespace e2e-tests-projected-gwmk9 deletion completed in 6.057525593s

â€¢ [SLOW TEST:8.146 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:23:54.179: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-731b8d57-f8dc-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:23:54.260: INFO: Waiting up to 5m0s for pod "pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-bqlxn" to be "success or failure"
Dec  5 22:23:54.262: INFO: Pod "pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91183ms
Dec  5 22:23:56.265: INFO: Pod "pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00493216s
STEP: Saw pod success
Dec  5 22:23:56.265: INFO: Pod "pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:23:56.267: INFO: Trying to get logs from node single-node pod pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb container secret-env-test: <nil>
STEP: delete the pod
Dec  5 22:23:56.279: INFO: Waiting for pod pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:23:56.294: INFO: Pod pod-secrets-731c12b8-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:23:56.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bqlxn" for this suite.
Dec  5 22:24:02.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:24:02.352: INFO: namespace: e2e-tests-secrets-bqlxn, resource: bindings, ignored listing per whitelist
Dec  5 22:24:02.360: INFO: namespace e2e-tests-secrets-bqlxn deletion completed in 6.061865008s

â€¢ [SLOW TEST:8.181 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:24:02.360: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-77fa05ae-f8dc-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:24:02.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-gpgpc" to be "success or failure"
Dec  5 22:24:02.428: INFO: Pod "pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.761174ms
Dec  5 22:24:04.431: INFO: Pod "pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005814939s
STEP: Saw pod success
Dec  5 22:24:04.431: INFO: Pod "pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:24:04.434: INFO: Trying to get logs from node single-node pod pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:24:04.451: INFO: Waiting for pod pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:24:04.458: INFO: Pod pod-configmaps-77fa87e2-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:24:04.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gpgpc" for this suite.
Dec  5 22:24:10.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:24:10.516: INFO: namespace: e2e-tests-configmap-gpgpc, resource: bindings, ignored listing per whitelist
Dec  5 22:24:10.517: INFO: namespace e2e-tests-configmap-gpgpc deletion completed in 6.055222743s

â€¢ [SLOW TEST:8.157 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:24:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7cd346a5-f8dc-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:24:10.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-5zlwh" to be "success or failure"
Dec  5 22:24:10.563: INFO: Pod "pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208868ms
Dec  5 22:24:12.568: INFO: Pod "pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010520541s
STEP: Saw pod success
Dec  5 22:24:12.570: INFO: Pod "pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:24:12.572: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:24:12.586: INFO: Waiting for pod pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:24:12.587: INFO: Pod pod-projected-configmaps-7cd39c9d-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:24:12.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5zlwh" for this suite.
Dec  5 22:24:18.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:24:18.643: INFO: namespace: e2e-tests-projected-5zlwh, resource: bindings, ignored listing per whitelist
Dec  5 22:24:18.653: INFO: namespace e2e-tests-projected-5zlwh deletion completed in 6.053357467s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:24:18.653: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 22:24:18.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:18.759: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 22:24:18.759: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec  5 22:24:18.773: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec  5 22:24:18.783: INFO: scanned /root for discovery docs: <nil>
Dec  5 22:24:18.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:34.542: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 22:24:34.542: INFO: stdout: "Created e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c\nScaling up e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  5 22:24:34.542: INFO: stdout: "Created e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c\nScaling up e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  5 22:24:34.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:34.592: INFO: stderr: ""
Dec  5 22:24:34.593: INFO: stdout: "e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c-dd4fv "
Dec  5 22:24:34.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c-dd4fv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:34.641: INFO: stderr: ""
Dec  5 22:24:34.641: INFO: stdout: "true"
Dec  5 22:24:34.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c-dd4fv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:34.705: INFO: stderr: ""
Dec  5 22:24:34.705: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  5 22:24:34.705: INFO: e2e-test-nginx-rc-e209b604e89638bc0b2db5bea8d9c37c-dd4fv is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Dec  5 22:24:34.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-dh4sp'
Dec  5 22:24:34.772: INFO: stderr: ""
Dec  5 22:24:34.772: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:24:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dh4sp" for this suite.
Dec  5 22:24:56.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:24:56.801: INFO: namespace: e2e-tests-kubectl-dh4sp, resource: bindings, ignored listing per whitelist
Dec  5 22:24:56.833: INFO: namespace e2e-tests-kubectl-dh4sp deletion completed in 22.055725689s

â€¢ [SLOW TEST:38.179 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:24:56.833: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  5 22:24:59.394: INFO: Successfully updated pod "annotationupdate986e57a6-f8dc-11e8-8a71-d2079f97accb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:25:03.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xhnr5" for this suite.
Dec  5 22:25:25.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:25:25.457: INFO: namespace: e2e-tests-downward-api-xhnr5, resource: bindings, ignored listing per whitelist
Dec  5 22:25:25.464: INFO: namespace e2e-tests-downward-api-xhnr5 deletion completed in 22.049723659s

â€¢ [SLOW TEST:28.631 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:25:25.469: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 22:25:28.028: INFO: Successfully updated pod "pod-update-a97fec77-f8dc-11e8-8a71-d2079f97accb"
STEP: verifying the updated pod is in kubernetes
Dec  5 22:25:28.037: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:25:28.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-k8shh" for this suite.
Dec  5 22:25:50.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:25:50.086: INFO: namespace: e2e-tests-pods-k8shh, resource: bindings, ignored listing per whitelist
Dec  5 22:25:50.088: INFO: namespace e2e-tests-pods-k8shh deletion completed in 22.048970648s

â€¢ [SLOW TEST:24.620 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:25:50.088: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-q9b9h
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-q9b9h
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-q9b9h
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-q9b9h
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-q9b9h
Dec  5 22:25:52.166: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-q9b9h, name: ss-0, uid: b907af57-f8dc-11e8-aca6-08002781145e, status phase: Pending. Waiting for statefulset controller to delete.
Dec  5 22:25:52.549: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-q9b9h, name: ss-0, uid: b907af57-f8dc-11e8-aca6-08002781145e, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 22:25:52.552: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-q9b9h, name: ss-0, uid: b907af57-f8dc-11e8-aca6-08002781145e, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 22:25:52.554: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-q9b9h
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-q9b9h
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-q9b9h and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 22:25:54.587: INFO: Deleting all statefulset in ns e2e-tests-statefulset-q9b9h
Dec  5 22:25:54.590: INFO: Scaling statefulset ss to 0
Dec  5 22:26:04.610: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:26:04.612: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:26:04.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-q9b9h" for this suite.
Dec  5 22:26:10.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:26:10.661: INFO: namespace: e2e-tests-statefulset-q9b9h, resource: bindings, ignored listing per whitelist
Dec  5 22:26:10.678: INFO: namespace e2e-tests-statefulset-q9b9h deletion completed in 6.058173745s

â€¢ [SLOW TEST:20.590 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:26:10.681: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:26:12.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hlx67" for this suite.
Dec  5 22:26:56.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:26:56.791: INFO: namespace: e2e-tests-kubelet-test-hlx67, resource: bindings, ignored listing per whitelist
Dec  5 22:26:56.794: INFO: namespace e2e-tests-kubelet-test-hlx67 deletion completed in 44.052132532s

â€¢ [SLOW TEST:46.114 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:26:56.795: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Dec  5 22:26:56.834: INFO: Waiting up to 5m0s for pod "client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-containers-7hc9s" to be "success or failure"
Dec  5 22:26:56.837: INFO: Pod "client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133488ms
Dec  5 22:26:58.840: INFO: Pod "client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005958234s
STEP: Saw pod success
Dec  5 22:26:58.840: INFO: Pod "client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:26:58.842: INFO: Trying to get logs from node single-node pod client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:26:58.859: INFO: Waiting for pod client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:26:58.865: INFO: Pod client-containers-dfef2ffd-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:26:58.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7hc9s" for this suite.
Dec  5 22:27:04.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:04.898: INFO: namespace: e2e-tests-containers-7hc9s, resource: bindings, ignored listing per whitelist
Dec  5 22:27:04.919: INFO: namespace e2e-tests-containers-7hc9s deletion completed in 6.050586458s

â€¢ [SLOW TEST:8.124 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:04.919: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Dec  5 22:27:04.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:05.520: INFO: stderr: ""
Dec  5 22:27:05.520: INFO: stdout: "pod/pause created\n"
Dec  5 22:27:05.520: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  5 22:27:05.520: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-pmvsk" to be "running and ready"
Dec  5 22:27:05.525: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465129ms
Dec  5 22:27:07.528: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007264596s
Dec  5 22:27:07.528: INFO: Pod "pause" satisfied condition "running and ready"
Dec  5 22:27:07.528: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  5 22:27:07.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.590: INFO: stderr: ""
Dec  5 22:27:07.590: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  5 22:27:07.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pod pause -L testing-label --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.667: INFO: stderr: ""
Dec  5 22:27:07.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  5 22:27:07.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 label pods pause testing-label- --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.730: INFO: stderr: ""
Dec  5 22:27:07.730: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  5 22:27:07.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pod pause -L testing-label --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.789: INFO: stderr: ""
Dec  5 22:27:07.789: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Dec  5 22:27:07.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.856: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:27:07.856: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  5 22:27:07.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-pmvsk'
Dec  5 22:27:07.914: INFO: stderr: "No resources found.\n"
Dec  5 22:27:07.914: INFO: stdout: ""
Dec  5 22:27:07.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -l name=pause --namespace=e2e-tests-kubectl-pmvsk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 22:27:07.989: INFO: stderr: ""
Dec  5 22:27:07.989: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:07.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pmvsk" for this suite.
Dec  5 22:27:14.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:14.027: INFO: namespace: e2e-tests-kubectl-pmvsk, resource: bindings, ignored listing per whitelist
Dec  5 22:27:14.047: INFO: namespace e2e-tests-kubectl-pmvsk deletion completed in 6.053714513s

â€¢ [SLOW TEST:9.127 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:14.047: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Dec  5 22:27:14.086: INFO: Waiting up to 5m0s for pod "client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-containers-wgj2k" to be "success or failure"
Dec  5 22:27:14.089: INFO: Pod "client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.689828ms
Dec  5 22:27:16.093: INFO: Pod "client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007344561s
STEP: Saw pod success
Dec  5 22:27:16.093: INFO: Pod "client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:27:16.096: INFO: Trying to get logs from node single-node pod client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:27:16.113: INFO: Waiting for pod client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:27:16.120: INFO: Pod client-containers-ea37ba14-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wgj2k" for this suite.
Dec  5 22:27:22.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:22.156: INFO: namespace: e2e-tests-containers-wgj2k, resource: bindings, ignored listing per whitelist
Dec  5 22:27:22.177: INFO: namespace e2e-tests-containers-wgj2k deletion completed in 6.053156233s

â€¢ [SLOW TEST:8.130 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:27:22.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-k42bh" to be "success or failure"
Dec  5 22:27:22.222: INFO: Pod "downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494082ms
Dec  5 22:27:24.229: INFO: Pod "downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010957606s
STEP: Saw pod success
Dec  5 22:27:24.229: INFO: Pod "downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:27:24.232: INFO: Trying to get logs from node single-node pod downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:27:24.248: INFO: Waiting for pod downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:27:24.254: INFO: Pod downwardapi-volume-ef10634f-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-k42bh" for this suite.
Dec  5 22:27:30.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:30.274: INFO: namespace: e2e-tests-downward-api-k42bh, resource: bindings, ignored listing per whitelist
Dec  5 22:27:30.303: INFO: namespace e2e-tests-downward-api-k42bh deletion completed in 6.045216779s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 22:27:30.341: INFO: Waiting up to 5m0s for pod "pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-thpm2" to be "success or failure"
Dec  5 22:27:30.345: INFO: Pod "pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025262ms
Dec  5 22:27:32.355: INFO: Pod "pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013618278s
STEP: Saw pod success
Dec  5 22:27:32.355: INFO: Pod "pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:27:32.357: INFO: Trying to get logs from node single-node pod pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:27:32.373: INFO: Waiting for pod pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:27:32.381: INFO: Pod pod-f3e83a1a-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:32.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-thpm2" for this suite.
Dec  5 22:27:38.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:38.420: INFO: namespace: e2e-tests-emptydir-thpm2, resource: bindings, ignored listing per whitelist
Dec  5 22:27:38.447: INFO: namespace e2e-tests-emptydir-thpm2 deletion completed in 6.06382603s

â€¢ [SLOW TEST:8.143 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:38.447: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-f8c35c46-f8dc-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:27:38.490: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-blq8l" to be "success or failure"
Dec  5 22:27:38.493: INFO: Pod "pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.482478ms
Dec  5 22:27:40.496: INFO: Pod "pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005093835s
STEP: Saw pod success
Dec  5 22:27:40.496: INFO: Pod "pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:27:40.499: INFO: Trying to get logs from node single-node pod pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:27:40.513: INFO: Waiting for pod pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:27:40.520: INFO: Pod pod-configmaps-f8c3bc46-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:40.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-blq8l" for this suite.
Dec  5 22:27:46.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:46.559: INFO: namespace: e2e-tests-configmap-blq8l, resource: bindings, ignored listing per whitelist
Dec  5 22:27:46.580: INFO: namespace e2e-tests-configmap-blq8l deletion completed in 6.056692371s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:46.580: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  5 22:27:46.622: INFO: Waiting up to 5m0s for pod "downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-hnlq5" to be "success or failure"
Dec  5 22:27:46.631: INFO: Pod "downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.357143ms
Dec  5 22:27:48.635: INFO: Pod "downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012916691s
STEP: Saw pod success
Dec  5 22:27:48.636: INFO: Pod "downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:27:48.639: INFO: Trying to get logs from node single-node pod downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:27:48.652: INFO: Waiting for pod downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb to disappear
Dec  5 22:27:48.657: INFO: Pod downward-api-fd9c438a-f8dc-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:27:48.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hnlq5" for this suite.
Dec  5 22:27:54.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:27:54.707: INFO: namespace: e2e-tests-downward-api-hnlq5, resource: bindings, ignored listing per whitelist
Dec  5 22:27:54.709: INFO: namespace e2e-tests-downward-api-hnlq5 deletion completed in 6.049840588s

â€¢ [SLOW TEST:8.129 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:27:54.710: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:27:54.751: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 22:27:54.759: INFO: Number of nodes with available pods: 0
Dec  5 22:27:54.759: INFO: Node single-node is running more than one daemon pod
Dec  5 22:27:55.762: INFO: Number of nodes with available pods: 0
Dec  5 22:27:55.762: INFO: Node single-node is running more than one daemon pod
Dec  5 22:27:56.765: INFO: Number of nodes with available pods: 1
Dec  5 22:27:56.765: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  5 22:27:56.780: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:27:57.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:27:58.788: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:27:59.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:00.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:01.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:02.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:03.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:04.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:05.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:06.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:07.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:08.789: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:09.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:10.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:11.784: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:12.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:13.784: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:14.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:15.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:16.784: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:17.784: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:18.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:19.784: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:20.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:21.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:22.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:23.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:24.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:25.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:26.786: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:27.785: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:28.787: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:29.788: INFO: Wrong image for pod: daemon-set-dmb97. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  5 22:28:29.788: INFO: Pod daemon-set-dmb97 is not available
Dec  5 22:28:30.786: INFO: Pod daemon-set-7gctd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  5 22:28:30.792: INFO: Number of nodes with available pods: 0
Dec  5 22:28:30.792: INFO: Node single-node is running more than one daemon pod
Dec  5 22:28:31.795: INFO: Number of nodes with available pods: 1
Dec  5 22:28:31.795: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-gwpnv, will wait for the garbage collector to delete the pods
Dec  5 22:28:31.863: INFO: Deleting DaemonSet.extensions daemon-set took: 9.638084ms
Dec  5 22:28:31.964: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.376017ms
Dec  5 22:28:45.768: INFO: Number of nodes with available pods: 0
Dec  5 22:28:45.768: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 22:28:45.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gwpnv/daemonsets","resourceVersion":"11873"},"items":null}

Dec  5 22:28:45.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gwpnv/pods","resourceVersion":"11873"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:28:45.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gwpnv" for this suite.
Dec  5 22:28:51.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:28:51.834: INFO: namespace: e2e-tests-daemonsets-gwpnv, resource: bindings, ignored listing per whitelist
Dec  5 22:28:51.836: INFO: namespace e2e-tests-daemonsets-gwpnv deletion completed in 6.055384954s

â€¢ [SLOW TEST:57.127 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:28:51.836: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:28:51.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-8ptzx" to be "success or failure"
Dec  5 22:28:51.882: INFO: Pod "downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91904ms
Dec  5 22:28:53.893: INFO: Pod "downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013473826s
STEP: Saw pod success
Dec  5 22:28:53.893: INFO: Pod "downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:28:53.897: INFO: Trying to get logs from node single-node pod downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:28:53.911: INFO: Waiting for pod downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:28:53.916: INFO: Pod downwardapi-volume-2481b429-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:28:53.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8ptzx" for this suite.
Dec  5 22:28:59.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:28:59.970: INFO: namespace: e2e-tests-projected-8ptzx, resource: bindings, ignored listing per whitelist
Dec  5 22:28:59.981: INFO: namespace e2e-tests-projected-8ptzx deletion completed in 6.058209681s

â€¢ [SLOW TEST:8.145 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:28:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-j2cx2/configmap-test-295e0507-f8dd-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:29:00.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-j2cx2" to be "success or failure"
Dec  5 22:29:00.043: INFO: Pod "pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.827902ms
Dec  5 22:29:02.048: INFO: Pod "pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011927764s
STEP: Saw pod success
Dec  5 22:29:02.048: INFO: Pod "pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:29:02.051: INFO: Trying to get logs from node single-node pod pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb container env-test: <nil>
STEP: delete the pod
Dec  5 22:29:02.066: INFO: Waiting for pod pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:29:02.076: INFO: Pod pod-configmaps-295e6752-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:02.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-j2cx2" for this suite.
Dec  5 22:29:08.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:08.115: INFO: namespace: e2e-tests-configmap-j2cx2, resource: bindings, ignored listing per whitelist
Dec  5 22:29:08.125: INFO: namespace e2e-tests-configmap-j2cx2 deletion completed in 6.046501209s

â€¢ [SLOW TEST:8.144 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:08.126: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:08.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-snwfj" for this suite.
Dec  5 22:29:14.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:14.192: INFO: namespace: e2e-tests-services-snwfj, resource: bindings, ignored listing per whitelist
Dec  5 22:29:14.217: INFO: namespace e2e-tests-services-snwfj deletion completed in 6.053156219s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.091 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:14.217: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Dec  5 22:29:14.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 --namespace=e2e-tests-kubectl-tnpmh run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  5 22:29:15.371: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  5 22:29:15.372: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:17.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tnpmh" for this suite.
Dec  5 22:29:27.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:27.437: INFO: namespace: e2e-tests-kubectl-tnpmh, resource: bindings, ignored listing per whitelist
Dec  5 22:29:27.438: INFO: namespace e2e-tests-kubectl-tnpmh deletion completed in 10.059726337s

â€¢ [SLOW TEST:13.222 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Dec  5 22:29:29.504: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-39b9c176-f8dd-11e8-8a71-d2079f97accb", GenerateName:"", Namespace:"e2e-tests-pods-m7m9b", SelfLink:"/api/v1/namespaces/e2e-tests-pods-m7m9b/pods/pod-submit-remove-39b9c176-f8dd-11e8-8a71-d2079f97accb", UID:"39ba8778-f8dd-11e8-aca6-08002781145e", ResourceVersion:"12077", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679645767, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"473602724"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.200.0.175/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8vmq4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001210580), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8vmq4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001215628), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"single-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0019307e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001215670)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001215690)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001215698), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00121569c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679645767, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679645768, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679645768, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679645767, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.100.50", PodIP:"10.200.0.175", StartTime:(*v1.Time)(0xc000e9ef60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000e9ef80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd", ContainerID:"containerd://d9666a3ac1d2bf369a31a8ea9eb12ba6d5b5740d78294cd110368568053681af"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  5 22:29:34.520: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:34.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-m7m9b" for this suite.
Dec  5 22:29:40.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:40.554: INFO: namespace: e2e-tests-pods-m7m9b, resource: bindings, ignored listing per whitelist
Dec  5 22:29:40.585: INFO: namespace e2e-tests-pods-m7m9b deletion completed in 6.057547457s

â€¢ [SLOW TEST:13.146 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:40.585: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  5 22:29:40.631: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-ql7td,SelfLink:/api/v1/namespaces/e2e-tests-watch-ql7td/configmaps/e2e-watch-test-watch-closed,UID:4190a1c2-f8dd-11e8-aca6-08002781145e,ResourceVersion:12118,Generation:0,CreationTimestamp:2018-12-05 22:29:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 22:29:40.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-ql7td,SelfLink:/api/v1/namespaces/e2e-tests-watch-ql7td/configmaps/e2e-watch-test-watch-closed,UID:4190a1c2-f8dd-11e8-aca6-08002781145e,ResourceVersion:12119,Generation:0,CreationTimestamp:2018-12-05 22:29:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  5 22:29:40.639: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-ql7td,SelfLink:/api/v1/namespaces/e2e-tests-watch-ql7td/configmaps/e2e-watch-test-watch-closed,UID:4190a1c2-f8dd-11e8-aca6-08002781145e,ResourceVersion:12120,Generation:0,CreationTimestamp:2018-12-05 22:29:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 22:29:40.640: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-ql7td,SelfLink:/api/v1/namespaces/e2e-tests-watch-ql7td/configmaps/e2e-watch-test-watch-closed,UID:4190a1c2-f8dd-11e8-aca6-08002781145e,ResourceVersion:12121,Generation:0,CreationTimestamp:2018-12-05 22:29:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-ql7td" for this suite.
Dec  5 22:29:46.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:46.675: INFO: namespace: e2e-tests-watch-ql7td, resource: bindings, ignored listing per whitelist
Dec  5 22:29:46.719: INFO: namespace e2e-tests-watch-ql7td deletion completed in 6.07523202s

â€¢ [SLOW TEST:6.134 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:46.721: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 22:29:46.769: INFO: Waiting up to 5m0s for pod "pod-453918d2-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-8dd86" to be "success or failure"
Dec  5 22:29:46.773: INFO: Pod "pod-453918d2-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.914199ms
Dec  5 22:29:48.776: INFO: Pod "pod-453918d2-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006987793s
STEP: Saw pod success
Dec  5 22:29:48.776: INFO: Pod "pod-453918d2-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:29:48.779: INFO: Trying to get logs from node single-node pod pod-453918d2-f8dd-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:29:48.806: INFO: Waiting for pod pod-453918d2-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:29:48.808: INFO: Pod pod-453918d2-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:29:48.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8dd86" for this suite.
Dec  5 22:29:54.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:29:54.859: INFO: namespace: e2e-tests-emptydir-8dd86, resource: bindings, ignored listing per whitelist
Dec  5 22:29:54.861: INFO: namespace e2e-tests-emptydir-8dd86 deletion completed in 6.046195062s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:29:54.862: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1205 22:30:01.068430      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 22:30:01.068: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:30:01.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hzmhk" for this suite.
Dec  5 22:30:07.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:30:07.207: INFO: namespace: e2e-tests-gc-hzmhk, resource: bindings, ignored listing per whitelist
Dec  5 22:30:07.207: INFO: namespace e2e-tests-gc-hzmhk deletion completed in 6.055915153s

â€¢ [SLOW TEST:12.346 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:30:07.208: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Dec  5 22:30:07.755: INFO: Waiting up to 5m0s for pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx" in namespace "e2e-tests-svcaccounts-85gbt" to be "success or failure"
Dec  5 22:30:07.759: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016928ms
Dec  5 22:30:09.763: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00799749s
STEP: Saw pod success
Dec  5 22:30:09.763: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx" satisfied condition "success or failure"
Dec  5 22:30:09.766: INFO: Trying to get logs from node single-node pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx container token-test: <nil>
STEP: delete the pod
Dec  5 22:30:09.780: INFO: Waiting for pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx to disappear
Dec  5 22:30:09.781: INFO: Pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-vk7qx no longer exists
STEP: Creating a pod to test consume service account root CA
Dec  5 22:30:09.784: INFO: Waiting up to 5m0s for pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8" in namespace "e2e-tests-svcaccounts-85gbt" to be "success or failure"
Dec  5 22:30:09.789: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66811ms
Dec  5 22:30:11.792: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008077865s
STEP: Saw pod success
Dec  5 22:30:11.792: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8" satisfied condition "success or failure"
Dec  5 22:30:11.794: INFO: Trying to get logs from node single-node pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8 container root-ca-test: <nil>
STEP: delete the pod
Dec  5 22:30:11.810: INFO: Waiting for pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8 to disappear
Dec  5 22:30:11.816: INFO: Pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-7qtb8 no longer exists
STEP: Creating a pod to test consume service account namespace
Dec  5 22:30:11.820: INFO: Waiting up to 5m0s for pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2" in namespace "e2e-tests-svcaccounts-85gbt" to be "success or failure"
Dec  5 22:30:11.823: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.483433ms
Dec  5 22:30:13.826: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00653555s
STEP: Saw pod success
Dec  5 22:30:13.827: INFO: Pod "pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2" satisfied condition "success or failure"
Dec  5 22:30:13.829: INFO: Trying to get logs from node single-node pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2 container namespace-test: <nil>
STEP: delete the pod
Dec  5 22:30:13.842: INFO: Waiting for pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2 to disappear
Dec  5 22:30:13.852: INFO: Pod pod-service-account-51bb862d-f8dd-11e8-8a71-d2079f97accb-276r2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:30:13.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-85gbt" for this suite.
Dec  5 22:30:19.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:30:19.891: INFO: namespace: e2e-tests-svcaccounts-85gbt, resource: bindings, ignored listing per whitelist
Dec  5 22:30:19.911: INFO: namespace e2e-tests-svcaccounts-85gbt deletion completed in 6.056236235s

â€¢ [SLOW TEST:12.704 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:30:19.911: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 22:30:23.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:24.001: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:26.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:26.007: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:28.004: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:28.008: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:30.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:30.005: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:32.002: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:32.005: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:34.002: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:34.006: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 22:30:36.002: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 22:30:36.006: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:30:36.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7khjz" for this suite.
Dec  5 22:30:58.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:30:58.058: INFO: namespace: e2e-tests-container-lifecycle-hook-7khjz, resource: bindings, ignored listing per whitelist
Dec  5 22:30:58.064: INFO: namespace e2e-tests-container-lifecycle-hook-7khjz deletion completed in 22.053546867s

â€¢ [SLOW TEST:38.153 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:30:58.064: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  5 22:31:00.114: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-6fbe483c-f8dd-11e8-8a71-d2079f97accb,GenerateName:,Namespace:e2e-tests-events-l2gw4,SelfLink:/api/v1/namespaces/e2e-tests-events-l2gw4/pods/send-events-6fbe483c-f8dd-11e8-8a71-d2079f97accb,UID:6fbeaa0d-f8dd-11e8-aca6-08002781145e,ResourceVersion:12636,Generation:0,CreationTimestamp:2018-12-05 22:30:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 100236154,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.192/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fmr49 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fmr49,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-fmr49 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c3090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c3140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:30:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:30:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:30:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:30:58 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.192,StartTime:2018-12-05 22:30:58 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-12-05 22:30:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://5dfbf1046eb3013fa3353d6f1c004e622a265848277e4c4c38bd36669f8bb278}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  5 22:31:02.120: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  5 22:31:04.124: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:31:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-l2gw4" for this suite.
Dec  5 22:31:42.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:31:42.189: INFO: namespace: e2e-tests-events-l2gw4, resource: bindings, ignored listing per whitelist
Dec  5 22:31:42.198: INFO: namespace e2e-tests-events-l2gw4 deletion completed in 38.053198412s

â€¢ [SLOW TEST:44.134 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:31:42.199: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Dec  5 22:31:51.454: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:32:08.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-5m6x8" for this suite.
Dec  5 22:32:14.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:32:14.352: INFO: namespace: e2e-tests-namespaces-5m6x8, resource: bindings, ignored listing per whitelist
Dec  5 22:32:14.363: INFO: namespace e2e-tests-namespaces-5m6x8 deletion completed in 6.058236297s
STEP: Destroying namespace "e2e-tests-nsdeletetest-l5kcf" for this suite.
Dec  5 22:32:14.364: INFO: Namespace e2e-tests-nsdeletetest-l5kcf was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-spjhj" for this suite.
Dec  5 22:32:20.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:32:20.420: INFO: namespace: e2e-tests-nsdeletetest-spjhj, resource: bindings, ignored listing per whitelist
Dec  5 22:32:20.424: INFO: namespace e2e-tests-nsdeletetest-spjhj deletion completed in 6.060069215s

â€¢ [SLOW TEST:38.225 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:32:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  5 22:32:20.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12852,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 22:32:20.472: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12853,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 22:32:20.472: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12854,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  5 22:32:30.490: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12874,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 22:32:30.490: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12875,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  5 22:32:30.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-252rq,SelfLink:/api/v1/namespaces/e2e-tests-watch-252rq/configmaps/e2e-watch-test-label-changed,UID:a0d5abb0-f8dd-11e8-aca6-08002781145e,ResourceVersion:12876,Generation:0,CreationTimestamp:2018-12-05 22:32:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:32:30.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-252rq" for this suite.
Dec  5 22:32:36.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:32:36.536: INFO: namespace: e2e-tests-watch-252rq, resource: bindings, ignored listing per whitelist
Dec  5 22:32:36.550: INFO: namespace e2e-tests-watch-252rq deletion completed in 6.057995552s

â€¢ [SLOW TEST:16.126 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:32:36.551: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  5 22:32:36.591: INFO: Waiting up to 5m0s for pod "downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-bzkpv" to be "success or failure"
Dec  5 22:32:36.604: INFO: Pod "downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.974151ms
Dec  5 22:32:38.606: INFO: Pod "downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015321657s
STEP: Saw pod success
Dec  5 22:32:38.606: INFO: Pod "downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:32:38.608: INFO: Trying to get logs from node single-node pod downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:32:38.630: INFO: Waiting for pod downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:32:38.633: INFO: Pod downward-api-aa72094d-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:32:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bzkpv" for this suite.
Dec  5 22:32:44.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:32:44.679: INFO: namespace: e2e-tests-downward-api-bzkpv, resource: bindings, ignored listing per whitelist
Dec  5 22:32:44.698: INFO: namespace e2e-tests-downward-api-bzkpv deletion completed in 6.06259384s

â€¢ [SLOW TEST:8.147 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:32:44.698: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 22:32:44.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-9kw45'
Dec  5 22:32:44.794: INFO: stderr: ""
Dec  5 22:32:44.794: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Dec  5 22:32:44.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-9kw45'
Dec  5 22:32:55.719: INFO: stderr: ""
Dec  5 22:32:55.719: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:32:55.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9kw45" for this suite.
Dec  5 22:33:01.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:33:01.756: INFO: namespace: e2e-tests-kubectl-9kw45, resource: bindings, ignored listing per whitelist
Dec  5 22:33:01.780: INFO: namespace e2e-tests-kubectl-9kw45 deletion completed in 6.057567766s

â€¢ [SLOW TEST:17.081 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:33:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:33:05.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-8ctzs" for this suite.
Dec  5 22:33:11.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:33:11.876: INFO: namespace: e2e-tests-kubelet-test-8ctzs, resource: bindings, ignored listing per whitelist
Dec  5 22:33:11.888: INFO: namespace e2e-tests-kubelet-test-8ctzs deletion completed in 6.051981451s

â€¢ [SLOW TEST:10.109 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:33:11.889: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-bf825c09-f8dd-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:33:11.935: INFO: Waiting up to 5m0s for pod "pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-b7dm5" to be "success or failure"
Dec  5 22:33:11.942: INFO: Pod "pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.983403ms
Dec  5 22:33:13.945: INFO: Pod "pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010336032s
STEP: Saw pod success
Dec  5 22:33:13.945: INFO: Pod "pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:33:13.948: INFO: Trying to get logs from node single-node pod pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:33:13.965: INFO: Waiting for pod pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:33:13.971: INFO: Pod pod-secrets-bf82ca67-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:33:13.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-b7dm5" for this suite.
Dec  5 22:33:19.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:33:19.994: INFO: namespace: e2e-tests-secrets-b7dm5, resource: bindings, ignored listing per whitelist
Dec  5 22:33:20.033: INFO: namespace e2e-tests-secrets-b7dm5 deletion completed in 6.059750463s

â€¢ [SLOW TEST:8.145 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:33:20.034: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Dec  5 22:33:20.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-j4dz9'
Dec  5 22:33:20.211: INFO: stderr: ""
Dec  5 22:33:20.211: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Dec  5 22:33:21.215: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:33:21.215: INFO: Found 0 / 1
Dec  5 22:33:22.215: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:33:22.215: INFO: Found 1 / 1
Dec  5 22:33:22.215: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 22:33:22.220: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:33:22.220: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  5 22:33:22.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 logs redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9'
Dec  5 22:33:22.282: INFO: stderr: ""
Dec  5 22:33:22.282: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 22:33:21.038 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 22:33:21.038 # Server started, Redis version 3.2.12\n1:M 05 Dec 22:33:21.038 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 22:33:21.039 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  5 22:33:22.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 log redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9 --tail=1'
Dec  5 22:33:22.344: INFO: stderr: ""
Dec  5 22:33:22.344: INFO: stdout: "1:M 05 Dec 22:33:21.039 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  5 22:33:22.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 log redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9 --limit-bytes=1'
Dec  5 22:33:22.411: INFO: stderr: ""
Dec  5 22:33:22.411: INFO: stdout: " "
STEP: exposing timestamps
Dec  5 22:33:22.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 log redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9 --tail=1 --timestamps'
Dec  5 22:33:22.480: INFO: stderr: ""
Dec  5 22:33:22.480: INFO: stdout: "2018-12-05T22:33:21.039141669Z 1:M 05 Dec 22:33:21.039 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  5 22:33:24.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 log redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9 --since=1s'
Dec  5 22:33:25.042: INFO: stderr: ""
Dec  5 22:33:25.042: INFO: stdout: ""
Dec  5 22:33:25.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 log redis-master-79747 redis-master --namespace=e2e-tests-kubectl-j4dz9 --since=24h'
Dec  5 22:33:25.106: INFO: stderr: ""
Dec  5 22:33:25.106: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 22:33:21.038 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 22:33:21.038 # Server started, Redis version 3.2.12\n1:M 05 Dec 22:33:21.038 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 22:33:21.039 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Dec  5 22:33:25.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-j4dz9'
Dec  5 22:33:25.159: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 22:33:25.159: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  5 22:33:25.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-j4dz9'
Dec  5 22:33:25.231: INFO: stderr: "No resources found.\n"
Dec  5 22:33:25.231: INFO: stdout: ""
Dec  5 22:33:25.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -l name=nginx --namespace=e2e-tests-kubectl-j4dz9 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 22:33:25.292: INFO: stderr: ""
Dec  5 22:33:25.292: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:33:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j4dz9" for this suite.
Dec  5 22:33:47.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:33:47.320: INFO: namespace: e2e-tests-kubectl-j4dz9, resource: bindings, ignored listing per whitelist
Dec  5 22:33:47.349: INFO: namespace e2e-tests-kubectl-j4dz9 deletion completed in 22.054382792s

â€¢ [SLOW TEST:27.315 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:33:47.350: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  5 22:33:58.414: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:33:58.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-p4b7g" for this suite.
Dec  5 22:34:20.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:34:20.495: INFO: namespace: e2e-tests-replicaset-p4b7g, resource: bindings, ignored listing per whitelist
Dec  5 22:34:20.514: INFO: namespace e2e-tests-replicaset-p4b7g deletion completed in 22.063953375s

â€¢ [SLOW TEST:33.163 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:34:20.514: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:34:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-dtb4c" for this suite.
Dec  5 22:34:32.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:34:32.625: INFO: namespace: e2e-tests-namespaces-dtb4c, resource: bindings, ignored listing per whitelist
Dec  5 22:34:32.660: INFO: namespace e2e-tests-namespaces-dtb4c deletion completed in 6.056703207s
STEP: Destroying namespace "e2e-tests-nsdeletetest-9mtgx" for this suite.
Dec  5 22:34:32.662: INFO: Namespace e2e-tests-nsdeletetest-9mtgx was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-fxbv5" for this suite.
Dec  5 22:34:38.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:34:38.710: INFO: namespace: e2e-tests-nsdeletetest-fxbv5, resource: bindings, ignored listing per whitelist
Dec  5 22:34:38.718: INFO: namespace e2e-tests-nsdeletetest-fxbv5 deletion completed in 6.05584199s

â€¢ [SLOW TEST:18.204 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:34:38.718: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:34:38.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-chzxc" to be "success or failure"
Dec  5 22:34:38.764: INFO: Pod "downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.607611ms
Dec  5 22:34:40.766: INFO: Pod "downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005971211s
Dec  5 22:34:42.770: INFO: Pod "downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0096112s
STEP: Saw pod success
Dec  5 22:34:42.770: INFO: Pod "downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:34:42.774: INFO: Trying to get logs from node single-node pod downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:34:42.788: INFO: Waiting for pod downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:34:42.798: INFO: Pod downwardapi-volume-f343677c-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:34:42.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-chzxc" for this suite.
Dec  5 22:34:48.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:34:48.825: INFO: namespace: e2e-tests-downward-api-chzxc, resource: bindings, ignored listing per whitelist
Dec  5 22:34:48.852: INFO: namespace e2e-tests-downward-api-chzxc deletion completed in 6.052054158s

â€¢ [SLOW TEST:10.134 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:34:48.852: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-f94e0b4d-f8dd-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:34:48.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-2kc5q" to be "success or failure"
Dec  5 22:34:48.899: INFO: Pod "pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325595ms
Dec  5 22:34:50.904: INFO: Pod "pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006790545s
STEP: Saw pod success
Dec  5 22:34:50.904: INFO: Pod "pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:34:50.907: INFO: Trying to get logs from node single-node pod pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:34:50.924: INFO: Waiting for pod pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb to disappear
Dec  5 22:34:50.930: INFO: Pod pod-projected-secrets-f94e6efe-f8dd-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:34:50.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2kc5q" for this suite.
Dec  5 22:34:56.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:34:56.976: INFO: namespace: e2e-tests-projected-2kc5q, resource: bindings, ignored listing per whitelist
Dec  5 22:34:56.988: INFO: namespace e2e-tests-projected-2kc5q deletion completed in 6.05160059s

â€¢ [SLOW TEST:8.136 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:34:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1205 22:35:27.546676      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 22:35:27.546: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:35:27.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kfzlq" for this suite.
Dec  5 22:35:33.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:35:33.593: INFO: namespace: e2e-tests-gc-kfzlq, resource: bindings, ignored listing per whitelist
Dec  5 22:35:33.613: INFO: namespace e2e-tests-gc-kfzlq deletion completed in 6.059994472s

â€¢ [SLOW TEST:36.624 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:35:33.613: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Dec  5 22:35:33.653: INFO: Waiting up to 5m0s for pod "client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-containers-8cqg9" to be "success or failure"
Dec  5 22:35:33.663: INFO: Pod "client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.936794ms
Dec  5 22:35:35.667: INFO: Pod "client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014024884s
STEP: Saw pod success
Dec  5 22:35:35.667: INFO: Pod "client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:35:35.670: INFO: Trying to get logs from node single-node pod client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:35:35.686: INFO: Waiting for pod client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:35:35.693: INFO: Pod client-containers-13fbb8ad-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:35:35.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-8cqg9" for this suite.
Dec  5 22:35:41.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:35:41.733: INFO: namespace: e2e-tests-containers-8cqg9, resource: bindings, ignored listing per whitelist
Dec  5 22:35:41.756: INFO: namespace e2e-tests-containers-8cqg9 deletion completed in 6.059035086s

â€¢ [SLOW TEST:8.143 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:35:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-7qbkc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-7qbkc to expose endpoints map[]
Dec  5 22:35:41.797: INFO: Get endpoints failed (1.601472ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec  5 22:35:42.800: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-7qbkc exposes endpoints map[] (1.005086775s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-7qbkc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-7qbkc to expose endpoints map[pod1:[100]]
Dec  5 22:35:44.828: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-7qbkc exposes endpoints map[pod1:[100]] (2.019784427s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-7qbkc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-7qbkc to expose endpoints map[pod2:[101] pod1:[100]]
Dec  5 22:35:46.864: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-7qbkc exposes endpoints map[pod1:[100] pod2:[101]] (2.027823793s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-7qbkc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-7qbkc to expose endpoints map[pod2:[101]]
Dec  5 22:35:46.882: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-7qbkc exposes endpoints map[pod2:[101]] (14.324956ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-7qbkc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-7qbkc to expose endpoints map[]
Dec  5 22:35:46.904: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-7qbkc exposes endpoints map[] (1.431879ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:35:46.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-7qbkc" for this suite.
Dec  5 22:36:08.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:36:08.993: INFO: namespace: e2e-tests-services-7qbkc, resource: bindings, ignored listing per whitelist
Dec  5 22:36:08.993: INFO: namespace e2e-tests-services-7qbkc deletion completed in 22.064743256s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:27.237 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:36:08.993: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:36:11.060: INFO: Waiting up to 5m0s for pod "client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-pods-25bwz" to be "success or failure"
Dec  5 22:36:11.065: INFO: Pod "client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848764ms
Dec  5 22:36:13.070: INFO: Pod "client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008844231s
STEP: Saw pod success
Dec  5 22:36:13.070: INFO: Pod "client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:36:13.074: INFO: Trying to get logs from node single-node pod client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb container env3cont: <nil>
STEP: delete the pod
Dec  5 22:36:13.089: INFO: Waiting for pod client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:36:13.092: INFO: Pod client-envvars-2a46ae31-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:36:13.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-25bwz" for this suite.
Dec  5 22:36:51.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:36:51.136: INFO: namespace: e2e-tests-pods-25bwz, resource: bindings, ignored listing per whitelist
Dec  5 22:36:51.155: INFO: namespace e2e-tests-pods-25bwz deletion completed in 38.060023045s

â€¢ [SLOW TEST:42.162 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:36:51.155: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 22:36:53.732: INFO: Successfully updated pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb"
Dec  5 22:36:53.732: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-pods-hs4rx" to be "terminated due to deadline exceeded"
Dec  5 22:36:53.736: INFO: Pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb": Phase="Running", Reason="", readiness=true. Elapsed: 3.223324ms
Dec  5 22:36:55.739: INFO: Pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007131611s
Dec  5 22:36:57.742: INFO: Pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009463758s
Dec  5 22:36:57.742: INFO: Pod "pod-update-activedeadlineseconds-42346fcd-f8de-11e8-8a71-d2079f97accb" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:36:57.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hs4rx" for this suite.
Dec  5 22:37:03.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:37:03.769: INFO: namespace: e2e-tests-pods-hs4rx, resource: bindings, ignored listing per whitelist
Dec  5 22:37:03.799: INFO: namespace e2e-tests-pods-hs4rx deletion completed in 6.055198112s

â€¢ [SLOW TEST:12.644 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:37:03.802: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  5 22:37:03.843: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  5 22:37:08.847: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:37:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-h8792" for this suite.
Dec  5 22:37:14.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:37:14.914: INFO: namespace: e2e-tests-replication-controller-h8792, resource: bindings, ignored listing per whitelist
Dec  5 22:37:14.934: INFO: namespace e2e-tests-replication-controller-h8792 deletion completed in 6.057101366s

â€¢ [SLOW TEST:11.133 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:37:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-qc9z
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 22:37:14.979: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qc9z" in namespace "e2e-tests-subpath-94jsz" to be "success or failure"
Dec  5 22:37:14.981: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.959912ms
Dec  5 22:37:16.984: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004358069s
Dec  5 22:37:18.989: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 4.009490179s
Dec  5 22:37:20.992: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 6.012891744s
Dec  5 22:37:22.995: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 8.015980882s
Dec  5 22:37:25.000: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 10.020670849s
Dec  5 22:37:27.002: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 12.023272036s
Dec  5 22:37:29.007: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 14.027306964s
Dec  5 22:37:31.008: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 16.029130603s
Dec  5 22:37:33.013: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 18.033858737s
Dec  5 22:37:35.018: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 20.03875273s
Dec  5 22:37:37.021: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Running", Reason="", readiness=false. Elapsed: 22.042096214s
Dec  5 22:37:39.030: INFO: Pod "pod-subpath-test-downwardapi-qc9z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050390524s
STEP: Saw pod success
Dec  5 22:37:39.030: INFO: Pod "pod-subpath-test-downwardapi-qc9z" satisfied condition "success or failure"
Dec  5 22:37:39.033: INFO: Trying to get logs from node single-node pod pod-subpath-test-downwardapi-qc9z container test-container-subpath-downwardapi-qc9z: <nil>
STEP: delete the pod
Dec  5 22:37:39.047: INFO: Waiting for pod pod-subpath-test-downwardapi-qc9z to disappear
Dec  5 22:37:39.055: INFO: Pod pod-subpath-test-downwardapi-qc9z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qc9z
Dec  5 22:37:39.055: INFO: Deleting pod "pod-subpath-test-downwardapi-qc9z" in namespace "e2e-tests-subpath-94jsz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:37:39.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-94jsz" for this suite.
Dec  5 22:37:45.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:37:45.106: INFO: namespace: e2e-tests-subpath-94jsz, resource: bindings, ignored listing per whitelist
Dec  5 22:37:45.112: INFO: namespace e2e-tests-subpath-94jsz deletion completed in 6.05365257s

â€¢ [SLOW TEST:30.176 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:37:45.114: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-625cf638-f8de-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:37:45.155: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-h6944" to be "success or failure"
Dec  5 22:37:45.168: INFO: Pod "pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.228948ms
Dec  5 22:37:47.173: INFO: Pod "pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017188323s
STEP: Saw pod success
Dec  5 22:37:47.173: INFO: Pod "pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:37:47.176: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:37:47.199: INFO: Waiting for pod pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:37:47.206: INFO: Pod pod-projected-configmaps-625d5b6f-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:37:47.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h6944" for this suite.
Dec  5 22:37:53.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:37:53.224: INFO: namespace: e2e-tests-projected-h6944, resource: bindings, ignored listing per whitelist
Dec  5 22:37:53.261: INFO: namespace e2e-tests-projected-h6944 deletion completed in 6.051157514s

â€¢ [SLOW TEST:8.147 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:37:53.262: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-67380150-f8de-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:37:53.303: INFO: Waiting up to 5m0s for pod "pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-ww82w" to be "success or failure"
Dec  5 22:37:53.312: INFO: Pod "pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.311916ms
Dec  5 22:37:55.315: INFO: Pod "pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011796601s
STEP: Saw pod success
Dec  5 22:37:55.315: INFO: Pod "pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:37:55.318: INFO: Trying to get logs from node single-node pod pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:37:55.331: INFO: Waiting for pod pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:37:55.332: INFO: Pod pod-configmaps-6738b2e5-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:37:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ww82w" for this suite.
Dec  5 22:38:01.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:01.375: INFO: namespace: e2e-tests-configmap-ww82w, resource: bindings, ignored listing per whitelist
Dec  5 22:38:01.394: INFO: namespace e2e-tests-configmap-ww82w deletion completed in 6.057581599s

â€¢ [SLOW TEST:8.133 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:01.396: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 22:38:01.436: INFO: Waiting up to 5m0s for pod "pod-6c11698d-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-p2bzg" to be "success or failure"
Dec  5 22:38:01.438: INFO: Pod "pod-6c11698d-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.52531ms
Dec  5 22:38:03.441: INFO: Pod "pod-6c11698d-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005553887s
STEP: Saw pod success
Dec  5 22:38:03.441: INFO: Pod "pod-6c11698d-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:38:03.445: INFO: Trying to get logs from node single-node pod pod-6c11698d-f8de-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:38:03.464: INFO: Waiting for pod pod-6c11698d-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:38:03.473: INFO: Pod pod-6c11698d-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:03.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-p2bzg" for this suite.
Dec  5 22:38:09.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:09.505: INFO: namespace: e2e-tests-emptydir-p2bzg, resource: bindings, ignored listing per whitelist
Dec  5 22:38:09.523: INFO: namespace e2e-tests-emptydir-p2bzg deletion completed in 6.048058768s

â€¢ [SLOW TEST:8.127 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:09.523: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 22:38:09.562: INFO: Waiting up to 5m0s for pod "pod-70e96460-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-kzf2c" to be "success or failure"
Dec  5 22:38:09.565: INFO: Pod "pod-70e96460-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439813ms
Dec  5 22:38:11.567: INFO: Pod "pod-70e96460-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004894589s
STEP: Saw pod success
Dec  5 22:38:11.567: INFO: Pod "pod-70e96460-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:38:11.570: INFO: Trying to get logs from node single-node pod pod-70e96460-f8de-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:38:11.583: INFO: Waiting for pod pod-70e96460-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:38:11.589: INFO: Pod pod-70e96460-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:11.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kzf2c" for this suite.
Dec  5 22:38:17.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:17.627: INFO: namespace: e2e-tests-emptydir-kzf2c, resource: bindings, ignored listing per whitelist
Dec  5 22:38:17.674: INFO: namespace e2e-tests-emptydir-kzf2c deletion completed in 6.083430311s

â€¢ [SLOW TEST:8.151 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:17.675: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-75c6ba44-f8de-11e8-8a71-d2079f97accb
STEP: Creating secret with name secret-projected-all-test-volume-75c6ba3a-f8de-11e8-8a71-d2079f97accb
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  5 22:38:17.732: INFO: Waiting up to 5m0s for pod "projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-v8gkm" to be "success or failure"
Dec  5 22:38:17.735: INFO: Pod "projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589366ms
Dec  5 22:38:19.738: INFO: Pod "projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006179447s
STEP: Saw pod success
Dec  5 22:38:19.738: INFO: Pod "projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:38:19.740: INFO: Trying to get logs from node single-node pod projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  5 22:38:19.752: INFO: Waiting for pod projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:38:19.767: INFO: Pod projected-volume-75c6ba10-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:19.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v8gkm" for this suite.
Dec  5 22:38:25.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:25.808: INFO: namespace: e2e-tests-projected-v8gkm, resource: bindings, ignored listing per whitelist
Dec  5 22:38:25.831: INFO: namespace e2e-tests-projected-v8gkm deletion completed in 6.061474979s

â€¢ [SLOW TEST:8.156 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:25.831: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-7aa2ce8b-f8de-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:38:25.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-hd2hl" to be "success or failure"
Dec  5 22:38:25.881: INFO: Pod "pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650057ms
Dec  5 22:38:27.883: INFO: Pod "pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005688636s
STEP: Saw pod success
Dec  5 22:38:27.883: INFO: Pod "pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:38:27.885: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:38:27.910: INFO: Waiting for pod pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:38:27.913: INFO: Pod pod-projected-configmaps-7aa32df6-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:27.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hd2hl" for this suite.
Dec  5 22:38:33.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:33.965: INFO: namespace: e2e-tests-projected-hd2hl, resource: bindings, ignored listing per whitelist
Dec  5 22:38:33.979: INFO: namespace e2e-tests-projected-hd2hl deletion completed in 6.061693547s

â€¢ [SLOW TEST:8.147 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 22:38:34.016: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 22:38:34.020: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 22:38:34.022: INFO: 
Logging pods the kubelet thinks is on node single-node before test
Dec  5 22:38:34.025: INFO: kube-proxy-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.025: INFO: tiller-deploy-6cf89f5895-gqbh8 from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container tiller ready: true, restart count 0
Dec  5 22:38:34.026: INFO: cert-manager-58898b5d55-hhsrf from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container cert-manager ready: true, restart count 0
Dec  5 22:38:34.026: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-05 21:38:34 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 22:38:34.026: INFO: kube-controller-manager-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.026: INFO: kube-apiserver-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.026: INFO: kubernetes-dashboard-7b8b594b7b-g5tmb from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  5 22:38:34.026: INFO: nginx-ingress-default-backend-7b84c755cc-d6fmh from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  5 22:38:34.026: INFO: kube-scheduler-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.026: INFO: nginx-ingress-controller-4v5js from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  5 22:38:34.026: INFO: coredns-74c648b76d-mn6rk from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container coredns ready: true, restart count 0
Dec  5 22:38:34.026: INFO: sonobuoy-systemd-logs-daemon-set-5b8b8e26ffb94402-m4fwd from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  5 22:38:34.026: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 22:38:34.026: INFO: calico-node-4fdbb from networking started at 2018-12-05 21:37:46 +0000 UTC (2 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 22:38:34.026: INFO: 	Container install-cni ready: true, restart count 0
Dec  5 22:38:34.026: INFO: etcd-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.026: INFO: gobetween-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:38:34.026: INFO: sonobuoy-e2e-job-d548272ad7774311 from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container e2e ready: true, restart count 0
Dec  5 22:38:34.026: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 22:38:34.026: INFO: coredns-74c648b76d-nr9vj from kube-system started at 2018-12-05 21:38:14 +0000 UTC (1 container statuses recorded)
Dec  5 22:38:34.026: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.156d90da2fa5fd18], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:35.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2zsnm" for this suite.
Dec  5 22:38:41.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:38:41.092: INFO: namespace: e2e-tests-sched-pred-2zsnm, resource: bindings, ignored listing per whitelist
Dec  5 22:38:41.092: INFO: namespace e2e-tests-sched-pred-2zsnm deletion completed in 6.047771806s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.114 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:38:41.092: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  5 22:38:41.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-pc6nc'
Dec  5 22:38:41.674: INFO: stderr: ""
Dec  5 22:38:41.674: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 22:38:42.676: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:38:42.676: INFO: Found 0 / 1
Dec  5 22:38:43.676: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:38:43.676: INFO: Found 1 / 1
Dec  5 22:38:43.676: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  5 22:38:43.679: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:38:43.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 22:38:43.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 patch pod redis-master-w882h --namespace=e2e-tests-kubectl-pc6nc -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  5 22:38:43.747: INFO: stderr: ""
Dec  5 22:38:43.747: INFO: stdout: "pod/redis-master-w882h patched\n"
STEP: checking annotations
Dec  5 22:38:43.752: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:38:43.752: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:38:43.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pc6nc" for this suite.
Dec  5 22:39:05.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:39:05.784: INFO: namespace: e2e-tests-kubectl-pc6nc, resource: bindings, ignored listing per whitelist
Dec  5 22:39:05.814: INFO: namespace e2e-tests-kubectl-pc6nc deletion completed in 22.059062458s

â€¢ [SLOW TEST:24.722 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:39:05.815: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  5 22:39:05.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14405,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 22:39:05.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14405,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  5 22:39:15.889: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14424,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 22:39:15.889: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14424,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  5 22:39:25.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14443,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 22:39:25.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14443,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  5 22:39:35.906: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14463,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 22:39:35.906: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-a,UID:927b9fe2-f8de-11e8-aca6-08002781145e,ResourceVersion:14463,Generation:0,CreationTimestamp:2018-12-05 22:39:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  5 22:39:45.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-b,UID:aa57e763-f8de-11e8-aca6-08002781145e,ResourceVersion:14482,Generation:0,CreationTimestamp:2018-12-05 22:39:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 22:39:45.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-b,UID:aa57e763-f8de-11e8-aca6-08002781145e,ResourceVersion:14482,Generation:0,CreationTimestamp:2018-12-05 22:39:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  5 22:39:55.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-b,UID:aa57e763-f8de-11e8-aca6-08002781145e,ResourceVersion:14501,Generation:0,CreationTimestamp:2018-12-05 22:39:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 22:39:55.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jw6t4,SelfLink:/api/v1/namespaces/e2e-tests-watch-jw6t4/configmaps/e2e-watch-test-configmap-b,UID:aa57e763-f8de-11e8-aca6-08002781145e,ResourceVersion:14501,Generation:0,CreationTimestamp:2018-12-05 22:39:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:40:05.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-jw6t4" for this suite.
Dec  5 22:40:11.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:40:11.987: INFO: namespace: e2e-tests-watch-jw6t4, resource: bindings, ignored listing per whitelist
Dec  5 22:40:11.993: INFO: namespace e2e-tests-watch-jw6t4 deletion completed in 6.059353239s

â€¢ [SLOW TEST:66.179 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:40:11.993: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-b9ea22cf-f8de-11e8-8a71-d2079f97accb
STEP: Creating configMap with name cm-test-opt-upd-b9ea22f2-f8de-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b9ea22cf-f8de-11e8-8a71-d2079f97accb
STEP: Updating configmap cm-test-opt-upd-b9ea22f2-f8de-11e8-8a71-d2079f97accb
STEP: Creating configMap with name cm-test-opt-create-b9ea22ff-f8de-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:41:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6rw5p" for this suite.
Dec  5 22:42:00.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:42:00.429: INFO: namespace: e2e-tests-configmap-6rw5p, resource: bindings, ignored listing per whitelist
Dec  5 22:42:00.446: INFO: namespace e2e-tests-configmap-6rw5p deletion completed in 22.047153577s

â€¢ [SLOW TEST:108.453 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:42:00.447: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:42:00.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-hmlvz" to be "success or failure"
Dec  5 22:42:00.487: INFO: Pod "downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226611ms
Dec  5 22:42:02.489: INFO: Pod "downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004849472s
STEP: Saw pod success
Dec  5 22:42:02.489: INFO: Pod "downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:42:02.491: INFO: Trying to get logs from node single-node pod downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:42:02.505: INFO: Waiting for pod downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb to disappear
Dec  5 22:42:02.507: INFO: Pod downwardapi-volume-fa8d88f3-f8de-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:42:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hmlvz" for this suite.
Dec  5 22:42:08.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:42:08.561: INFO: namespace: e2e-tests-projected-hmlvz, resource: bindings, ignored listing per whitelist
Dec  5 22:42:08.578: INFO: namespace e2e-tests-projected-hmlvz deletion completed in 6.066157644s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:42:08.578: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:42:08.652: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ff6a4561-f8de-11e8-aca6-08002781145e", Controller:(*bool)(0xc0018c2a66), BlockOwnerDeletion:(*bool)(0xc0018c2a67)}}
Dec  5 22:42:08.670: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ff67df3a-f8de-11e8-aca6-08002781145e", Controller:(*bool)(0xc002b4355e), BlockOwnerDeletion:(*bool)(0xc002b4355f)}}
Dec  5 22:42:08.683: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ff688959-f8de-11e8-aca6-08002781145e", Controller:(*bool)(0xc0018c2f4e), BlockOwnerDeletion:(*bool)(0xc0018c2f4f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:42:13.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-bxt4l" for this suite.
Dec  5 22:42:19.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:42:19.741: INFO: namespace: e2e-tests-gc-bxt4l, resource: bindings, ignored listing per whitelist
Dec  5 22:42:19.764: INFO: namespace e2e-tests-gc-bxt4l deletion completed in 6.054921331s

â€¢ [SLOW TEST:11.186 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:42:19.767: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-4vf78
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-4vf78
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-4vf78
Dec  5 22:42:19.821: INFO: Found 0 stateful pods, waiting for 1
Dec  5 22:42:29.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  5 22:42:29.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:42:29.969: INFO: stderr: ""
Dec  5 22:42:29.969: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:42:29.969: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:42:29.972: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 22:42:39.976: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:42:39.976: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:42:39.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999422s
Dec  5 22:42:40.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997544295s
Dec  5 22:42:41.995: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992880857s
Dec  5 22:42:43.000: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988642332s
Dec  5 22:42:44.003: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983731089s
Dec  5 22:42:45.006: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979711759s
Dec  5 22:42:46.011: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976893144s
Dec  5 22:42:47.013: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971990777s
Dec  5 22:42:48.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970011954s
Dec  5 22:42:49.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.631464ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-4vf78
Dec  5 22:42:50.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:42:50.190: INFO: stderr: ""
Dec  5 22:42:50.190: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:42:50.190: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:42:50.191: INFO: Found 1 stateful pods, waiting for 3
Dec  5 22:43:00.198: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 22:43:00.198: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 22:43:00.198: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  5 22:43:00.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:43:00.334: INFO: stderr: ""
Dec  5 22:43:00.334: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:43:00.334: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:43:00.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:43:00.462: INFO: stderr: ""
Dec  5 22:43:00.462: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:43:00.462: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:43:00.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 22:43:00.585: INFO: stderr: ""
Dec  5 22:43:00.585: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 22:43:00.585: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 22:43:00.585: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:43:00.587: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec  5 22:43:10.594: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:43:10.594: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:43:10.594: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 22:43:10.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999604s
Dec  5 22:43:11.609: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99741327s
Dec  5 22:43:12.613: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992869192s
Dec  5 22:43:13.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989485678s
Dec  5 22:43:14.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986991208s
Dec  5 22:43:15.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980530388s
Dec  5 22:43:16.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975360526s
Dec  5 22:43:17.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973298637s
Dec  5 22:43:18.634: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970554099s
Dec  5 22:43:19.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.694699ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-4vf78
Dec  5 22:43:20.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:43:20.778: INFO: stderr: ""
Dec  5 22:43:20.778: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:43:20.778: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:43:20.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:43:20.926: INFO: stderr: ""
Dec  5 22:43:20.926: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:43:20.926: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:43:20.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-4vf78 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 22:43:21.051: INFO: stderr: ""
Dec  5 22:43:21.051: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 22:43:21.051: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 22:43:21.051: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 22:43:31.062: INFO: Deleting all statefulset in ns e2e-tests-statefulset-4vf78
Dec  5 22:43:31.064: INFO: Scaling statefulset ss to 0
Dec  5 22:43:31.069: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 22:43:31.071: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:43:31.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-4vf78" for this suite.
Dec  5 22:43:37.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:43:37.143: INFO: namespace: e2e-tests-statefulset-4vf78, resource: bindings, ignored listing per whitelist
Dec  5 22:43:37.145: INFO: namespace e2e-tests-statefulset-4vf78 deletion completed in 6.065953867s

â€¢ [SLOW TEST:77.378 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:43:37.147: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:43:37.188: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  5 22:43:42.192: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 22:43:42.192: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 22:43:42.205: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-69wmr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-69wmr/deployments/test-cleanup-deployment,UID:372e399c-f8df-11e8-aca6-08002781145e,ResourceVersion:15264,Generation:1,CreationTimestamp:2018-12-05 22:43:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec  5 22:43:42.210: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec  5 22:43:42.210: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec  5 22:43:42.210: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-69wmr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-69wmr/replicasets/test-cleanup-controller,UID:34316eb1-f8df-11e8-aca6-08002781145e,ResourceVersion:15265,Generation:1,CreationTimestamp:2018-12-05 22:43:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 372e399c-f8df-11e8-aca6-08002781145e 0xc0008a2fa7 0xc0008a2fa8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 22:43:42.219: INFO: Pod "test-cleanup-controller-xpdgr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-xpdgr,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-69wmr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-69wmr/pods/test-cleanup-controller-xpdgr,UID:3432668c-f8df-11e8-aca6-08002781145e,ResourceVersion:15255,Generation:0,CreationTimestamp:2018-12-05 22:43:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.225/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 34316eb1-f8df-11e8-aca6-08002781145e 0xc000945c67 0xc000945c68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vbv96 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vbv96,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vbv96 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000945ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000945d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:43:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:43:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:43:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:43:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.225,StartTime:2018-12-05 22:43:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-05 22:43:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd containerd://51b570e83a74a0a49cc0c91dadaa04340d8d894d04e40167473221092c8515d1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:43:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-69wmr" for this suite.
Dec  5 22:43:48.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:43:48.276: INFO: namespace: e2e-tests-deployment-69wmr, resource: bindings, ignored listing per whitelist
Dec  5 22:43:48.298: INFO: namespace e2e-tests-deployment-69wmr deletion completed in 6.062805715s

â€¢ [SLOW TEST:11.152 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:43:48.298: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-69sdh
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 22:43:48.334: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 22:44:10.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.0.228:8080/dial?request=hostName&protocol=udp&host=10.200.0.227&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-69sdh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 22:44:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 22:44:10.457: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:44:10.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-69sdh" for this suite.
Dec  5 22:44:32.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:44:32.505: INFO: namespace: e2e-tests-pod-network-test-69sdh, resource: bindings, ignored listing per whitelist
Dec  5 22:44:32.519: INFO: namespace e2e-tests-pod-network-test-69sdh deletion completed in 22.060418343s

â€¢ [SLOW TEST:44.221 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:44:32.522: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-5533a310-f8df-11e8-8a71-d2079f97accb
STEP: Creating configMap with name cm-test-opt-upd-5533a37b-f8df-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5533a310-f8df-11e8-8a71-d2079f97accb
STEP: Updating configmap cm-test-opt-upd-5533a37b-f8df-11e8-8a71-d2079f97accb
STEP: Creating configMap with name cm-test-opt-create-5533a392-f8df-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:44:38.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4twhx" for this suite.
Dec  5 22:45:00.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:45:00.669: INFO: namespace: e2e-tests-projected-4twhx, resource: bindings, ignored listing per whitelist
Dec  5 22:45:00.671: INFO: namespace e2e-tests-projected-4twhx deletion completed in 22.049924498s

â€¢ [SLOW TEST:28.149 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:45:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 22:45:00.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-nznqw'
Dec  5 22:45:00.802: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 22:45:00.802: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Dec  5 22:45:00.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-nznqw'
Dec  5 22:45:00.872: INFO: stderr: ""
Dec  5 22:45:00.873: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:45:00.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nznqw" for this suite.
Dec  5 22:45:06.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:45:06.892: INFO: namespace: e2e-tests-kubectl-nznqw, resource: bindings, ignored listing per whitelist
Dec  5 22:45:06.939: INFO: namespace e2e-tests-kubectl-nznqw deletion completed in 6.063637141s

â€¢ [SLOW TEST:6.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:45:06.939: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 22:45:11.034: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:11.038: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:13.040: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:13.045: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:15.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:15.044: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:17.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:17.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:19.039: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:19.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:21.039: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:21.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:23.039: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:23.044: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:25.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:25.050: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:27.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:27.040: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:29.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:29.044: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:31.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:31.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:33.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:33.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:35.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:35.043: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 22:45:37.038: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 22:45:37.041: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:45:37.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-nl4j9" for this suite.
Dec  5 22:45:59.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:45:59.084: INFO: namespace: e2e-tests-container-lifecycle-hook-nl4j9, resource: bindings, ignored listing per whitelist
Dec  5 22:45:59.097: INFO: namespace e2e-tests-container-lifecycle-hook-nl4j9 deletion completed in 22.054535199s

â€¢ [SLOW TEST:52.159 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:45:59.098: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Dec  5 22:45:59.134: INFO: Waiting up to 5m0s for pod "var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb" in namespace "e2e-tests-var-expansion-k7l2c" to be "success or failure"
Dec  5 22:45:59.140: INFO: Pod "var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.628473ms
Dec  5 22:46:01.144: INFO: Pod "var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010425226s
STEP: Saw pod success
Dec  5 22:46:01.144: INFO: Pod "var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:46:01.147: INFO: Trying to get logs from node single-node pod var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:46:01.164: INFO: Waiting for pod var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb to disappear
Dec  5 22:46:01.168: INFO: Pod var-expansion-88cc6507-f8df-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:46:01.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-k7l2c" for this suite.
Dec  5 22:46:07.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:46:07.222: INFO: namespace: e2e-tests-var-expansion-k7l2c, resource: bindings, ignored listing per whitelist
Dec  5 22:46:07.229: INFO: namespace e2e-tests-var-expansion-k7l2c deletion completed in 6.056991336s

â€¢ [SLOW TEST:8.131 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:46:07.231: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:46:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-pp2z4" for this suite.
Dec  5 22:46:37.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:46:37.539: INFO: namespace: e2e-tests-container-runtime-pp2z4, resource: bindings, ignored listing per whitelist
Dec  5 22:46:37.541: INFO: namespace e2e-tests-container-runtime-pp2z4 deletion completed in 6.056079722s

â€¢ [SLOW TEST:30.310 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:46:37.542: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4jj8j
Dec  5 22:46:41.594: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4jj8j
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 22:46:41.597: INFO: Initial restart count of pod liveness-http is 0
Dec  5 22:46:59.635: INFO: Restart count of pod e2e-tests-container-probe-4jj8j/liveness-http is now 1 (18.037434176s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:46:59.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4jj8j" for this suite.
Dec  5 22:47:05.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:47:05.695: INFO: namespace: e2e-tests-container-probe-4jj8j, resource: bindings, ignored listing per whitelist
Dec  5 22:47:05.723: INFO: namespace e2e-tests-container-probe-4jj8j deletion completed in 6.07564992s

â€¢ [SLOW TEST:28.181 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:47:05.724: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 22:47:05.767: INFO: Waiting up to 5m0s for pod "pod-b083b50b-f8df-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-zjwsf" to be "success or failure"
Dec  5 22:47:05.769: INFO: Pod "pod-b083b50b-f8df-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291407ms
Dec  5 22:47:07.771: INFO: Pod "pod-b083b50b-f8df-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003875396s
STEP: Saw pod success
Dec  5 22:47:07.771: INFO: Pod "pod-b083b50b-f8df-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:47:07.772: INFO: Trying to get logs from node single-node pod pod-b083b50b-f8df-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:47:07.788: INFO: Waiting for pod pod-b083b50b-f8df-11e8-8a71-d2079f97accb to disappear
Dec  5 22:47:07.791: INFO: Pod pod-b083b50b-f8df-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:47:07.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zjwsf" for this suite.
Dec  5 22:47:13.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:47:13.852: INFO: namespace: e2e-tests-emptydir-zjwsf, resource: bindings, ignored listing per whitelist
Dec  5 22:47:13.861: INFO: namespace e2e-tests-emptydir-zjwsf deletion completed in 6.065849757s

â€¢ [SLOW TEST:8.137 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:47:13.862: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:47:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-669br" for this suite.
Dec  5 22:47:19.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:47:19.960: INFO: namespace: e2e-tests-kubelet-test-669br, resource: bindings, ignored listing per whitelist
Dec  5 22:47:19.985: INFO: namespace e2e-tests-kubelet-test-669br deletion completed in 6.051563889s

â€¢ [SLOW TEST:6.124 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:47:19.986: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:47:20.042: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  5 22:47:25.047: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 22:47:25.047: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  5 22:47:27.051: INFO: Creating deployment "test-rollover-deployment"
Dec  5 22:47:27.060: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  5 22:47:29.065: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  5 22:47:29.069: INFO: Ensure that both replica sets have 1 created replica
Dec  5 22:47:29.071: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  5 22:47:29.074: INFO: Updating deployment test-rollover-deployment
Dec  5 22:47:29.075: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  5 22:47:31.079: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  5 22:47:31.086: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  5 22:47:31.095: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 22:47:31.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646850, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 22:47:33.104: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 22:47:33.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646850, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 22:47:35.099: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 22:47:35.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646850, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 22:47:37.102: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 22:47:37.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646850, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 22:47:39.106: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 22:47:39.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646850, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679646847, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 22:47:41.104: INFO: 
Dec  5 22:47:41.104: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 22:47:41.108: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-qstjh,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qstjh/deployments/test-rollover-deployment,UID:bd3471b3-f8df-11e8-aca6-08002781145e,ResourceVersion:16179,Generation:2,CreationTimestamp:2018-12-05 22:47:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-05 22:47:27 +0000 UTC 2018-12-05 22:47:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-05 22:47:40 +0000 UTC 2018-12-05 22:47:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 22:47:41.110: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-qstjh,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qstjh/replicasets/test-rollover-deployment-6b7f9d6597,UID:be691ac0-f8df-11e8-aca6-08002781145e,ResourceVersion:16169,Generation:2,CreationTimestamp:2018-12-05 22:47:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bd3471b3-f8df-11e8-aca6-08002781145e 0xc001a11947 0xc001a11948}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 22:47:41.110: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  5 22:47:41.110: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-qstjh,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qstjh/replicasets/test-rollover-controller,UID:b906007f-f8df-11e8-aca6-08002781145e,ResourceVersion:16178,Generation:2,CreationTimestamp:2018-12-05 22:47:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bd3471b3-f8df-11e8-aca6-08002781145e 0xc001a117b7 0xc001a117b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 22:47:41.110: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-qstjh,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qstjh/replicasets/test-rollover-deployment-6586df867b,UID:bd360c44-f8df-11e8-aca6-08002781145e,ResourceVersion:16136,Generation:2,CreationTimestamp:2018-12-05 22:47:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bd3471b3-f8df-11e8-aca6-08002781145e 0xc001a11877 0xc001a11878}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 22:47:41.111: INFO: Pod "test-rollover-deployment-6b7f9d6597-l77m4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-l77m4,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-qstjh,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qstjh/pods/test-rollover-deployment-6b7f9d6597-l77m4,UID:be6e0518-f8df-11e8-aca6-08002781145e,ResourceVersion:16149,Generation:0,CreationTimestamp:2018-12-05 22:47:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.200.0.240/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 be691ac0-f8df-11e8-aca6-08002781145e 0xc002aa4e07 0xc002aa4e08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-h9cm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h9cm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-h9cm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:single-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002aa4f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002aa4f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:47:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:47:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:47:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-05 22:47:29 +0000 UTC  }],Message:,Reason:,HostIP:192.168.100.50,PodIP:10.200.0.240,StartTime:2018-12-05 22:47:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-05 22:47:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://84de5c1b541dbc997c96e885fbb592fccb5a8369a9397a6a999ec5127d9705e2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:47:41.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qstjh" for this suite.
Dec  5 22:47:47.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:47:47.163: INFO: namespace: e2e-tests-deployment-qstjh, resource: bindings, ignored listing per whitelist
Dec  5 22:47:47.171: INFO: namespace e2e-tests-deployment-qstjh deletion completed in 6.057578528s

â€¢ [SLOW TEST:27.185 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:47:47.171: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb
Dec  5 22:47:47.219: INFO: Pod name my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb: Found 0 pods out of 1
Dec  5 22:47:52.225: INFO: Pod name my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb: Found 1 pods out of 1
Dec  5 22:47:52.225: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb" are running
Dec  5 22:47:52.233: INFO: Pod "my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb-vhgwq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 22:47:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 22:47:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 22:47:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 22:47:47 +0000 UTC Reason: Message:}])
Dec  5 22:47:52.233: INFO: Trying to dial the pod
Dec  5 22:47:57.241: INFO: Controller my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb: Got expected result from replica 1 [my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb-vhgwq]: "my-hostname-basic-c9388331-f8df-11e8-8a71-d2079f97accb-vhgwq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:47:57.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-nfkds" for this suite.
Dec  5 22:48:03.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:48:03.307: INFO: namespace: e2e-tests-replication-controller-nfkds, resource: bindings, ignored listing per whitelist
Dec  5 22:48:03.316: INFO: namespace e2e-tests-replication-controller-nfkds deletion completed in 6.071735506s

â€¢ [SLOW TEST:16.144 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:48:03.316: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-d2dbb368-f8df-11e8-8a71-d2079f97accb
STEP: Creating secret with name s-test-opt-upd-d2dbb3ab-f8df-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d2dbb368-f8df-11e8-8a71-d2079f97accb
STEP: Updating secret s-test-opt-upd-d2dbb3ab-f8df-11e8-8a71-d2079f97accb
STEP: Creating secret with name s-test-opt-create-d2dbb3c0-f8df-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:49:35.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-btpk9" for this suite.
Dec  5 22:49:57.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:49:57.776: INFO: namespace: e2e-tests-secrets-btpk9, resource: bindings, ignored listing per whitelist
Dec  5 22:49:57.808: INFO: namespace e2e-tests-secrets-btpk9 deletion completed in 22.04535829s

â€¢ [SLOW TEST:114.491 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:49:57.808: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:49:59.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-mqr4h" for this suite.
Dec  5 22:50:05.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:50:05.950: INFO: namespace: e2e-tests-emptydir-wrapper-mqr4h, resource: bindings, ignored listing per whitelist
Dec  5 22:50:05.950: INFO: namespace e2e-tests-emptydir-wrapper-mqr4h deletion completed in 6.055157433s

â€¢ [SLOW TEST:8.143 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:50:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:50:05.986: INFO: (0) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.477621ms)
Dec  5 22:50:05.988: INFO: (1) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.738018ms)
Dec  5 22:50:05.990: INFO: (2) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.694392ms)
Dec  5 22:50:05.991: INFO: (3) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.517425ms)
Dec  5 22:50:05.993: INFO: (4) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.664861ms)
Dec  5 22:50:05.994: INFO: (5) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.459321ms)
Dec  5 22:50:05.996: INFO: (6) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.741383ms)
Dec  5 22:50:05.998: INFO: (7) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.593736ms)
Dec  5 22:50:05.999: INFO: (8) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.654415ms)
Dec  5 22:50:06.002: INFO: (9) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.981548ms)
Dec  5 22:50:06.004: INFO: (10) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.486099ms)
Dec  5 22:50:06.006: INFO: (11) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.131361ms)
Dec  5 22:50:06.008: INFO: (12) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.893988ms)
Dec  5 22:50:06.010: INFO: (13) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.100622ms)
Dec  5 22:50:06.013: INFO: (14) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.289791ms)
Dec  5 22:50:06.015: INFO: (15) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.210762ms)
Dec  5 22:50:06.018: INFO: (16) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.389675ms)
Dec  5 22:50:06.021: INFO: (17) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.007348ms)
Dec  5 22:50:06.024: INFO: (18) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.33086ms)
Dec  5 22:50:06.026: INFO: (19) /api/v1/nodes/single-node:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.285057ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:50:06.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-x44gf" for this suite.
Dec  5 22:50:12.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:50:12.071: INFO: namespace: e2e-tests-proxy-x44gf, resource: bindings, ignored listing per whitelist
Dec  5 22:50:12.081: INFO: namespace e2e-tests-proxy-x44gf deletion completed in 6.052705293s

â€¢ [SLOW TEST:6.131 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:50:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 22:50:12.124: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  5 22:50:12.128: INFO: Number of nodes with available pods: 0
Dec  5 22:50:12.128: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  5 22:50:12.137: INFO: Number of nodes with available pods: 0
Dec  5 22:50:12.137: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:13.139: INFO: Number of nodes with available pods: 1
Dec  5 22:50:13.139: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  5 22:50:13.152: INFO: Number of nodes with available pods: 1
Dec  5 22:50:13.152: INFO: Number of running nodes: 0, number of available pods: 1
Dec  5 22:50:14.155: INFO: Number of nodes with available pods: 0
Dec  5 22:50:14.155: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  5 22:50:14.163: INFO: Number of nodes with available pods: 0
Dec  5 22:50:14.163: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:15.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:15.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:16.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:16.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:17.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:17.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:18.172: INFO: Number of nodes with available pods: 0
Dec  5 22:50:18.172: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:19.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:19.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:20.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:20.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:21.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:21.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:22.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:22.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:23.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:23.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:24.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:24.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:25.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:25.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:26.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:26.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:27.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:27.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:28.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:28.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:29.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:29.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:30.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:30.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:31.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:31.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:32.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:32.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:33.165: INFO: Number of nodes with available pods: 0
Dec  5 22:50:33.165: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:34.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:34.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:35.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:35.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:36.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:36.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:37.171: INFO: Number of nodes with available pods: 0
Dec  5 22:50:37.171: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:38.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:38.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:39.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:39.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:40.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:40.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:41.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:41.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:42.169: INFO: Number of nodes with available pods: 0
Dec  5 22:50:42.169: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:43.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:43.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:44.167: INFO: Number of nodes with available pods: 0
Dec  5 22:50:44.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:45.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:45.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:46.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:46.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:47.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:47.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:48.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:48.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:49.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:49.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:50.168: INFO: Number of nodes with available pods: 0
Dec  5 22:50:50.168: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:51.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:51.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:52.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:52.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:53.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:53.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:54.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:54.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:55.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:55.167: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:56.166: INFO: Number of nodes with available pods: 0
Dec  5 22:50:56.166: INFO: Node single-node is running more than one daemon pod
Dec  5 22:50:57.168: INFO: Number of nodes with available pods: 1
Dec  5 22:50:57.168: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-47k7h, will wait for the garbage collector to delete the pods
Dec  5 22:50:57.244: INFO: Deleting DaemonSet.extensions daemon-set took: 12.027881ms
Dec  5 22:50:57.345: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.290085ms
Dec  5 22:51:35.748: INFO: Number of nodes with available pods: 0
Dec  5 22:51:35.748: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 22:51:35.750: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-47k7h/daemonsets","resourceVersion":"16825"},"items":null}

Dec  5 22:51:35.752: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-47k7h/pods","resourceVersion":"16825"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:51:35.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-47k7h" for this suite.
Dec  5 22:51:41.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:51:41.809: INFO: namespace: e2e-tests-daemonsets-47k7h, resource: bindings, ignored listing per whitelist
Dec  5 22:51:41.812: INFO: namespace e2e-tests-daemonsets-47k7h deletion completed in 6.048019732s

â€¢ [SLOW TEST:89.731 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:51:41.812: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1205 22:51:51.869872      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 22:51:51.869: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:51:51.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-77wqw" for this suite.
Dec  5 22:51:57.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:51:57.895: INFO: namespace: e2e-tests-gc-77wqw, resource: bindings, ignored listing per whitelist
Dec  5 22:51:57.926: INFO: namespace e2e-tests-gc-77wqw deletion completed in 6.051303285s

â€¢ [SLOW TEST:16.114 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:51:57.927: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-5ead7106-f8e0-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:51:57.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-ndnmj" to be "success or failure"
Dec  5 22:51:57.968: INFO: Pod "pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319899ms
Dec  5 22:51:59.973: INFO: Pod "pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008326799s
STEP: Saw pod success
Dec  5 22:51:59.973: INFO: Pod "pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:51:59.978: INFO: Trying to get logs from node single-node pod pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:51:59.992: INFO: Waiting for pod pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:00.001: INFO: Pod pod-configmaps-5eaddbd8-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:00.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ndnmj" for this suite.
Dec  5 22:52:06.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:06.056: INFO: namespace: e2e-tests-configmap-ndnmj, resource: bindings, ignored listing per whitelist
Dec  5 22:52:06.058: INFO: namespace e2e-tests-configmap-ndnmj deletion completed in 6.052109027s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:06.059: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-63863538-f8e0-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:52:06.096: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-t58kp" to be "success or failure"
Dec  5 22:52:06.098: INFO: Pod "pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.760248ms
Dec  5 22:52:08.101: INFO: Pod "pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004679989s
STEP: Saw pod success
Dec  5 22:52:08.101: INFO: Pod "pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:08.105: INFO: Trying to get logs from node single-node pod pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:52:08.120: INFO: Waiting for pod pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:08.124: INFO: Pod pod-projected-secrets-63869d6f-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:08.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t58kp" for this suite.
Dec  5 22:52:14.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:14.164: INFO: namespace: e2e-tests-projected-t58kp, resource: bindings, ignored listing per whitelist
Dec  5 22:52:14.179: INFO: namespace e2e-tests-projected-t58kp deletion completed in 6.046355089s

â€¢ [SLOW TEST:8.120 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:52:14.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-dh4pb" to be "success or failure"
Dec  5 22:52:14.222: INFO: Pod "downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767771ms
Dec  5 22:52:16.224: INFO: Pod "downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004320721s
STEP: Saw pod success
Dec  5 22:52:16.224: INFO: Pod "downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:16.228: INFO: Trying to get logs from node single-node pod downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:52:16.250: INFO: Waiting for pod downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:16.252: INFO: Pod downwardapi-volume-685e107a-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:16.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dh4pb" for this suite.
Dec  5 22:52:22.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:22.293: INFO: namespace: e2e-tests-downward-api-dh4pb, resource: bindings, ignored listing per whitelist
Dec  5 22:52:22.303: INFO: namespace e2e-tests-downward-api-dh4pb deletion completed in 6.048606693s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 22:52:22.351: INFO: Waiting up to 5m0s for pod "pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-fkk4s" to be "success or failure"
Dec  5 22:52:22.354: INFO: Pod "pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345051ms
Dec  5 22:52:24.361: INFO: Pod "pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009732131s
STEP: Saw pod success
Dec  5 22:52:24.361: INFO: Pod "pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:24.363: INFO: Trying to get logs from node single-node pod pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:52:24.395: INFO: Waiting for pod pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:24.398: INFO: Pod pod-6d36f76a-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:24.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fkk4s" for this suite.
Dec  5 22:52:30.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:30.458: INFO: namespace: e2e-tests-emptydir-fkk4s, resource: bindings, ignored listing per whitelist
Dec  5 22:52:30.491: INFO: namespace e2e-tests-emptydir-fkk4s deletion completed in 6.08870979s

â€¢ [SLOW TEST:8.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 22:52:30.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-c6bt5" to be "success or failure"
Dec  5 22:52:30.537: INFO: Pod "downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137776ms
Dec  5 22:52:32.541: INFO: Pod "downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008738099s
STEP: Saw pod success
Dec  5 22:52:32.541: INFO: Pod "downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:32.548: INFO: Trying to get logs from node single-node pod downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 22:52:32.575: INFO: Waiting for pod downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:32.576: INFO: Pod downwardapi-volume-7216eb48-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:32.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c6bt5" for this suite.
Dec  5 22:52:38.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:38.615: INFO: namespace: e2e-tests-projected-c6bt5, resource: bindings, ignored listing per whitelist
Dec  5 22:52:38.653: INFO: namespace e2e-tests-projected-c6bt5 deletion completed in 6.074541066s

â€¢ [SLOW TEST:8.162 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 22:52:38.709: INFO: Waiting up to 5m0s for pod "pod-76f68f52-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-g9mmk" to be "success or failure"
Dec  5 22:52:38.712: INFO: Pod "pod-76f68f52-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367299ms
Dec  5 22:52:40.714: INFO: Pod "pod-76f68f52-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00436138s
STEP: Saw pod success
Dec  5 22:52:40.714: INFO: Pod "pod-76f68f52-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:40.715: INFO: Trying to get logs from node single-node pod pod-76f68f52-f8e0-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:52:40.727: INFO: Waiting for pod pod-76f68f52-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:40.730: INFO: Pod pod-76f68f52-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:40.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-g9mmk" for this suite.
Dec  5 22:52:46.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:46.772: INFO: namespace: e2e-tests-emptydir-g9mmk, resource: bindings, ignored listing per whitelist
Dec  5 22:52:46.844: INFO: namespace e2e-tests-emptydir-g9mmk deletion completed in 6.108069872s

â€¢ [SLOW TEST:8.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:46.844: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 22:52:46.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-jznrj'
Dec  5 22:52:47.408: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 22:52:47.408: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  5 22:52:47.417: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-l279l]
Dec  5 22:52:47.417: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-l279l" in namespace "e2e-tests-kubectl-jznrj" to be "running and ready"
Dec  5 22:52:47.421: INFO: Pod "e2e-test-nginx-rc-l279l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174964ms
Dec  5 22:52:49.423: INFO: Pod "e2e-test-nginx-rc-l279l": Phase="Running", Reason="", readiness=true. Elapsed: 2.006196321s
Dec  5 22:52:49.423: INFO: Pod "e2e-test-nginx-rc-l279l" satisfied condition "running and ready"
Dec  5 22:52:49.423: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-l279l]
Dec  5 22:52:49.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-jznrj'
Dec  5 22:52:49.502: INFO: stderr: ""
Dec  5 22:52:49.502: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Dec  5 22:52:49.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-jznrj'
Dec  5 22:52:49.569: INFO: stderr: ""
Dec  5 22:52:49.569: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:49.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jznrj" for this suite.
Dec  5 22:52:55.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:52:55.621: INFO: namespace: e2e-tests-kubectl-jznrj, resource: bindings, ignored listing per whitelist
Dec  5 22:52:55.638: INFO: namespace e2e-tests-kubectl-jznrj deletion completed in 6.064245305s

â€¢ [SLOW TEST:8.794 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:52:55.638: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  5 22:52:55.698: INFO: Waiting up to 5m0s for pod "downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-hl8nn" to be "success or failure"
Dec  5 22:52:55.702: INFO: Pod "downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57953ms
Dec  5 22:52:57.704: INFO: Pod "downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006853291s
STEP: Saw pod success
Dec  5 22:52:57.704: INFO: Pod "downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:52:57.706: INFO: Trying to get logs from node single-node pod downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 22:52:57.730: INFO: Waiting for pod downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:52:57.738: INFO: Pod downward-api-8116ed1b-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:52:57.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hl8nn" for this suite.
Dec  5 22:53:03.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:53:03.780: INFO: namespace: e2e-tests-downward-api-hl8nn, resource: bindings, ignored listing per whitelist
Dec  5 22:53:03.790: INFO: namespace e2e-tests-downward-api-hl8nn deletion completed in 6.050774728s

â€¢ [SLOW TEST:8.153 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:53:03.791: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  5 22:53:04.037: INFO: Pod name wrapped-volume-race-8608255b-f8e0-11e8-8a71-d2079f97accb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8608255b-f8e0-11e8-8a71-d2079f97accb in namespace e2e-tests-emptydir-wrapper-8gv4l, will wait for the garbage collector to delete the pods
Dec  5 22:53:18.140: INFO: Deleting ReplicationController wrapped-volume-race-8608255b-f8e0-11e8-8a71-d2079f97accb took: 13.230281ms
Dec  5 22:53:18.242: INFO: Terminating ReplicationController wrapped-volume-race-8608255b-f8e0-11e8-8a71-d2079f97accb pods took: 101.984486ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 22:53:56.760: INFO: Pod name wrapped-volume-race-a57a8d10-f8e0-11e8-8a71-d2079f97accb: Found 0 pods out of 5
Dec  5 22:54:01.765: INFO: Pod name wrapped-volume-race-a57a8d10-f8e0-11e8-8a71-d2079f97accb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a57a8d10-f8e0-11e8-8a71-d2079f97accb in namespace e2e-tests-emptydir-wrapper-8gv4l, will wait for the garbage collector to delete the pods
Dec  5 22:54:11.836: INFO: Deleting ReplicationController wrapped-volume-race-a57a8d10-f8e0-11e8-8a71-d2079f97accb took: 6.167246ms
Dec  5 22:54:11.937: INFO: Terminating ReplicationController wrapped-volume-race-a57a8d10-f8e0-11e8-8a71-d2079f97accb pods took: 100.443576ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 22:54:55.850: INFO: Pod name wrapped-volume-race-c8b3a417-f8e0-11e8-8a71-d2079f97accb: Found 0 pods out of 5
Dec  5 22:55:00.913: INFO: Pod name wrapped-volume-race-c8b3a417-f8e0-11e8-8a71-d2079f97accb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c8b3a417-f8e0-11e8-8a71-d2079f97accb in namespace e2e-tests-emptydir-wrapper-8gv4l, will wait for the garbage collector to delete the pods
Dec  5 22:55:11.008: INFO: Deleting ReplicationController wrapped-volume-race-c8b3a417-f8e0-11e8-8a71-d2079f97accb took: 12.52484ms
Dec  5 22:55:11.109: INFO: Terminating ReplicationController wrapped-volume-race-c8b3a417-f8e0-11e8-8a71-d2079f97accb pods took: 101.13737ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:55:56.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-8gv4l" for this suite.
Dec  5 22:56:02.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:56:02.423: INFO: namespace: e2e-tests-emptydir-wrapper-8gv4l, resource: bindings, ignored listing per whitelist
Dec  5 22:56:02.464: INFO: namespace e2e-tests-emptydir-wrapper-8gv4l deletion completed in 6.07166704s

â€¢ [SLOW TEST:178.673 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:56:02.464: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 22:56:02.506: INFO: Waiting up to 5m0s for pod "pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-8fkqf" to be "success or failure"
Dec  5 22:56:02.509: INFO: Pod "pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908882ms
Dec  5 22:56:04.512: INFO: Pod "pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006456158s
STEP: Saw pod success
Dec  5 22:56:04.512: INFO: Pod "pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:56:04.517: INFO: Trying to get logs from node single-node pod pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 22:56:04.533: INFO: Waiting for pod pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb to disappear
Dec  5 22:56:04.542: INFO: Pod pod-f06fd0ea-f8e0-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:56:04.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8fkqf" for this suite.
Dec  5 22:56:10.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:56:10.603: INFO: namespace: e2e-tests-emptydir-8fkqf, resource: bindings, ignored listing per whitelist
Dec  5 22:56:10.604: INFO: namespace e2e-tests-emptydir-8fkqf deletion completed in 6.059829398s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:56:10.605: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  5 22:56:10.635: INFO: namespace e2e-tests-kubectl-wfvs7
Dec  5 22:56:10.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-wfvs7'
Dec  5 22:56:10.760: INFO: stderr: ""
Dec  5 22:56:10.760: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 22:56:11.765: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:56:11.765: INFO: Found 0 / 1
Dec  5 22:56:12.767: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:56:12.767: INFO: Found 1 / 1
Dec  5 22:56:12.767: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 22:56:12.772: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 22:56:12.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 22:56:12.772: INFO: wait on redis-master startup in e2e-tests-kubectl-wfvs7 
Dec  5 22:56:12.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 logs redis-master-b8r9v redis-master --namespace=e2e-tests-kubectl-wfvs7'
Dec  5 22:56:12.844: INFO: stderr: ""
Dec  5 22:56:12.844: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 22:56:11.552 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 22:56:11.552 # Server started, Redis version 3.2.12\n1:M 05 Dec 22:56:11.552 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 22:56:11.552 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  5 22:56:12.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-wfvs7'
Dec  5 22:56:12.921: INFO: stderr: ""
Dec  5 22:56:12.921: INFO: stdout: "service/rm2 exposed\n"
Dec  5 22:56:12.923: INFO: Service rm2 in namespace e2e-tests-kubectl-wfvs7 found.
STEP: exposing service
Dec  5 22:56:14.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-wfvs7'
Dec  5 22:56:15.007: INFO: stderr: ""
Dec  5 22:56:15.007: INFO: stdout: "service/rm3 exposed\n"
Dec  5 22:56:15.011: INFO: Service rm3 in namespace e2e-tests-kubectl-wfvs7 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:56:17.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wfvs7" for this suite.
Dec  5 22:56:39.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:56:39.056: INFO: namespace: e2e-tests-kubectl-wfvs7, resource: bindings, ignored listing per whitelist
Dec  5 22:56:39.076: INFO: namespace e2e-tests-kubectl-wfvs7 deletion completed in 22.058088507s

â€¢ [SLOW TEST:28.471 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:56:39.077: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-v484
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 22:56:39.129: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v484" in namespace "e2e-tests-subpath-99ms6" to be "success or failure"
Dec  5 22:56:39.135: INFO: Pod "pod-subpath-test-projected-v484": Phase="Pending", Reason="", readiness=false. Elapsed: 6.126043ms
Dec  5 22:56:41.139: INFO: Pod "pod-subpath-test-projected-v484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010026559s
Dec  5 22:56:43.140: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 4.011660678s
Dec  5 22:56:45.144: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 6.014966201s
Dec  5 22:56:47.146: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 8.017115326s
Dec  5 22:56:49.149: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 10.019818608s
Dec  5 22:56:51.154: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 12.024921399s
Dec  5 22:56:53.192: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 14.063436682s
Dec  5 22:56:55.196: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 16.067366127s
Dec  5 22:56:57.201: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 18.071944745s
Dec  5 22:56:59.204: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 20.075565602s
Dec  5 22:57:01.209: INFO: Pod "pod-subpath-test-projected-v484": Phase="Running", Reason="", readiness=false. Elapsed: 22.079900617s
Dec  5 22:57:03.213: INFO: Pod "pod-subpath-test-projected-v484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084373092s
STEP: Saw pod success
Dec  5 22:57:03.213: INFO: Pod "pod-subpath-test-projected-v484" satisfied condition "success or failure"
Dec  5 22:57:03.215: INFO: Trying to get logs from node single-node pod pod-subpath-test-projected-v484 container test-container-subpath-projected-v484: <nil>
STEP: delete the pod
Dec  5 22:57:03.232: INFO: Waiting for pod pod-subpath-test-projected-v484 to disappear
Dec  5 22:57:03.236: INFO: Pod pod-subpath-test-projected-v484 no longer exists
STEP: Deleting pod pod-subpath-test-projected-v484
Dec  5 22:57:03.236: INFO: Deleting pod "pod-subpath-test-projected-v484" in namespace "e2e-tests-subpath-99ms6"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:57:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-99ms6" for this suite.
Dec  5 22:57:09.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:57:09.268: INFO: namespace: e2e-tests-subpath-99ms6, resource: bindings, ignored listing per whitelist
Dec  5 22:57:09.296: INFO: namespace e2e-tests-subpath-99ms6 deletion completed in 6.054569632s

â€¢ [SLOW TEST:30.219 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:57:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-18450d8f-f8e1-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 22:57:09.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb" in namespace "e2e-tests-configmap-gqzwr" to be "success or failure"
Dec  5 22:57:09.350: INFO: Pod "pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.80074ms
Dec  5 22:57:11.354: INFO: Pod "pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014273158s
STEP: Saw pod success
Dec  5 22:57:11.354: INFO: Pod "pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:57:11.357: INFO: Trying to get logs from node single-node pod pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 22:57:11.372: INFO: Waiting for pod pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb to disappear
Dec  5 22:57:11.378: INFO: Pod pod-configmaps-18457684-f8e1-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:57:11.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gqzwr" for this suite.
Dec  5 22:57:17.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:57:17.410: INFO: namespace: e2e-tests-configmap-gqzwr, resource: bindings, ignored listing per whitelist
Dec  5 22:57:17.436: INFO: namespace e2e-tests-configmap-gqzwr deletion completed in 6.0543581s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:57:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 22:57:17.467: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 22:57:17.471: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 22:57:17.472: INFO: 
Logging pods the kubelet thinks is on node single-node before test
Dec  5 22:57:17.477: INFO: gobetween-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:57:17.477: INFO: sonobuoy-e2e-job-d548272ad7774311 from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container e2e ready: true, restart count 0
Dec  5 22:57:17.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 22:57:17.477: INFO: coredns-74c648b76d-nr9vj from kube-system started at 2018-12-05 21:38:14 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container coredns ready: true, restart count 0
Dec  5 22:57:17.477: INFO: cert-manager-58898b5d55-hhsrf from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container cert-manager ready: true, restart count 0
Dec  5 22:57:17.477: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-05 21:38:34 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 22:57:17.477: INFO: kube-proxy-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:57:17.477: INFO: tiller-deploy-6cf89f5895-gqbh8 from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container tiller ready: true, restart count 0
Dec  5 22:57:17.477: INFO: kubernetes-dashboard-7b8b594b7b-g5tmb from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  5 22:57:17.477: INFO: nginx-ingress-default-backend-7b84c755cc-d6fmh from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  5 22:57:17.477: INFO: kube-controller-manager-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:57:17.477: INFO: kube-apiserver-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:57:17.477: INFO: kube-scheduler-single-node from kube-system started at <nil> (0 container statuses recorded)
Dec  5 22:57:17.477: INFO: nginx-ingress-controller-4v5js from networking started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  5 22:57:17.477: INFO: coredns-74c648b76d-mn6rk from kube-system started at 2018-12-05 21:38:04 +0000 UTC (1 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container coredns ready: true, restart count 0
Dec  5 22:57:17.477: INFO: sonobuoy-systemd-logs-daemon-set-5b8b8e26ffb94402-m4fwd from heptio-sonobuoy started at 2018-12-05 21:39:24 +0000 UTC (2 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  5 22:57:17.477: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  5 22:57:17.477: INFO: calico-node-4fdbb from networking started at 2018-12-05 21:37:46 +0000 UTC (2 container statuses recorded)
Dec  5 22:57:17.477: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 22:57:17.477: INFO: 	Container install-cni ready: true, restart count 0
Dec  5 22:57:17.477: INFO: etcd-single-node from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1e54a716-f8e1-11e8-8a71-d2079f97accb 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1e54a716-f8e1-11e8-8a71-d2079f97accb off the node single-node
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1e54a716-f8e1-11e8-8a71-d2079f97accb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:57:21.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-mhltb" for this suite.
Dec  5 22:57:29.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:57:29.577: INFO: namespace: e2e-tests-sched-pred-mhltb, resource: bindings, ignored listing per whitelist
Dec  5 22:57:29.599: INFO: namespace e2e-tests-sched-pred-mhltb deletion completed in 8.055846675s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:12.163 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:57:29.599: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Dec  5 22:57:29.641: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-502647789 proxy --unix-socket=/tmp/kubectl-proxy-unix145227368/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:57:29.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rm9kh" for this suite.
Dec  5 22:57:35.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:57:35.713: INFO: namespace: e2e-tests-kubectl-rm9kh, resource: bindings, ignored listing per whitelist
Dec  5 22:57:35.729: INFO: namespace e2e-tests-kubectl-rm9kh deletion completed in 6.045168659s

â€¢ [SLOW TEST:6.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:57:35.729: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  5 22:57:38.302: INFO: Successfully updated pod "labelsupdate28060a5d-f8e1-11e8-8a71-d2079f97accb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:57:40.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w99f6" for this suite.
Dec  5 22:58:02.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:58:02.349: INFO: namespace: e2e-tests-projected-w99f6, resource: bindings, ignored listing per whitelist
Dec  5 22:58:02.378: INFO: namespace e2e-tests-projected-w99f6 deletion completed in 22.060238262s

â€¢ [SLOW TEST:26.648 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:58:02.378: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-37e8db34-f8e1-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 22:58:02.420: INFO: Waiting up to 5m0s for pod "pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-rj5wn" to be "success or failure"
Dec  5 22:58:02.422: INFO: Pod "pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867388ms
Dec  5 22:58:04.426: INFO: Pod "pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005789952s
STEP: Saw pod success
Dec  5 22:58:04.427: INFO: Pod "pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 22:58:04.429: INFO: Trying to get logs from node single-node pod pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 22:58:04.440: INFO: Waiting for pod pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb to disappear
Dec  5 22:58:04.447: INFO: Pod pod-secrets-37e94681-f8e1-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:58:04.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rj5wn" for this suite.
Dec  5 22:58:10.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:58:10.488: INFO: namespace: e2e-tests-secrets-rj5wn, resource: bindings, ignored listing per whitelist
Dec  5 22:58:10.498: INFO: namespace e2e-tests-secrets-rj5wn deletion completed in 6.048614986s

â€¢ [SLOW TEST:8.120 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:58:10.498: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1205 22:58:50.554599      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  5 22:58:50.554: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:58:50.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hxmvw" for this suite.
Dec  5 22:58:56.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:58:56.605: INFO: namespace: e2e-tests-gc-hxmvw, resource: bindings, ignored listing per whitelist
Dec  5 22:58:56.649: INFO: namespace e2e-tests-gc-hxmvw deletion completed in 6.089385345s

â€¢ [SLOW TEST:46.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:58:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hg77p
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 22:58:56.694: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 22:59:12.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.0.44:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-hg77p PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 22:59:12.755: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 22:59:12.830: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 22:59:12.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hg77p" for this suite.
Dec  5 22:59:34.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 22:59:34.857: INFO: namespace: e2e-tests-pod-network-test-hg77p, resource: bindings, ignored listing per whitelist
Dec  5 22:59:34.887: INFO: namespace e2e-tests-pod-network-test-hg77p deletion completed in 22.054737119s

â€¢ [SLOW TEST:38.238 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 22:59:34.888: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  5 22:59:34.924: INFO: PodSpec: initContainers in spec.initContainers
Dec  5 23:00:23.158: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6f0cde7d-f8e1-11e8-8a71-d2079f97accb", GenerateName:"", Namespace:"e2e-tests-init-container-pkn52", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-pkn52/pods/pod-init-6f0cde7d-f8e1-11e8-8a71-d2079f97accb", UID:"6f0d44da-f8e1-11e8-aca6-08002781145e", ResourceVersion:"19478", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679647574, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"924458252"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.200.0.46/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b6bf6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000eb9200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b6bf6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b6bf6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b6bf6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c66c28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"single-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0013d75c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c66cb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c66cd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c66cd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c66cdc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679647574, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679647574, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679647574, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679647574, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.100.50", PodIP:"10.200.0.46", StartTime:(*v1.Time)(0xc0014c8e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0014c8f00), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000e80e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:2a03a6059f21e150ae84b0973863609494aad70f0a80eaeb64bddd8d92465812", ContainerID:"containerd://428f4e49a5f64a3ee46a729964809a9009acae1f6238af602b8ab9eb8f1fb607"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014c8f20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014c8e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:00:23.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-pkn52" for this suite.
Dec  5 23:00:45.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:00:45.217: INFO: namespace: e2e-tests-init-container-pkn52, resource: bindings, ignored listing per whitelist
Dec  5 23:00:45.223: INFO: namespace e2e-tests-init-container-pkn52 deletion completed in 22.059990283s

â€¢ [SLOW TEST:70.335 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:00:45.224: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-98f995eb-f8e1-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 23:00:45.270: INFO: Waiting up to 5m0s for pod "pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb" in namespace "e2e-tests-secrets-n6kvp" to be "success or failure"
Dec  5 23:00:45.276: INFO: Pod "pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031821ms
Dec  5 23:00:47.278: INFO: Pod "pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008402813s
STEP: Saw pod success
Dec  5 23:00:47.278: INFO: Pod "pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:00:47.280: INFO: Trying to get logs from node single-node pod pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 23:00:47.294: INFO: Waiting for pod pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb to disappear
Dec  5 23:00:47.303: INFO: Pod pod-secrets-98f9f3c7-f8e1-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:00:47.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n6kvp" for this suite.
Dec  5 23:00:53.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:00:53.353: INFO: namespace: e2e-tests-secrets-n6kvp, resource: bindings, ignored listing per whitelist
Dec  5 23:00:53.362: INFO: namespace e2e-tests-secrets-n6kvp deletion completed in 6.058096537s

â€¢ [SLOW TEST:8.139 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:00:53.364: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 23:00:53.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-cchm6" to be "success or failure"
Dec  5 23:00:53.411: INFO: Pod "downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.520828ms
Dec  5 23:00:55.413: INFO: Pod "downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00951752s
STEP: Saw pod success
Dec  5 23:00:55.413: INFO: Pod "downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:00:55.414: INFO: Trying to get logs from node single-node pod downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 23:00:55.428: INFO: Waiting for pod downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb to disappear
Dec  5 23:00:55.437: INFO: Pod downwardapi-volume-9dd2d467-f8e1-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:00:55.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cchm6" for this suite.
Dec  5 23:01:01.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:01:01.489: INFO: namespace: e2e-tests-downward-api-cchm6, resource: bindings, ignored listing per whitelist
Dec  5 23:01:01.499: INFO: namespace e2e-tests-downward-api-cchm6 deletion completed in 6.057550963s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:01:01.507: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-px2tc
Dec  5 23:01:03.560: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-px2tc
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 23:01:03.564: INFO: Initial restart count of pod liveness-http is 0
Dec  5 23:01:21.602: INFO: Restart count of pod e2e-tests-container-probe-px2tc/liveness-http is now 1 (18.03766821s elapsed)
Dec  5 23:01:41.633: INFO: Restart count of pod e2e-tests-container-probe-px2tc/liveness-http is now 2 (38.069381883s elapsed)
Dec  5 23:02:03.672: INFO: Restart count of pod e2e-tests-container-probe-px2tc/liveness-http is now 3 (1m0.108037152s elapsed)
Dec  5 23:02:23.717: INFO: Restart count of pod e2e-tests-container-probe-px2tc/liveness-http is now 4 (1m20.152940964s elapsed)
Dec  5 23:03:23.830: INFO: Restart count of pod e2e-tests-container-probe-px2tc/liveness-http is now 5 (2m20.266428278s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:03:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-px2tc" for this suite.
Dec  5 23:03:29.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:03:29.910: INFO: namespace: e2e-tests-container-probe-px2tc, resource: bindings, ignored listing per whitelist
Dec  5 23:03:29.919: INFO: namespace e2e-tests-container-probe-px2tc deletion completed in 6.062450225s

â€¢ [SLOW TEST:148.412 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:03:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-ddtnm
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Dec  5 23:03:29.965: INFO: Found 0 stateful pods, waiting for 3
Dec  5 23:03:39.970: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 23:03:39.970: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 23:03:39.972: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 23:03:39.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-ddtnm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 23:03:40.126: INFO: stderr: ""
Dec  5 23:03:40.126: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 23:03:40.126: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 23:03:50.151: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  5 23:04:00.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-ddtnm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 23:04:00.309: INFO: stderr: ""
Dec  5 23:04:00.309: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 23:04:00.309: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 23:04:20.323: INFO: Waiting for StatefulSet e2e-tests-statefulset-ddtnm/ss2 to complete update
Dec  5 23:04:20.323: INFO: Waiting for Pod e2e-tests-statefulset-ddtnm/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Dec  5 23:04:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-ddtnm ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 23:04:30.469: INFO: stderr: ""
Dec  5 23:04:30.469: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 23:04:30.469: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 23:04:40.494: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  5 23:04:50.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 exec --namespace=e2e-tests-statefulset-ddtnm ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 23:04:50.673: INFO: stderr: ""
Dec  5 23:04:50.674: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 23:04:50.674: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 23:05:10.690: INFO: Waiting for StatefulSet e2e-tests-statefulset-ddtnm/ss2 to complete update
Dec  5 23:05:10.690: INFO: Waiting for Pod e2e-tests-statefulset-ddtnm/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 23:05:20.694: INFO: Deleting all statefulset in ns e2e-tests-statefulset-ddtnm
Dec  5 23:05:20.697: INFO: Scaling statefulset ss2 to 0
Dec  5 23:05:50.724: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 23:05:50.729: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:05:50.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-ddtnm" for this suite.
Dec  5 23:05:56.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:05:56.789: INFO: namespace: e2e-tests-statefulset-ddtnm, resource: bindings, ignored listing per whitelist
Dec  5 23:05:56.800: INFO: namespace e2e-tests-statefulset-ddtnm deletion completed in 6.059522476s

â€¢ [SLOW TEST:146.882 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:05:56.801: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-52b06e68-f8e2-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume configMaps
Dec  5 23:05:56.844: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-djm8r" to be "success or failure"
Dec  5 23:05:56.846: INFO: Pod "pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656988ms
Dec  5 23:05:58.848: INFO: Pod "pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003668773s
STEP: Saw pod success
Dec  5 23:05:58.848: INFO: Pod "pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:05:58.849: INFO: Trying to get logs from node single-node pod pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 23:05:58.871: INFO: Waiting for pod pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb to disappear
Dec  5 23:05:58.874: INFO: Pod pod-projected-configmaps-52b0c636-f8e2-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:05:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-djm8r" for this suite.
Dec  5 23:06:04.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:06:04.970: INFO: namespace: e2e-tests-projected-djm8r, resource: bindings, ignored listing per whitelist
Dec  5 23:06:04.975: INFO: namespace e2e-tests-projected-djm8r deletion completed in 6.093734318s

â€¢ [SLOW TEST:8.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:06:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 23:06:05.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-92krr" to be "success or failure"
Dec  5 23:06:05.031: INFO: Pod "downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426843ms
Dec  5 23:06:07.034: INFO: Pod "downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005768504s
STEP: Saw pod success
Dec  5 23:06:07.034: INFO: Pod "downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:06:07.037: INFO: Trying to get logs from node single-node pod downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 23:06:07.053: INFO: Waiting for pod downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb to disappear
Dec  5 23:06:07.067: INFO: Pod downwardapi-volume-579127f4-f8e2-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:06:07.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-92krr" for this suite.
Dec  5 23:06:13.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:06:13.115: INFO: namespace: e2e-tests-downward-api-92krr, resource: bindings, ignored listing per whitelist
Dec  5 23:06:13.130: INFO: namespace e2e-tests-downward-api-92krr deletion completed in 6.059768112s

â€¢ [SLOW TEST:8.155 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:06:13.132: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:06:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6mnr9" for this suite.
Dec  5 23:06:35.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:06:35.219: INFO: namespace: e2e-tests-pods-6mnr9, resource: bindings, ignored listing per whitelist
Dec  5 23:06:35.269: INFO: namespace e2e-tests-pods-6mnr9 deletion completed in 22.072227176s

â€¢ [SLOW TEST:22.137 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:06:35.270: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 23:06:35.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2tv5p'
Dec  5 23:06:35.829: INFO: stderr: ""
Dec  5 23:06:35.829: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  5 23:06:40.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2tv5p -o json'
Dec  5 23:06:40.941: INFO: stderr: ""
Dec  5 23:06:40.941: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.200.0.62/32\"\n        },\n        \"creationTimestamp\": \"2018-12-05T23:06:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-2tv5p\",\n        \"resourceVersion\": \"20731\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-2tv5p/pods/e2e-test-nginx-pod\",\n        \"uid\": \"69ea84c2-f8e2-11e8-aca6-08002781145e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wl76c\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"single-node\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wl76c\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wl76c\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-05T23:06:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-05T23:06:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-05T23:06:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-05T23:06:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://74fb9992a18c7e2cec7e05d9ce360053d25f2810dd3a8432ff2e20d4a7a80bf9\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:3c1380fd5f6f0e4c468a922ed6331831e60cea9db317b8ac4a8ad36335e53bbd\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-12-05T23:06:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.100.50\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.0.62\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-12-05T23:06:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  5 23:06:40.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 replace -f - --namespace=e2e-tests-kubectl-2tv5p'
Dec  5 23:06:41.230: INFO: stderr: ""
Dec  5 23:06:41.230: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Dec  5 23:06:41.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-2tv5p'
Dec  5 23:06:45.724: INFO: stderr: ""
Dec  5 23:06:45.724: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:06:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2tv5p" for this suite.
Dec  5 23:06:51.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:06:51.770: INFO: namespace: e2e-tests-kubectl-2tv5p, resource: bindings, ignored listing per whitelist
Dec  5 23:06:51.790: INFO: namespace e2e-tests-kubectl-2tv5p deletion completed in 6.064606752s

â€¢ [SLOW TEST:16.521 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:06:51.791: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-73786db1-f8e2-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-73786db1-f8e2-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:06:55.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cb2nc" for this suite.
Dec  5 23:07:17.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:07:17.924: INFO: namespace: e2e-tests-configmap-cb2nc, resource: bindings, ignored listing per whitelist
Dec  5 23:07:17.942: INFO: namespace e2e-tests-configmap-cb2nc deletion completed in 22.057640763s

â€¢ [SLOW TEST:26.152 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:07:17.942: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Dec  5 23:07:17.988: INFO: Waiting up to 5m0s for pod "var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb" in namespace "e2e-tests-var-expansion-nvnnm" to be "success or failure"
Dec  5 23:07:18.001: INFO: Pod "var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.555278ms
Dec  5 23:07:20.004: INFO: Pod "var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01654594s
STEP: Saw pod success
Dec  5 23:07:20.004: INFO: Pod "var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:07:20.006: INFO: Trying to get logs from node single-node pod var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb container dapi-container: <nil>
STEP: delete the pod
Dec  5 23:07:20.021: INFO: Waiting for pod var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb to disappear
Dec  5 23:07:20.030: INFO: Pod var-expansion-830de1d7-f8e2-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:07:20.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-nvnnm" for this suite.
Dec  5 23:07:26.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:07:26.112: INFO: namespace: e2e-tests-var-expansion-nvnnm, resource: bindings, ignored listing per whitelist
Dec  5 23:07:26.142: INFO: namespace e2e-tests-var-expansion-nvnnm deletion completed in 6.109024156s

â€¢ [SLOW TEST:8.199 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:07:26.142: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  5 23:07:26.177: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:07:30.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-kzbxf" for this suite.
Dec  5 23:07:54.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:07:54.330: INFO: namespace: e2e-tests-init-container-kzbxf, resource: bindings, ignored listing per whitelist
Dec  5 23:07:54.348: INFO: namespace e2e-tests-init-container-kzbxf deletion completed in 24.051964648s

â€¢ [SLOW TEST:28.206 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:07:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  5 23:07:58.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.416: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.488: INFO: Exec stderr: ""
Dec  5 23:07:58.488: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.488: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.552: INFO: Exec stderr: ""
Dec  5 23:07:58.552: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.625: INFO: Exec stderr: ""
Dec  5 23:07:58.625: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.625: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.692: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  5 23:07:58.692: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.762: INFO: Exec stderr: ""
Dec  5 23:07:58.762: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.762: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.841: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  5 23:07:58.841: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.902: INFO: Exec stderr: ""
Dec  5 23:07:58.902: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.902: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:58.968: INFO: Exec stderr: ""
Dec  5 23:07:58.969: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:58.969: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:59.036: INFO: Exec stderr: ""
Dec  5 23:07:59.036: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-npr69 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 23:07:59.036: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
Dec  5 23:07:59.120: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:07:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-npr69" for this suite.
Dec  5 23:08:37.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:08:37.147: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-npr69, resource: bindings, ignored listing per whitelist
Dec  5 23:08:37.181: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-npr69 deletion completed in 38.05855247s

â€¢ [SLOW TEST:42.833 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:08:37.181: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  5 23:08:37.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb" in namespace "e2e-tests-downward-api-kmv4x" to be "success or failure"
Dec  5 23:08:37.228: INFO: Pod "downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190974ms
Dec  5 23:08:39.230: INFO: Pod "downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009931201s
STEP: Saw pod success
Dec  5 23:08:39.231: INFO: Pod "downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:08:39.235: INFO: Trying to get logs from node single-node pod downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb container client-container: <nil>
STEP: delete the pod
Dec  5 23:08:39.253: INFO: Waiting for pod downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb to disappear
Dec  5 23:08:39.258: INFO: Pod downwardapi-volume-b247fafe-f8e2-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:08:39.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kmv4x" for this suite.
Dec  5 23:08:45.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:08:45.282: INFO: namespace: e2e-tests-downward-api-kmv4x, resource: bindings, ignored listing per whitelist
Dec  5 23:08:45.383: INFO: namespace e2e-tests-downward-api-kmv4x deletion completed in 6.121481152s

â€¢ [SLOW TEST:8.202 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:08:45.383: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 23:08:45.489: INFO: (0) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 8.008739ms)
Dec  5 23:08:45.503: INFO: (1) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 13.283593ms)
Dec  5 23:08:45.508: INFO: (2) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 4.855151ms)
Dec  5 23:08:45.513: INFO: (3) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 4.994177ms)
Dec  5 23:08:45.518: INFO: (4) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 5.240708ms)
Dec  5 23:08:45.522: INFO: (5) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.671709ms)
Dec  5 23:08:45.525: INFO: (6) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.293182ms)
Dec  5 23:08:45.530: INFO: (7) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 4.523641ms)
Dec  5 23:08:45.535: INFO: (8) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 4.926201ms)
Dec  5 23:08:45.538: INFO: (9) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.258873ms)
Dec  5 23:08:45.540: INFO: (10) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.274715ms)
Dec  5 23:08:45.545: INFO: (11) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 4.472548ms)
Dec  5 23:08:45.548: INFO: (12) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.184726ms)
Dec  5 23:08:45.556: INFO: (13) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 7.559011ms)
Dec  5 23:08:45.559: INFO: (14) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.647223ms)
Dec  5 23:08:45.562: INFO: (15) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.731266ms)
Dec  5 23:08:45.565: INFO: (16) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 3.19223ms)
Dec  5 23:08:45.568: INFO: (17) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.845651ms)
Dec  5 23:08:45.571: INFO: (18) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.839766ms)
Dec  5 23:08:45.574: INFO: (19) /api/v1/nodes/single-node/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.921574ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:08:45.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-q27mm" for this suite.
Dec  5 23:08:51.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:08:51.614: INFO: namespace: e2e-tests-proxy-q27mm, resource: bindings, ignored listing per whitelist
Dec  5 23:08:51.644: INFO: namespace e2e-tests-proxy-q27mm deletion completed in 6.066110553s

â€¢ [SLOW TEST:6.260 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:08:51.644: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bae7c85b-f8e2-11e8-8a71-d2079f97accb
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-bae7c85b-f8e2-11e8-8a71-d2079f97accb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:10:20.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-szngk" for this suite.
Dec  5 23:10:42.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:10:42.071: INFO: namespace: e2e-tests-projected-szngk, resource: bindings, ignored listing per whitelist
Dec  5 23:10:42.097: INFO: namespace e2e-tests-projected-szngk deletion completed in 22.060895485s

â€¢ [SLOW TEST:110.453 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:10:42.097: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l68t8 in namespace e2e-tests-proxy-7hk69
I1205 23:10:42.149545      17 runners.go:184] Created replication controller with name: proxy-service-l68t8, namespace: e2e-tests-proxy-7hk69, replica count: 1
I1205 23:10:43.201520      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:44.202277      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:45.203909      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:46.205682      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:47.206457      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:48.206865      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:49.207576      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:50.208102      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 23:10:51.208719      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:52.210536      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:53.211223      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:54.212165      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:55.212870      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:56.215851      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 23:10:57.217810      17 runners.go:184] proxy-service-l68t8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 23:10:57.220: INFO: setup took 15.081464375s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.030998ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.979034ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 7.220999ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.060783ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.158ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.076831ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 8.002444ms)
Dec  5 23:10:57.228: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.710744ms)
Dec  5 23:10:57.232: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 11.422187ms)
Dec  5 23:10:57.232: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 11.71231ms)
Dec  5 23:10:57.232: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 11.577941ms)
Dec  5 23:10:57.236: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 15.31835ms)
Dec  5 23:10:57.236: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 15.692605ms)
Dec  5 23:10:57.237: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 16.401191ms)
Dec  5 23:10:57.238: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 17.846107ms)
Dec  5 23:10:57.238: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 17.62197ms)
Dec  5 23:10:57.244: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 5.159525ms)
Dec  5 23:10:57.245: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 6.651353ms)
Dec  5 23:10:57.245: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.653463ms)
Dec  5 23:10:57.245: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 6.643217ms)
Dec  5 23:10:57.245: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.704845ms)
Dec  5 23:10:57.245: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.021146ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 7.042127ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.105705ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 7.191885ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 7.200867ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 7.232453ms)
Dec  5 23:10:57.246: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 7.295134ms)
Dec  5 23:10:57.248: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 9.369962ms)
Dec  5 23:10:57.248: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 9.287239ms)
Dec  5 23:10:57.248: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 10.106932ms)
Dec  5 23:10:57.249: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 10.226823ms)
Dec  5 23:10:57.251: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 2.617315ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.483433ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.958425ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 6.639565ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.824737ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 6.770547ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.826426ms)
Dec  5 23:10:57.256: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 6.73014ms)
Dec  5 23:10:57.257: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 7.640384ms)
Dec  5 23:10:57.257: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.233851ms)
Dec  5 23:10:57.257: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.74257ms)
Dec  5 23:10:57.258: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 9.458138ms)
Dec  5 23:10:57.258: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 7.477404ms)
Dec  5 23:10:57.258: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 8.768078ms)
Dec  5 23:10:57.259: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 8.061427ms)
Dec  5 23:10:57.259: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 10.269167ms)
Dec  5 23:10:57.262: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 1.504818ms)
Dec  5 23:10:57.262: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 2.923514ms)
Dec  5 23:10:57.263: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 2.092316ms)
Dec  5 23:10:57.263: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 3.690451ms)
Dec  5 23:10:57.264: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 4.201096ms)
Dec  5 23:10:57.264: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 4.106004ms)
Dec  5 23:10:57.266: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 5.447142ms)
Dec  5 23:10:57.266: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 5.25038ms)
Dec  5 23:10:57.267: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 5.995682ms)
Dec  5 23:10:57.267: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 6.192841ms)
Dec  5 23:10:57.267: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.127805ms)
Dec  5 23:10:57.268: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.313012ms)
Dec  5 23:10:57.268: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.353251ms)
Dec  5 23:10:57.268: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.399792ms)
Dec  5 23:10:57.269: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.972522ms)
Dec  5 23:10:57.269: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 7.955463ms)
Dec  5 23:10:57.272: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 2.660439ms)
Dec  5 23:10:57.273: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.273288ms)
Dec  5 23:10:57.273: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 3.735164ms)
Dec  5 23:10:57.274: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 4.507379ms)
Dec  5 23:10:57.276: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.456158ms)
Dec  5 23:10:57.276: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.47102ms)
Dec  5 23:10:57.276: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 6.465973ms)
Dec  5 23:10:57.276: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 6.49706ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.081864ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 8.52948ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 7.318792ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 8.424356ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 7.482539ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 7.450449ms)
Dec  5 23:10:57.277: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.54286ms)
Dec  5 23:10:57.279: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 9.838081ms)
Dec  5 23:10:57.284: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 3.749622ms)
Dec  5 23:10:57.285: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.530358ms)
Dec  5 23:10:57.286: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 5.764923ms)
Dec  5 23:10:57.286: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 5.417988ms)
Dec  5 23:10:57.287: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.021083ms)
Dec  5 23:10:57.287: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 8.04509ms)
Dec  5 23:10:57.287: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 7.129297ms)
Dec  5 23:10:57.288: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.260513ms)
Dec  5 23:10:57.288: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.698721ms)
Dec  5 23:10:57.288: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 7.718098ms)
Dec  5 23:10:57.290: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 9.924553ms)
Dec  5 23:10:57.291: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 10.626177ms)
Dec  5 23:10:57.291: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 10.685879ms)
Dec  5 23:10:57.291: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 11.262707ms)
Dec  5 23:10:57.292: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 11.406187ms)
Dec  5 23:10:57.292: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 12.149443ms)
Dec  5 23:10:57.297: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 4.642015ms)
Dec  5 23:10:57.297: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.760033ms)
Dec  5 23:10:57.297: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.525749ms)
Dec  5 23:10:57.298: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 5.712078ms)
Dec  5 23:10:57.298: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 5.868703ms)
Dec  5 23:10:57.299: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.429913ms)
Dec  5 23:10:57.299: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.413905ms)
Dec  5 23:10:57.299: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.662995ms)
Dec  5 23:10:57.299: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.128752ms)
Dec  5 23:10:57.300: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 7.287962ms)
Dec  5 23:10:57.302: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 9.694851ms)
Dec  5 23:10:57.302: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 9.455629ms)
Dec  5 23:10:57.303: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 10.508489ms)
Dec  5 23:10:57.303: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 10.566873ms)
Dec  5 23:10:57.303: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 10.577037ms)
Dec  5 23:10:57.304: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 11.363229ms)
Dec  5 23:10:57.308: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.628389ms)
Dec  5 23:10:57.308: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.74881ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 6.68365ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.354078ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 6.763572ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 6.797ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 6.526213ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 6.405349ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.40101ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.613654ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.646601ms)
Dec  5 23:10:57.311: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.80976ms)
Dec  5 23:10:57.313: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 8.594341ms)
Dec  5 23:10:57.313: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 8.524256ms)
Dec  5 23:10:57.313: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 9.238941ms)
Dec  5 23:10:57.314: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 9.21768ms)
Dec  5 23:10:57.316: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 2.108088ms)
Dec  5 23:10:57.319: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 4.792769ms)
Dec  5 23:10:57.319: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 4.815846ms)
Dec  5 23:10:57.319: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 4.961081ms)
Dec  5 23:10:57.319: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 5.043234ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.717587ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 6.706652ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.012527ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.750315ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 6.791208ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.826275ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.745072ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.947523ms)
Dec  5 23:10:57.322: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 7.326102ms)
Dec  5 23:10:57.321: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.728552ms)
Dec  5 23:10:57.323: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 8.397918ms)
Dec  5 23:10:57.326: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 3.481526ms)
Dec  5 23:10:57.326: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 3.726345ms)
Dec  5 23:10:57.326: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 3.500792ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 5.964674ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 6.025942ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.309246ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.157636ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 6.40396ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 6.163356ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.232256ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.221478ms)
Dec  5 23:10:57.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 6.513869ms)
Dec  5 23:10:57.330: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.736057ms)
Dec  5 23:10:57.330: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 7.080475ms)
Dec  5 23:10:57.330: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 7.033233ms)
Dec  5 23:10:57.330: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.274217ms)
Dec  5 23:10:57.334: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 4.015601ms)
Dec  5 23:10:57.336: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 5.090422ms)
Dec  5 23:10:57.336: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 5.859488ms)
Dec  5 23:10:57.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 5.972609ms)
Dec  5 23:10:57.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.026991ms)
Dec  5 23:10:57.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 5.915718ms)
Dec  5 23:10:57.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 5.985539ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.352386ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 7.77863ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 7.71643ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.856875ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.865167ms)
Dec  5 23:10:57.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.784026ms)
Dec  5 23:10:57.339: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.996457ms)
Dec  5 23:10:57.339: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 8.618242ms)
Dec  5 23:10:57.339: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 8.675969ms)
Dec  5 23:10:57.342: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 2.671501ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 4.001468ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 3.84568ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.90027ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 3.892572ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 3.915412ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 3.865035ms)
Dec  5 23:10:57.343: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.101ms)
Dec  5 23:10:57.345: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 5.164716ms)
Dec  5 23:10:57.347: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.347427ms)
Dec  5 23:10:57.347: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.965678ms)
Dec  5 23:10:57.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 9.116998ms)
Dec  5 23:10:57.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 9.227613ms)
Dec  5 23:10:57.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 9.177892ms)
Dec  5 23:10:57.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 7.777496ms)
Dec  5 23:10:57.350: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 9.978398ms)
Dec  5 23:10:57.353: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 3.375625ms)
Dec  5 23:10:57.355: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 4.913693ms)
Dec  5 23:10:57.355: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 4.886825ms)
Dec  5 23:10:57.355: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 4.919675ms)
Dec  5 23:10:57.357: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.442481ms)
Dec  5 23:10:57.357: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 7.436331ms)
Dec  5 23:10:57.358: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 8.709133ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 11.159084ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 11.267674ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 10.978679ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 10.974905ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 9.203658ms)
Dec  5 23:10:57.361: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 11.059582ms)
Dec  5 23:10:57.364: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 14.318636ms)
Dec  5 23:10:57.364: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 14.478555ms)
Dec  5 23:10:57.365: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 15.613691ms)
Dec  5 23:10:57.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.440017ms)
Dec  5 23:10:57.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 6.479029ms)
Dec  5 23:10:57.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.820324ms)
Dec  5 23:10:57.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.943606ms)
Dec  5 23:10:57.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 6.992936ms)
Dec  5 23:10:57.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.276079ms)
Dec  5 23:10:57.373: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.512444ms)
Dec  5 23:10:57.372: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.54132ms)
Dec  5 23:10:57.375: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 9.349351ms)
Dec  5 23:10:57.376: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 10.072166ms)
Dec  5 23:10:57.376: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 10.232183ms)
Dec  5 23:10:57.377: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 10.106848ms)
Dec  5 23:10:57.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 12.059303ms)
Dec  5 23:10:57.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 12.230014ms)
Dec  5 23:10:57.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 12.445593ms)
Dec  5 23:10:57.378: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 12.967294ms)
Dec  5 23:10:57.382: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 2.997447ms)
Dec  5 23:10:57.382: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.01795ms)
Dec  5 23:10:57.383: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 3.803252ms)
Dec  5 23:10:57.383: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 3.406876ms)
Dec  5 23:10:57.383: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.7381ms)
Dec  5 23:10:57.383: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 3.898874ms)
Dec  5 23:10:57.384: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.725596ms)
Dec  5 23:10:57.384: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 4.64486ms)
Dec  5 23:10:57.384: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 5.274811ms)
Dec  5 23:10:57.385: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 5.39468ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.765063ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.648076ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 7.513856ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 7.229904ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 7.594547ms)
Dec  5 23:10:57.387: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.781138ms)
Dec  5 23:10:57.391: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 3.106529ms)
Dec  5 23:10:57.391: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 3.19123ms)
Dec  5 23:10:57.391: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 3.156038ms)
Dec  5 23:10:57.391: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.774526ms)
Dec  5 23:10:57.392: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 4.30176ms)
Dec  5 23:10:57.395: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.621115ms)
Dec  5 23:10:57.395: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.441716ms)
Dec  5 23:10:57.395: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 7.501267ms)
Dec  5 23:10:57.397: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 9.339525ms)
Dec  5 23:10:57.397: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 9.464569ms)
Dec  5 23:10:57.397: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 9.452696ms)
Dec  5 23:10:57.397: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 9.4503ms)
Dec  5 23:10:57.398: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 10.294745ms)
Dec  5 23:10:57.398: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 10.377305ms)
Dec  5 23:10:57.398: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 10.341253ms)
Dec  5 23:10:57.400: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 12.000976ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 8.425669ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 8.297718ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 8.216862ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 8.315872ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 8.478987ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 8.278452ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 8.413229ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 8.257928ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 8.259712ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 8.330466ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 8.543713ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 8.333754ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 8.517121ms)
Dec  5 23:10:57.408: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 8.548553ms)
Dec  5 23:10:57.409: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 8.434687ms)
Dec  5 23:10:57.409: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 8.843414ms)
Dec  5 23:10:57.414: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.033412ms)
Dec  5 23:10:57.414: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 3.51753ms)
Dec  5 23:10:57.414: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 4.716872ms)
Dec  5 23:10:57.414: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 3.800463ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 6.583401ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 6.630483ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 6.307502ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 6.187188ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.24313ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.10595ms)
Dec  5 23:10:57.416: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 6.48981ms)
Dec  5 23:10:57.417: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.340776ms)
Dec  5 23:10:57.417: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 6.182945ms)
Dec  5 23:10:57.417: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.284606ms)
Dec  5 23:10:57.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 8.388175ms)
Dec  5 23:10:57.419: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 9.117482ms)
Dec  5 23:10:57.424: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.301455ms)
Dec  5 23:10:57.425: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 5.324274ms)
Dec  5 23:10:57.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 6.853937ms)
Dec  5 23:10:57.426: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 6.693075ms)
Dec  5 23:10:57.427: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.933989ms)
Dec  5 23:10:57.428: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 7.880327ms)
Dec  5 23:10:57.428: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 7.957275ms)
Dec  5 23:10:57.428: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 8.101196ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 9.051556ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 9.686787ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 9.759629ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 9.77765ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 9.928213ms)
Dec  5 23:10:57.429: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 9.824458ms)
Dec  5 23:10:57.430: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 10.095769ms)
Dec  5 23:10:57.430: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 10.234352ms)
Dec  5 23:10:57.433: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 3.292424ms)
Dec  5 23:10:57.433: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:1080/proxy/rewri... (200; 2.818359ms)
Dec  5 23:10:57.435: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 4.060836ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/proxy-service-l68t8-pdmlc/proxy/rewriteme"... (200; 6.818231ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:443/proxy/... (200; 7.002023ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:162/proxy/: bar (200; 7.037127ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:160/proxy/: foo (200; 7.068927ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname1/proxy/: foo (200; 7.039476ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/proxy-service-l68t8:portname2/proxy/: bar (200; 6.876681ms)
Dec  5 23:10:57.438: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:462/proxy/: tls qux (200; 7.009702ms)
Dec  5 23:10:57.438: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname2/proxy/: bar (200; 7.222386ms)
Dec  5 23:10:57.437: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-7hk69/pods/http:proxy-service-l68t8-pdmlc:1080/proxy/... (200; 6.841794ms)
Dec  5 23:10:57.438: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname1/proxy/: tls baz (200; 7.794451ms)
Dec  5 23:10:57.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/http:proxy-service-l68t8:portname1/proxy/: foo (200; 8.064825ms)
Dec  5 23:10:57.439: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/services/https:proxy-service-l68t8:tlsportname2/proxy/: tls qux (200; 8.494875ms)
Dec  5 23:10:57.440: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-7hk69/pods/https:proxy-service-l68t8-pdmlc:460/proxy/: tls baz (200; 9.451327ms)
STEP: deleting ReplicationController proxy-service-l68t8 in namespace e2e-tests-proxy-7hk69, will wait for the garbage collector to delete the pods
Dec  5 23:10:57.499: INFO: Deleting ReplicationController proxy-service-l68t8 took: 6.794505ms
Dec  5 23:10:57.599: INFO: Terminating ReplicationController proxy-service-l68t8 pods took: 100.103587ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:10:59.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-7hk69" for this suite.
Dec  5 23:11:05.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:11:05.926: INFO: namespace: e2e-tests-proxy-7hk69, resource: bindings, ignored listing per whitelist
Dec  5 23:11:05.961: INFO: namespace e2e-tests-proxy-7hk69 deletion completed in 6.056173672s

â€¢ [SLOW TEST:23.865 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:11:05.962: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 23:11:06.009: INFO: Waiting up to 5m0s for pod "pod-0af740cb-f8e3-11e8-8a71-d2079f97accb" in namespace "e2e-tests-emptydir-pcjmc" to be "success or failure"
Dec  5 23:11:06.014: INFO: Pod "pod-0af740cb-f8e3-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532268ms
Dec  5 23:11:08.016: INFO: Pod "pod-0af740cb-f8e3-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006321213s
STEP: Saw pod success
Dec  5 23:11:08.016: INFO: Pod "pod-0af740cb-f8e3-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:11:08.017: INFO: Trying to get logs from node single-node pod pod-0af740cb-f8e3-11e8-8a71-d2079f97accb container test-container: <nil>
STEP: delete the pod
Dec  5 23:11:08.033: INFO: Waiting for pod pod-0af740cb-f8e3-11e8-8a71-d2079f97accb to disappear
Dec  5 23:11:08.040: INFO: Pod pod-0af740cb-f8e3-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:11:08.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pcjmc" for this suite.
Dec  5 23:11:14.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:11:14.073: INFO: namespace: e2e-tests-emptydir-pcjmc, resource: bindings, ignored listing per whitelist
Dec  5 23:11:14.096: INFO: namespace e2e-tests-emptydir-pcjmc deletion completed in 6.054157823s

â€¢ [SLOW TEST:8.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:11:14.097: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  5 23:11:14.133: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:11:17.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2dz4s" for this suite.
Dec  5 23:11:23.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:11:23.918: INFO: namespace: e2e-tests-init-container-2dz4s, resource: bindings, ignored listing per whitelist
Dec  5 23:11:23.942: INFO: namespace e2e-tests-init-container-2dz4s deletion completed in 6.052388544s

â€¢ [SLOW TEST:9.846 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:11:23.943: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 23:11:23.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 version'
Dec  5 23:11:24.025: INFO: stderr: ""
Dec  5 23:11:24.025: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T20:56:12Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:11:24.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4jxvj" for this suite.
Dec  5 23:11:30.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:11:30.087: INFO: namespace: e2e-tests-kubectl-4jxvj, resource: bindings, ignored listing per whitelist
Dec  5 23:11:30.087: INFO: namespace e2e-tests-kubectl-4jxvj deletion completed in 6.05828792s

â€¢ [SLOW TEST:6.144 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:11:30.087: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Dec  5 23:11:30.654: INFO: created pod pod-service-account-defaultsa
Dec  5 23:11:30.654: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  5 23:11:30.665: INFO: created pod pod-service-account-mountsa
Dec  5 23:11:30.665: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  5 23:11:30.676: INFO: created pod pod-service-account-nomountsa
Dec  5 23:11:30.676: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  5 23:11:30.684: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  5 23:11:30.684: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  5 23:11:30.696: INFO: created pod pod-service-account-mountsa-mountspec
Dec  5 23:11:30.696: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  5 23:11:30.705: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  5 23:11:30.705: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  5 23:11:30.712: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  5 23:11:30.712: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  5 23:11:30.722: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  5 23:11:30.722: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  5 23:11:30.730: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  5 23:11:30.730: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:11:30.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-mck67" for this suite.
Dec  5 23:12:34.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:12:34.768: INFO: namespace: e2e-tests-svcaccounts-mck67, resource: bindings, ignored listing per whitelist
Dec  5 23:12:34.798: INFO: namespace e2e-tests-svcaccounts-mck67 deletion completed in 1m4.064527282s

â€¢ [SLOW TEST:64.711 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:12:34.798: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-3fea7ef8-f8e3-11e8-8a71-d2079f97accb
STEP: Creating a pod to test consume secrets
Dec  5 23:12:34.847: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb" in namespace "e2e-tests-projected-lctrr" to be "success or failure"
Dec  5 23:12:34.850: INFO: Pod "pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.977299ms
Dec  5 23:12:36.852: INFO: Pod "pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005269164s
Dec  5 23:12:38.855: INFO: Pod "pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007699376s
STEP: Saw pod success
Dec  5 23:12:38.855: INFO: Pod "pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb" satisfied condition "success or failure"
Dec  5 23:12:38.858: INFO: Trying to get logs from node single-node pod pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 23:12:38.871: INFO: Waiting for pod pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb to disappear
Dec  5 23:12:38.879: INFO: Pod pod-projected-secrets-3feae4a2-f8e3-11e8-8a71-d2079f97accb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:12:38.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lctrr" for this suite.
Dec  5 23:12:44.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:12:44.906: INFO: namespace: e2e-tests-projected-lctrr, resource: bindings, ignored listing per whitelist
Dec  5 23:12:44.940: INFO: namespace e2e-tests-projected-lctrr deletion completed in 6.056389181s

â€¢ [SLOW TEST:10.142 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:12:44.941: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-lglg
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 23:12:45.010: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lglg" in namespace "e2e-tests-subpath-rz2hq" to be "success or failure"
Dec  5 23:12:45.081: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Pending", Reason="", readiness=false. Elapsed: 71.333962ms
Dec  5 23:12:47.084: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074190521s
Dec  5 23:12:49.087: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 4.077150541s
Dec  5 23:12:51.090: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 6.080497902s
Dec  5 23:12:53.095: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 8.084717973s
Dec  5 23:12:55.102: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 10.092038719s
Dec  5 23:12:57.105: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 12.094990296s
Dec  5 23:12:59.107: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 14.097288247s
Dec  5 23:13:01.110: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 16.100487304s
Dec  5 23:13:03.114: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 18.104414577s
Dec  5 23:13:05.117: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 20.107475561s
Dec  5 23:13:07.121: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Running", Reason="", readiness=false. Elapsed: 22.110768059s
Dec  5 23:13:09.124: INFO: Pod "pod-subpath-test-configmap-lglg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.113939367s
STEP: Saw pod success
Dec  5 23:13:09.124: INFO: Pod "pod-subpath-test-configmap-lglg" satisfied condition "success or failure"
Dec  5 23:13:09.127: INFO: Trying to get logs from node single-node pod pod-subpath-test-configmap-lglg container test-container-subpath-configmap-lglg: <nil>
STEP: delete the pod
Dec  5 23:13:09.141: INFO: Waiting for pod pod-subpath-test-configmap-lglg to disappear
Dec  5 23:13:09.145: INFO: Pod pod-subpath-test-configmap-lglg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lglg
Dec  5 23:13:09.145: INFO: Deleting pod "pod-subpath-test-configmap-lglg" in namespace "e2e-tests-subpath-rz2hq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:13:09.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-rz2hq" for this suite.
Dec  5 23:13:15.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:13:15.185: INFO: namespace: e2e-tests-subpath-rz2hq, resource: bindings, ignored listing per whitelist
Dec  5 23:13:15.203: INFO: namespace e2e-tests-subpath-rz2hq deletion completed in 6.053634012s

â€¢ [SLOW TEST:30.262 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:13:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:14:15.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bzdn6" for this suite.
Dec  5 23:14:37.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:14:37.277: INFO: namespace: e2e-tests-container-probe-bzdn6, resource: bindings, ignored listing per whitelist
Dec  5 23:14:37.312: INFO: namespace e2e-tests-container-probe-bzdn6 deletion completed in 22.059601699s

â€¢ [SLOW TEST:82.104 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:14:37.313: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 23:14:37.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-9clnw'
Dec  5 23:14:37.432: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 23:14:37.432: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Dec  5 23:14:39.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-9clnw'
Dec  5 23:14:39.519: INFO: stderr: ""
Dec  5 23:14:39.519: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:14:39.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9clnw" for this suite.
Dec  5 23:15:01.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:15:01.554: INFO: namespace: e2e-tests-kubectl-9clnw, resource: bindings, ignored listing per whitelist
Dec  5 23:15:01.576: INFO: namespace e2e-tests-kubectl-9clnw deletion completed in 22.054105627s

â€¢ [SLOW TEST:24.263 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:15:01.577: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  5 23:15:01.610: INFO: Creating ReplicaSet my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb
Dec  5 23:15:01.615: INFO: Pod name my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb: Found 0 pods out of 1
Dec  5 23:15:06.619: INFO: Pod name my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb: Found 1 pods out of 1
Dec  5 23:15:06.620: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb" is running
Dec  5 23:15:06.623: INFO: Pod "my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb-fvmvw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 23:15:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 23:15:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 23:15:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-05 23:15:01 +0000 UTC Reason: Message:}])
Dec  5 23:15:06.623: INFO: Trying to dial the pod
Dec  5 23:15:11.630: INFO: Controller my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb: Got expected result from replica 1 [my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb-fvmvw]: "my-hostname-basic-9765eefa-f8e3-11e8-8a71-d2079f97accb-fvmvw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:15:11.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-g56fg" for this suite.
Dec  5 23:15:17.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:15:17.698: INFO: namespace: e2e-tests-replicaset-g56fg, resource: bindings, ignored listing per whitelist
Dec  5 23:15:17.715: INFO: namespace e2e-tests-replicaset-g56fg deletion completed in 6.083159272s

â€¢ [SLOW TEST:16.139 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:15:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  5 23:15:17.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 create -f - --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:17.904: INFO: stderr: ""
Dec  5 23:15:17.904: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 23:15:17.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:17.988: INFO: stderr: ""
Dec  5 23:15:17.988: INFO: stdout: "update-demo-nautilus-9zjk6 update-demo-nautilus-kkbxr "
Dec  5 23:15:17.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-9zjk6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:18.044: INFO: stderr: ""
Dec  5 23:15:18.044: INFO: stdout: ""
Dec  5 23:15:18.044: INFO: update-demo-nautilus-9zjk6 is created but not running
Dec  5 23:15:23.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:23.100: INFO: stderr: ""
Dec  5 23:15:23.100: INFO: stdout: "update-demo-nautilus-9zjk6 update-demo-nautilus-kkbxr "
Dec  5 23:15:23.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-9zjk6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:23.169: INFO: stderr: ""
Dec  5 23:15:23.169: INFO: stdout: "true"
Dec  5 23:15:23.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-9zjk6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:23.224: INFO: stderr: ""
Dec  5 23:15:23.224: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 23:15:23.224: INFO: validating pod update-demo-nautilus-9zjk6
Dec  5 23:15:23.226: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 23:15:23.226: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 23:15:23.226: INFO: update-demo-nautilus-9zjk6 is verified up and running
Dec  5 23:15:23.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:23.288: INFO: stderr: ""
Dec  5 23:15:23.288: INFO: stdout: "true"
Dec  5 23:15:23.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:23.358: INFO: stderr: ""
Dec  5 23:15:23.358: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 23:15:23.358: INFO: validating pod update-demo-nautilus-kkbxr
Dec  5 23:15:23.362: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 23:15:23.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 23:15:23.362: INFO: update-demo-nautilus-kkbxr is verified up and running
STEP: scaling down the replication controller
Dec  5 23:15:23.363: INFO: scanned /root for discovery docs: <nil>
Dec  5 23:15:23.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:24.455: INFO: stderr: ""
Dec  5 23:15:24.455: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 23:15:24.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:24.519: INFO: stderr: ""
Dec  5 23:15:24.519: INFO: stdout: "update-demo-nautilus-9zjk6 update-demo-nautilus-kkbxr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 23:15:29.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:29.577: INFO: stderr: ""
Dec  5 23:15:29.577: INFO: stdout: "update-demo-nautilus-9zjk6 update-demo-nautilus-kkbxr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 23:15:34.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:34.645: INFO: stderr: ""
Dec  5 23:15:34.645: INFO: stdout: "update-demo-nautilus-9zjk6 update-demo-nautilus-kkbxr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 23:15:39.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:39.712: INFO: stderr: ""
Dec  5 23:15:39.712: INFO: stdout: "update-demo-nautilus-kkbxr "
Dec  5 23:15:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:39.767: INFO: stderr: ""
Dec  5 23:15:39.767: INFO: stdout: "true"
Dec  5 23:15:39.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:39.827: INFO: stderr: ""
Dec  5 23:15:39.827: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 23:15:39.827: INFO: validating pod update-demo-nautilus-kkbxr
Dec  5 23:15:39.830: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 23:15:39.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 23:15:39.830: INFO: update-demo-nautilus-kkbxr is verified up and running
STEP: scaling up the replication controller
Dec  5 23:15:39.832: INFO: scanned /root for discovery docs: <nil>
Dec  5 23:15:39.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:40.908: INFO: stderr: ""
Dec  5 23:15:40.908: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 23:15:40.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:40.967: INFO: stderr: ""
Dec  5 23:15:40.967: INFO: stdout: "update-demo-nautilus-5w46l update-demo-nautilus-kkbxr "
Dec  5 23:15:40.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5w46l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:41.031: INFO: stderr: ""
Dec  5 23:15:41.031: INFO: stdout: ""
Dec  5 23:15:41.032: INFO: update-demo-nautilus-5w46l is created but not running
Dec  5 23:15:46.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.090: INFO: stderr: ""
Dec  5 23:15:46.090: INFO: stdout: "update-demo-nautilus-5w46l update-demo-nautilus-kkbxr "
Dec  5 23:15:46.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5w46l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.141: INFO: stderr: ""
Dec  5 23:15:46.141: INFO: stdout: "true"
Dec  5 23:15:46.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-5w46l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.196: INFO: stderr: ""
Dec  5 23:15:46.196: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 23:15:46.196: INFO: validating pod update-demo-nautilus-5w46l
Dec  5 23:15:46.199: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 23:15:46.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 23:15:46.199: INFO: update-demo-nautilus-5w46l is verified up and running
Dec  5 23:15:46.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.261: INFO: stderr: ""
Dec  5 23:15:46.261: INFO: stdout: "true"
Dec  5 23:15:46.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods update-demo-nautilus-kkbxr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.314: INFO: stderr: ""
Dec  5 23:15:46.314: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 23:15:46.314: INFO: validating pod update-demo-nautilus-kkbxr
Dec  5 23:15:46.317: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 23:15:46.317: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 23:15:46.317: INFO: update-demo-nautilus-kkbxr is verified up and running
STEP: using delete to clean up resources
Dec  5 23:15:46.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.384: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 23:15:46.384: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 23:15:46.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-hsmkr'
Dec  5 23:15:46.470: INFO: stderr: "No resources found.\n"
Dec  5 23:15:46.470: INFO: stdout: ""
Dec  5 23:15:46.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-502647789 get pods -l name=update-demo --namespace=e2e-tests-kubectl-hsmkr -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 23:15:46.532: INFO: stderr: ""
Dec  5 23:15:46.532: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:15:46.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hsmkr" for this suite.
Dec  5 23:16:08.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:16:08.567: INFO: namespace: e2e-tests-kubectl-hsmkr, resource: bindings, ignored listing per whitelist
Dec  5 23:16:08.593: INFO: namespace e2e-tests-kubectl-hsmkr deletion completed in 22.05813973s

â€¢ [SLOW TEST:50.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec  5 23:16:08.595: INFO: >>> kubeConfig: /tmp/kubeconfig-502647789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 23:16:08.650: INFO: Number of nodes with available pods: 0
Dec  5 23:16:08.650: INFO: Node single-node is running more than one daemon pod
Dec  5 23:16:09.658: INFO: Number of nodes with available pods: 0
Dec  5 23:16:09.658: INFO: Node single-node is running more than one daemon pod
Dec  5 23:16:10.657: INFO: Number of nodes with available pods: 1
Dec  5 23:16:10.657: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  5 23:16:10.681: INFO: Number of nodes with available pods: 0
Dec  5 23:16:10.681: INFO: Node single-node is running more than one daemon pod
Dec  5 23:16:11.685: INFO: Number of nodes with available pods: 0
Dec  5 23:16:11.685: INFO: Node single-node is running more than one daemon pod
Dec  5 23:16:12.695: INFO: Number of nodes with available pods: 1
Dec  5 23:16:12.695: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-bjdsp, will wait for the garbage collector to delete the pods
Dec  5 23:16:12.757: INFO: Deleting DaemonSet.extensions daemon-set took: 3.628187ms
Dec  5 23:16:12.857: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.166597ms
Dec  5 23:16:45.863: INFO: Number of nodes with available pods: 0
Dec  5 23:16:45.863: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 23:16:45.865: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-bjdsp/daemonsets","resourceVersion":"22617"},"items":null}

Dec  5 23:16:45.867: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-bjdsp/pods","resourceVersion":"22617"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec  5 23:16:45.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-bjdsp" for this suite.
Dec  5 23:16:51.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 23:16:51.899: INFO: namespace: e2e-tests-daemonsets-bjdsp, resource: bindings, ignored listing per whitelist
Dec  5 23:16:51.917: INFO: namespace e2e-tests-daemonsets-bjdsp deletion completed in 6.04500487s

â€¢ [SLOW TEST:43.322 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSDec  5 23:16:51.917: INFO: Running AfterSuite actions on all nodes
Dec  5 23:16:51.917: INFO: Running AfterSuite actions on node 1
Dec  5 23:16:51.917: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5751.227 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h35m51.806353291s
Test Suite Passed
