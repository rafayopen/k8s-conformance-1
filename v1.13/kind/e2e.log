I0123 00:14:18.638894      19 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-311263258
I0123 00:14:18.638984      19 e2e.go:224] Starting e2e run "d320b24c-1ea3-11e9-b567-9e07e353f0d4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1548202457 - Will randomize all specs
Will run 201 of 1946 specs

Jan 23 00:14:18.767: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:14:18.769: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 23 00:14:34.117: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 23 00:14:34.143: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 23 00:14:34.143: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 23 00:14:34.143: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 23 00:14:34.149: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 23 00:14:34.149: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jan 23 00:14:34.149: INFO: e2e test version: v1.13.0
Jan 23 00:14:34.150: INFO: kube-apiserver version: v1.13.2
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:14:34.150: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename statefulset
Jan 23 00:14:34.208: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-q87rd
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-q87rd
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-q87rd
Jan 23 00:14:34.218: INFO: Found 0 stateful pods, waiting for 1
Jan 23 00:14:44.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 23 00:14:54.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 23 00:14:54.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:14:54.632: INFO: stderr: ""
Jan 23 00:14:54.632: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:14:54.632: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:14:54.635: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 00:15:04.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:15:04.639: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:15:04.654: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999519s
Jan 23 00:15:05.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992763478s
Jan 23 00:15:06.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988921228s
Jan 23 00:15:07.665: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984979225s
Jan 23 00:15:08.669: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981444941s
Jan 23 00:15:09.671: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977695637s
Jan 23 00:15:10.674: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97525737s
Jan 23 00:15:11.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972842464s
Jan 23 00:15:12.681: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969280818s
Jan 23 00:15:13.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.648115ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-q87rd
Jan 23 00:15:14.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:15:14.879: INFO: stderr: ""
Jan 23 00:15:14.879: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:15:14.879: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:15:14.882: INFO: Found 1 stateful pods, waiting for 3
Jan 23 00:15:24.886: INFO: Found 2 stateful pods, waiting for 3
Jan 23 00:15:34.885: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:15:34.885: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:15:34.885: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 23 00:15:34.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:15:35.091: INFO: stderr: ""
Jan 23 00:15:35.091: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:15:35.091: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:15:35.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:15:35.278: INFO: stderr: ""
Jan 23 00:15:35.278: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:15:35.278: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:15:35.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:15:35.465: INFO: stderr: ""
Jan 23 00:15:35.465: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:15:35.465: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:15:35.465: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:15:35.468: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 23 00:15:45.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:15:45.475: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:15:45.475: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:15:45.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999473s
Jan 23 00:15:46.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996620302s
Jan 23 00:15:47.493: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992681107s
Jan 23 00:15:48.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988129453s
Jan 23 00:15:49.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984930389s
Jan 23 00:15:50.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980373175s
Jan 23 00:15:51.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975857681s
Jan 23 00:15:52.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971607901s
Jan 23 00:15:53.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96756803s
Jan 23 00:15:54.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.791175ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-q87rd
Jan 23 00:15:55.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:15:55.693: INFO: stderr: ""
Jan 23 00:15:55.693: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:15:55.693: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:15:55.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:15:55.860: INFO: stderr: ""
Jan 23 00:15:55.860: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:15:55.860: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:15:55.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-q87rd ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:15:56.041: INFO: stderr: ""
Jan 23 00:15:56.041: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:15:56.041: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:15:56.041: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 23 00:16:16.053: INFO: Deleting all statefulset in ns e2e-tests-statefulset-q87rd
Jan 23 00:16:16.056: INFO: Scaling statefulset ss to 0
Jan 23 00:16:16.065: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:16:16.067: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:16:16.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-q87rd" for this suite.
Jan 23 00:16:22.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:16:22.100: INFO: namespace: e2e-tests-statefulset-q87rd, resource: bindings, ignored listing per whitelist
Jan 23 00:16:22.150: INFO: namespace e2e-tests-statefulset-q87rd deletion completed in 6.071166385s

• [SLOW TEST:108.000 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:16:22.151: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jan 23 00:16:22.717: INFO: Waiting up to 5m0s for pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq" in namespace "e2e-tests-svcaccounts-vspn2" to be "success or failure"
Jan 23 00:16:22.724: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.078559ms
Jan 23 00:16:24.727: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010171691s
Jan 23 00:16:26.731: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013706276s
Jan 23 00:16:28.734: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016628855s
Jan 23 00:16:30.737: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019794821s
Jan 23 00:16:32.739: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02217212s
Jan 23 00:16:34.744: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027110142s
Jan 23 00:16:36.747: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.030468237s
STEP: Saw pod success
Jan 23 00:16:36.748: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq" satisfied condition "success or failure"
Jan 23 00:16:36.750: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq container token-test: <nil>
STEP: delete the pod
Jan 23 00:16:36.779: INFO: Waiting for pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq to disappear
Jan 23 00:16:36.783: INFO: Pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-58xxq no longer exists
STEP: Creating a pod to test consume service account root CA
Jan 23 00:16:36.786: INFO: Waiting up to 5m0s for pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl" in namespace "e2e-tests-svcaccounts-vspn2" to be "success or failure"
Jan 23 00:16:36.790: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652139ms
Jan 23 00:16:38.793: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006963068s
Jan 23 00:16:40.796: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010457395s
Jan 23 00:16:42.799: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013498239s
Jan 23 00:16:44.803: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016969406s
Jan 23 00:16:46.806: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020041614s
Jan 23 00:16:48.810: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024035268s
Jan 23 00:16:50.813: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.027315398s
STEP: Saw pod success
Jan 23 00:16:50.813: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl" satisfied condition "success or failure"
Jan 23 00:16:50.816: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl container root-ca-test: <nil>
STEP: delete the pod
Jan 23 00:16:50.840: INFO: Waiting for pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl to disappear
Jan 23 00:16:50.849: INFO: Pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-fxzfl no longer exists
STEP: Creating a pod to test consume service account namespace
Jan 23 00:16:50.852: INFO: Waiting up to 5m0s for pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt" in namespace "e2e-tests-svcaccounts-vspn2" to be "success or failure"
Jan 23 00:16:50.858: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508509ms
Jan 23 00:16:52.861: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008420605s
Jan 23 00:16:54.864: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011847478s
STEP: Saw pod success
Jan 23 00:16:54.864: INFO: Pod "pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt" satisfied condition "success or failure"
Jan 23 00:16:54.867: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt container namespace-test: <nil>
STEP: delete the pod
Jan 23 00:16:54.890: INFO: Waiting for pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt to disappear
Jan 23 00:16:54.901: INFO: Pod pod-service-account-1d55638f-1ea4-11e9-b567-9e07e353f0d4-p5pvt no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:16:54.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-vspn2" for this suite.
Jan 23 00:17:00.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:17:00.945: INFO: namespace: e2e-tests-svcaccounts-vspn2, resource: bindings, ignored listing per whitelist
Jan 23 00:17:00.982: INFO: namespace e2e-tests-svcaccounts-vspn2 deletion completed in 6.079073902s

• [SLOW TEST:38.831 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:17:00.982: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:17:01.041: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:17:11.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-c9fgx" for this suite.
Jan 23 00:17:49.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:17:49.107: INFO: namespace: e2e-tests-pods-c9fgx, resource: bindings, ignored listing per whitelist
Jan 23 00:17:49.134: INFO: namespace e2e-tests-pods-c9fgx deletion completed in 38.066442429s

• [SLOW TEST:48.151 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:17:49.134: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 23 00:17:49.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:49.671: INFO: stderr: ""
Jan 23 00:17:49.671: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 00:17:49.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:49.744: INFO: stderr: ""
Jan 23 00:17:49.744: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-ngzf9 "
Jan 23 00:17:49.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:49.824: INFO: stderr: ""
Jan 23 00:17:49.824: INFO: stdout: ""
Jan 23 00:17:49.824: INFO: update-demo-nautilus-7mbkf is created but not running
Jan 23 00:17:54.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:54.899: INFO: stderr: ""
Jan 23 00:17:54.899: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-ngzf9 "
Jan 23 00:17:54.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:54.961: INFO: stderr: ""
Jan 23 00:17:54.961: INFO: stdout: "true"
Jan 23 00:17:54.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:55.026: INFO: stderr: ""
Jan 23 00:17:55.026: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:17:55.026: INFO: validating pod update-demo-nautilus-7mbkf
Jan 23 00:17:55.029: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:17:55.029: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:17:55.029: INFO: update-demo-nautilus-7mbkf is verified up and running
Jan 23 00:17:55.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-ngzf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:55.094: INFO: stderr: ""
Jan 23 00:17:55.094: INFO: stdout: "true"
Jan 23 00:17:55.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-ngzf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:55.160: INFO: stderr: ""
Jan 23 00:17:55.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:17:55.160: INFO: validating pod update-demo-nautilus-ngzf9
Jan 23 00:17:55.163: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:17:55.163: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:17:55.163: INFO: update-demo-nautilus-ngzf9 is verified up and running
STEP: scaling down the replication controller
Jan 23 00:17:55.166: INFO: scanned /root for discovery docs: <nil>
Jan 23 00:17:55.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:56.275: INFO: stderr: ""
Jan 23 00:17:56.275: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 00:17:56.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:17:56.344: INFO: stderr: ""
Jan 23 00:17:56.344: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-ngzf9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 23 00:18:01.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:01.432: INFO: stderr: ""
Jan 23 00:18:01.432: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-ngzf9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 23 00:18:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:06.524: INFO: stderr: ""
Jan 23 00:18:06.524: INFO: stdout: "update-demo-nautilus-7mbkf "
Jan 23 00:18:06.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:06.596: INFO: stderr: ""
Jan 23 00:18:06.596: INFO: stdout: "true"
Jan 23 00:18:06.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:06.671: INFO: stderr: ""
Jan 23 00:18:06.671: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:18:06.671: INFO: validating pod update-demo-nautilus-7mbkf
Jan 23 00:18:06.672: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:18:06.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:18:06.673: INFO: update-demo-nautilus-7mbkf is verified up and running
STEP: scaling up the replication controller
Jan 23 00:18:06.674: INFO: scanned /root for discovery docs: <nil>
Jan 23 00:18:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:07.773: INFO: stderr: ""
Jan 23 00:18:07.773: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 00:18:07.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:07.846: INFO: stderr: ""
Jan 23 00:18:07.846: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-t8bf6 "
Jan 23 00:18:07.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:07.922: INFO: stderr: ""
Jan 23 00:18:07.922: INFO: stdout: "true"
Jan 23 00:18:07.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:07.993: INFO: stderr: ""
Jan 23 00:18:07.993: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:18:07.993: INFO: validating pod update-demo-nautilus-7mbkf
Jan 23 00:18:07.995: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:18:07.995: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:18:07.995: INFO: update-demo-nautilus-7mbkf is verified up and running
Jan 23 00:18:07.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-t8bf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:08.060: INFO: stderr: ""
Jan 23 00:18:08.060: INFO: stdout: ""
Jan 23 00:18:08.060: INFO: update-demo-nautilus-t8bf6 is created but not running
Jan 23 00:18:13.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.162: INFO: stderr: ""
Jan 23 00:18:13.162: INFO: stdout: "update-demo-nautilus-7mbkf update-demo-nautilus-t8bf6 "
Jan 23 00:18:13.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.254: INFO: stderr: ""
Jan 23 00:18:13.254: INFO: stdout: "true"
Jan 23 00:18:13.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7mbkf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.322: INFO: stderr: ""
Jan 23 00:18:13.322: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:18:13.322: INFO: validating pod update-demo-nautilus-7mbkf
Jan 23 00:18:13.325: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:18:13.325: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:18:13.325: INFO: update-demo-nautilus-7mbkf is verified up and running
Jan 23 00:18:13.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-t8bf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.400: INFO: stderr: ""
Jan 23 00:18:13.400: INFO: stdout: "true"
Jan 23 00:18:13.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-t8bf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.458: INFO: stderr: ""
Jan 23 00:18:13.459: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:18:13.459: INFO: validating pod update-demo-nautilus-t8bf6
Jan 23 00:18:13.462: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:18:13.462: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:18:13.462: INFO: update-demo-nautilus-t8bf6 is verified up and running
STEP: using delete to clean up resources
Jan 23 00:18:13.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.539: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 00:18:13.539: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 00:18:13.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-7gvmq'
Jan 23 00:18:13.613: INFO: stderr: "No resources found.\n"
Jan 23 00:18:13.613: INFO: stdout: ""
Jan 23 00:18:13.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -l name=update-demo --namespace=e2e-tests-kubectl-7gvmq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 00:18:13.681: INFO: stderr: ""
Jan 23 00:18:13.681: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:18:13.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7gvmq" for this suite.
Jan 23 00:18:35.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:18:35.709: INFO: namespace: e2e-tests-kubectl-7gvmq, resource: bindings, ignored listing per whitelist
Jan 23 00:18:35.750: INFO: namespace e2e-tests-kubectl-7gvmq deletion completed in 22.066456155s

• [SLOW TEST:46.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:18:35.750: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-6caafefc-1ea4-11e9-b567-9e07e353f0d4
STEP: Creating configMap with name cm-test-opt-upd-6caaff2b-1ea4-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6caafefc-1ea4-11e9-b567-9e07e353f0d4
STEP: Updating configmap cm-test-opt-upd-6caaff2b-1ea4-11e9-b567-9e07e353f0d4
STEP: Creating configMap with name cm-test-opt-create-6caaff3e-1ea4-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:19:46.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7j4cq" for this suite.
Jan 23 00:20:08.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:20:08.282: INFO: namespace: e2e-tests-configmap-7j4cq, resource: bindings, ignored listing per whitelist
Jan 23 00:20:08.305: INFO: namespace e2e-tests-configmap-7j4cq deletion completed in 22.07602198s

• [SLOW TEST:92.555 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:20:08.305: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a3d4b049-1ea4-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:20:08.367: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-lbk9k" to be "success or failure"
Jan 23 00:20:08.369: INFO: Pod "pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.502874ms
Jan 23 00:20:10.372: INFO: Pod "pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004985194s
STEP: Saw pod success
Jan 23 00:20:10.372: INFO: Pod "pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:20:10.375: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:20:10.391: INFO: Waiting for pod pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:20:10.393: INFO: Pod pod-configmaps-a3d4f361-1ea4-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:20:10.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lbk9k" for this suite.
Jan 23 00:20:16.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:20:16.424: INFO: namespace: e2e-tests-configmap-lbk9k, resource: bindings, ignored listing per whitelist
Jan 23 00:20:16.476: INFO: namespace e2e-tests-configmap-lbk9k deletion completed in 6.080543912s

• [SLOW TEST:8.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:20:16.476: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:20:16.537: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 23 00:20:16.540: INFO: Number of nodes with available pods: 0
Jan 23 00:20:16.540: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 23 00:20:16.551: INFO: Number of nodes with available pods: 0
Jan 23 00:20:16.551: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:17.554: INFO: Number of nodes with available pods: 0
Jan 23 00:20:17.554: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:18.554: INFO: Number of nodes with available pods: 0
Jan 23 00:20:18.554: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:19.553: INFO: Number of nodes with available pods: 0
Jan 23 00:20:19.554: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:20.554: INFO: Number of nodes with available pods: 1
Jan 23 00:20:20.554: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 23 00:20:20.569: INFO: Number of nodes with available pods: 1
Jan 23 00:20:20.569: INFO: Number of running nodes: 0, number of available pods: 1
Jan 23 00:20:21.573: INFO: Number of nodes with available pods: 0
Jan 23 00:20:21.573: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 23 00:20:21.584: INFO: Number of nodes with available pods: 0
Jan 23 00:20:21.584: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:22.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:22.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:23.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:23.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:24.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:24.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:25.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:25.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:26.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:26.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:27.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:27.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:28.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:28.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:29.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:29.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:30.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:30.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:31.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:31.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:32.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:32.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:33.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:33.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:34.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:34.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:35.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:35.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:36.589: INFO: Number of nodes with available pods: 0
Jan 23 00:20:36.589: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:37.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:37.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:38.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:38.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:39.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:39.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:40.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:40.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:41.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:41.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:42.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:42.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:43.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:43.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:44.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:44.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:45.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:45.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:46.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:46.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:47.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:47.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:48.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:48.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:49.588: INFO: Number of nodes with available pods: 0
Jan 23 00:20:49.588: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:50.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:50.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:51.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:51.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:52.588: INFO: Number of nodes with available pods: 0
Jan 23 00:20:52.588: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:53.587: INFO: Number of nodes with available pods: 0
Jan 23 00:20:53.587: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:54.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:54.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:55.586: INFO: Number of nodes with available pods: 0
Jan 23 00:20:55.586: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:20:56.586: INFO: Number of nodes with available pods: 1
Jan 23 00:20:56.586: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tw9g7, will wait for the garbage collector to delete the pods
Jan 23 00:20:56.648: INFO: Deleting DaemonSet.extensions daemon-set took: 5.547966ms
Jan 23 00:20:56.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.236174ms
Jan 23 00:21:41.650: INFO: Number of nodes with available pods: 0
Jan 23 00:21:41.650: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 00:21:41.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tw9g7/daemonsets","resourceVersion":"2245"},"items":null}

Jan 23 00:21:41.656: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tw9g7/pods","resourceVersion":"2245"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:21:41.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tw9g7" for this suite.
Jan 23 00:21:47.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:21:47.695: INFO: namespace: e2e-tests-daemonsets-tw9g7, resource: bindings, ignored listing per whitelist
Jan 23 00:21:47.736: INFO: namespace e2e-tests-daemonsets-tw9g7 deletion completed in 6.06920544s

• [SLOW TEST:91.260 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:21:47.736: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-q259
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 00:21:47.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q259" in namespace "e2e-tests-subpath-tmvnt" to be "success or failure"
Jan 23 00:21:47.801: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Pending", Reason="", readiness=false. Elapsed: 1.392533ms
Jan 23 00:21:49.805: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004659095s
Jan 23 00:21:51.808: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008195326s
Jan 23 00:21:53.812: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 6.011580229s
Jan 23 00:21:55.815: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 8.015022467s
Jan 23 00:21:57.818: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 10.018092903s
Jan 23 00:21:59.821: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 12.02127314s
Jan 23 00:22:01.825: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 14.0245701s
Jan 23 00:22:03.828: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 16.02812279s
Jan 23 00:22:05.832: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 18.031560546s
Jan 23 00:22:07.834: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 20.034372148s
Jan 23 00:22:09.837: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 22.037358291s
Jan 23 00:22:11.841: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Running", Reason="", readiness=false. Elapsed: 24.040847369s
Jan 23 00:22:13.845: INFO: Pod "pod-subpath-test-configmap-q259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.044465217s
STEP: Saw pod success
Jan 23 00:22:13.845: INFO: Pod "pod-subpath-test-configmap-q259" satisfied condition "success or failure"
Jan 23 00:22:13.847: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-subpath-test-configmap-q259 container test-container-subpath-configmap-q259: <nil>
STEP: delete the pod
Jan 23 00:22:13.870: INFO: Waiting for pod pod-subpath-test-configmap-q259 to disappear
Jan 23 00:22:13.871: INFO: Pod pod-subpath-test-configmap-q259 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q259
Jan 23 00:22:13.871: INFO: Deleting pod "pod-subpath-test-configmap-q259" in namespace "e2e-tests-subpath-tmvnt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:22:13.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-tmvnt" for this suite.
Jan 23 00:22:19.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:22:19.939: INFO: namespace: e2e-tests-subpath-tmvnt, resource: bindings, ignored listing per whitelist
Jan 23 00:22:19.945: INFO: namespace e2e-tests-subpath-tmvnt deletion completed in 6.070105111s

• [SLOW TEST:32.209 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:22:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mqgcl in namespace e2e-tests-proxy-wkxql
I0123 00:22:20.019768      19 runners.go:184] Created replication controller with name: proxy-service-mqgcl, namespace: e2e-tests-proxy-wkxql, replica count: 1
I0123 00:22:21.070165      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:22.070337      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:23.070553      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:24.070792      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:25.070978      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:26.071163      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:27.071322      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:28.071465      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 00:22:29.071709      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:30.071954      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:31.072208      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:32.072425      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:33.072714      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:34.073054      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 00:22:35.073263      19 runners.go:184] proxy-service-mqgcl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 00:22:35.076: INFO: setup took 15.06984174s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 23 00:22:35.086: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 9.885387ms)
Jan 23 00:22:35.086: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 9.814279ms)
Jan 23 00:22:35.086: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 10.039221ms)
Jan 23 00:22:35.086: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 9.834182ms)
Jan 23 00:22:35.087: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 10.341356ms)
Jan 23 00:22:35.091: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 14.404916ms)
Jan 23 00:22:35.091: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 14.709205ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 19.560465ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 19.591188ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 19.727342ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 19.685327ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 19.814825ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 19.582657ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 19.667054ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 19.717044ms)
Jan 23 00:22:35.096: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 20.169059ms)
Jan 23 00:22:35.099: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.000585ms)
Jan 23 00:22:35.099: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.037949ms)
Jan 23 00:22:35.099: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.026022ms)
Jan 23 00:22:35.100: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.232139ms)
Jan 23 00:22:35.100: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.373031ms)
Jan 23 00:22:35.100: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.986179ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.196104ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 4.499119ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 4.420282ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 5.003152ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 5.110781ms)
Jan 23 00:22:35.101: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 5.06748ms)
Jan 23 00:22:35.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 5.255878ms)
Jan 23 00:22:35.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 5.227132ms)
Jan 23 00:22:35.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 5.235063ms)
Jan 23 00:22:35.102: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 5.366721ms)
Jan 23 00:22:35.104: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.069615ms)
Jan 23 00:22:35.105: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.845602ms)
Jan 23 00:22:35.105: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.488076ms)
Jan 23 00:22:35.105: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.53704ms)
Jan 23 00:22:35.105: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.501208ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.808431ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.20678ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 4.261641ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 4.373534ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 4.430776ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 4.457379ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 4.534915ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 4.466643ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 4.523458ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 4.6131ms)
Jan 23 00:22:35.106: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 4.604463ms)
Jan 23 00:22:35.109: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 2.91739ms)
Jan 23 00:22:35.110: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.08857ms)
Jan 23 00:22:35.110: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.131588ms)
Jan 23 00:22:35.110: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.229198ms)
Jan 23 00:22:35.110: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 3.408432ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 4.132984ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 4.105031ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 4.115669ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 4.25227ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 4.316613ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 4.30663ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 4.459263ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 4.569368ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 4.571876ms)
Jan 23 00:22:35.111: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 4.594973ms)
Jan 23 00:22:35.112: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 5.230655ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.668542ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.919628ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.976737ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.942464ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 3.059702ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.113298ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.106372ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.344168ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.292553ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.319848ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.404978ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.345745ms)
Jan 23 00:22:35.115: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.65988ms)
Jan 23 00:22:35.116: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.68639ms)
Jan 23 00:22:35.116: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.662401ms)
Jan 23 00:22:35.116: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.8163ms)
Jan 23 00:22:35.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.492761ms)
Jan 23 00:22:35.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.600387ms)
Jan 23 00:22:35.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.600933ms)
Jan 23 00:22:35.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.674348ms)
Jan 23 00:22:35.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.711883ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 2.806189ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 2.90908ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 2.985065ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.06334ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.014225ms)
Jan 23 00:22:35.119: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.160874ms)
Jan 23 00:22:35.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.77346ms)
Jan 23 00:22:35.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.834851ms)
Jan 23 00:22:35.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.961642ms)
Jan 23 00:22:35.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.98201ms)
Jan 23 00:22:35.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.994398ms)
Jan 23 00:22:35.122: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.308663ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.806754ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.921061ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 3.308657ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.473757ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.500301ms)
Jan 23 00:22:35.123: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.556794ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.923299ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.901017ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.910189ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.902651ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.925641ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.94224ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.988899ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 4.25905ms)
Jan 23 00:22:35.124: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 4.540064ms)
Jan 23 00:22:35.126: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 1.903675ms)
Jan 23 00:22:35.126: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 1.82642ms)
Jan 23 00:22:35.127: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.64815ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.491452ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.523909ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.581368ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.525045ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.552663ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.635661ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.670251ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.673536ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.782386ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.866778ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.784682ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.89825ms)
Jan 23 00:22:35.128: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.894139ms)
Jan 23 00:22:35.131: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.614724ms)
Jan 23 00:22:35.131: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.537021ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.128091ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.133483ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.197212ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.16989ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.258661ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.225801ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.341889ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.279279ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.27677ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.403709ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.605807ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.592749ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.796849ms)
Jan 23 00:22:35.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.806298ms)
Jan 23 00:22:35.134: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 1.792413ms)
Jan 23 00:22:35.135: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.606262ms)
Jan 23 00:22:35.135: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.748625ms)
Jan 23 00:22:35.135: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.767732ms)
Jan 23 00:22:35.135: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.84138ms)
Jan 23 00:22:35.135: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 2.819466ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.344742ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.361099ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.324828ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.313323ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.317172ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.479449ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.394552ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.588263ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.646228ms)
Jan 23 00:22:35.136: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.652524ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.547746ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.932239ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.983822ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.022082ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.037423ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.042427ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.10211ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.024358ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.018041ms)
Jan 23 00:22:35.139: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.11567ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.710853ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.853596ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.923443ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.936183ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.950662ms)
Jan 23 00:22:35.140: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 4.006671ms)
Jan 23 00:22:35.142: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 2.254596ms)
Jan 23 00:22:35.143: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.427033ms)
Jan 23 00:22:35.143: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.651173ms)
Jan 23 00:22:35.143: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.845983ms)
Jan 23 00:22:35.143: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.995918ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.336698ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.377591ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.440231ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.492379ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.615133ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.621024ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.622492ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.723918ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.783723ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.767638ms)
Jan 23 00:22:35.144: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.839949ms)
Jan 23 00:22:35.146: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 1.99645ms)
Jan 23 00:22:35.146: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 1.999763ms)
Jan 23 00:22:35.146: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.023925ms)
Jan 23 00:22:35.146: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.089778ms)
Jan 23 00:22:35.146: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 2.022913ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.781565ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 2.974817ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.001486ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.293027ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.326331ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.317363ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.39442ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.368347ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.363428ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.337863ms)
Jan 23 00:22:35.147: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.334837ms)
Jan 23 00:22:35.149: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 1.634815ms)
Jan 23 00:22:35.150: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.604308ms)
Jan 23 00:22:35.150: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 2.59075ms)
Jan 23 00:22:35.150: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.784132ms)
Jan 23 00:22:35.150: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.904299ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.914003ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.08753ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.136712ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.235464ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.419545ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.496479ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.427279ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.570488ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.529843ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.556664ms)
Jan 23 00:22:35.151: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.723634ms)
Jan 23 00:22:35.153: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 1.997077ms)
Jan 23 00:22:35.153: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 2.07447ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.448253ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.399479ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.371662ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.459877ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.41973ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.462883ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.459395ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.516598ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.554524ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.5582ms)
Jan 23 00:22:35.155: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.850614ms)
Jan 23 00:22:35.157: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 5.943614ms)
Jan 23 00:22:35.157: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 6.00759ms)
Jan 23 00:22:35.157: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 5.95694ms)
Jan 23 00:22:35.160: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.209851ms)
Jan 23 00:22:35.160: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 2.19268ms)
Jan 23 00:22:35.160: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 2.946483ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.036942ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.012858ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.014819ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.053823ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.169633ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 3.231965ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 3.339964ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 3.249061ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.863292ms)
Jan 23 00:22:35.161: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.867832ms)
Jan 23 00:22:35.162: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 4.107823ms)
Jan 23 00:22:35.162: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 4.113432ms)
Jan 23 00:22:35.162: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 4.096743ms)
Jan 23 00:22:35.164: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.437541ms)
Jan 23 00:22:35.164: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.553472ms)
Jan 23 00:22:35.164: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 2.685373ms)
Jan 23 00:22:35.166: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.059453ms)
Jan 23 00:22:35.166: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 4.448465ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 4.716798ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 4.739872ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 4.809001ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.797004ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 4.8024ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 4.838486ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 4.853502ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 5.296691ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 5.403145ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 5.419249ms)
Jan 23 00:22:35.167: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 5.41167ms)
Jan 23 00:22:35.169: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.173073ms)
Jan 23 00:22:35.170: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 2.201316ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 7.583895ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 7.568394ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 7.605923ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 7.884974ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 7.906871ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 7.965907ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 8.048656ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 7.971524ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 8.025415ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 8.006528ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 8.001263ms)
Jan 23 00:22:35.175: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 7.990388ms)
Jan 23 00:22:35.176: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 8.236203ms)
Jan 23 00:22:35.176: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 8.292436ms)
Jan 23 00:22:35.178: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 2.480449ms)
Jan 23 00:22:35.178: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 2.489973ms)
Jan 23 00:22:35.178: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.553852ms)
Jan 23 00:22:35.178: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 2.693801ms)
Jan 23 00:22:35.179: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.628639ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.870797ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.897969ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 3.954334ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.954006ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 4.193318ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.404507ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 4.418096ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 4.424083ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 4.431621ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 4.428841ms)
Jan 23 00:22:35.180: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 4.455869ms)
Jan 23 00:22:35.182: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb/proxy/rewriteme"... (200; 1.778758ms)
Jan 23 00:22:35.182: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:1080/proxy/... (200; 1.900553ms)
Jan 23 00:22:35.182: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:443/proxy/... (200; 2.005975ms)
Jan 23 00:22:35.183: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 2.973328ms)
Jan 23 00:22:35.183: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:162/proxy/: bar (200; 3.123955ms)
Jan 23 00:22:35.183: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:1080/proxy/rewri... (200; 3.104533ms)
Jan 23 00:22:35.183: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:462/proxy/: tls qux (200; 3.182612ms)
Jan 23 00:22:35.183: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname1/proxy/: foo (200; 3.183897ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname1/proxy/: tls baz (200; 3.241438ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.310862ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname2/proxy/: bar (200; 3.402958ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/proxy-service-mqgcl:portname2/proxy/: bar (200; 3.425214ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/https:proxy-service-mqgcl:tlsportname2/proxy/: tls qux (200; 3.60574ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/http:proxy-service-mqgcl-h88cb:160/proxy/: foo (200; 3.590448ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/services/http:proxy-service-mqgcl:portname1/proxy/: foo (200; 3.700889ms)
Jan 23 00:22:35.184: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-wkxql/pods/https:proxy-service-mqgcl-h88cb:460/proxy/: tls baz (200; 3.685879ms)
STEP: deleting ReplicationController proxy-service-mqgcl in namespace e2e-tests-proxy-wkxql, will wait for the garbage collector to delete the pods
Jan 23 00:22:35.241: INFO: Deleting ReplicationController proxy-service-mqgcl took: 5.9456ms
Jan 23 00:22:35.342: INFO: Terminating ReplicationController proxy-service-mqgcl pods took: 100.220247ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:22:41.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-wkxql" for this suite.
Jan 23 00:22:47.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:22:47.672: INFO: namespace: e2e-tests-proxy-wkxql, resource: bindings, ignored listing per whitelist
Jan 23 00:22:47.712: INFO: namespace e2e-tests-proxy-wkxql deletion completed in 6.068001173s

• [SLOW TEST:27.767 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:22:47.713: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:22:47.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-llfkc" to be "success or failure"
Jan 23 00:22:47.776: INFO: Pod "downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524698ms
Jan 23 00:22:49.779: INFO: Pod "downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004301212s
STEP: Saw pod success
Jan 23 00:22:49.779: INFO: Pod "downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:22:49.781: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:22:49.797: INFO: Waiting for pod downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:22:49.799: INFO: Pod downwardapi-volume-02d8e60e-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:22:49.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-llfkc" for this suite.
Jan 23 00:22:55.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:22:55.824: INFO: namespace: e2e-tests-projected-llfkc, resource: bindings, ignored listing per whitelist
Jan 23 00:22:55.879: INFO: namespace e2e-tests-projected-llfkc deletion completed in 6.076804011s

• [SLOW TEST:8.166 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:22:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:22:57.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-bw4cd" for this suite.
Jan 23 00:23:47.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:23:47.981: INFO: namespace: e2e-tests-kubelet-test-bw4cd, resource: bindings, ignored listing per whitelist
Jan 23 00:23:48.022: INFO: namespace e2e-tests-kubelet-test-bw4cd deletion completed in 50.075863552s

• [SLOW TEST:52.143 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:23:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 23 00:23:48.086: INFO: Waiting up to 5m0s for pod "pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-kjkt9" to be "success or failure"
Jan 23 00:23:48.088: INFO: Pod "pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630559ms
Jan 23 00:23:50.091: INFO: Pod "pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005169944s
STEP: Saw pod success
Jan 23 00:23:50.092: INFO: Pod "pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:23:50.094: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:23:50.115: INFO: Waiting for pod pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:23:50.116: INFO: Pod pod-26cbbf64-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:23:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kjkt9" for this suite.
Jan 23 00:23:56.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:23:56.147: INFO: namespace: e2e-tests-emptydir-kjkt9, resource: bindings, ignored listing per whitelist
Jan 23 00:23:56.197: INFO: namespace e2e-tests-emptydir-kjkt9 deletion completed in 6.077447777s

• [SLOW TEST:8.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:23:56.197: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Jan 23 00:23:56.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 --namespace=e2e-tests-kubectl-kpfzt run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 23 00:23:58.692: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 23 00:23:58.692: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:24:00.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kpfzt" for this suite.
Jan 23 00:24:06.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:24:06.774: INFO: namespace: e2e-tests-kubectl-kpfzt, resource: bindings, ignored listing per whitelist
Jan 23 00:24:06.778: INFO: namespace e2e-tests-kubectl-kpfzt deletion completed in 6.076013484s

• [SLOW TEST:10.581 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:24:06.779: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wsk7j
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-wsk7j
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-wsk7j
Jan 23 00:24:06.838: INFO: Found 0 stateful pods, waiting for 1
Jan 23 00:24:16.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 23 00:24:16.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:24:17.077: INFO: stderr: ""
Jan 23 00:24:17.077: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:24:17.077: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:24:17.080: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 00:24:27.084: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:24:27.084: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:24:27.098: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jan 23 00:24:27.098: INFO: ss-0  kind-conformance-worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  }]
Jan 23 00:24:27.098: INFO: 
Jan 23 00:24:27.098: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 23 00:24:28.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993383853s
Jan 23 00:24:29.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989990168s
Jan 23 00:24:30.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985839434s
Jan 23 00:24:31.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981945104s
Jan 23 00:24:32.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977855352s
Jan 23 00:24:33.122: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973850605s
Jan 23 00:24:34.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969857878s
Jan 23 00:24:35.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966045886s
Jan 23 00:24:36.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.955685ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-wsk7j
Jan 23 00:24:37.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:24:37.341: INFO: stderr: ""
Jan 23 00:24:37.341: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:24:37.341: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:24:37.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:24:37.485: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 23 00:24:37.486: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:24:37.486: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:24:37.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 00:24:37.645: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 23 00:24:37.645: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 00:24:37.645: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 00:24:37.647: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:24:37.647: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:24:37.647: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 23 00:24:37.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:24:37.812: INFO: stderr: ""
Jan 23 00:24:37.812: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:24:37.812: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:24:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:24:37.992: INFO: stderr: ""
Jan 23 00:24:37.992: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:24:37.992: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:24:37.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-wsk7j ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 00:24:38.175: INFO: stderr: ""
Jan 23 00:24:38.175: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 00:24:38.175: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 00:24:38.175: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:24:38.177: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 23 00:24:48.184: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:24:48.184: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:24:48.184: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 00:24:48.194: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jan 23 00:24:48.194: INFO: ss-0  kind-conformance-worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  }]
Jan 23 00:24:48.194: INFO: ss-1  kind-conformance-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  }]
Jan 23 00:24:48.195: INFO: ss-2  kind-conformance-worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  }]
Jan 23 00:24:48.195: INFO: 
Jan 23 00:24:48.195: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 00:24:49.199: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jan 23 00:24:49.199: INFO: ss-0  kind-conformance-worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  }]
Jan 23 00:24:49.199: INFO: ss-1  kind-conformance-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  }]
Jan 23 00:24:49.199: INFO: ss-2  kind-conformance-worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  }]
Jan 23 00:24:49.199: INFO: 
Jan 23 00:24:49.199: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 00:24:50.202: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jan 23 00:24:50.202: INFO: ss-0  kind-conformance-worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  }]
Jan 23 00:24:50.202: INFO: ss-2  kind-conformance-worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:27 +0000 UTC  }]
Jan 23 00:24:50.202: INFO: 
Jan 23 00:24:50.202: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 23 00:24:51.206: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jan 23 00:24:51.206: INFO: ss-0  kind-conformance-worker1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:24:06 +0000 UTC  }]
Jan 23 00:24:51.206: INFO: 
Jan 23 00:24:51.206: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 23 00:24:52.209: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984797191s
Jan 23 00:24:53.213: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982199334s
Jan 23 00:24:54.216: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.977859443s
Jan 23 00:24:55.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.974674447s
Jan 23 00:24:56.223: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.971176464s
Jan 23 00:24:57.227: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.777016ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-wsk7j
Jan 23 00:24:58.230: INFO: Scaling statefulset ss to 0
Jan 23 00:24:58.236: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 23 00:24:58.238: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wsk7j
Jan 23 00:24:58.240: INFO: Scaling statefulset ss to 0
Jan 23 00:24:58.246: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:24:58.248: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:24:58.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wsk7j" for this suite.
Jan 23 00:25:04.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:25:04.323: INFO: namespace: e2e-tests-statefulset-wsk7j, resource: bindings, ignored listing per whitelist
Jan 23 00:25:04.340: INFO: namespace e2e-tests-statefulset-wsk7j deletion completed in 6.080934775s

• [SLOW TEST:57.561 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:25:04.340: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 23 00:25:04.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:04.550: INFO: stderr: ""
Jan 23 00:25:04.550: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 00:25:04.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:04.622: INFO: stderr: ""
Jan 23 00:25:04.622: INFO: stdout: "update-demo-nautilus-cbqsn update-demo-nautilus-ks8f7 "
Jan 23 00:25:04.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-cbqsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:04.690: INFO: stderr: ""
Jan 23 00:25:04.690: INFO: stdout: ""
Jan 23 00:25:04.690: INFO: update-demo-nautilus-cbqsn is created but not running
Jan 23 00:25:09.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:09.772: INFO: stderr: ""
Jan 23 00:25:09.772: INFO: stdout: "update-demo-nautilus-cbqsn update-demo-nautilus-ks8f7 "
Jan 23 00:25:09.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-cbqsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:09.846: INFO: stderr: ""
Jan 23 00:25:09.846: INFO: stdout: "true"
Jan 23 00:25:09.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-cbqsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:09.926: INFO: stderr: ""
Jan 23 00:25:09.926: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:25:09.926: INFO: validating pod update-demo-nautilus-cbqsn
Jan 23 00:25:09.930: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:25:09.930: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:25:09.930: INFO: update-demo-nautilus-cbqsn is verified up and running
Jan 23 00:25:09.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-ks8f7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:10.006: INFO: stderr: ""
Jan 23 00:25:10.006: INFO: stdout: "true"
Jan 23 00:25:10.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-ks8f7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:10.079: INFO: stderr: ""
Jan 23 00:25:10.079: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 00:25:10.079: INFO: validating pod update-demo-nautilus-ks8f7
Jan 23 00:25:10.083: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 00:25:10.083: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 00:25:10.083: INFO: update-demo-nautilus-ks8f7 is verified up and running
STEP: using delete to clean up resources
Jan 23 00:25:10.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:10.162: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 00:25:10.162: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 00:25:10.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-zvnx8'
Jan 23 00:25:10.234: INFO: stderr: "No resources found.\n"
Jan 23 00:25:10.234: INFO: stdout: ""
Jan 23 00:25:10.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -l name=update-demo --namespace=e2e-tests-kubectl-zvnx8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 00:25:10.308: INFO: stderr: ""
Jan 23 00:25:10.308: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:25:10.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zvnx8" for this suite.
Jan 23 00:25:16.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:25:16.335: INFO: namespace: e2e-tests-kubectl-zvnx8, resource: bindings, ignored listing per whitelist
Jan 23 00:25:16.384: INFO: namespace e2e-tests-kubectl-zvnx8 deletion completed in 6.072594995s

• [SLOW TEST:12.044 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:25:16.384: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 00:25:16.459: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:16.460: INFO: Number of nodes with available pods: 0
Jan 23 00:25:16.460: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:25:17.465: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:17.468: INFO: Number of nodes with available pods: 0
Jan 23 00:25:17.468: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:25:18.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:18.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:18.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:19.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:19.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:19.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:20.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:20.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:20.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:21.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:21.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:21.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:22.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:22.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:22.468: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:23.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:23.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:23.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:24.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:24.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:24.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:25.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:25.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:25.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:26.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:26.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:26.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:27.463: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:27.465: INFO: Number of nodes with available pods: 1
Jan 23 00:25:27.465: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:28.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:28.467: INFO: Number of nodes with available pods: 1
Jan 23 00:25:28.467: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:29.462: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:29.466: INFO: Number of nodes with available pods: 1
Jan 23 00:25:29.466: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:25:30.464: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:30.473: INFO: Number of nodes with available pods: 2
Jan 23 00:25:30.473: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 23 00:25:30.486: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:30.492: INFO: Number of nodes with available pods: 1
Jan 23 00:25:30.492: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:25:31.496: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:31.498: INFO: Number of nodes with available pods: 1
Jan 23 00:25:31.498: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:25:32.497: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:32.500: INFO: Number of nodes with available pods: 1
Jan 23 00:25:32.500: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:25:33.496: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:25:33.499: INFO: Number of nodes with available pods: 2
Jan 23 00:25:33.499: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-28xrk, will wait for the garbage collector to delete the pods
Jan 23 00:25:33.561: INFO: Deleting DaemonSet.extensions daemon-set took: 5.048837ms
Jan 23 00:25:33.661: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164145ms
Jan 23 00:26:11.664: INFO: Number of nodes with available pods: 0
Jan 23 00:26:11.664: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 00:26:11.665: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-28xrk/daemonsets","resourceVersion":"3199"},"items":null}

Jan 23 00:26:11.667: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-28xrk/pods","resourceVersion":"3199"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:26:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-28xrk" for this suite.
Jan 23 00:26:17.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:26:17.720: INFO: namespace: e2e-tests-daemonsets-28xrk, resource: bindings, ignored listing per whitelist
Jan 23 00:26:17.734: INFO: namespace e2e-tests-daemonsets-28xrk deletion completed in 6.0609411s

• [SLOW TEST:61.350 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:26:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:26:17.789: INFO: Creating deployment "nginx-deployment"
Jan 23 00:26:17.793: INFO: Waiting for observed generation 1
Jan 23 00:26:19.797: INFO: Waiting for all required pods to come up
Jan 23 00:26:19.799: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 23 00:26:21.806: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 23 00:26:21.810: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 23 00:26:21.814: INFO: Updating deployment nginx-deployment
Jan 23 00:26:21.814: INFO: Waiting for observed generation 2
Jan 23 00:26:23.818: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 23 00:26:23.821: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 23 00:26:23.823: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 23 00:26:23.829: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 23 00:26:23.829: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 23 00:26:23.831: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 23 00:26:23.835: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 23 00:26:23.835: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 23 00:26:23.840: INFO: Updating deployment nginx-deployment
Jan 23 00:26:23.840: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 23 00:26:23.847: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 23 00:26:23.849: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 23 00:26:23.864: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-jfssp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jfssp/deployments/nginx-deployment,UID:80073095-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3473,Generation:3,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-01-23 00:26:21 +0000 UTC 2019-01-23 00:26:17 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-01-23 00:26:23 +0000 UTC 2019-01-23 00:26:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 23 00:26:23.881: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-jfssp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jfssp/replicasets/nginx-deployment-65bbdb5f8,UID:826d5e28-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3465,Generation:3,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 80073095-1ea5-11e9-9fdb-02424cc6ba99 0xc000bafe07 0xc000bafe08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 00:26:23.881: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 23 00:26:23.881: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-jfssp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jfssp/replicasets/nginx-deployment-555b55d965,UID:8008369d-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3464,Generation:3,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 80073095-1ea5-11e9-9fdb-02424cc6ba99 0xc000bafb67 0xc000bafb68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 23 00:26:23.896: INFO: Pod "nginx-deployment-555b55d965-44hzs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-44hzs,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-44hzs,UID:83a75997-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3514,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000c8b030 0xc000c8b031}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c8b090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c8b0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.896: INFO: Pod "nginx-deployment-555b55d965-72pj9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-72pj9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-72pj9,UID:83a3022d-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3515,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000c8b1e0 0xc000c8b1e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c8b250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c8b270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-01-23 00:26:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.896: INFO: Pod "nginx-deployment-555b55d965-7v2vt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7v2vt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-7v2vt,UID:800a962b-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3373,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000c8b350 0xc000c8b351}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c8b540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c8b560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.7,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://26f80fc91d068760bb8a45b0dc4bb9d9374ec35f8054afab6103e969c377c272}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.896: INFO: Pod "nginx-deployment-555b55d965-b87r7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b87r7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-b87r7,UID:83a576c4-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3500,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000c8b620 0xc000c8b621}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa0020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-bpwjt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bpwjt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-bpwjt,UID:83a578fa-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3503,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0090 0xc000aa0091}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa00f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa0110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-crwct" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-crwct,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-crwct,UID:83a3b848-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3489,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0190 0xc000aa0191}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa02a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-dkhd4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dkhd4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-dkhd4,UID:83a56a2b-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3497,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0470 0xc000aa0471}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa04d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa04f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-flbpw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-flbpw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-flbpw,UID:83a76bae-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3517,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0650 0xc000aa0651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa06b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa06d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-ftvwc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ftvwc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-ftvwc,UID:83a3c106-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3491,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0760 0xc000aa0761}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa0be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-glxb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-glxb5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-glxb5,UID:83a57734-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3502,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0c50 0xc000aa0c51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa0ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-hjp2m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hjp2m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-hjp2m,UID:80094207-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3337,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0db0 0xc000aa0db1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa0e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.3,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://b308a28904899729833d0bc845f8113e2d909f8944f4d6ca325f98c2c6009254}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-j52z4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j52z4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-j52z4,UID:83a73922-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3513,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa0f50 0xc000aa0f51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa0fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa1000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-j7sm7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j7sm7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-j7sm7,UID:800a90f5-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3357,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa1070 0xc000aa1071}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa10d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa10f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.5,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://8ddebeb9f10fe46559750bcffcf1da82fa448dc017a2e583905ec8f967631f32}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-n94qg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-n94qg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-n94qg,UID:800a9993-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3348,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa1250 0xc000aa1251}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa12b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa12d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.6,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://b488c07b6a5fbb061d9842cf9288c4ffd83923c8c9f1e65222ceb5865da64cc0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-rf9rm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rf9rm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-rf9rm,UID:800ccb24-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3361,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa1590 0xc000aa1591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa15f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa1610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.8,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://e8e80ba6c0fd975493b6f3c2519614ccba0cef9572f29381796ae323137d3c9a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-sbz9c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sbz9c,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-sbz9c,UID:800a9470-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3375,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc000aa16d0 0xc000aa16d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aa1730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aa1750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.5,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://76be594daa3dafa8073c756cbfc454fbb12d816fc3c8f243d37971962384a404}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.897: INFO: Pod "nginx-deployment-555b55d965-sf64t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sf64t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-sf64t,UID:83a76441-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3518,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e0b0 0xc00139e0b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-555b55d965-sfnmv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sfnmv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-sfnmv,UID:83a762f2-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3516,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e1e0 0xc00139e1e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e2b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-555b55d965-w5w2r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-w5w2r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-w5w2r,UID:800cc033-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3341,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e320 0xc00139e321}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e3a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.6,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://f7e550bb8e4600e325d4265193dc3f9c5c028a5cad6d6e7701c89bcc53dadb8d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-555b55d965-x9lzg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x9lzg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-555b55d965-x9lzg,UID:8009ab20-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3379,Generation:0,CreationTimestamp:2019-01-23 00:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8008369d-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e460 0xc00139e461}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.4,StartTime:2019-01-23 00:26:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:26:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://c309cb79eba65380d54bec590ee7dea01cf3dd50bcc6d9b6a95fa529050aad60}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-65bbdb5f8-2dkss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2dkss,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-2dkss,UID:83a561f0-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3499,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e610 0xc00139e611}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-65bbdb5f8-5xb6r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-5xb6r,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-5xb6r,UID:83a5576a-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3498,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e720 0xc00139e721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-65bbdb5f8-8b7zp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8b7zp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-8b7zp,UID:83a51f93-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3504,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e820 0xc00139e821}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e8b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.898: INFO: Pod "nginx-deployment-65bbdb5f8-cbmhm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cbmhm,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-cbmhm,UID:83a67bb0-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3512,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139e920 0xc00139e921}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139e990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139e9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-ct47g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ct47g,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-ct47g,UID:83a3b11c-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3475,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139ea20 0xc00139ea21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139ea90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139eab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-cwhgr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cwhgr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-cwhgr,UID:83a57643-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3501,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139eb20 0xc00139eb21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139eb90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139ebb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-dxkbz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dxkbz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-dxkbz,UID:826db625-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3419,Generation:0,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139ec20 0xc00139ec21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139ec90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139ecb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:,StartTime:2019-01-23 00:26:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-jzkvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jzkvc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-jzkvc,UID:83a423b9-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3493,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139ed70 0xc00139ed71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139f5b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139f5d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-l6tb6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l6tb6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-l6tb6,UID:82724d29-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3439,Generation:0,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139f640 0xc00139f641}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139f6b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139f6d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-01-23 00:26:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-rm8nw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rm8nw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-rm8nw,UID:83a41b81-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3492,Generation:0,CreationTimestamp:2019-01-23 00:26:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139f810 0xc00139f811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139f880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139f8a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.899: INFO: Pod "nginx-deployment-65bbdb5f8-s5dlr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-s5dlr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-s5dlr,UID:826e3871-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3437,Generation:0,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139f910 0xc00139f911}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139fa00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139fa20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:,StartTime:2019-01-23 00:26:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.900: INFO: Pod "nginx-deployment-65bbdb5f8-s6jqd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-s6jqd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-s6jqd,UID:826e342f-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3424,Generation:0,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139fae0 0xc00139fae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139fbc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139fbe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-01-23 00:26:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 23 00:26:23.900: INFO: Pod "nginx-deployment-65bbdb5f8-x6km6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-x6km6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-jfssp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jfssp/pods/nginx-deployment-65bbdb5f8-x6km6,UID:8272fe77-1ea5-11e9-9fdb-02424cc6ba99,ResourceVersion:3440,Generation:0,CreationTimestamp:2019-01-23 00:26:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 826d5e28-1ea5-11e9-9fdb-02424cc6ba99 0xc00139fca0 0xc00139fca1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-knqkl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knqkl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-knqkl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00139fd10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00139fda0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:26:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:,StartTime:2019-01-23 00:26:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:26:23.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jfssp" for this suite.
Jan 23 00:26:29.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:26:29.984: INFO: namespace: e2e-tests-deployment-jfssp, resource: bindings, ignored listing per whitelist
Jan 23 00:26:29.992: INFO: namespace e2e-tests-deployment-jfssp deletion completed in 6.082545617s

• [SLOW TEST:12.258 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:26:29.992: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 23 00:26:36.054: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-8753a125-1ea5-11e9-b567-9e07e353f0d4", GenerateName:"", Namespace:"e2e-tests-pods-scp9z", SelfLink:"/api/v1/namespaces/e2e-tests-pods-scp9z/pods/pod-submit-remove-8753a125-1ea5-11e9-b567-9e07e353f0d4", UID:"875489cf-1ea5-11e9-9fdb-02424cc6ba99", ResourceVersion:"3792", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683799990, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"35285821"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qxbsl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001610380), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qxbsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001dce978), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-conformance-worker1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001bd99e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001dce9b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001dce9d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001dce9d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001dce9dc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683799990, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683799993, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683799993, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683799990, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.9.3", PodIP:"10.40.0.4", StartTime:(*v1.Time)(0xc001c5ea60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001c5ea80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96", ContainerID:"docker://b08a9e0dfc9a52b82cb722e7094bf8569e26581c7f2952b7d6eff636d8e66cc1"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 23 00:26:41.064: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:26:41.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-scp9z" for this suite.
Jan 23 00:26:47.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:26:47.095: INFO: namespace: e2e-tests-pods-scp9z, resource: bindings, ignored listing per whitelist
Jan 23 00:26:47.141: INFO: namespace e2e-tests-pods-scp9z deletion completed in 6.070453648s

• [SLOW TEST:17.148 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:26:47.141: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 00:26:47.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-4zhwt'
Jan 23 00:26:47.279: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 00:26:47.280: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Jan 23 00:26:49.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-4zhwt'
Jan 23 00:26:49.376: INFO: stderr: ""
Jan 23 00:26:49.376: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:26:49.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4zhwt" for this suite.
Jan 23 00:28:09.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:28:09.411: INFO: namespace: e2e-tests-kubectl-4zhwt, resource: bindings, ignored listing per whitelist
Jan 23 00:28:09.438: INFO: namespace e2e-tests-kubectl-4zhwt deletion completed in 1m20.060342431s

• [SLOW TEST:82.297 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:28:09.438: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-c29d9dc3-1ea5-11e9-b567-9e07e353f0d4
STEP: Creating secret with name secret-projected-all-test-volume-c29d9db7-1ea5-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 23 00:28:09.514: INFO: Waiting up to 5m0s for pod "projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-tlpjr" to be "success or failure"
Jan 23 00:28:09.516: INFO: Pod "projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660778ms
Jan 23 00:28:11.519: INFO: Pod "projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005005658s
Jan 23 00:28:13.523: INFO: Pod "projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008667122s
STEP: Saw pod success
Jan 23 00:28:13.523: INFO: Pod "projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:28:13.525: INFO: Trying to get logs from node kind-conformance-worker1 pod projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 23 00:28:13.542: INFO: Waiting for pod projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:28:13.544: INFO: Pod projected-volume-c29d9d86-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:28:13.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tlpjr" for this suite.
Jan 23 00:28:19.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:28:19.607: INFO: namespace: e2e-tests-projected-tlpjr, resource: bindings, ignored listing per whitelist
Jan 23 00:28:19.617: INFO: namespace e2e-tests-projected-tlpjr deletion completed in 6.069965812s

• [SLOW TEST:10.178 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:28:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:28:19.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-dnfcq" to be "success or failure"
Jan 23 00:28:19.685: INFO: Pod "downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.614532ms
Jan 23 00:28:21.689: INFO: Pod "downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011492548s
Jan 23 00:28:23.692: INFO: Pod "downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01478918s
STEP: Saw pod success
Jan 23 00:28:23.692: INFO: Pod "downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:28:23.695: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:28:23.708: INFO: Waiting for pod downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:28:23.710: INFO: Pod downwardapi-volume-c8ac88f4-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:28:23.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dnfcq" for this suite.
Jan 23 00:28:29.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:28:29.781: INFO: namespace: e2e-tests-downward-api-dnfcq, resource: bindings, ignored listing per whitelist
Jan 23 00:28:29.783: INFO: namespace e2e-tests-downward-api-dnfcq deletion completed in 6.071185435s

• [SLOW TEST:10.166 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:28:29.784: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:28:29.861: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Jan 23 00:28:29.864: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-zlccf/daemonsets","resourceVersion":"4070"},"items":null}

Jan 23 00:28:29.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-zlccf/pods","resourceVersion":"4070"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:28:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-zlccf" for this suite.
Jan 23 00:28:35.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:28:35.920: INFO: namespace: e2e-tests-daemonsets-zlccf, resource: bindings, ignored listing per whitelist
Jan 23 00:28:35.933: INFO: namespace e2e-tests-daemonsets-zlccf deletion completed in 6.061326037s

S [SKIPPING] [6.149 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Jan 23 00:28:29.861: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:28:35.933: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:28:36.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-859fj" to be "success or failure"
Jan 23 00:28:36.040: INFO: Pod "downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657676ms
Jan 23 00:28:38.043: INFO: Pod "downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005281482s
STEP: Saw pod success
Jan 23 00:28:38.043: INFO: Pod "downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:28:38.046: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:28:38.059: INFO: Waiting for pod downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:28:38.061: INFO: Pod downwardapi-volume-d26d5a8d-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:28:38.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-859fj" for this suite.
Jan 23 00:28:44.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:28:44.095: INFO: namespace: e2e-tests-downward-api-859fj, resource: bindings, ignored listing per whitelist
Jan 23 00:28:44.137: INFO: namespace e2e-tests-downward-api-859fj deletion completed in 6.074349157s

• [SLOW TEST:8.204 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:28:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Jan 23 00:28:44.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-bmmfv'
Jan 23 00:28:44.651: INFO: stderr: ""
Jan 23 00:28:44.651: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Jan 23 00:28:45.654: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:45.654: INFO: Found 0 / 1
Jan 23 00:28:46.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:46.655: INFO: Found 0 / 1
Jan 23 00:28:47.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:47.655: INFO: Found 0 / 1
Jan 23 00:28:48.656: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:48.656: INFO: Found 0 / 1
Jan 23 00:28:49.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:49.655: INFO: Found 0 / 1
Jan 23 00:28:50.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:50.655: INFO: Found 0 / 1
Jan 23 00:28:51.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:51.655: INFO: Found 0 / 1
Jan 23 00:28:52.655: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:52.655: INFO: Found 0 / 1
Jan 23 00:28:53.654: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:53.655: INFO: Found 1 / 1
Jan 23 00:28:53.655: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 00:28:53.657: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:28:53.657: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 23 00:28:53.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 logs redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv'
Jan 23 00:28:53.742: INFO: stderr: ""
Jan 23 00:28:53.742: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jan 00:28:52.258 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jan 00:28:52.258 # Server started, Redis version 3.2.12\n1:M 23 Jan 00:28:52.258 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jan 00:28:52.258 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 23 00:28:53.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 log redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv --tail=1'
Jan 23 00:28:53.817: INFO: stderr: ""
Jan 23 00:28:53.817: INFO: stdout: "1:M 23 Jan 00:28:52.258 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 23 00:28:53.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 log redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv --limit-bytes=1'
Jan 23 00:28:53.912: INFO: stderr: ""
Jan 23 00:28:53.912: INFO: stdout: " "
STEP: exposing timestamps
Jan 23 00:28:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 log redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv --tail=1 --timestamps'
Jan 23 00:28:53.991: INFO: stderr: ""
Jan 23 00:28:53.991: INFO: stdout: "2019-01-23T00:28:52.258680887Z 1:M 23 Jan 00:28:52.258 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 23 00:28:56.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 log redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv --since=1s'
Jan 23 00:28:56.579: INFO: stderr: ""
Jan 23 00:28:56.579: INFO: stdout: ""
Jan 23 00:28:56.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 log redis-master-mm6nr redis-master --namespace=e2e-tests-kubectl-bmmfv --since=24h'
Jan 23 00:28:56.673: INFO: stderr: ""
Jan 23 00:28:56.674: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jan 00:28:52.258 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jan 00:28:52.258 # Server started, Redis version 3.2.12\n1:M 23 Jan 00:28:52.258 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jan 00:28:52.258 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Jan 23 00:28:56.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-bmmfv'
Jan 23 00:28:56.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 00:28:56.745: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 23 00:28:56.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-bmmfv'
Jan 23 00:28:56.826: INFO: stderr: "No resources found.\n"
Jan 23 00:28:56.826: INFO: stdout: ""
Jan 23 00:28:56.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -l name=nginx --namespace=e2e-tests-kubectl-bmmfv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 00:28:56.905: INFO: stderr: ""
Jan 23 00:28:56.905: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:28:56.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bmmfv" for this suite.
Jan 23 00:29:18.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:29:18.928: INFO: namespace: e2e-tests-kubectl-bmmfv, resource: bindings, ignored listing per whitelist
Jan 23 00:29:18.986: INFO: namespace e2e-tests-kubectl-bmmfv deletion completed in 22.078827455s

• [SLOW TEST:34.849 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:29:18.987: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-qq6wj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qq6wj to expose endpoints map[]
Jan 23 00:29:19.059: INFO: Get endpoints failed (8.489182ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 23 00:29:20.063: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qq6wj exposes endpoints map[] (1.011770239s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-qq6wj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qq6wj to expose endpoints map[pod1:[80]]
Jan 23 00:29:22.089: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qq6wj exposes endpoints map[pod1:[80]] (2.020154597s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-qq6wj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qq6wj to expose endpoints map[pod1:[80] pod2:[80]]
Jan 23 00:29:24.127: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qq6wj exposes endpoints map[pod2:[80] pod1:[80]] (2.034352512s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-qq6wj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qq6wj to expose endpoints map[pod2:[80]]
Jan 23 00:29:25.147: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qq6wj exposes endpoints map[pod2:[80]] (1.01758326s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-qq6wj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qq6wj to expose endpoints map[]
Jan 23 00:29:26.159: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qq6wj exposes endpoints map[] (1.008397541s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:29:26.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-qq6wj" for this suite.
Jan 23 00:29:48.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:29:48.251: INFO: namespace: e2e-tests-services-qq6wj, resource: bindings, ignored listing per whitelist
Jan 23 00:29:48.256: INFO: namespace e2e-tests-services-qq6wj deletion completed in 22.073099202s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:29.269 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:29:48.256: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-fd8189ff-1ea5-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 00:29:48.312: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-ht7dc" to be "success or failure"
Jan 23 00:29:48.321: INFO: Pod "pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.087137ms
Jan 23 00:29:50.324: INFO: Pod "pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012164802s
STEP: Saw pod success
Jan 23 00:29:50.324: INFO: Pod "pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:29:50.326: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 00:29:50.344: INFO: Waiting for pod pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:29:50.346: INFO: Pod pod-projected-secrets-fd81c406-1ea5-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:29:50.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ht7dc" for this suite.
Jan 23 00:29:56.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:29:56.427: INFO: namespace: e2e-tests-projected-ht7dc, resource: bindings, ignored listing per whitelist
Jan 23 00:29:56.427: INFO: namespace e2e-tests-projected-ht7dc deletion completed in 6.079849465s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:29:56.428: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:29:56.476: INFO: (0) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.075022ms)
Jan 23 00:29:56.478: INFO: (1) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.095287ms)
Jan 23 00:29:56.480: INFO: (2) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.774243ms)
Jan 23 00:29:56.482: INFO: (3) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.769977ms)
Jan 23 00:29:56.483: INFO: (4) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.525375ms)
Jan 23 00:29:56.485: INFO: (5) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.541888ms)
Jan 23 00:29:56.487: INFO: (6) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.785347ms)
Jan 23 00:29:56.489: INFO: (7) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.411494ms)
Jan 23 00:29:56.491: INFO: (8) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.146346ms)
Jan 23 00:29:56.493: INFO: (9) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.108976ms)
Jan 23 00:29:56.495: INFO: (10) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.830783ms)
Jan 23 00:29:56.498: INFO: (11) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.015872ms)
Jan 23 00:29:56.501: INFO: (12) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.652679ms)
Jan 23 00:29:56.503: INFO: (13) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.048604ms)
Jan 23 00:29:56.505: INFO: (14) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.783203ms)
Jan 23 00:29:56.507: INFO: (15) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.035737ms)
Jan 23 00:29:56.511: INFO: (16) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.097462ms)
Jan 23 00:29:56.513: INFO: (17) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.152444ms)
Jan 23 00:29:56.515: INFO: (18) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.730751ms)
Jan 23 00:29:56.517: INFO: (19) /api/v1/nodes/kind-conformance-worker1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.451957ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:29:56.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-djg95" for this suite.
Jan 23 00:30:02.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:30:02.540: INFO: namespace: e2e-tests-proxy-djg95, resource: bindings, ignored listing per whitelist
Jan 23 00:30:02.588: INFO: namespace e2e-tests-proxy-djg95 deletion completed in 6.070345096s

• [SLOW TEST:6.161 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:30:02.589: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 23 00:30:02.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-r8ts6'
Jan 23 00:30:02.787: INFO: stderr: ""
Jan 23 00:30:02.787: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 23 00:30:03.790: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:30:03.790: INFO: Found 0 / 1
Jan 23 00:30:04.791: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:30:04.791: INFO: Found 0 / 1
Jan 23 00:30:05.791: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:30:05.791: INFO: Found 1 / 1
Jan 23 00:30:05.791: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 23 00:30:05.793: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:30:05.793: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 00:30:05.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 patch pod redis-master-7c7h2 --namespace=e2e-tests-kubectl-r8ts6 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 23 00:30:05.881: INFO: stderr: ""
Jan 23 00:30:05.881: INFO: stdout: "pod/redis-master-7c7h2 patched\n"
STEP: checking annotations
Jan 23 00:30:05.884: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 00:30:05.884: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:30:05.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r8ts6" for this suite.
Jan 23 00:30:25.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:30:25.907: INFO: namespace: e2e-tests-kubectl-r8ts6, resource: bindings, ignored listing per whitelist
Jan 23 00:30:25.944: INFO: namespace e2e-tests-kubectl-r8ts6 deletion completed in 20.058825249s

• [SLOW TEST:23.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:30:25.944: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Jan 23 00:30:26.000: INFO: Waiting up to 5m0s for pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-containers-xgtmg" to be "success or failure"
Jan 23 00:30:26.002: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535391ms
Jan 23 00:30:28.005: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004733521s
Jan 23 00:30:30.008: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008092373s
Jan 23 00:30:32.011: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011245011s
Jan 23 00:30:34.014: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014256608s
Jan 23 00:30:36.017: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017037363s
STEP: Saw pod success
Jan 23 00:30:36.017: INFO: Pod "client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:30:36.019: INFO: Trying to get logs from node kind-conformance-worker2 pod client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:30:36.037: INFO: Waiting for pod client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:30:36.039: INFO: Pod client-containers-13f88e3c-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:30:36.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-xgtmg" for this suite.
Jan 23 00:30:42.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:30:42.060: INFO: namespace: e2e-tests-containers-xgtmg, resource: bindings, ignored listing per whitelist
Jan 23 00:30:42.114: INFO: namespace e2e-tests-containers-xgtmg deletion completed in 6.072446996s

• [SLOW TEST:16.169 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:30:42.114: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 23 00:30:42.180: INFO: Waiting up to 5m0s for pod "downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-fnld7" to be "success or failure"
Jan 23 00:30:42.182: INFO: Pod "downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830555ms
Jan 23 00:30:44.185: INFO: Pod "downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004731817s
Jan 23 00:30:46.188: INFO: Pod "downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007527496s
STEP: Saw pod success
Jan 23 00:30:46.188: INFO: Pod "downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:30:46.190: INFO: Trying to get logs from node kind-conformance-worker2 pod downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 00:30:46.205: INFO: Waiting for pod downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:30:46.207: INFO: Pod downward-api-1d9d33d9-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:30:46.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fnld7" for this suite.
Jan 23 00:30:52.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:30:52.275: INFO: namespace: e2e-tests-downward-api-fnld7, resource: bindings, ignored listing per whitelist
Jan 23 00:30:52.282: INFO: namespace e2e-tests-downward-api-fnld7 deletion completed in 6.073059904s

• [SLOW TEST:10.168 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:30:52.282: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-23ab7a5a-1ea6-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 00:30:52.343: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-gt8lx" to be "success or failure"
Jan 23 00:30:52.344: INFO: Pod "pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5372ms
Jan 23 00:30:54.347: INFO: Pod "pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003973474s
Jan 23 00:30:56.349: INFO: Pod "pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006414225s
STEP: Saw pod success
Jan 23 00:30:56.349: INFO: Pod "pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:30:56.351: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 00:30:56.365: INFO: Waiting for pod pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:30:56.370: INFO: Pod pod-projected-secrets-23abd77d-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:30:56.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gt8lx" for this suite.
Jan 23 00:31:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:31:02.407: INFO: namespace: e2e-tests-projected-gt8lx, resource: bindings, ignored listing per whitelist
Jan 23 00:31:02.447: INFO: namespace e2e-tests-projected-gt8lx deletion completed in 6.074366139s

• [SLOW TEST:10.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:31:02.447: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Jan 23 00:31:02.507: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-jwv7f" to be "success or failure"
Jan 23 00:31:02.510: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601805ms
Jan 23 00:31:04.513: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006525313s
Jan 23 00:31:06.521: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014042231s
STEP: Saw pod success
Jan 23 00:31:06.521: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 23 00:31:06.523: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 23 00:31:06.543: INFO: Waiting for pod pod-host-path-test to disappear
Jan 23 00:31:06.545: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:31:06.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-jwv7f" for this suite.
Jan 23 00:31:12.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:31:12.603: INFO: namespace: e2e-tests-hostpath-jwv7f, resource: bindings, ignored listing per whitelist
Jan 23 00:31:12.605: INFO: namespace e2e-tests-hostpath-jwv7f deletion completed in 6.058712451s

• [SLOW TEST:10.158 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:31:12.606: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:31:12.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-fljk6" to be "success or failure"
Jan 23 00:31:12.661: INFO: Pod "downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.265872ms
Jan 23 00:31:14.664: INFO: Pod "downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006559271s
Jan 23 00:31:16.667: INFO: Pod "downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009763787s
STEP: Saw pod success
Jan 23 00:31:16.667: INFO: Pod "downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:31:16.669: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:31:16.683: INFO: Waiting for pod downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:31:16.685: INFO: Pod downwardapi-volume-2fc7f96b-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:31:16.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fljk6" for this suite.
Jan 23 00:31:22.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:31:22.735: INFO: namespace: e2e-tests-downward-api-fljk6, resource: bindings, ignored listing per whitelist
Jan 23 00:31:22.752: INFO: namespace e2e-tests-downward-api-fljk6 deletion completed in 6.064904572s

• [SLOW TEST:10.147 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:31:22.752: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 23 00:31:31.827: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:31:32.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-jj5nb" for this suite.
Jan 23 00:31:54.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:31:54.887: INFO: namespace: e2e-tests-replicaset-jj5nb, resource: bindings, ignored listing per whitelist
Jan 23 00:31:54.916: INFO: namespace e2e-tests-replicaset-jj5nb deletion completed in 22.071789559s

• [SLOW TEST:32.164 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:31:54.916: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 23 00:31:54.978: INFO: Waiting up to 5m0s for pod "pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-gph2v" to be "success or failure"
Jan 23 00:31:54.980: INFO: Pod "pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.773803ms
Jan 23 00:31:56.983: INFO: Pod "pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00486765s
Jan 23 00:31:58.986: INFO: Pod "pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008406455s
STEP: Saw pod success
Jan 23 00:31:58.986: INFO: Pod "pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:31:58.989: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:31:59.006: INFO: Waiting for pod pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:31:59.008: INFO: Pod pod-4900e92d-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:31:59.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gph2v" for this suite.
Jan 23 00:32:05.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:32:05.042: INFO: namespace: e2e-tests-emptydir-gph2v, resource: bindings, ignored listing per whitelist
Jan 23 00:32:05.082: INFO: namespace e2e-tests-emptydir-gph2v deletion completed in 6.070756s

• [SLOW TEST:10.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:32:05.082: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:32:05.153: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 00:32:05.157: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:05.159: INFO: Number of nodes with available pods: 0
Jan 23 00:32:05.159: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:32:06.163: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:06.166: INFO: Number of nodes with available pods: 0
Jan 23 00:32:06.166: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:32:07.163: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:07.167: INFO: Number of nodes with available pods: 0
Jan 23 00:32:07.167: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:32:08.163: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:08.167: INFO: Number of nodes with available pods: 2
Jan 23 00:32:08.167: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 23 00:32:08.181: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:08.181: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:08.186: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:09.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:09.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:09.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:10.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:10.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:10.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:11.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:11.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:11.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:12.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:12.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:12.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:13.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:13.190: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:13.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:14.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:14.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:14.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:15.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:15.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:15.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:16.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:16.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:16.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:17.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:17.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:17.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:18.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:18.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:18.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:19.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:19.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:19.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:20.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:20.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:20.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:21.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:21.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:21.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:22.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:22.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:22.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:23.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:23.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:23.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:24.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:24.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:24.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:25.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:25.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:25.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:26.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:26.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:26.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:27.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:27.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:27.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:28.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:28.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:28.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:29.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:29.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:29.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:30.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:30.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:30.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:31.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:31.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:31.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:32.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:32.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:32.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:33.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:33.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:33.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:34.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:34.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:34.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:35.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:35.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:35.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:36.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:36.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:36.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:37.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:37.188: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:37.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:38.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:38.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:38.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:39.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:39.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:39.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:40.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:40.189: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:40.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:41.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:41.190: INFO: Wrong image for pod: daemon-set-cgwkp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:41.190: INFO: Pod daemon-set-cgwkp is not available
Jan 23 00:32:41.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:42.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:42.189: INFO: Pod daemon-set-g7vx6 is not available
Jan 23 00:32:42.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:43.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:43.190: INFO: Pod daemon-set-g7vx6 is not available
Jan 23 00:32:43.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:44.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:44.188: INFO: Pod daemon-set-g7vx6 is not available
Jan 23 00:32:44.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:45.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:45.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:46.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:46.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:47.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:47.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:48.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:48.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:49.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:49.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:50.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:50.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:51.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:51.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:52.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:52.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:53.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:53.191: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:54.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:54.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:55.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:55.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:56.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:56.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:57.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:57.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:58.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:58.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:32:59.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:32:59.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:00.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:00.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:01.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:01.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:02.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:02.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:03.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:03.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:04.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:04.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:05.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:05.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:06.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:06.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:07.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:07.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:08.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:08.194: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:09.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:09.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:10.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:10.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:11.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:11.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:12.192: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:12.195: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:13.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:13.194: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:14.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:14.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:15.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:15.190: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:16.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:16.352: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:17.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:17.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:17.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:18.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:18.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:18.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:19.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:19.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:19.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:20.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:20.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:20.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:21.190: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:21.190: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:21.193: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:22.188: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:22.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:22.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:23.189: INFO: Wrong image for pod: daemon-set-4trtk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 23 00:33:23.189: INFO: Pod daemon-set-4trtk is not available
Jan 23 00:33:23.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:24.189: INFO: Pod daemon-set-pz44f is not available
Jan 23 00:33:24.192: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 23 00:33:24.196: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:24.198: INFO: Number of nodes with available pods: 1
Jan 23 00:33:24.198: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:33:25.201: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:25.204: INFO: Number of nodes with available pods: 1
Jan 23 00:33:25.204: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:33:26.202: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:33:26.204: INFO: Number of nodes with available pods: 2
Jan 23 00:33:26.204: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-qrqph, will wait for the garbage collector to delete the pods
Jan 23 00:33:26.270: INFO: Deleting DaemonSet.extensions daemon-set took: 5.859403ms
Jan 23 00:33:26.370: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.172218ms
Jan 23 00:33:33.773: INFO: Number of nodes with available pods: 0
Jan 23 00:33:33.773: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 00:33:33.776: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qrqph/daemonsets","resourceVersion":"5072"},"items":null}

Jan 23 00:33:33.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qrqph/pods","resourceVersion":"5072"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:33:33.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qrqph" for this suite.
Jan 23 00:33:39.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:33:39.928: INFO: namespace: e2e-tests-daemonsets-qrqph, resource: bindings, ignored listing per whitelist
Jan 23 00:33:39.935: INFO: namespace e2e-tests-daemonsets-qrqph deletion completed in 6.149947246s

• [SLOW TEST:94.853 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:33:39.935: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-ghcbv
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Jan 23 00:33:39.996: INFO: Found 0 stateful pods, waiting for 3
Jan 23 00:33:49.999: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:33:49.999: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:33:49.999: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 23 00:33:50.023: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 23 00:34:00.051: INFO: Updating stateful set ss2
Jan 23 00:34:00.062: INFO: Waiting for Pod e2e-tests-statefulset-ghcbv/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jan 23 00:34:10.096: INFO: Found 1 stateful pods, waiting for 3
Jan 23 00:34:20.100: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:34:20.100: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:34:20.100: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 23 00:34:30.100: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:34:30.100: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 00:34:30.100: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 23 00:34:30.122: INFO: Updating stateful set ss2
Jan 23 00:34:30.131: INFO: Waiting for Pod e2e-tests-statefulset-ghcbv/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 23 00:34:40.155: INFO: Updating stateful set ss2
Jan 23 00:34:40.171: INFO: Waiting for StatefulSet e2e-tests-statefulset-ghcbv/ss2 to complete update
Jan 23 00:34:40.171: INFO: Waiting for Pod e2e-tests-statefulset-ghcbv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 23 00:34:50.178: INFO: Waiting for StatefulSet e2e-tests-statefulset-ghcbv/ss2 to complete update
Jan 23 00:34:50.178: INFO: Waiting for Pod e2e-tests-statefulset-ghcbv/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 23 00:35:00.178: INFO: Deleting all statefulset in ns e2e-tests-statefulset-ghcbv
Jan 23 00:35:00.181: INFO: Scaling statefulset ss2 to 0
Jan 23 00:35:30.192: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 00:35:30.194: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:35:30.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-ghcbv" for this suite.
Jan 23 00:35:36.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:35:36.262: INFO: namespace: e2e-tests-statefulset-ghcbv, resource: bindings, ignored listing per whitelist
Jan 23 00:35:36.263: INFO: namespace e2e-tests-statefulset-ghcbv deletion completed in 6.058553777s

• [SLOW TEST:116.328 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:35:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:35:36.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-85hjz" to be "success or failure"
Jan 23 00:35:36.317: INFO: Pod "downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652647ms
Jan 23 00:35:38.319: INFO: Pod "downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005819368s
STEP: Saw pod success
Jan 23 00:35:38.319: INFO: Pod "downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:35:38.320: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:35:38.333: INFO: Waiting for pod downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:35:38.335: INFO: Pod downwardapi-volume-cceea64d-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:35:38.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-85hjz" for this suite.
Jan 23 00:35:44.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:35:44.379: INFO: namespace: e2e-tests-downward-api-85hjz, resource: bindings, ignored listing per whitelist
Jan 23 00:35:44.391: INFO: namespace e2e-tests-downward-api-85hjz deletion completed in 6.054109392s

• [SLOW TEST:8.128 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:35:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 23 00:35:44.435: INFO: Waiting up to 5m0s for pod "downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-p8crc" to be "success or failure"
Jan 23 00:35:44.437: INFO: Pod "downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695352ms
Jan 23 00:35:46.439: INFO: Pod "downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004318326s
STEP: Saw pod success
Jan 23 00:35:46.439: INFO: Pod "downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:35:46.441: INFO: Trying to get logs from node kind-conformance-worker2 pod downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 00:35:46.456: INFO: Waiting for pod downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:35:46.457: INFO: Pod downward-api-d1c532a4-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:35:46.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p8crc" for this suite.
Jan 23 00:35:52.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:35:52.472: INFO: namespace: e2e-tests-downward-api-p8crc, resource: bindings, ignored listing per whitelist
Jan 23 00:35:52.512: INFO: namespace e2e-tests-downward-api-p8crc deletion completed in 6.052883754s

• [SLOW TEST:8.121 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:35:52.512: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-d69ee62f-1ea6-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d69ee62f-1ea6-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:35:56.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6pbkz" for this suite.
Jan 23 00:36:18.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:36:18.674: INFO: namespace: e2e-tests-configmap-6pbkz, resource: bindings, ignored listing per whitelist
Jan 23 00:36:18.678: INFO: namespace e2e-tests-configmap-6pbkz deletion completed in 22.080086957s

• [SLOW TEST:26.166 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:36:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 00:36:18.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-l2qw2'
Jan 23 00:36:18.807: INFO: stderr: ""
Jan 23 00:36:18.807: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 23 00:36:23.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-l2qw2 -o json'
Jan 23 00:36:23.925: INFO: stderr: ""
Jan 23 00:36:23.925: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-01-23T00:36:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-l2qw2\",\n        \"resourceVersion\": \"5740\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-l2qw2/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e6417671-1ea6-11e9-9fdb-02424cc6ba99\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9ns8z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-conformance-worker2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9ns8z\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9ns8z\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-23T00:36:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-23T00:36:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-23T00:36:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-23T00:36:18Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://d753d986cd189bb7af16fec1d10a26a1382183d14fd9b0e2459b837163f0ea0b\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-01-23T00:36:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.9.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.0.3\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-01-23T00:36:18Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 23 00:36:23.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 replace -f - --namespace=e2e-tests-kubectl-l2qw2'
Jan 23 00:36:24.083: INFO: stderr: ""
Jan 23 00:36:24.083: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Jan 23 00:36:24.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-l2qw2'
Jan 23 00:36:25.714: INFO: stderr: ""
Jan 23 00:36:25.714: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:36:25.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l2qw2" for this suite.
Jan 23 00:36:31.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:36:31.755: INFO: namespace: e2e-tests-kubectl-l2qw2, resource: bindings, ignored listing per whitelist
Jan 23 00:36:31.770: INFO: namespace e2e-tests-kubectl-l2qw2 deletion completed in 6.053432627s

• [SLOW TEST:13.092 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:36:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Jan 23 00:36:31.815: INFO: Waiting up to 5m0s for pod "var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-var-expansion-6jrcp" to be "success or failure"
Jan 23 00:36:31.816: INFO: Pod "var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.449417ms
Jan 23 00:36:33.819: INFO: Pod "var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004182948s
STEP: Saw pod success
Jan 23 00:36:33.819: INFO: Pod "var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:36:33.821: INFO: Trying to get logs from node kind-conformance-worker1 pod var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 00:36:33.835: INFO: Waiting for pod var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:36:33.837: INFO: Pod var-expansion-ee0394e9-1ea6-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:36:33.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-6jrcp" for this suite.
Jan 23 00:36:39.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:36:39.898: INFO: namespace: e2e-tests-var-expansion-6jrcp, resource: bindings, ignored listing per whitelist
Jan 23 00:36:39.911: INFO: namespace e2e-tests-var-expansion-6jrcp deletion completed in 6.072849353s

• [SLOW TEST:8.141 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:36:39.912: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:37:06.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-wnrgl" for this suite.
Jan 23 00:37:12.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:37:12.178: INFO: namespace: e2e-tests-container-runtime-wnrgl, resource: bindings, ignored listing per whitelist
Jan 23 00:37:12.185: INFO: namespace e2e-tests-container-runtime-wnrgl deletion completed in 6.055484645s

• [SLOW TEST:32.274 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:37:12.186: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-061b0722-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:37:12.249: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-jbjph" to be "success or failure"
Jan 23 00:37:12.251: INFO: Pod "pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671415ms
Jan 23 00:37:14.254: INFO: Pod "pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004462556s
Jan 23 00:37:16.257: INFO: Pod "pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007736677s
STEP: Saw pod success
Jan 23 00:37:16.257: INFO: Pod "pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:37:16.259: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:37:16.276: INFO: Waiting for pod pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:37:16.277: INFO: Pod pod-projected-configmaps-061c0eb5-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:37:16.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jbjph" for this suite.
Jan 23 00:37:22.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:37:22.312: INFO: namespace: e2e-tests-projected-jbjph, resource: bindings, ignored listing per whitelist
Jan 23 00:37:22.343: INFO: namespace e2e-tests-projected-jbjph deletion completed in 6.064100251s

• [SLOW TEST:10.158 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:37:22.344: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-0c2adbfc-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 00:37:22.406: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-68rmg" to be "success or failure"
Jan 23 00:37:22.407: INFO: Pod "pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29055ms
Jan 23 00:37:24.410: INFO: Pod "pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004020295s
STEP: Saw pod success
Jan 23 00:37:24.410: INFO: Pod "pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:37:24.412: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 00:37:24.430: INFO: Waiting for pod pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:37:24.432: INFO: Pod pod-projected-secrets-0c2b153c-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:37:24.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-68rmg" for this suite.
Jan 23 00:37:30.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:37:30.499: INFO: namespace: e2e-tests-projected-68rmg, resource: bindings, ignored listing per whitelist
Jan 23 00:37:30.509: INFO: namespace e2e-tests-projected-68rmg deletion completed in 6.07416467s

• [SLOW TEST:8.165 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:37:30.509: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 00:37:30.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-nczsg'
Jan 23 00:37:30.630: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 00:37:30.631: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Jan 23 00:37:30.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-nczsg'
Jan 23 00:37:30.707: INFO: stderr: ""
Jan 23 00:37:30.707: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:37:30.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nczsg" for this suite.
Jan 23 00:37:52.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:37:52.748: INFO: namespace: e2e-tests-kubectl-nczsg, resource: bindings, ignored listing per whitelist
Jan 23 00:37:52.765: INFO: namespace e2e-tests-kubectl-nczsg deletion completed in 22.05494006s

• [SLOW TEST:22.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:37:52.765: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 23 00:37:52.818: INFO: Waiting up to 5m0s for pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-kq9jv" to be "success or failure"
Jan 23 00:37:52.819: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39939ms
Jan 23 00:37:54.822: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004260355s
Jan 23 00:37:56.826: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007844348s
Jan 23 00:37:58.830: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011669388s
Jan 23 00:38:00.833: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015070422s
Jan 23 00:38:02.836: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.018316833s
STEP: Saw pod success
Jan 23 00:38:02.836: INFO: Pod "pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:38:02.839: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:38:02.857: INFO: Waiting for pod pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:38:02.861: INFO: Pod pod-1e4aed78-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:38:02.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kq9jv" for this suite.
Jan 23 00:38:08.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:38:08.895: INFO: namespace: e2e-tests-emptydir-kq9jv, resource: bindings, ignored listing per whitelist
Jan 23 00:38:08.931: INFO: namespace e2e-tests-emptydir-kq9jv deletion completed in 6.067961294s

• [SLOW TEST:16.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:38:08.931: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-cn6fw
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 00:38:08.993: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 00:38:41.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.5:8080/dial?request=hostName&protocol=http&host=10.32.0.3&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-cn6fw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:38:41.044: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:38:41.183: INFO: Waiting for endpoints: map[]
Jan 23 00:38:41.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.5:8080/dial?request=hostName&protocol=http&host=10.40.0.4&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-cn6fw PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:38:41.186: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:38:41.284: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:38:41.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-cn6fw" for this suite.
Jan 23 00:39:03.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:39:03.351: INFO: namespace: e2e-tests-pod-network-test-cn6fw, resource: bindings, ignored listing per whitelist
Jan 23 00:39:03.360: INFO: namespace e2e-tests-pod-network-test-cn6fw deletion completed in 22.073102772s

• [SLOW TEST:54.429 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:39:03.360: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 23 00:39:03.422: INFO: Waiting up to 5m0s for pod "pod-4860d578-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-zxm2j" to be "success or failure"
Jan 23 00:39:03.426: INFO: Pod "pod-4860d578-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145932ms
Jan 23 00:39:05.429: INFO: Pod "pod-4860d578-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006704536s
Jan 23 00:39:07.432: INFO: Pod "pod-4860d578-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009590087s
STEP: Saw pod success
Jan 23 00:39:07.432: INFO: Pod "pod-4860d578-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:39:07.434: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-4860d578-1ea7-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:39:07.445: INFO: Waiting for pod pod-4860d578-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:39:07.446: INFO: Pod pod-4860d578-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:39:07.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zxm2j" for this suite.
Jan 23 00:39:13.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:39:13.466: INFO: namespace: e2e-tests-emptydir-zxm2j, resource: bindings, ignored listing per whitelist
Jan 23 00:39:13.522: INFO: namespace e2e-tests-emptydir-zxm2j deletion completed in 6.074633708s

• [SLOW TEST:10.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:39:13.523: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 23 00:39:17.605: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:17.605: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:17.702: INFO: Exec stderr: ""
Jan 23 00:39:17.702: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:17.702: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:17.785: INFO: Exec stderr: ""
Jan 23 00:39:17.785: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:17.785: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:17.873: INFO: Exec stderr: ""
Jan 23 00:39:17.873: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:17.962: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 23 00:39:17.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.040: INFO: Exec stderr: ""
Jan 23 00:39:18.040: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:18.040: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.125: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 23 00:39:18.125: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:18.125: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.207: INFO: Exec stderr: ""
Jan 23 00:39:18.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:18.207: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.298: INFO: Exec stderr: ""
Jan 23 00:39:18.298: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:18.298: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.390: INFO: Exec stderr: ""
Jan 23 00:39:18.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-jxj5h PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:39:18.390: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:39:18.542: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:39:18.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-jxj5h" for this suite.
Jan 23 00:39:56.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:39:56.565: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-jxj5h, resource: bindings, ignored listing per whitelist
Jan 23 00:39:56.618: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-jxj5h deletion completed in 38.073492776s

• [SLOW TEST:43.095 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:39:56.618: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Jan 23 00:39:56.678: INFO: Waiting up to 5m0s for pod "var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-var-expansion-sfxjf" to be "success or failure"
Jan 23 00:39:56.680: INFO: Pod "var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51088ms
Jan 23 00:39:58.684: INFO: Pod "var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005142415s
STEP: Saw pod success
Jan 23 00:39:58.684: INFO: Pod "var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:39:58.686: INFO: Trying to get logs from node kind-conformance-worker2 pod var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 00:39:58.707: INFO: Waiting for pod var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:39:58.709: INFO: Pod var-expansion-681f2725-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:39:58.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-sfxjf" for this suite.
Jan 23 00:40:04.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:40:04.770: INFO: namespace: e2e-tests-var-expansion-sfxjf, resource: bindings, ignored listing per whitelist
Jan 23 00:40:04.781: INFO: namespace e2e-tests-var-expansion-sfxjf deletion completed in 6.069451716s

• [SLOW TEST:8.163 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:40:04.781: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-6cfccabb-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:40:04.843: INFO: Waiting up to 5m0s for pod "pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-x6ll9" to be "success or failure"
Jan 23 00:40:04.846: INFO: Pod "pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469192ms
Jan 23 00:40:06.849: INFO: Pod "pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005782516s
STEP: Saw pod success
Jan 23 00:40:06.849: INFO: Pod "pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:40:06.851: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:40:06.865: INFO: Waiting for pod pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:40:06.869: INFO: Pod pod-configmaps-6cfd0d34-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:40:06.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x6ll9" for this suite.
Jan 23 00:40:12.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:40:12.937: INFO: namespace: e2e-tests-configmap-x6ll9, resource: bindings, ignored listing per whitelist
Jan 23 00:40:12.942: INFO: namespace e2e-tests-configmap-x6ll9 deletion completed in 6.07020217s

• [SLOW TEST:8.160 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:40:12.942: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:41:13.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-f6kw9" for this suite.
Jan 23 00:41:35.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:41:35.061: INFO: namespace: e2e-tests-container-probe-f6kw9, resource: bindings, ignored listing per whitelist
Jan 23 00:41:35.089: INFO: namespace e2e-tests-container-probe-f6kw9 deletion completed in 22.073772916s

• [SLOW TEST:82.147 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:41:35.089: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Jan 23 00:41:39.162: INFO: Pod pod-hostip-a2d0eff6-1ea7-11e9-b567-9e07e353f0d4 has hostIP: 192.168.9.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:41:39.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-27lb4" for this suite.
Jan 23 00:42:01.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:42:01.198: INFO: namespace: e2e-tests-pods-27lb4, resource: bindings, ignored listing per whitelist
Jan 23 00:42:01.221: INFO: namespace e2e-tests-pods-27lb4 deletion completed in 22.05588354s

• [SLOW TEST:26.132 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:42:01.221: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b263ddcb-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:42:01.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-gcsr4" to be "success or failure"
Jan 23 00:42:01.282: INFO: Pod "pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.272713ms
Jan 23 00:42:03.286: INFO: Pod "pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004946246s
Jan 23 00:42:05.289: INFO: Pod "pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007850385s
STEP: Saw pod success
Jan 23 00:42:05.289: INFO: Pod "pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:42:05.291: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:42:05.307: INFO: Waiting for pod pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:42:05.309: INFO: Pod pod-configmaps-b2642555-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:42:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gcsr4" for this suite.
Jan 23 00:42:11.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:42:11.381: INFO: namespace: e2e-tests-configmap-gcsr4, resource: bindings, ignored listing per whitelist
Jan 23 00:42:11.385: INFO: namespace e2e-tests-configmap-gcsr4 deletion completed in 6.07356616s

• [SLOW TEST:10.164 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:42:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0123 00:42:21.472286      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 00:42:21.472: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:42:21.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mjvjc" for this suite.
Jan 23 00:42:27.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:42:27.497: INFO: namespace: e2e-tests-gc-mjvjc, resource: bindings, ignored listing per whitelist
Jan 23 00:42:27.537: INFO: namespace e2e-tests-gc-mjvjc deletion completed in 6.06126658s

• [SLOW TEST:16.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:42:27.537: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Jan 23 00:42:27.578: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-311263258 proxy --unix-socket=/tmp/kubectl-proxy-unix288542897/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:42:27.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wccdc" for this suite.
Jan 23 00:42:33.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:42:33.671: INFO: namespace: e2e-tests-kubectl-wccdc, resource: bindings, ignored listing per whitelist
Jan 23 00:42:33.699: INFO: namespace e2e-tests-kubectl-wccdc deletion completed in 6.068711145s

• [SLOW TEST:6.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:42:33.699: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 23 00:42:33.752: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:42:37.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-6l7s8" for this suite.
Jan 23 00:42:43.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:42:43.138: INFO: namespace: e2e-tests-init-container-6l7s8, resource: bindings, ignored listing per whitelist
Jan 23 00:42:43.163: INFO: namespace e2e-tests-init-container-6l7s8 deletion completed in 6.050501316s

• [SLOW TEST:9.464 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:42:43.163: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-cb621edc-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating configMap with name cm-test-opt-upd-cb621f0a-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cb621edc-1ea7-11e9-b567-9e07e353f0d4
STEP: Updating configmap cm-test-opt-upd-cb621f0a-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating configMap with name cm-test-opt-create-cb621f2c-1ea7-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:42:47.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m8btk" for this suite.
Jan 23 00:43:09.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:43:09.316: INFO: namespace: e2e-tests-projected-m8btk, resource: bindings, ignored listing per whitelist
Jan 23 00:43:09.351: INFO: namespace e2e-tests-projected-m8btk deletion completed in 22.074174714s

• [SLOW TEST:26.188 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:43:09.352: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:43:09.408: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 23 00:43:09.412: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 00:43:14.416: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 00:43:14.416: INFO: Creating deployment "test-rolling-update-deployment"
Jan 23 00:43:14.419: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 23 00:43:14.424: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 23 00:43:16.431: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 23 00:43:16.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683800994, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683800994, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683800994, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683800994, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 00:43:18.437: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 23 00:43:18.445: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-mghj5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mghj5/deployments/test-rolling-update-deployment,UID:ddfc23b2-1ea7-11e9-9fdb-02424cc6ba99,ResourceVersion:7155,Generation:1,CreationTimestamp:2019-01-23 00:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-23 00:43:14 +0000 UTC 2019-01-23 00:43:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-23 00:43:16 +0000 UTC 2019-01-23 00:43:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 00:43:18.448: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-mghj5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mghj5/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:ddfe7f31-1ea7-11e9-9fdb-02424cc6ba99,ResourceVersion:7146,Generation:1,CreationTimestamp:2019-01-23 00:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ddfc23b2-1ea7-11e9-9fdb-02424cc6ba99 0xc001da7637 0xc001da7638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 23 00:43:18.448: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 23 00:43:18.449: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-mghj5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-mghj5/replicasets/test-rolling-update-controller,UID:dafffe37-1ea7-11e9-9fdb-02424cc6ba99,ResourceVersion:7154,Generation:2,CreationTimestamp:2019-01-23 00:43:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ddfc23b2-1ea7-11e9-9fdb-02424cc6ba99 0xc001da756f 0xc001da7580}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 00:43:18.451: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-t6z4j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-t6z4j,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-mghj5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-mghj5/pods/test-rolling-update-deployment-68b55d7bc6-t6z4j,UID:ddfed966-1ea7-11e9-9fdb-02424cc6ba99,ResourceVersion:7145,Generation:0,CreationTimestamp:2019-01-23 00:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 ddfe7f31-1ea7-11e9-9fdb-02424cc6ba99 0xc0022a5857 0xc0022a5858}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-l5bp9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l5bp9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-l5bp9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022a58c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022a58e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:43:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:43:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:43:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.4,PodIP:10.32.0.3,StartTime:2019-01-23 00:43:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-23 00:43:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1c8daee1ba2d6af2fea64bc4ee8f4ca493de98cec7567cf1c587c6f0d92feb71}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:43:18.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-mghj5" for this suite.
Jan 23 00:43:24.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:43:24.521: INFO: namespace: e2e-tests-deployment-mghj5, resource: bindings, ignored listing per whitelist
Jan 23 00:43:24.531: INFO: namespace e2e-tests-deployment-mghj5 deletion completed in 6.076787341s

• [SLOW TEST:15.180 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:43:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 23 00:43:24.594: INFO: Waiting up to 5m0s for pod "pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-cw7ng" to be "success or failure"
Jan 23 00:43:24.600: INFO: Pod "pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.128491ms
Jan 23 00:43:26.603: INFO: Pod "pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008574599s
Jan 23 00:43:28.606: INFO: Pod "pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011961456s
STEP: Saw pod success
Jan 23 00:43:28.606: INFO: Pod "pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:43:28.609: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:43:28.626: INFO: Waiting for pod pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:43:28.628: INFO: Pod pod-e40c9114-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:43:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cw7ng" for this suite.
Jan 23 00:43:34.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:43:34.646: INFO: namespace: e2e-tests-emptydir-cw7ng, resource: bindings, ignored listing per whitelist
Jan 23 00:43:34.702: INFO: namespace e2e-tests-emptydir-cw7ng deletion completed in 6.071502108s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:43:34.702: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Jan 23 00:43:34.768: INFO: Waiting up to 5m0s for pod "client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-containers-brhpf" to be "success or failure"
Jan 23 00:43:34.770: INFO: Pod "client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680448ms
Jan 23 00:43:36.773: INFO: Pod "client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005753683s
Jan 23 00:43:38.777: INFO: Pod "client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009147033s
STEP: Saw pod success
Jan 23 00:43:38.777: INFO: Pod "client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:43:38.779: INFO: Trying to get logs from node kind-conformance-worker2 pod client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:43:38.803: INFO: Waiting for pod client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:43:38.806: INFO: Pod client-containers-ea1cf563-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:43:38.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-brhpf" for this suite.
Jan 23 00:43:44.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:43:44.875: INFO: namespace: e2e-tests-containers-brhpf, resource: bindings, ignored listing per whitelist
Jan 23 00:43:44.896: INFO: namespace e2e-tests-containers-brhpf deletion completed in 6.087119156s

• [SLOW TEST:10.194 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:43:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:43:44.958: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-vpht2" to be "success or failure"
Jan 23 00:43:44.960: INFO: Pod "downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59512ms
Jan 23 00:43:46.963: INFO: Pod "downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00484002s
Jan 23 00:43:48.966: INFO: Pod "downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00797162s
STEP: Saw pod success
Jan 23 00:43:48.966: INFO: Pod "downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:43:48.969: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:43:48.986: INFO: Waiting for pod downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:43:48.988: INFO: Pod downwardapi-volume-f02fedde-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:43:48.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vpht2" for this suite.
Jan 23 00:43:54.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:43:55.020: INFO: namespace: e2e-tests-downward-api-vpht2, resource: bindings, ignored listing per whitelist
Jan 23 00:43:55.053: INFO: namespace e2e-tests-downward-api-vpht2 deletion completed in 6.061708514s

• [SLOW TEST:10.156 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:43:55.053: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f63c3e16-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 00:43:55.109: INFO: Waiting up to 5m0s for pod "pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-5fgc8" to be "success or failure"
Jan 23 00:43:55.113: INFO: Pod "pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.857169ms
Jan 23 00:43:57.115: INFO: Pod "pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732653s
Jan 23 00:43:59.119: INFO: Pod "pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009820964s
STEP: Saw pod success
Jan 23 00:43:59.119: INFO: Pod "pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:43:59.121: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 00:43:59.139: INFO: Waiting for pod pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:43:59.141: INFO: Pod pod-secrets-f63cc7f8-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:43:59.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5fgc8" for this suite.
Jan 23 00:44:05.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:44:05.159: INFO: namespace: e2e-tests-secrets-5fgc8, resource: bindings, ignored listing per whitelist
Jan 23 00:44:05.208: INFO: namespace e2e-tests-secrets-5fgc8 deletion completed in 6.064884228s

• [SLOW TEST:10.155 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:44:05.208: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8t9tx/configmap-test-fc4afc6d-1ea7-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:44:05.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-8t9tx" to be "success or failure"
Jan 23 00:44:05.272: INFO: Pod "pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.614388ms
Jan 23 00:44:07.275: INFO: Pod "pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004794894s
Jan 23 00:44:09.278: INFO: Pod "pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008151644s
STEP: Saw pod success
Jan 23 00:44:09.278: INFO: Pod "pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:44:09.281: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4 container env-test: <nil>
STEP: delete the pod
Jan 23 00:44:09.302: INFO: Waiting for pod pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:44:09.306: INFO: Pod pod-configmaps-fc4b3964-1ea7-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:44:09.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8t9tx" for this suite.
Jan 23 00:44:15.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:44:15.329: INFO: namespace: e2e-tests-configmap-8t9tx, resource: bindings, ignored listing per whitelist
Jan 23 00:44:15.381: INFO: namespace e2e-tests-configmap-8t9tx deletion completed in 6.072458839s

• [SLOW TEST:10.172 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:44:15.381: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 00:44:15.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-xdtd4'
Jan 23 00:44:15.811: INFO: stderr: ""
Jan 23 00:44:15.811: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Jan 23 00:44:15.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xdtd4'
Jan 23 00:44:23.702: INFO: stderr: ""
Jan 23 00:44:23.702: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:44:23.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xdtd4" for this suite.
Jan 23 00:44:29.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:44:29.723: INFO: namespace: e2e-tests-kubectl-xdtd4, resource: bindings, ignored listing per whitelist
Jan 23 00:44:29.776: INFO: namespace e2e-tests-kubectl-xdtd4 deletion completed in 6.071683596s

• [SLOW TEST:14.395 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:44:29.776: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:44:31.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-7hvzq" for this suite.
Jan 23 00:45:13.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:45:13.909: INFO: namespace: e2e-tests-kubelet-test-7hvzq, resource: bindings, ignored listing per whitelist
Jan 23 00:45:13.931: INFO: namespace e2e-tests-kubelet-test-7hvzq deletion completed in 42.076459642s

• [SLOW TEST:44.155 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:45:13.931: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Jan 23 00:45:13.997: INFO: Waiting up to 5m0s for pod "client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-containers-2lzwz" to be "success or failure"
Jan 23 00:45:13.999: INFO: Pod "client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.670009ms
Jan 23 00:45:16.002: INFO: Pod "client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005128477s
Jan 23 00:45:18.006: INFO: Pod "client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008345986s
STEP: Saw pod success
Jan 23 00:45:18.006: INFO: Pod "client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:45:18.008: INFO: Trying to get logs from node kind-conformance-worker2 pod client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:45:18.033: INFO: Waiting for pod client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:45:18.036: INFO: Pod client-containers-254232c8-1ea8-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:45:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2lzwz" for this suite.
Jan 23 00:45:24.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:45:24.112: INFO: namespace: e2e-tests-containers-2lzwz, resource: bindings, ignored listing per whitelist
Jan 23 00:45:24.123: INFO: namespace e2e-tests-containers-2lzwz deletion completed in 6.084417879s

• [SLOW TEST:10.192 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:45:24.123: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2b55859d-1ea8-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:45:24.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-hx7rg" to be "success or failure"
Jan 23 00:45:24.196: INFO: Pod "pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948556ms
Jan 23 00:45:26.199: INFO: Pod "pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006030258s
Jan 23 00:45:28.203: INFO: Pod "pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518369s
STEP: Saw pod success
Jan 23 00:45:28.203: INFO: Pod "pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:45:28.205: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:45:28.222: INFO: Waiting for pod pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:45:28.223: INFO: Pod pod-projected-configmaps-2b55c8f0-1ea8-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:45:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hx7rg" for this suite.
Jan 23 00:45:34.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:45:34.240: INFO: namespace: e2e-tests-projected-hx7rg, resource: bindings, ignored listing per whitelist
Jan 23 00:45:34.295: INFO: namespace e2e-tests-projected-hx7rg deletion completed in 6.068995183s

• [SLOW TEST:10.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:45:34.295: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-dl29m
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-dl29m
STEP: Deleting pre-stop pod
Jan 23 00:45:57.394: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:45:57.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-dl29m" for this suite.
Jan 23 00:46:35.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:46:35.472: INFO: namespace: e2e-tests-prestop-dl29m, resource: bindings, ignored listing per whitelist
Jan 23 00:46:35.484: INFO: namespace e2e-tests-prestop-dl29m deletion completed in 38.079898794s

• [SLOW TEST:61.189 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:46:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-5gmzp
Jan 23 00:46:37.540: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-5gmzp
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 00:46:37.543: INFO: Initial restart count of pod liveness-exec is 0
Jan 23 00:47:29.631: INFO: Restart count of pod e2e-tests-container-probe-5gmzp/liveness-exec is now 1 (52.0884234s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:47:29.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5gmzp" for this suite.
Jan 23 00:47:35.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:47:35.670: INFO: namespace: e2e-tests-container-probe-5gmzp, resource: bindings, ignored listing per whitelist
Jan 23 00:47:35.710: INFO: namespace e2e-tests-container-probe-5gmzp deletion completed in 6.060576814s

• [SLOW TEST:60.226 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:47:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 23 00:47:35.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-rxwnq,SelfLink:/api/v1/namespaces/e2e-tests-watch-rxwnq/configmaps/e2e-watch-test-resource-version,UID:79c1cbec-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:7910,Generation:0,CreationTimestamp:2019-01-23 00:47:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 00:47:35.782: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-rxwnq,SelfLink:/api/v1/namespaces/e2e-tests-watch-rxwnq/configmaps/e2e-watch-test-resource-version,UID:79c1cbec-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:7911,Generation:0,CreationTimestamp:2019-01-23 00:47:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:47:35.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-rxwnq" for this suite.
Jan 23 00:47:41.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:47:41.816: INFO: namespace: e2e-tests-watch-rxwnq, resource: bindings, ignored listing per whitelist
Jan 23 00:47:41.854: INFO: namespace e2e-tests-watch-rxwnq deletion completed in 6.070001482s

• [SLOW TEST:6.144 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:47:41.854: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 23 00:47:41.899: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 00:47:41.903: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 00:47:41.904: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker1 before test
Jan 23 00:47:41.907: INFO: coredns-86c58d9df4-2wct2 from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container coredns ready: true, restart count 0
Jan 23 00:47:41.907: INFO: coredns-86c58d9df4-84xdw from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container coredns ready: true, restart count 0
Jan 23 00:47:41.907: INFO: kube-proxy-4tfgc from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 00:47:41.907: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-23 00:13:23 +0000 UTC (1 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 00:47:41.907: INFO: weave-net-6h6xx from kube-system started at 2019-01-23 00:10:21 +0000 UTC (2 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container weave ready: true, restart count 0
Jan 23 00:47:41.907: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 00:47:41.907: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-ghcfq from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:47:41.907: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 23 00:47:41.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 00:47:41.907: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker2 before test
Jan 23 00:47:41.911: INFO: kube-proxy-85kj4 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (1 container statuses recorded)
Jan 23 00:47:41.911: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 00:47:41.911: INFO: weave-net-29887 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (2 container statuses recorded)
Jan 23 00:47:41.911: INFO: 	Container weave ready: true, restart count 0
Jan 23 00:47:41.911: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 00:47:41.911: INFO: sonobuoy-e2e-job-9907aed0bf024cec from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:47:41.911: INFO: 	Container e2e ready: true, restart count 0
Jan 23 00:47:41.911: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 00:47:41.911: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-6bn97 from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:47:41.911: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 23 00:47:41.911: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7fd1fbfc-1ea8-11e9-b567-9e07e353f0d4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7fd1fbfc-1ea8-11e9-b567-9e07e353f0d4 off the node kind-conformance-worker1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7fd1fbfc-1ea8-11e9-b567-9e07e353f0d4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:47:47.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-b99wb" for this suite.
Jan 23 00:48:05.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:06.003: INFO: namespace: e2e-tests-sched-pred-b99wb, resource: bindings, ignored listing per whitelist
Jan 23 00:48:06.035: INFO: namespace e2e-tests-sched-pred-b99wb deletion completed in 18.06904609s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8bd54a8c-1ea8-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 00:48:06.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-4gpxq" to be "success or failure"
Jan 23 00:48:06.095: INFO: Pod "pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777318ms
Jan 23 00:48:08.098: INFO: Pod "pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00673981s
STEP: Saw pod success
Jan 23 00:48:08.098: INFO: Pod "pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:48:08.101: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 00:48:08.119: INFO: Waiting for pod pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:48:08.122: INFO: Pod pod-projected-configmaps-8bd58e3f-1ea8-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:08.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4gpxq" for this suite.
Jan 23 00:48:14.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:14.187: INFO: namespace: e2e-tests-projected-4gpxq, resource: bindings, ignored listing per whitelist
Jan 23 00:48:14.188: INFO: namespace e2e-tests-projected-4gpxq deletion completed in 6.064090358s

• [SLOW TEST:8.153 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:14.188: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 23 00:48:14.240: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 00:48:14.244: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 00:48:14.246: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker1 before test
Jan 23 00:48:14.252: INFO: kube-proxy-4tfgc from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 00:48:14.252: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-23 00:13:23 +0000 UTC (1 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 00:48:14.252: INFO: weave-net-6h6xx from kube-system started at 2019-01-23 00:10:21 +0000 UTC (2 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container weave ready: true, restart count 0
Jan 23 00:48:14.252: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 00:48:14.252: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-ghcfq from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 23 00:48:14.252: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 00:48:14.252: INFO: coredns-86c58d9df4-2wct2 from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container coredns ready: true, restart count 0
Jan 23 00:48:14.252: INFO: coredns-86c58d9df4-84xdw from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 00:48:14.252: INFO: 	Container coredns ready: true, restart count 0
Jan 23 00:48:14.252: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker2 before test
Jan 23 00:48:14.256: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-6bn97 from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:48:14.256: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 23 00:48:14.256: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 00:48:14.256: INFO: weave-net-29887 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (2 container statuses recorded)
Jan 23 00:48:14.256: INFO: 	Container weave ready: true, restart count 0
Jan 23 00:48:14.256: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 00:48:14.256: INFO: sonobuoy-e2e-job-9907aed0bf024cec from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 00:48:14.256: INFO: 	Container e2e ready: true, restart count 0
Jan 23 00:48:14.256: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 00:48:14.256: INFO: kube-proxy-85kj4 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (1 container statuses recorded)
Jan 23 00:48:14.256: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node kind-conformance-worker1
STEP: verifying the node has the label node kind-conformance-worker2
Jan 23 00:48:14.279: INFO: Pod sonobuoy requesting resource cpu=0m on Node kind-conformance-worker1
Jan 23 00:48:14.279: INFO: Pod sonobuoy-e2e-job-9907aed0bf024cec requesting resource cpu=0m on Node kind-conformance-worker2
Jan 23 00:48:14.279: INFO: Pod sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-6bn97 requesting resource cpu=0m on Node kind-conformance-worker2
Jan 23 00:48:14.279: INFO: Pod sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-ghcfq requesting resource cpu=0m on Node kind-conformance-worker1
Jan 23 00:48:14.279: INFO: Pod coredns-86c58d9df4-2wct2 requesting resource cpu=100m on Node kind-conformance-worker1
Jan 23 00:48:14.279: INFO: Pod coredns-86c58d9df4-84xdw requesting resource cpu=100m on Node kind-conformance-worker1
Jan 23 00:48:14.279: INFO: Pod kube-proxy-4tfgc requesting resource cpu=0m on Node kind-conformance-worker1
Jan 23 00:48:14.279: INFO: Pod kube-proxy-85kj4 requesting resource cpu=0m on Node kind-conformance-worker2
Jan 23 00:48:14.279: INFO: Pod weave-net-29887 requesting resource cpu=20m on Node kind-conformance-worker2
Jan 23 00:48:14.279: INFO: Pod weave-net-6h6xx requesting resource cpu=20m on Node kind-conformance-worker1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4.157c53c8e9da95c8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-rmmkd/filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4 to kind-conformance-worker1]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4.157c53c8fc520efd], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-sched-pred-rmmkd.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4.157c53c923f521ed], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4.157c53c92adf1327], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b77497-1ea8-11e9-b567-9e07e353f0d4.157c53c935e130e2], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4.157c53c8e9e83470], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-rmmkd/filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4 to kind-conformance-worker2]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4.157c53c8fc85f42f], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-sched-pred-rmmkd.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4.157c53c923ebc8bd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4.157c53c92a5533c0], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90b87abb-1ea8-11e9-b567-9e07e353f0d4.157c53c9360af6cf], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.157c53c9d924e3c4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node kind-conformance-worker1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kind-conformance-worker2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:19.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rmmkd" for this suite.
Jan 23 00:48:25.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:25.358: INFO: namespace: e2e-tests-sched-pred-rmmkd, resource: bindings, ignored listing per whitelist
Jan 23 00:48:25.396: INFO: namespace e2e-tests-sched-pred-rmmkd deletion completed in 6.06570834s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.208 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:25.396: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 23 00:48:25.454: INFO: Waiting up to 5m0s for pod "downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-f2fks" to be "success or failure"
Jan 23 00:48:25.456: INFO: Pod "downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.814984ms
Jan 23 00:48:27.459: INFO: Pod "downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00482401s
STEP: Saw pod success
Jan 23 00:48:27.459: INFO: Pod "downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:48:27.461: INFO: Trying to get logs from node kind-conformance-worker1 pod downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 00:48:27.475: INFO: Waiting for pod downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:48:27.477: INFO: Pod downward-api-97602b33-1ea8-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f2fks" for this suite.
Jan 23 00:48:33.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:33.501: INFO: namespace: e2e-tests-downward-api-f2fks, resource: bindings, ignored listing per whitelist
Jan 23 00:48:33.538: INFO: namespace e2e-tests-downward-api-f2fks deletion completed in 6.059451763s

• [SLOW TEST:8.142 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:33.538: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:48:33.584: INFO: Creating deployment "test-recreate-deployment"
Jan 23 00:48:33.590: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 23 00:48:33.592: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 23 00:48:35.597: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 23 00:48:35.599: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 23 00:48:35.602: INFO: Updating deployment test-recreate-deployment
Jan 23 00:48:35.602: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 23 00:48:35.645: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-fm8hp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fm8hp/deployments/test-recreate-deployment,UID:9c394d21-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:8216,Generation:2,CreationTimestamp:2019-01-23 00:48:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-01-23 00:48:35 +0000 UTC 2019-01-23 00:48:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-01-23 00:48:35 +0000 UTC 2019-01-23 00:48:33 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 00:48:35.647: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-fm8hp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fm8hp/replicasets/test-recreate-deployment-697fbf54bf,UID:9d70dbaa-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:8215,Generation:1,CreationTimestamp:2019-01-23 00:48:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9c394d21-1ea8-11e9-9fdb-02424cc6ba99 0xc00211e9e7 0xc00211e9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 00:48:35.647: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 23 00:48:35.648: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-fm8hp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fm8hp/replicasets/test-recreate-deployment-5dfdcc846d,UID:9c3a94b1-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:8205,Generation:2,CreationTimestamp:2019-01-23 00:48:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9c394d21-1ea8-11e9-9fdb-02424cc6ba99 0xc00211e937 0xc00211e938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 00:48:35.649: INFO: Pod "test-recreate-deployment-697fbf54bf-9cldp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-9cldp,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-fm8hp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fm8hp/pods/test-recreate-deployment-697fbf54bf-9cldp,UID:9d711ba6-1ea8-11e9-9fdb-02424cc6ba99,ResourceVersion:8217,Generation:0,CreationTimestamp:2019-01-23 00:48:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 9d70dbaa-1ea8-11e9-9fdb-02424cc6ba99 0xc001d92737 0xc001d92738}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-hm5b5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hm5b5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hm5b5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d927a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d927c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:48:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:48:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:48:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-01-23 00:48:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:35.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-fm8hp" for this suite.
Jan 23 00:48:41.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:41.717: INFO: namespace: e2e-tests-deployment-fm8hp, resource: bindings, ignored listing per whitelist
Jan 23 00:48:41.721: INFO: namespace e2e-tests-deployment-fm8hp deletion completed in 6.070427744s

• [SLOW TEST:8.183 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:41.722: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:45.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-dfgr7" for this suite.
Jan 23 00:48:51.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:48:51.819: INFO: namespace: e2e-tests-kubelet-test-dfgr7, resource: bindings, ignored listing per whitelist
Jan 23 00:48:51.861: INFO: namespace e2e-tests-kubelet-test-dfgr7 deletion completed in 6.075104384s

• [SLOW TEST:10.139 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:48:51.861: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:48:51.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-89zns" to be "success or failure"
Jan 23 00:48:51.937: INFO: Pod "downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816123ms
Jan 23 00:48:53.940: INFO: Pod "downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005202005s
STEP: Saw pod success
Jan 23 00:48:53.940: INFO: Pod "downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:48:53.943: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:48:53.960: INFO: Waiting for pod downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:48:53.962: INFO: Pod downwardapi-volume-a728b5d9-1ea8-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:48:53.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-89zns" for this suite.
Jan 23 00:48:59.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:49:00.009: INFO: namespace: e2e-tests-downward-api-89zns, resource: bindings, ignored listing per whitelist
Jan 23 00:49:00.041: INFO: namespace e2e-tests-downward-api-89zns deletion completed in 6.076466758s

• [SLOW TEST:8.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:49:00.041: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Jan 23 00:49:00.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 api-versions'
Jan 23 00:49:00.147: INFO: stderr: ""
Jan 23 00:49:00.147: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:49:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6cqzk" for this suite.
Jan 23 00:49:06.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:49:06.204: INFO: namespace: e2e-tests-kubectl-6cqzk, resource: bindings, ignored listing per whitelist
Jan 23 00:49:06.218: INFO: namespace e2e-tests-kubectl-6cqzk deletion completed in 6.068224779s

• [SLOW TEST:6.177 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:49:06.218: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-gnc57
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 00:49:06.276: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 00:49:30.326: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.3:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-gnc57 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:49:30.326: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:49:30.453: INFO: Found all expected endpoints: [netserver-0]
Jan 23 00:49:30.455: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.4:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-gnc57 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 00:49:30.455: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 00:49:30.611: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:49:30.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-gnc57" for this suite.
Jan 23 00:49:52.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:49:52.663: INFO: namespace: e2e-tests-pod-network-test-gnc57, resource: bindings, ignored listing per whitelist
Jan 23 00:49:52.688: INFO: namespace e2e-tests-pod-network-test-gnc57 deletion completed in 22.073760137s

• [SLOW TEST:46.469 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:49:52.688: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 23 00:49:55.275: INFO: Successfully updated pod "annotationupdatecb685813-1ea8-11e9-b567-9e07e353f0d4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:49:57.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hx9p6" for this suite.
Jan 23 00:50:19.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:50:19.346: INFO: namespace: e2e-tests-projected-hx9p6, resource: bindings, ignored listing per whitelist
Jan 23 00:50:19.398: INFO: namespace e2e-tests-projected-hx9p6 deletion completed in 22.088662002s

• [SLOW TEST:26.710 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:50:19.398: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:50:23.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-45lw9" for this suite.
Jan 23 00:51:13.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:51:13.506: INFO: namespace: e2e-tests-kubelet-test-45lw9, resource: bindings, ignored listing per whitelist
Jan 23 00:51:13.545: INFO: namespace e2e-tests-kubelet-test-45lw9 deletion completed in 50.059594267s

• [SLOW TEST:54.147 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:51:13.545: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-fb98d4e5-1ea8-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:51:15.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mrdhq" for this suite.
Jan 23 00:51:37.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:51:37.652: INFO: namespace: e2e-tests-configmap-mrdhq, resource: bindings, ignored listing per whitelist
Jan 23 00:51:37.692: INFO: namespace e2e-tests-configmap-mrdhq deletion completed in 22.072046s

• [SLOW TEST:24.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:51:37.693: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:51:37.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-8jbmb" to be "success or failure"
Jan 23 00:51:37.741: INFO: Pod "downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.939127ms
Jan 23 00:51:39.745: INFO: Pod "downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005607866s
STEP: Saw pod success
Jan 23 00:51:39.745: INFO: Pod "downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:51:39.748: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:51:39.764: INFO: Waiting for pod downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:51:39.766: INFO: Pod downwardapi-volume-09fc7bc8-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:51:39.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8jbmb" for this suite.
Jan 23 00:51:45.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:51:45.795: INFO: namespace: e2e-tests-projected-8jbmb, resource: bindings, ignored listing per whitelist
Jan 23 00:51:45.834: INFO: namespace e2e-tests-projected-8jbmb deletion completed in 6.06550805s

• [SLOW TEST:8.141 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:51:45.834: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-9w4c
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 00:51:45.897: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9w4c" in namespace "e2e-tests-subpath-ppkqh" to be "success or failure"
Jan 23 00:51:45.900: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147403ms
Jan 23 00:51:47.904: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007010455s
Jan 23 00:51:49.907: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 4.010136283s
Jan 23 00:51:51.910: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 6.013331107s
Jan 23 00:51:53.914: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 8.016734354s
Jan 23 00:51:55.917: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 10.020207114s
Jan 23 00:51:57.920: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 12.023553595s
Jan 23 00:51:59.924: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 14.026993305s
Jan 23 00:52:01.927: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 16.030337554s
Jan 23 00:52:03.931: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 18.033768215s
Jan 23 00:52:05.933: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 20.036344975s
Jan 23 00:52:07.936: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Running", Reason="", readiness=false. Elapsed: 22.038841621s
Jan 23 00:52:09.939: INFO: Pod "pod-subpath-test-projected-9w4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042134692s
STEP: Saw pod success
Jan 23 00:52:09.939: INFO: Pod "pod-subpath-test-projected-9w4c" satisfied condition "success or failure"
Jan 23 00:52:09.942: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-subpath-test-projected-9w4c container test-container-subpath-projected-9w4c: <nil>
STEP: delete the pod
Jan 23 00:52:09.966: INFO: Waiting for pod pod-subpath-test-projected-9w4c to disappear
Jan 23 00:52:09.967: INFO: Pod pod-subpath-test-projected-9w4c no longer exists
STEP: Deleting pod pod-subpath-test-projected-9w4c
Jan 23 00:52:09.968: INFO: Deleting pod "pod-subpath-test-projected-9w4c" in namespace "e2e-tests-subpath-ppkqh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:52:09.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-ppkqh" for this suite.
Jan 23 00:52:15.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:52:15.996: INFO: namespace: e2e-tests-subpath-ppkqh, resource: bindings, ignored listing per whitelist
Jan 23 00:52:16.041: INFO: namespace e2e-tests-subpath-ppkqh deletion completed in 6.069857753s

• [SLOW TEST:30.207 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:52:16.041: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:52:30.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-dxllc" for this suite.
Jan 23 00:52:36.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:52:36.149: INFO: namespace: e2e-tests-emptydir-wrapper-dxllc, resource: bindings, ignored listing per whitelist
Jan 23 00:52:36.192: INFO: namespace e2e-tests-emptydir-wrapper-dxllc deletion completed in 6.060391415s

• [SLOW TEST:20.151 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:52:36.192: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-28rwg;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-28rwg;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-28rwg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.152.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.152.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.152.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.152.234_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-28rwg;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-28rwg;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-28rwg.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-28rwg.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-28rwg.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.152.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.152.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.152.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.152.234_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 00:52:50.320: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.322: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.325: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-28rwg from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.328: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-28rwg from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.331: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.333: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.336: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.338: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.340: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.343: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.345: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.351: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4)
Jan 23 00:52:50.356: INFO: Lookups using e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-28rwg jessie_tcp@dns-test-service.e2e-tests-dns-28rwg jessie_udp@dns-test-service.e2e-tests-dns-28rwg.svc jessie_tcp@dns-test-service.e2e-tests-dns-28rwg.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-28rwg.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-28rwg.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Jan 23 00:52:55.420: INFO: DNS probes using e2e-tests-dns-28rwg/dns-test-2cdf9eb3-1ea9-11e9-b567-9e07e353f0d4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:52:55.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-28rwg" for this suite.
Jan 23 00:53:01.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:53:01.537: INFO: namespace: e2e-tests-dns-28rwg, resource: bindings, ignored listing per whitelist
Jan 23 00:53:01.551: INFO: namespace e2e-tests-dns-28rwg deletion completed in 6.066731728s

• [SLOW TEST:25.359 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:53:01.551: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 23 00:53:01.612: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9045,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 00:53:01.612: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9045,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 23 00:53:11.620: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9060,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 23 00:53:11.620: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9060,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 23 00:53:21.625: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9075,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 00:53:21.625: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9075,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 23 00:53:31.631: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9090,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 00:53:31.631: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-a,UID:3bfa7bd9-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9090,Generation:0,CreationTimestamp:2019-01-23 00:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 23 00:53:41.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-b,UID:53d5a882-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9105,Generation:0,CreationTimestamp:2019-01-23 00:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 00:53:41.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-b,UID:53d5a882-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9105,Generation:0,CreationTimestamp:2019-01-23 00:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 23 00:53:51.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-b,UID:53d5a882-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9121,Generation:0,CreationTimestamp:2019-01-23 00:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 00:53:51.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fw5hp,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw5hp/configmaps/e2e-watch-test-configmap-b,UID:53d5a882-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:9121,Generation:0,CreationTimestamp:2019-01-23 00:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:54:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fw5hp" for this suite.
Jan 23 00:54:07.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:54:07.696: INFO: namespace: e2e-tests-watch-fw5hp, resource: bindings, ignored listing per whitelist
Jan 23 00:54:07.717: INFO: namespace e2e-tests-watch-fw5hp deletion completed in 6.069031998s

• [SLOW TEST:66.165 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:54:07.717: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-k9nn
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 00:54:07.792: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-k9nn" in namespace "e2e-tests-subpath-p4kx2" to be "success or failure"
Jan 23 00:54:07.795: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391196ms
Jan 23 00:54:09.799: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006981939s
Jan 23 00:54:11.803: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 4.010210985s
Jan 23 00:54:13.806: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 6.013767658s
Jan 23 00:54:15.810: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 8.017349173s
Jan 23 00:54:17.813: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 10.020687733s
Jan 23 00:54:19.816: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 12.023873921s
Jan 23 00:54:21.819: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 14.026697104s
Jan 23 00:54:23.822: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 16.029845562s
Jan 23 00:54:25.825: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 18.03310287s
Jan 23 00:54:27.829: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 20.036603157s
Jan 23 00:54:29.832: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Running", Reason="", readiness=false. Elapsed: 22.039812088s
Jan 23 00:54:31.836: INFO: Pod "pod-subpath-test-downwardapi-k9nn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.043162846s
STEP: Saw pod success
Jan 23 00:54:31.836: INFO: Pod "pod-subpath-test-downwardapi-k9nn" satisfied condition "success or failure"
Jan 23 00:54:31.838: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-subpath-test-downwardapi-k9nn container test-container-subpath-downwardapi-k9nn: <nil>
STEP: delete the pod
Jan 23 00:54:31.863: INFO: Waiting for pod pod-subpath-test-downwardapi-k9nn to disappear
Jan 23 00:54:31.867: INFO: Pod pod-subpath-test-downwardapi-k9nn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-k9nn
Jan 23 00:54:31.867: INFO: Deleting pod "pod-subpath-test-downwardapi-k9nn" in namespace "e2e-tests-subpath-p4kx2"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:54:31.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-p4kx2" for this suite.
Jan 23 00:54:37.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:54:37.905: INFO: namespace: e2e-tests-subpath-p4kx2, resource: bindings, ignored listing per whitelist
Jan 23 00:54:37.955: INFO: namespace e2e-tests-subpath-p4kx2 deletion completed in 6.082488651s

• [SLOW TEST:30.238 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:54:37.955: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 23 00:54:38.027: INFO: Waiting up to 5m0s for pod "pod-7571f960-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-8xg66" to be "success or failure"
Jan 23 00:54:38.033: INFO: Pod "pod-7571f960-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.359487ms
Jan 23 00:54:40.035: INFO: Pod "pod-7571f960-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008168271s
STEP: Saw pod success
Jan 23 00:54:40.035: INFO: Pod "pod-7571f960-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:54:40.037: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-7571f960-1ea9-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:54:40.050: INFO: Waiting for pod pod-7571f960-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:54:40.052: INFO: Pod pod-7571f960-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:54:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8xg66" for this suite.
Jan 23 00:54:46.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:54:46.113: INFO: namespace: e2e-tests-emptydir-8xg66, resource: bindings, ignored listing per whitelist
Jan 23 00:54:46.124: INFO: namespace e2e-tests-emptydir-8xg66 deletion completed in 6.070595324s

• [SLOW TEST:8.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:54:46.125: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0123 00:54:52.202169      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 00:54:52.202: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:54:52.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dt2vz" for this suite.
Jan 23 00:54:58.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:54:58.260: INFO: namespace: e2e-tests-gc-dt2vz, resource: bindings, ignored listing per whitelist
Jan 23 00:54:58.264: INFO: namespace e2e-tests-gc-dt2vz deletion completed in 6.058665405s

• [SLOW TEST:12.139 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:54:58.264: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:54:58.314: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:54:59.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-lvkqw" for this suite.
Jan 23 00:55:05.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:55:05.413: INFO: namespace: e2e-tests-custom-resource-definition-lvkqw, resource: bindings, ignored listing per whitelist
Jan 23 00:55:05.441: INFO: namespace e2e-tests-custom-resource-definition-lvkqw deletion completed in 6.070298864s

• [SLOW TEST:7.177 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:55:05.442: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:55:10.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-qxrkg" for this suite.
Jan 23 00:55:26.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:55:26.627: INFO: namespace: e2e-tests-replication-controller-qxrkg, resource: bindings, ignored listing per whitelist
Jan 23 00:55:26.636: INFO: namespace e2e-tests-replication-controller-qxrkg deletion completed in 16.071828396s

• [SLOW TEST:21.194 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:55:26.636: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0123 00:55:36.741776      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 00:55:36.741: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:55:36.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-chhpd" for this suite.
Jan 23 00:55:42.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:55:42.775: INFO: namespace: e2e-tests-gc-chhpd, resource: bindings, ignored listing per whitelist
Jan 23 00:55:42.803: INFO: namespace e2e-tests-gc-chhpd deletion completed in 6.058657711s

• [SLOW TEST:16.167 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:55:42.803: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:55:42.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-6gppj" to be "success or failure"
Jan 23 00:55:42.859: INFO: Pod "downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448303ms
Jan 23 00:55:44.863: INFO: Pod "downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00701055s
STEP: Saw pod success
Jan 23 00:55:44.863: INFO: Pod "downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:55:44.866: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:55:44.886: INFO: Waiting for pod downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:55:44.888: INFO: Pod downwardapi-volume-9c163edc-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:55:44.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6gppj" for this suite.
Jan 23 00:55:50.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:55:50.939: INFO: namespace: e2e-tests-projected-6gppj, resource: bindings, ignored listing per whitelist
Jan 23 00:55:50.965: INFO: namespace e2e-tests-projected-6gppj deletion completed in 6.0742579s

• [SLOW TEST:8.162 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:55:50.965: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 00:55:51.024: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 23 00:55:56.028: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 00:55:56.028: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 23 00:55:56.042: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-nn2rj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-nn2rj/deployments/test-cleanup-deployment,UID:a3f1a10e-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:10020,Generation:1,CreationTimestamp:2019-01-23 00:55:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 00:55:56.048: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 23 00:55:56.048: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 23 00:55:56.048: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-nn2rj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-nn2rj/replicasets/test-cleanup-controller,UID:a0f423c6-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:10021,Generation:1,CreationTimestamp:2019-01-23 00:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a3f1a10e-1ea9-11e9-9fdb-02424cc6ba99 0xc00264cc67 0xc00264cc68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 23 00:55:56.053: INFO: Pod "test-cleanup-controller-4p6rr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-4p6rr,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-nn2rj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-nn2rj/pods/test-cleanup-controller-4p6rr,UID:a0f5ad7a-1ea9-11e9-9fdb-02424cc6ba99,ResourceVersion:10010,Generation:0,CreationTimestamp:2019-01-23 00:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a0f423c6-1ea9-11e9-9fdb-02424cc6ba99 0xc0023840b7 0xc0023840b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-crtzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-crtzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-crtzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002384120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002384140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:55:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:55:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:55:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 00:55:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.4,StartTime:2019-01-23 00:55:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-23 00:55:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://e2b852cdca5ebb986018fe8c6216b3898277fce978349691aa946121d9d4bb0c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:55:56.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-nn2rj" for this suite.
Jan 23 00:56:02.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:56:02.092: INFO: namespace: e2e-tests-deployment-nn2rj, resource: bindings, ignored listing per whitelist
Jan 23 00:56:02.134: INFO: namespace e2e-tests-deployment-nn2rj deletion completed in 6.076534252s

• [SLOW TEST:11.169 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:56:02.134: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:56:02.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-7wmzc" for this suite.
Jan 23 00:56:08.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:56:08.218: INFO: namespace: e2e-tests-services-7wmzc, resource: bindings, ignored listing per whitelist
Jan 23 00:56:08.271: INFO: namespace e2e-tests-services-7wmzc deletion completed in 6.072695349s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.137 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:56:08.271: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ab4478f4-1ea9-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 00:56:08.327: INFO: Waiting up to 5m0s for pod "pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-5djkt" to be "success or failure"
Jan 23 00:56:08.331: INFO: Pod "pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658368ms
Jan 23 00:56:10.334: INFO: Pod "pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007183832s
STEP: Saw pod success
Jan 23 00:56:10.334: INFO: Pod "pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:56:10.337: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 00:56:10.352: INFO: Waiting for pod pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:56:10.356: INFO: Pod pod-secrets-ab44c4eb-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:56:10.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5djkt" for this suite.
Jan 23 00:56:16.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:56:16.421: INFO: namespace: e2e-tests-secrets-5djkt, resource: bindings, ignored listing per whitelist
Jan 23 00:56:16.430: INFO: namespace e2e-tests-secrets-5djkt deletion completed in 6.070659519s

• [SLOW TEST:8.159 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:56:16.431: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0123 00:56:47.021629      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 00:56:47.021: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:56:47.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4956f" for this suite.
Jan 23 00:56:53.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:56:53.059: INFO: namespace: e2e-tests-gc-4956f, resource: bindings, ignored listing per whitelist
Jan 23 00:56:53.090: INFO: namespace e2e-tests-gc-4956f deletion completed in 6.065576258s

• [SLOW TEST:36.659 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:56:53.090: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:56:53.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-4rbbn" to be "success or failure"
Jan 23 00:56:53.152: INFO: Pod "downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180724ms
Jan 23 00:56:55.156: INFO: Pod "downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009079162s
Jan 23 00:56:57.160: INFO: Pod "downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0126321s
STEP: Saw pod success
Jan 23 00:56:57.160: INFO: Pod "downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:56:57.163: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:56:57.181: INFO: Waiting for pod downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:56:57.183: INFO: Pod downwardapi-volume-c5fb6686-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:56:57.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4rbbn" for this suite.
Jan 23 00:57:03.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:57:03.239: INFO: namespace: e2e-tests-projected-4rbbn, resource: bindings, ignored listing per whitelist
Jan 23 00:57:03.253: INFO: namespace e2e-tests-projected-4rbbn deletion completed in 6.067581046s

• [SLOW TEST:10.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:57:03.253: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 00:57:03.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-zm8nh" to be "success or failure"
Jan 23 00:57:03.320: INFO: Pod "downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62699ms
Jan 23 00:57:05.323: INFO: Pod "downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826417s
Jan 23 00:57:07.327: INFO: Pod "downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010243853s
STEP: Saw pod success
Jan 23 00:57:07.327: INFO: Pod "downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:57:07.329: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 00:57:07.344: INFO: Waiting for pod downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:57:07.346: INFO: Pod downwardapi-volume-cc0ba7d5-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:57:07.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zm8nh" for this suite.
Jan 23 00:57:13.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:57:13.383: INFO: namespace: e2e-tests-downward-api-zm8nh, resource: bindings, ignored listing per whitelist
Jan 23 00:57:13.411: INFO: namespace e2e-tests-downward-api-zm8nh deletion completed in 6.059353541s

• [SLOW TEST:10.158 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:57:13.411: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 23 00:57:13.472: INFO: Waiting up to 5m0s for pod "pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-6ckkv" to be "success or failure"
Jan 23 00:57:13.475: INFO: Pod "pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.517797ms
Jan 23 00:57:15.479: INFO: Pod "pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007360682s
Jan 23 00:57:17.483: INFO: Pod "pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010875622s
STEP: Saw pod success
Jan 23 00:57:17.483: INFO: Pod "pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:57:17.485: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:57:17.504: INFO: Waiting for pod pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:57:17.506: INFO: Pod pod-d2193a46-1ea9-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:57:17.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6ckkv" for this suite.
Jan 23 00:57:23.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:57:23.582: INFO: namespace: e2e-tests-emptydir-6ckkv, resource: bindings, ignored listing per whitelist
Jan 23 00:57:23.590: INFO: namespace e2e-tests-emptydir-6ckkv deletion completed in 6.082112334s

• [SLOW TEST:10.179 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:57:23.590: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Jan 23 00:57:24.166: INFO: created pod pod-service-account-defaultsa
Jan 23 00:57:24.166: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 23 00:57:24.169: INFO: created pod pod-service-account-mountsa
Jan 23 00:57:24.169: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 23 00:57:24.172: INFO: created pod pod-service-account-nomountsa
Jan 23 00:57:24.172: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 23 00:57:24.176: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 23 00:57:24.176: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 23 00:57:24.185: INFO: created pod pod-service-account-mountsa-mountspec
Jan 23 00:57:24.185: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 23 00:57:24.190: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 23 00:57:24.190: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 23 00:57:24.194: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 23 00:57:24.194: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 23 00:57:24.202: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 23 00:57:24.202: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 23 00:57:24.207: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 23 00:57:24.207: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:57:24.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-jgbsf" for this suite.
Jan 23 00:57:30.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:57:30.271: INFO: namespace: e2e-tests-svcaccounts-jgbsf, resource: bindings, ignored listing per whitelist
Jan 23 00:57:30.294: INFO: namespace e2e-tests-svcaccounts-jgbsf deletion completed in 6.076431086s

• [SLOW TEST:6.703 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:57:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Jan 23 00:57:30.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 cluster-info'
Jan 23 00:57:30.691: INFO: stderr: ""
Jan 23 00:57:30.691: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:57:30.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v7ktw" for this suite.
Jan 23 00:57:36.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:57:36.743: INFO: namespace: e2e-tests-kubectl-v7ktw, resource: bindings, ignored listing per whitelist
Jan 23 00:57:36.763: INFO: namespace e2e-tests-kubectl-v7ktw deletion completed in 6.068943115s

• [SLOW TEST:6.469 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:57:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 00:57:36.831: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:36.832: INFO: Number of nodes with available pods: 0
Jan 23 00:57:36.832: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:57:37.835: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:37.838: INFO: Number of nodes with available pods: 0
Jan 23 00:57:37.838: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:57:38.835: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:38.839: INFO: Number of nodes with available pods: 1
Jan 23 00:57:38.839: INFO: Node kind-conformance-worker1 is running more than one daemon pod
Jan 23 00:57:39.835: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:39.838: INFO: Number of nodes with available pods: 2
Jan 23 00:57:39.838: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 23 00:57:39.852: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:39.854: INFO: Number of nodes with available pods: 1
Jan 23 00:57:39.854: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:40.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:40.860: INFO: Number of nodes with available pods: 1
Jan 23 00:57:40.860: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:41.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:41.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:41.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:42.857: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:42.860: INFO: Number of nodes with available pods: 1
Jan 23 00:57:42.860: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:43.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:43.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:43.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:44.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:44.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:44.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:45.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:45.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:45.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:46.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:46.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:46.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:47.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:47.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:47.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:48.857: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:48.860: INFO: Number of nodes with available pods: 1
Jan 23 00:57:48.860: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:49.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:49.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:49.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:50.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:50.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:50.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:51.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:51.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:51.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:52.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:52.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:52.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:53.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:53.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:53.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:54.857: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:54.860: INFO: Number of nodes with available pods: 1
Jan 23 00:57:54.860: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:55.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:55.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:55.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:56.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:56.861: INFO: Number of nodes with available pods: 1
Jan 23 00:57:56.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:57.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:57.860: INFO: Number of nodes with available pods: 1
Jan 23 00:57:57.860: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:58.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:58.864: INFO: Number of nodes with available pods: 1
Jan 23 00:57:58.864: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:57:59.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:57:59.862: INFO: Number of nodes with available pods: 1
Jan 23 00:57:59.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:00.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:00.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:00.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:01.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:01.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:01.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:02.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:02.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:02.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:03.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:03.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:03.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:04.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:04.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:04.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:05.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:05.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:05.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:06.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:06.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:06.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:07.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:07.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:07.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:08.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:08.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:08.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:09.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:09.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:09.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:10.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:10.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:10.863: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:11.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:11.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:11.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:12.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:12.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:12.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:13.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:13.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:13.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:14.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:14.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:14.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:15.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:15.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:15.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:16.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:16.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:16.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:17.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:17.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:17.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:18.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:18.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:18.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:19.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:19.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:19.863: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:20.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:20.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:20.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:21.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:21.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:21.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:22.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:22.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:22.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:23.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:23.861: INFO: Number of nodes with available pods: 1
Jan 23 00:58:23.861: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:24.859: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:24.862: INFO: Number of nodes with available pods: 1
Jan 23 00:58:24.862: INFO: Node kind-conformance-worker2 is running more than one daemon pod
Jan 23 00:58:25.858: INFO: DaemonSet pods can't tolerate node kind-conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 00:58:25.861: INFO: Number of nodes with available pods: 2
Jan 23 00:58:25.861: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-24gsw, will wait for the garbage collector to delete the pods
Jan 23 00:58:25.920: INFO: Deleting DaemonSet.extensions daemon-set took: 4.406436ms
Jan 23 00:58:26.020: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.298857ms
Jan 23 00:58:59.923: INFO: Number of nodes with available pods: 0
Jan 23 00:58:59.923: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 00:58:59.925: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-24gsw/daemonsets","resourceVersion":"10699"},"items":null}

Jan 23 00:58:59.927: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-24gsw/pods","resourceVersion":"10699"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:58:59.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-24gsw" for this suite.
Jan 23 00:59:05.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:59:05.991: INFO: namespace: e2e-tests-daemonsets-24gsw, resource: bindings, ignored listing per whitelist
Jan 23 00:59:05.997: INFO: namespace e2e-tests-daemonsets-24gsw deletion completed in 6.060772609s

• [SLOW TEST:89.233 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:59:05.997: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 23 00:59:06.048: INFO: Waiting up to 5m0s for pod "pod-153305d8-1eaa-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-p2zj4" to be "success or failure"
Jan 23 00:59:06.050: INFO: Pod "pod-153305d8-1eaa-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098287ms
Jan 23 00:59:08.053: INFO: Pod "pod-153305d8-1eaa-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004874609s
STEP: Saw pod success
Jan 23 00:59:08.053: INFO: Pod "pod-153305d8-1eaa-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 00:59:08.055: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-153305d8-1eaa-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 00:59:08.073: INFO: Waiting for pod pod-153305d8-1eaa-11e9-b567-9e07e353f0d4 to disappear
Jan 23 00:59:08.075: INFO: Pod pod-153305d8-1eaa-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 00:59:08.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-p2zj4" for this suite.
Jan 23 00:59:14.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 00:59:14.123: INFO: namespace: e2e-tests-emptydir-p2zj4, resource: bindings, ignored listing per whitelist
Jan 23 00:59:14.132: INFO: namespace e2e-tests-emptydir-p2zj4 deletion completed in 6.055008339s

• [SLOW TEST:8.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 00:59:14.132: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-7xkpt
Jan 23 00:59:34.189: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-7xkpt
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 00:59:34.192: INFO: Initial restart count of pod liveness-http is 0
Jan 23 00:59:54.224: INFO: Restart count of pod e2e-tests-container-probe-7xkpt/liveness-http is now 1 (20.032073447s elapsed)
Jan 23 01:00:14.261: INFO: Restart count of pod e2e-tests-container-probe-7xkpt/liveness-http is now 2 (40.069099533s elapsed)
Jan 23 01:00:34.292: INFO: Restart count of pod e2e-tests-container-probe-7xkpt/liveness-http is now 3 (1m0.099567552s elapsed)
Jan 23 01:00:52.320: INFO: Restart count of pod e2e-tests-container-probe-7xkpt/liveness-http is now 4 (1m18.127500913s elapsed)
Jan 23 01:02:02.430: INFO: Restart count of pod e2e-tests-container-probe-7xkpt/liveness-http is now 5 (2m28.238161689s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:02:02.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-7xkpt" for this suite.
Jan 23 01:02:08.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:02:08.475: INFO: namespace: e2e-tests-container-probe-7xkpt, resource: bindings, ignored listing per whitelist
Jan 23 01:02:08.513: INFO: namespace e2e-tests-container-probe-7xkpt deletion completed in 6.06478765s

• [SLOW TEST:174.381 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:02:08.513: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Jan 23 01:02:12.602: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:02:36.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-w29hf" for this suite.
Jan 23 01:02:42.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:02:42.666: INFO: namespace: e2e-tests-namespaces-w29hf, resource: bindings, ignored listing per whitelist
Jan 23 01:02:42.703: INFO: namespace e2e-tests-namespaces-w29hf deletion completed in 6.063723693s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mvhtg" for this suite.
Jan 23 01:02:42.705: INFO: Namespace e2e-tests-nsdeletetest-mvhtg was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-cgmsd" for this suite.
Jan 23 01:02:48.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:02:48.761: INFO: namespace: e2e-tests-nsdeletetest-cgmsd, resource: bindings, ignored listing per whitelist
Jan 23 01:02:48.776: INFO: namespace e2e-tests-nsdeletetest-cgmsd deletion completed in 6.070043122s

• [SLOW TEST:40.262 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:02:48.776: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 23 01:02:48.823: INFO: PodSpec: initContainers in spec.initContainers
Jan 23 01:03:35.809: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-99fc4ffc-1eaa-11e9-b567-9e07e353f0d4", GenerateName:"", Namespace:"e2e-tests-init-container-vnj8k", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-vnj8k/pods/pod-init-99fc4ffc-1eaa-11e9-b567-9e07e353f0d4", UID:"99fc9dde-1eaa-11e9-9fdb-02424cc6ba99", ResourceVersion:"11312", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683802168, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"823403217"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-v7zzf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002718c40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v7zzf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v7zzf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v7zzf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002406448), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-conformance-worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001a96780), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0024064c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0024064e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0024064e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0024064ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683802168, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683802168, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683802168, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683802168, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.9.4", PodIP:"10.32.0.3", StartTime:(*v1.Time)(0xc001e5a260), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022c2d20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022c2e00)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d9b187a7e982662f3d746124f3f0aa5a5491d36134a554849519eb0a1d155aff"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e5a2a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e5a280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:03:35.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-vnj8k" for this suite.
Jan 23 01:03:57.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:03:57.882: INFO: namespace: e2e-tests-init-container-vnj8k, resource: bindings, ignored listing per whitelist
Jan 23 01:03:57.882: INFO: namespace e2e-tests-init-container-vnj8k deletion completed in 22.068237023s

• [SLOW TEST:69.106 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:03:57.882: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:03:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:04:00.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nct5j" for this suite.
Jan 23 01:04:44.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:04:44.128: INFO: namespace: e2e-tests-pods-nct5j, resource: bindings, ignored listing per whitelist
Jan 23 01:04:44.150: INFO: namespace e2e-tests-pods-nct5j deletion completed in 44.06731265s

• [SLOW TEST:46.269 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:04:44.150: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-dec14ba3-1eaa-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:04:44.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-vw4vb" to be "success or failure"
Jan 23 01:04:44.214: INFO: Pod "pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.221539ms
Jan 23 01:04:46.217: INFO: Pod "pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01208926s
STEP: Saw pod success
Jan 23 01:04:46.217: INFO: Pod "pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:04:46.219: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:04:46.236: INFO: Waiting for pod pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:04:46.238: INFO: Pod pod-projected-configmaps-dec196bf-1eaa-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:04:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vw4vb" for this suite.
Jan 23 01:04:52.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:04:52.278: INFO: namespace: e2e-tests-projected-vw4vb, resource: bindings, ignored listing per whitelist
Jan 23 01:04:52.293: INFO: namespace e2e-tests-projected-vw4vb deletion completed in 6.052233005s

• [SLOW TEST:8.142 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:04:52.293: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:04:52.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 version --client'
Jan 23 01:04:52.389: INFO: stderr: ""
Jan 23 01:04:52.389: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jan 23 01:04:52.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-bpblx'
Jan 23 01:04:52.560: INFO: stderr: ""
Jan 23 01:04:52.561: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 23 01:04:52.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-bpblx'
Jan 23 01:04:52.728: INFO: stderr: ""
Jan 23 01:04:52.728: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 23 01:04:53.732: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:04:53.732: INFO: Found 0 / 1
Jan 23 01:04:54.732: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:04:54.732: INFO: Found 1 / 1
Jan 23 01:04:54.732: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 01:04:54.736: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:04:54.736: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 01:04:54.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 describe pod redis-master-lqcf5 --namespace=e2e-tests-kubectl-bpblx'
Jan 23 01:04:54.828: INFO: stderr: ""
Jan 23 01:04:54.828: INFO: stdout: "Name:               redis-master-lqcf5\nNamespace:          e2e-tests-kubectl-bpblx\nPriority:           0\nPriorityClassName:  <none>\nNode:               kind-conformance-worker1/192.168.9.3\nStart Time:         Wed, 23 Jan 2019 01:04:52 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.40.0.4\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://da9b0c20f33e371914aab3f3fe9ddc6cd7258c418ffb82da986b3b2e0dd894c3\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 23 Jan 2019 01:04:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-w9r8g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-w9r8g:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-w9r8g\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason            Age              From                               Message\n  ----     ------            ----             ----                               -------\n  Normal   Scheduled         2s               default-scheduler                  Successfully assigned e2e-tests-kubectl-bpblx/redis-master-lqcf5 to kind-conformance-worker1\n  Warning  DNSConfigForming  1s (x3 over 2s)  kubelet, kind-conformance-worker1  Search Line limits were exceeded, some search paths have been omitted, the applied search line is: e2e-tests-kubectl-bpblx.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com\n  Normal   Pulled            1s               kubelet, kind-conformance-worker1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal   Created           1s               kubelet, kind-conformance-worker1  Created container\n  Normal   Started           1s               kubelet, kind-conformance-worker1  Started container\n"
Jan 23 01:04:54.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 describe rc redis-master --namespace=e2e-tests-kubectl-bpblx'
Jan 23 01:04:54.919: INFO: stderr: ""
Jan 23 01:04:54.919: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-bpblx\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-lqcf5\n"
Jan 23 01:04:54.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 describe service redis-master --namespace=e2e-tests-kubectl-bpblx'
Jan 23 01:04:55.000: INFO: stderr: ""
Jan 23 01:04:55.000: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-bpblx\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.110.219.209\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.40.0.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 23 01:04:55.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 describe node kind-conformance-control-plane'
Jan 23 01:04:55.099: INFO: stderr: ""
Jan 23 01:04:55.099: INFO: stdout: "Name:               kind-conformance-control-plane\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=kind-conformance-control-plane\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 23 Jan 2019 00:10:09 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 23 Jan 2019 00:10:41 +0000   Wed, 23 Jan 2019 00:10:41 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Wed, 23 Jan 2019 01:04:53 +0000   Wed, 23 Jan 2019 00:10:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 23 Jan 2019 01:04:53 +0000   Wed, 23 Jan 2019 00:10:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 23 Jan 2019 01:04:53 +0000   Wed, 23 Jan 2019 00:10:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 23 Jan 2019 01:04:53 +0000   Wed, 23 Jan 2019 00:10:49 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.9.2\n  Hostname:    kind-conformance-control-plane\nCapacity:\n cpu:                12\n ephemeral-storage:  367702144Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65953676Ki\n pods:               110\nAllocatable:\n cpu:                12\n ephemeral-storage:  338874295350\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65851276Ki\n pods:               110\nSystem Info:\n Machine ID:                 2f04d39e62f549b78e9309c77b3deea4\n System UUID:                9f029e76-4671-11e7-9c43-bc0000fe0000\n Boot ID:                    8e5d24e3-5e59-4782-ad1e-cd2cd98e9f31\n Kernel Version:             4.17.0-3rodete2-amd64\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-nqrwq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                etcd-kind-conformance-control-plane                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                kube-apiserver-kind-conformance-control-plane              250m (2%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                kube-controller-manager-kind-conformance-control-plane     200m (1%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                kube-proxy-kvq2f                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                kube-scheduler-kind-conformance-control-plane              100m (0%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                weave-net-pd5nn                                            20m (0%)      0 (0%)      0 (0%)           0 (0%)         54m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                570m (4%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:\n  Type     Reason                    Age                From                                        Message\n  ----     ------                    ----               ----                                        -------\n  Normal   Starting                  54m                kubelet, kind-conformance-control-plane     Starting kubelet.\n  Normal   NodeHasSufficientMemory   54m (x7 over 54m)  kubelet, kind-conformance-control-plane     Node kind-conformance-control-plane status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     54m (x7 over 54m)  kubelet, kind-conformance-control-plane     Node kind-conformance-control-plane status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      54m (x6 over 54m)  kubelet, kind-conformance-control-plane     Node kind-conformance-control-plane status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced   54m                kubelet, kind-conformance-control-plane     Updated Node Allocatable limit across pods\n  Warning  CheckLimitsForResolvConf  54m (x3 over 54m)  kubelet, kind-conformance-control-plane     Resolv.conf file '/etc/resolv.conf' contains search line consisting of more than 3 domains!\n  Warning  readOnlySysFS             54m                kube-proxy, kind-conformance-control-plane  DOCKER RESTART NEEDED (docker issue #24000): /sys is read-only: cannot modify conntrack limits, problems may arise later.\n  Normal   Starting                  54m                kube-proxy, kind-conformance-control-plane  Starting kube-proxy.\n"
Jan 23 01:04:55.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 describe namespace e2e-tests-kubectl-bpblx'
Jan 23 01:04:55.179: INFO: stderr: ""
Jan 23 01:04:55.179: INFO: stdout: "Name:         e2e-tests-kubectl-bpblx\nLabels:       e2e-framework=kubectl\n              e2e-run=d320b24c-1ea3-11e9-b567-9e07e353f0d4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:04:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bpblx" for this suite.
Jan 23 01:05:17.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:05:17.243: INFO: namespace: e2e-tests-kubectl-bpblx, resource: bindings, ignored listing per whitelist
Jan 23 01:05:17.248: INFO: namespace e2e-tests-kubectl-bpblx deletion completed in 22.066121801s

• [SLOW TEST:24.955 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:05:17.248: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:05:17.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-ggv6j" to be "success or failure"
Jan 23 01:05:17.306: INFO: Pod "downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897349ms
Jan 23 01:05:19.309: INFO: Pod "downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004864356s
STEP: Saw pod success
Jan 23 01:05:19.309: INFO: Pod "downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:05:19.311: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:05:19.329: INFO: Waiting for pod downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:05:19.331: INFO: Pod downwardapi-volume-f27c1d3c-1eaa-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:05:19.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ggv6j" for this suite.
Jan 23 01:05:25.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:05:25.373: INFO: namespace: e2e-tests-projected-ggv6j, resource: bindings, ignored listing per whitelist
Jan 23 01:05:25.399: INFO: namespace e2e-tests-projected-ggv6j deletion completed in 6.066023866s

• [SLOW TEST:8.151 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:05:25.400: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 23 01:05:25.463: INFO: Waiting up to 5m0s for pod "pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-x9xv5" to be "success or failure"
Jan 23 01:05:25.468: INFO: Pod "pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.93049ms
Jan 23 01:05:27.470: INFO: Pod "pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007344202s
STEP: Saw pod success
Jan 23 01:05:27.470: INFO: Pod "pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:05:27.472: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:05:27.483: INFO: Waiting for pod pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:05:27.485: INFO: Pod pod-f758afe1-1eaa-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:05:27.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-x9xv5" for this suite.
Jan 23 01:05:33.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:05:33.530: INFO: namespace: e2e-tests-emptydir-x9xv5, resource: bindings, ignored listing per whitelist
Jan 23 01:05:33.554: INFO: namespace e2e-tests-emptydir-x9xv5 deletion completed in 6.067352187s

• [SLOW TEST:8.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:05:33.555: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 23 01:08:17.643: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:17.651: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:19.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:19.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:21.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:21.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:23.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:23.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:25.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:25.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:27.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:27.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:29.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:29.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:31.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:31.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:33.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:33.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:35.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:35.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:37.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:37.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:39.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:39.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:41.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:41.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:43.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:43.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:45.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:45.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:47.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:47.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:49.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:49.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:51.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:51.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:53.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:53.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:55.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:55.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:57.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:57.656: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:08:59.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:08:59.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:01.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:01.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:03.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:03.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:05.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:05.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:07.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:07.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:09.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:09.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:11.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:11.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:13.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:13.656: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:15.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:15.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:17.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:17.656: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:19.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:19.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:21.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:21.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:23.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:23.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:25.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:25.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:27.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:27.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:29.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:29.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:31.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:31.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:33.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:33.656: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:35.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:35.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:37.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:37.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:39.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:39.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:41.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:41.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:43.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:43.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:45.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:45.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:47.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:47.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:49.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:49.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:51.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:51.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:53.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:53.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:55.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:55.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:57.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:57.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:09:59.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:09:59.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:01.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:01.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:03.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:03.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:05.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:05.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:07.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:07.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:09.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:09.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 01:10:11.651: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 01:10:11.656: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:10:11.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7zp25" for this suite.
Jan 23 01:10:25.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:10:25.690: INFO: namespace: e2e-tests-container-lifecycle-hook-7zp25, resource: bindings, ignored listing per whitelist
Jan 23 01:10:25.726: INFO: namespace e2e-tests-container-lifecycle-hook-7zp25 deletion completed in 14.066671023s

• [SLOW TEST:292.171 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:10:25.726: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:10:25.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-vw2px" to be "success or failure"
Jan 23 01:10:25.788: INFO: Pod "downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803023ms
Jan 23 01:10:27.792: INFO: Pod "downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005229673s
STEP: Saw pod success
Jan 23 01:10:27.792: INFO: Pod "downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:10:27.794: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:10:27.810: INFO: Waiting for pod downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:10:27.811: INFO: Pod downwardapi-volume-aa5aa378-1eab-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:10:27.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vw2px" for this suite.
Jan 23 01:10:33.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:10:33.833: INFO: namespace: e2e-tests-projected-vw2px, resource: bindings, ignored listing per whitelist
Jan 23 01:10:33.887: INFO: namespace e2e-tests-projected-vw2px deletion completed in 6.073080504s

• [SLOW TEST:8.161 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:10:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:10:33.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-zrjtq" to be "success or failure"
Jan 23 01:10:33.948: INFO: Pod "downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282699ms
Jan 23 01:10:35.951: INFO: Pod "downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005056351s
STEP: Saw pod success
Jan 23 01:10:35.951: INFO: Pod "downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:10:35.953: INFO: Trying to get logs from node kind-conformance-worker1 pod downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:10:35.969: INFO: Waiting for pod downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:10:35.971: INFO: Pod downwardapi-volume-af37e6f4-1eab-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:10:35.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zrjtq" for this suite.
Jan 23 01:10:41.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:10:42.012: INFO: namespace: e2e-tests-projected-zrjtq, resource: bindings, ignored listing per whitelist
Jan 23 01:10:42.038: INFO: namespace e2e-tests-projected-zrjtq deletion completed in 6.063922732s

• [SLOW TEST:8.150 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:10:42.038: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vh4m7
Jan 23 01:10:44.100: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vh4m7
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 01:10:44.102: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:14:44.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vh4m7" for this suite.
Jan 23 01:14:50.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:14:50.552: INFO: namespace: e2e-tests-container-probe-vh4m7, resource: bindings, ignored listing per whitelist
Jan 23 01:14:50.582: INFO: namespace e2e-tests-container-probe-vh4m7 deletion completed in 6.068671892s

• [SLOW TEST:248.544 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:14:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-4838a305-1eac-11e9-b567-9e07e353f0d4
STEP: Creating secret with name s-test-opt-upd-4838a342-1eac-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4838a305-1eac-11e9-b567-9e07e353f0d4
STEP: Updating secret s-test-opt-upd-4838a342-1eac-11e9-b567-9e07e353f0d4
STEP: Creating secret with name s-test-opt-create-4838a360-1eac-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:14:54.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p2fkb" for this suite.
Jan 23 01:15:16.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:15:16.737: INFO: namespace: e2e-tests-projected-p2fkb, resource: bindings, ignored listing per whitelist
Jan 23 01:15:16.762: INFO: namespace e2e-tests-projected-p2fkb deletion completed in 22.056114799s

• [SLOW TEST:26.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:15:16.763: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-n5gbs/configmap-test-57d3fcfd-1eac-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:15:16.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-n5gbs" to be "success or failure"
Jan 23 01:15:16.832: INFO: Pod "pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411399ms
Jan 23 01:15:18.835: INFO: Pod "pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005293512s
Jan 23 01:15:20.838: INFO: Pod "pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007826578s
STEP: Saw pod success
Jan 23 01:15:20.838: INFO: Pod "pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:15:20.840: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4 container env-test: <nil>
STEP: delete the pod
Jan 23 01:15:20.853: INFO: Waiting for pod pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:15:20.855: INFO: Pod pod-configmaps-57d44c2b-1eac-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:15:20.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n5gbs" for this suite.
Jan 23 01:15:26.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:15:26.878: INFO: namespace: e2e-tests-configmap-n5gbs, resource: bindings, ignored listing per whitelist
Jan 23 01:15:26.922: INFO: namespace e2e-tests-configmap-n5gbs deletion completed in 6.064724572s

• [SLOW TEST:10.159 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:15:26.922: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:15:31.017: INFO: Waiting up to 5m0s for pod "client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-pods-5sr8x" to be "success or failure"
Jan 23 01:15:31.020: INFO: Pod "client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296981ms
Jan 23 01:15:33.023: INFO: Pod "client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006505643s
Jan 23 01:15:35.027: INFO: Pod "client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010109997s
STEP: Saw pod success
Jan 23 01:15:35.027: INFO: Pod "client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:15:35.030: INFO: Trying to get logs from node kind-conformance-worker2 pod client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4 container env3cont: <nil>
STEP: delete the pod
Jan 23 01:15:35.053: INFO: Waiting for pod client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:15:35.055: INFO: Pod client-envvars-60494bb1-1eac-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:15:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5sr8x" for this suite.
Jan 23 01:16:25.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:16:25.123: INFO: namespace: e2e-tests-pods-5sr8x, resource: bindings, ignored listing per whitelist
Jan 23 01:16:25.124: INFO: namespace e2e-tests-pods-5sr8x deletion completed in 50.066607091s

• [SLOW TEST:58.203 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:16:25.125: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-2wx8
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 01:16:25.181: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2wx8" in namespace "e2e-tests-subpath-xm2sn" to be "success or failure"
Jan 23 01:16:25.182: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531308ms
Jan 23 01:16:27.184: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003682268s
Jan 23 01:16:29.187: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 4.00655274s
Jan 23 01:16:31.190: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 6.009149029s
Jan 23 01:16:33.193: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 8.012333877s
Jan 23 01:16:35.196: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 10.015666228s
Jan 23 01:16:37.200: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 12.018839991s
Jan 23 01:16:39.202: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 14.021725202s
Jan 23 01:16:41.205: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 16.024309838s
Jan 23 01:16:43.208: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 18.027206001s
Jan 23 01:16:45.210: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 20.029518974s
Jan 23 01:16:47.214: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Running", Reason="", readiness=false. Elapsed: 22.032959449s
Jan 23 01:16:49.217: INFO: Pod "pod-subpath-test-configmap-2wx8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036190843s
STEP: Saw pod success
Jan 23 01:16:49.217: INFO: Pod "pod-subpath-test-configmap-2wx8" satisfied condition "success or failure"
Jan 23 01:16:49.220: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-subpath-test-configmap-2wx8 container test-container-subpath-configmap-2wx8: <nil>
STEP: delete the pod
Jan 23 01:16:49.243: INFO: Waiting for pod pod-subpath-test-configmap-2wx8 to disappear
Jan 23 01:16:49.247: INFO: Pod pod-subpath-test-configmap-2wx8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2wx8
Jan 23 01:16:49.247: INFO: Deleting pod "pod-subpath-test-configmap-2wx8" in namespace "e2e-tests-subpath-xm2sn"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:16:49.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-xm2sn" for this suite.
Jan 23 01:16:55.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:16:55.307: INFO: namespace: e2e-tests-subpath-xm2sn, resource: bindings, ignored listing per whitelist
Jan 23 01:16:55.321: INFO: namespace e2e-tests-subpath-xm2sn deletion completed in 6.069227685s

• [SLOW TEST:30.196 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:16:55.321: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-9290a729-1eac-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:16:55.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-thn4j" to be "success or failure"
Jan 23 01:16:55.374: INFO: Pod "pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047801ms
Jan 23 01:16:57.376: INFO: Pod "pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00475924s
STEP: Saw pod success
Jan 23 01:16:57.376: INFO: Pod "pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:16:57.379: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:16:57.395: INFO: Waiting for pod pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:16:57.397: INFO: Pod pod-configmaps-9290e05d-1eac-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:16:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-thn4j" for this suite.
Jan 23 01:17:03.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:17:03.423: INFO: namespace: e2e-tests-configmap-thn4j, resource: bindings, ignored listing per whitelist
Jan 23 01:17:03.464: INFO: namespace e2e-tests-configmap-thn4j deletion completed in 6.065083573s

• [SLOW TEST:8.143 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:17:03.464: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 23 01:17:06.032: INFO: Successfully updated pod "annotationupdate976a5f01-1eac-11e9-b567-9e07e353f0d4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:17:08.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xjlr2" for this suite.
Jan 23 01:17:30.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:17:30.097: INFO: namespace: e2e-tests-downward-api-xjlr2, resource: bindings, ignored listing per whitelist
Jan 23 01:17:30.120: INFO: namespace e2e-tests-downward-api-xjlr2 deletion completed in 22.063447631s

• [SLOW TEST:26.656 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:17:30.120: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4
Jan 23 01:17:30.173: INFO: Pod name my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4: Found 0 pods out of 1
Jan 23 01:17:35.176: INFO: Pod name my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4: Found 1 pods out of 1
Jan 23 01:17:35.176: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4" are running
Jan 23 01:17:35.178: INFO: Pod "my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4-plxln" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:17:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:17:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:17:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:17:30 +0000 UTC Reason: Message:}])
Jan 23 01:17:35.178: INFO: Trying to dial the pod
Jan 23 01:17:40.188: INFO: Controller my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4: Got expected result from replica 1 [my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4-plxln]: "my-hostname-basic-a74eee19-1eac-11e9-b567-9e07e353f0d4-plxln", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:17:40.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-5s5gw" for this suite.
Jan 23 01:17:46.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:17:46.225: INFO: namespace: e2e-tests-replication-controller-5s5gw, resource: bindings, ignored listing per whitelist
Jan 23 01:17:46.256: INFO: namespace e2e-tests-replication-controller-5s5gw deletion completed in 6.065273216s

• [SLOW TEST:16.136 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:17:46.256: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Jan 23 01:17:46.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:46.808: INFO: stderr: ""
Jan 23 01:17:46.808: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 01:17:46.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:46.875: INFO: stderr: ""
Jan 23 01:17:46.875: INFO: stdout: "update-demo-nautilus-7dk49 update-demo-nautilus-8zpkv "
Jan 23 01:17:46.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7dk49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:46.952: INFO: stderr: ""
Jan 23 01:17:46.952: INFO: stdout: ""
Jan 23 01:17:46.952: INFO: update-demo-nautilus-7dk49 is created but not running
Jan 23 01:17:51.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:52.018: INFO: stderr: ""
Jan 23 01:17:52.018: INFO: stdout: "update-demo-nautilus-7dk49 update-demo-nautilus-8zpkv "
Jan 23 01:17:52.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7dk49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:52.085: INFO: stderr: ""
Jan 23 01:17:52.085: INFO: stdout: "true"
Jan 23 01:17:52.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-7dk49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:52.160: INFO: stderr: ""
Jan 23 01:17:52.161: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 01:17:52.161: INFO: validating pod update-demo-nautilus-7dk49
Jan 23 01:17:52.164: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 01:17:52.164: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 01:17:52.164: INFO: update-demo-nautilus-7dk49 is verified up and running
Jan 23 01:17:52.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-8zpkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:52.225: INFO: stderr: ""
Jan 23 01:17:52.225: INFO: stdout: "true"
Jan 23 01:17:52.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-nautilus-8zpkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:17:52.293: INFO: stderr: ""
Jan 23 01:17:52.293: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 01:17:52.293: INFO: validating pod update-demo-nautilus-8zpkv
Jan 23 01:17:52.296: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 01:17:52.296: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 01:17:52.297: INFO: update-demo-nautilus-8zpkv is verified up and running
STEP: rolling-update to new replication controller
Jan 23 01:17:52.298: INFO: scanned /root for discovery docs: <nil>
Jan 23 01:17:52.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:23.661: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 23 01:18:23.661: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 01:18:23.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:23.734: INFO: stderr: ""
Jan 23 01:18:23.734: INFO: stdout: "update-demo-kitten-bms57 update-demo-kitten-ddhm8 "
Jan 23 01:18:23.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-kitten-bms57 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:23.801: INFO: stderr: ""
Jan 23 01:18:23.801: INFO: stdout: "true"
Jan 23 01:18:23.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-kitten-bms57 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:23.881: INFO: stderr: ""
Jan 23 01:18:23.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 23 01:18:23.881: INFO: validating pod update-demo-kitten-bms57
Jan 23 01:18:23.885: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 23 01:18:23.885: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 23 01:18:23.885: INFO: update-demo-kitten-bms57 is verified up and running
Jan 23 01:18:23.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-kitten-ddhm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:23.958: INFO: stderr: ""
Jan 23 01:18:23.958: INFO: stdout: "true"
Jan 23 01:18:23.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods update-demo-kitten-ddhm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9nf2b'
Jan 23 01:18:24.024: INFO: stderr: ""
Jan 23 01:18:24.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 23 01:18:24.024: INFO: validating pod update-demo-kitten-ddhm8
Jan 23 01:18:24.027: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 23 01:18:24.027: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 23 01:18:24.027: INFO: update-demo-kitten-ddhm8 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:18:24.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9nf2b" for this suite.
Jan 23 01:18:46.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:18:46.061: INFO: namespace: e2e-tests-kubectl-9nf2b, resource: bindings, ignored listing per whitelist
Jan 23 01:18:46.094: INFO: namespace e2e-tests-kubectl-9nf2b deletion completed in 22.064709655s

• [SLOW TEST:59.838 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:18:46.094: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-bwtkl
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-bwtkl
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-bwtkl
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-bwtkl
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-bwtkl
Jan 23 01:18:48.159: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bwtkl, name: ss-0, uid: d4a3df20-1eac-11e9-9fdb-02424cc6ba99, status phase: Pending. Waiting for statefulset controller to delete.
Jan 23 01:18:51.611: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bwtkl, name: ss-0, uid: d4a3df20-1eac-11e9-9fdb-02424cc6ba99, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 01:18:51.619: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-bwtkl, name: ss-0, uid: d4a3df20-1eac-11e9-9fdb-02424cc6ba99, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 01:18:51.624: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-bwtkl
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-bwtkl
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-bwtkl and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 23 01:19:01.652: INFO: Deleting all statefulset in ns e2e-tests-statefulset-bwtkl
Jan 23 01:19:01.654: INFO: Scaling statefulset ss to 0
Jan 23 01:19:11.665: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 01:19:11.666: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:19:11.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-bwtkl" for this suite.
Jan 23 01:19:17.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:19:17.728: INFO: namespace: e2e-tests-statefulset-bwtkl, resource: bindings, ignored listing per whitelist
Jan 23 01:19:17.754: INFO: namespace e2e-tests-statefulset-bwtkl deletion completed in 6.073680455s

• [SLOW TEST:31.660 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:19:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zgf5v.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zgf5v.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 01:19:41.830: INFO: Unable to read wheezy_udp@kubernetes.default from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.833: INFO: Unable to read wheezy_tcp@kubernetes.default from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.836: INFO: Unable to read wheezy_udp@kubernetes.default.svc from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.838: INFO: Unable to read wheezy_tcp@kubernetes.default.svc from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.840: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.842: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.844: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.852: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.854: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.856: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.857: INFO: Unable to read jessie_udp@kubernetes.default from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.860: INFO: Unable to read jessie_tcp@kubernetes.default from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.862: INFO: Unable to read jessie_udp@kubernetes.default.svc from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.868: INFO: Unable to read jessie_tcp@kubernetes.default.svc from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.870: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.873: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.875: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.876: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.879: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.880: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4: the server could not find the requested resource (get pods dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4)
Jan 23 01:19:41.880: INFO: Lookups using e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4 failed for: [wheezy_udp@kubernetes.default wheezy_tcp@kubernetes.default wheezy_udp@kubernetes.default.svc wheezy_tcp@kubernetes.default.svc wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default jessie_tcp@kubernetes.default jessie_udp@kubernetes.default.svc jessie_tcp@kubernetes.default.svc jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-zgf5v.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jan 23 01:19:46.923: INFO: DNS probes using e2e-tests-dns-zgf5v/dns-test-e7777f47-1eac-11e9-b567-9e07e353f0d4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:19:46.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-zgf5v" for this suite.
Jan 23 01:19:52.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:19:52.985: INFO: namespace: e2e-tests-dns-zgf5v, resource: bindings, ignored listing per whitelist
Jan 23 01:19:53.003: INFO: namespace e2e-tests-dns-zgf5v deletion completed in 6.063640279s

• [SLOW TEST:35.249 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:19:53.003: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-27vm
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 01:19:53.057: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-27vm" in namespace "e2e-tests-subpath-55d67" to be "success or failure"
Jan 23 01:19:53.061: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.822787ms
Jan 23 01:19:55.064: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00716041s
Jan 23 01:19:57.067: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 4.010468221s
Jan 23 01:19:59.071: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 6.013810307s
Jan 23 01:20:01.074: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 8.017199588s
Jan 23 01:20:03.076: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 10.019567886s
Jan 23 01:20:05.079: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 12.022370512s
Jan 23 01:20:07.083: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 14.026309525s
Jan 23 01:20:09.086: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 16.028678722s
Jan 23 01:20:11.089: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 18.032086301s
Jan 23 01:20:13.092: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 20.035330163s
Jan 23 01:20:15.096: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Running", Reason="", readiness=false. Elapsed: 22.038934217s
Jan 23 01:20:17.099: INFO: Pod "pod-subpath-test-secret-27vm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042381849s
STEP: Saw pod success
Jan 23 01:20:17.099: INFO: Pod "pod-subpath-test-secret-27vm" satisfied condition "success or failure"
Jan 23 01:20:17.102: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-subpath-test-secret-27vm container test-container-subpath-secret-27vm: <nil>
STEP: delete the pod
Jan 23 01:20:17.119: INFO: Waiting for pod pod-subpath-test-secret-27vm to disappear
Jan 23 01:20:17.121: INFO: Pod pod-subpath-test-secret-27vm no longer exists
STEP: Deleting pod pod-subpath-test-secret-27vm
Jan 23 01:20:17.121: INFO: Deleting pod "pod-subpath-test-secret-27vm" in namespace "e2e-tests-subpath-55d67"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:20:17.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-55d67" for this suite.
Jan 23 01:20:23.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:20:23.179: INFO: namespace: e2e-tests-subpath-55d67, resource: bindings, ignored listing per whitelist
Jan 23 01:20:23.196: INFO: namespace e2e-tests-subpath-55d67 deletion completed in 6.070317087s

• [SLOW TEST:30.193 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:20:23.196: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 23 01:20:23.242: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:20:27.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8kpsz" for this suite.
Jan 23 01:20:33.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:20:33.132: INFO: namespace: e2e-tests-init-container-8kpsz, resource: bindings, ignored listing per whitelist
Jan 23 01:20:33.148: INFO: namespace e2e-tests-init-container-8kpsz deletion completed in 6.072261616s

• [SLOW TEST:9.952 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:20:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 23 01:20:33.382: INFO: Pod name wrapped-volume-race-147637c1-1ead-11e9-b567-9e07e353f0d4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-147637c1-1ead-11e9-b567-9e07e353f0d4 in namespace e2e-tests-emptydir-wrapper-vcjvb, will wait for the garbage collector to delete the pods
Jan 23 01:20:49.498: INFO: Deleting ReplicationController wrapped-volume-race-147637c1-1ead-11e9-b567-9e07e353f0d4 took: 4.504445ms
Jan 23 01:20:49.598: INFO: Terminating ReplicationController wrapped-volume-race-147637c1-1ead-11e9-b567-9e07e353f0d4 pods took: 100.204123ms
STEP: Creating RC which spawns configmap-volume pods
Jan 23 01:21:32.117: INFO: Pod name wrapped-volume-race-378299ff-1ead-11e9-b567-9e07e353f0d4: Found 0 pods out of 5
Jan 23 01:21:37.122: INFO: Pod name wrapped-volume-race-378299ff-1ead-11e9-b567-9e07e353f0d4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-378299ff-1ead-11e9-b567-9e07e353f0d4 in namespace e2e-tests-emptydir-wrapper-vcjvb, will wait for the garbage collector to delete the pods
Jan 23 01:21:49.264: INFO: Deleting ReplicationController wrapped-volume-race-378299ff-1ead-11e9-b567-9e07e353f0d4 took: 69.131839ms
Jan 23 01:21:49.464: INFO: Terminating ReplicationController wrapped-volume-race-378299ff-1ead-11e9-b567-9e07e353f0d4 pods took: 200.187009ms
STEP: Creating RC which spawns configmap-volume pods
Jan 23 01:22:31.780: INFO: Pod name wrapped-volume-race-5b12d054-1ead-11e9-b567-9e07e353f0d4: Found 0 pods out of 5
Jan 23 01:22:36.785: INFO: Pod name wrapped-volume-race-5b12d054-1ead-11e9-b567-9e07e353f0d4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5b12d054-1ead-11e9-b567-9e07e353f0d4 in namespace e2e-tests-emptydir-wrapper-vcjvb, will wait for the garbage collector to delete the pods
Jan 23 01:22:46.862: INFO: Deleting ReplicationController wrapped-volume-race-5b12d054-1ead-11e9-b567-9e07e353f0d4 took: 6.12969ms
Jan 23 01:22:46.962: INFO: Terminating ReplicationController wrapped-volume-race-5b12d054-1ead-11e9-b567-9e07e353f0d4 pods took: 100.207581ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:23:23.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-vcjvb" for this suite.
Jan 23 01:23:29.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:23:29.305: INFO: namespace: e2e-tests-emptydir-wrapper-vcjvb, resource: bindings, ignored listing per whitelist
Jan 23 01:23:29.320: INFO: namespace e2e-tests-emptydir-wrapper-vcjvb deletion completed in 6.08020283s

• [SLOW TEST:176.172 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:23:29.320: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-c2r59
I0123 01:23:29.383370      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-c2r59, replica count: 1
I0123 01:23:30.433732      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 01:23:31.433905      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 01:23:31.545: INFO: Created: latency-svc-mjf65
Jan 23 01:23:31.562: INFO: Got endpoints: latency-svc-mjf65 [28.379698ms]
Jan 23 01:23:31.573: INFO: Created: latency-svc-x7mbh
Jan 23 01:23:31.583: INFO: Created: latency-svc-m8rhc
Jan 23 01:23:31.583: INFO: Got endpoints: latency-svc-x7mbh [20.942702ms]
Jan 23 01:23:31.591: INFO: Got endpoints: latency-svc-m8rhc [28.292176ms]
Jan 23 01:23:31.591: INFO: Created: latency-svc-pt2hr
Jan 23 01:23:31.593: INFO: Got endpoints: latency-svc-pt2hr [30.823552ms]
Jan 23 01:23:31.606: INFO: Created: latency-svc-nw24w
Jan 23 01:23:31.609: INFO: Got endpoints: latency-svc-nw24w [46.539224ms]
Jan 23 01:23:31.618: INFO: Created: latency-svc-mnbjl
Jan 23 01:23:31.625: INFO: Got endpoints: latency-svc-mnbjl [62.174872ms]
Jan 23 01:23:31.631: INFO: Created: latency-svc-zqrcv
Jan 23 01:23:31.634: INFO: Got endpoints: latency-svc-zqrcv [71.823747ms]
Jan 23 01:23:31.642: INFO: Created: latency-svc-btk29
Jan 23 01:23:31.648: INFO: Got endpoints: latency-svc-btk29 [85.965289ms]
Jan 23 01:23:31.656: INFO: Created: latency-svc-w7b2t
Jan 23 01:23:31.677: INFO: Got endpoints: latency-svc-w7b2t [114.371777ms]
Jan 23 01:23:31.678: INFO: Created: latency-svc-g5ckx
Jan 23 01:23:31.681: INFO: Got endpoints: latency-svc-g5ckx [118.671987ms]
Jan 23 01:23:31.689: INFO: Created: latency-svc-tfxxg
Jan 23 01:23:31.697: INFO: Got endpoints: latency-svc-tfxxg [134.884865ms]
Jan 23 01:23:31.704: INFO: Created: latency-svc-qftfd
Jan 23 01:23:31.710: INFO: Got endpoints: latency-svc-qftfd [147.417566ms]
Jan 23 01:23:31.716: INFO: Created: latency-svc-qs8z5
Jan 23 01:23:31.717: INFO: Got endpoints: latency-svc-qs8z5 [153.941204ms]
Jan 23 01:23:31.731: INFO: Created: latency-svc-qlzkp
Jan 23 01:23:31.735: INFO: Got endpoints: latency-svc-qlzkp [172.765132ms]
Jan 23 01:23:31.741: INFO: Created: latency-svc-z5vbb
Jan 23 01:23:31.744: INFO: Got endpoints: latency-svc-z5vbb [181.740533ms]
Jan 23 01:23:31.753: INFO: Created: latency-svc-j2x2l
Jan 23 01:23:31.756: INFO: Got endpoints: latency-svc-j2x2l [193.708538ms]
Jan 23 01:23:31.769: INFO: Created: latency-svc-4plpt
Jan 23 01:23:31.772: INFO: Got endpoints: latency-svc-4plpt [188.977064ms]
Jan 23 01:23:31.857: INFO: Created: latency-svc-zq72r
Jan 23 01:23:31.858: INFO: Got endpoints: latency-svc-zq72r [266.77241ms]
Jan 23 01:23:31.866: INFO: Created: latency-svc-ml9xb
Jan 23 01:23:31.871: INFO: Got endpoints: latency-svc-ml9xb [278.098779ms]
Jan 23 01:23:31.882: INFO: Created: latency-svc-s7zgk
Jan 23 01:23:31.884: INFO: Got endpoints: latency-svc-s7zgk [275.104705ms]
Jan 23 01:23:31.952: INFO: Created: latency-svc-lf52j
Jan 23 01:23:31.954: INFO: Got endpoints: latency-svc-lf52j [328.944095ms]
Jan 23 01:23:31.960: INFO: Created: latency-svc-nlbfs
Jan 23 01:23:31.965: INFO: Got endpoints: latency-svc-nlbfs [330.530168ms]
Jan 23 01:23:31.970: INFO: Created: latency-svc-qcnnr
Jan 23 01:23:31.973: INFO: Got endpoints: latency-svc-qcnnr [324.327048ms]
Jan 23 01:23:31.982: INFO: Created: latency-svc-b5jcw
Jan 23 01:23:31.984: INFO: Got endpoints: latency-svc-b5jcw [307.53481ms]
Jan 23 01:23:31.991: INFO: Created: latency-svc-mrg2k
Jan 23 01:23:31.994: INFO: Got endpoints: latency-svc-mrg2k [312.240132ms]
Jan 23 01:23:32.004: INFO: Created: latency-svc-srbml
Jan 23 01:23:32.020: INFO: Got endpoints: latency-svc-srbml [322.34941ms]
Jan 23 01:23:32.023: INFO: Created: latency-svc-bzbxn
Jan 23 01:23:32.025: INFO: Got endpoints: latency-svc-bzbxn [315.104349ms]
Jan 23 01:23:32.035: INFO: Created: latency-svc-lvghr
Jan 23 01:23:32.038: INFO: Got endpoints: latency-svc-lvghr [321.28522ms]
Jan 23 01:23:32.050: INFO: Created: latency-svc-vnmw7
Jan 23 01:23:32.051: INFO: Got endpoints: latency-svc-vnmw7 [315.412427ms]
Jan 23 01:23:32.058: INFO: Created: latency-svc-m54tk
Jan 23 01:23:32.060: INFO: Got endpoints: latency-svc-m54tk [315.89439ms]
Jan 23 01:23:32.067: INFO: Created: latency-svc-26bhd
Jan 23 01:23:32.075: INFO: Created: latency-svc-w55dw
Jan 23 01:23:32.075: INFO: Got endpoints: latency-svc-26bhd [318.884745ms]
Jan 23 01:23:32.083: INFO: Got endpoints: latency-svc-w55dw [310.365887ms]
Jan 23 01:23:32.092: INFO: Created: latency-svc-jdhzn
Jan 23 01:23:32.095: INFO: Got endpoints: latency-svc-jdhzn [237.446123ms]
Jan 23 01:23:32.101: INFO: Created: latency-svc-zbgc8
Jan 23 01:23:32.103: INFO: Got endpoints: latency-svc-zbgc8 [231.616304ms]
Jan 23 01:23:32.115: INFO: Created: latency-svc-qbjsc
Jan 23 01:23:32.134: INFO: Got endpoints: latency-svc-qbjsc [250.486397ms]
Jan 23 01:23:32.136: INFO: Created: latency-svc-f48fq
Jan 23 01:23:32.139: INFO: Got endpoints: latency-svc-f48fq [184.922923ms]
Jan 23 01:23:32.148: INFO: Created: latency-svc-ws8zm
Jan 23 01:23:32.153: INFO: Got endpoints: latency-svc-ws8zm [188.235898ms]
Jan 23 01:23:32.158: INFO: Created: latency-svc-gqrxt
Jan 23 01:23:32.161: INFO: Got endpoints: latency-svc-gqrxt [188.234646ms]
Jan 23 01:23:32.171: INFO: Created: latency-svc-49bkz
Jan 23 01:23:32.174: INFO: Got endpoints: latency-svc-49bkz [189.258979ms]
Jan 23 01:23:32.183: INFO: Created: latency-svc-nr6fk
Jan 23 01:23:32.193: INFO: Got endpoints: latency-svc-nr6fk [198.94645ms]
Jan 23 01:23:32.193: INFO: Created: latency-svc-6txbv
Jan 23 01:23:32.197: INFO: Got endpoints: latency-svc-6txbv [177.47849ms]
Jan 23 01:23:32.205: INFO: Created: latency-svc-wgbzf
Jan 23 01:23:32.209: INFO: Got endpoints: latency-svc-wgbzf [183.671261ms]
Jan 23 01:23:32.215: INFO: Created: latency-svc-vg27v
Jan 23 01:23:32.219: INFO: Got endpoints: latency-svc-vg27v [181.111086ms]
Jan 23 01:23:32.233: INFO: Created: latency-svc-5lkcz
Jan 23 01:23:32.255: INFO: Got endpoints: latency-svc-5lkcz [204.259702ms]
Jan 23 01:23:32.255: INFO: Created: latency-svc-7ns9p
Jan 23 01:23:32.267: INFO: Created: latency-svc-7427x
Jan 23 01:23:32.282: INFO: Created: latency-svc-xmjs2
Jan 23 01:23:32.290: INFO: Created: latency-svc-878h2
Jan 23 01:23:32.303: INFO: Created: latency-svc-9rsrr
Jan 23 01:23:32.303: INFO: Got endpoints: latency-svc-7ns9p [243.058982ms]
Jan 23 01:23:32.314: INFO: Created: latency-svc-dswq9
Jan 23 01:23:32.323: INFO: Created: latency-svc-6lxgk
Jan 23 01:23:32.331: INFO: Created: latency-svc-hhvzh
Jan 23 01:23:32.345: INFO: Created: latency-svc-ld7pd
Jan 23 01:23:32.349: INFO: Got endpoints: latency-svc-7427x [273.670718ms]
Jan 23 01:23:32.387: INFO: Created: latency-svc-sm9bn
Jan 23 01:23:32.404: INFO: Got endpoints: latency-svc-xmjs2 [321.041001ms]
Jan 23 01:23:32.404: INFO: Created: latency-svc-n7zzt
Jan 23 01:23:32.415: INFO: Created: latency-svc-295rn
Jan 23 01:23:32.430: INFO: Created: latency-svc-fw2c2
Jan 23 01:23:32.450: INFO: Got endpoints: latency-svc-878h2 [354.435914ms]
Jan 23 01:23:32.450: INFO: Created: latency-svc-jmvjv
Jan 23 01:23:32.463: INFO: Created: latency-svc-w8crl
Jan 23 01:23:32.474: INFO: Created: latency-svc-6nppx
Jan 23 01:23:32.497: INFO: Created: latency-svc-bpph8
Jan 23 01:23:32.500: INFO: Got endpoints: latency-svc-9rsrr [397.075968ms]
Jan 23 01:23:32.508: INFO: Created: latency-svc-7ncvl
Jan 23 01:23:32.519: INFO: Created: latency-svc-cpngq
Jan 23 01:23:32.530: INFO: Created: latency-svc-79hnh
Jan 23 01:23:32.548: INFO: Got endpoints: latency-svc-dswq9 [413.925395ms]
Jan 23 01:23:32.561: INFO: Created: latency-svc-hnhpv
Jan 23 01:23:32.609: INFO: Got endpoints: latency-svc-6lxgk [470.811631ms]
Jan 23 01:23:32.623: INFO: Created: latency-svc-gsqsg
Jan 23 01:23:32.648: INFO: Got endpoints: latency-svc-hhvzh [495.033542ms]
Jan 23 01:23:32.660: INFO: Created: latency-svc-pwc5f
Jan 23 01:23:32.698: INFO: Got endpoints: latency-svc-ld7pd [536.655953ms]
Jan 23 01:23:32.707: INFO: Created: latency-svc-wbqdv
Jan 23 01:23:32.748: INFO: Got endpoints: latency-svc-sm9bn [574.504523ms]
Jan 23 01:23:32.759: INFO: Created: latency-svc-57r4q
Jan 23 01:23:32.800: INFO: Got endpoints: latency-svc-n7zzt [607.039991ms]
Jan 23 01:23:32.828: INFO: Created: latency-svc-ft76b
Jan 23 01:23:32.850: INFO: Got endpoints: latency-svc-295rn [652.401496ms]
Jan 23 01:23:32.860: INFO: Created: latency-svc-zh2vc
Jan 23 01:23:32.898: INFO: Got endpoints: latency-svc-fw2c2 [689.354538ms]
Jan 23 01:23:32.908: INFO: Created: latency-svc-gcvzk
Jan 23 01:23:32.949: INFO: Got endpoints: latency-svc-jmvjv [729.587379ms]
Jan 23 01:23:32.960: INFO: Created: latency-svc-pd56w
Jan 23 01:23:32.998: INFO: Got endpoints: latency-svc-w8crl [743.166981ms]
Jan 23 01:23:33.016: INFO: Created: latency-svc-jwl6l
Jan 23 01:23:33.054: INFO: Got endpoints: latency-svc-6nppx [750.437438ms]
Jan 23 01:23:33.067: INFO: Created: latency-svc-sm58z
Jan 23 01:23:33.098: INFO: Got endpoints: latency-svc-bpph8 [749.599066ms]
Jan 23 01:23:33.108: INFO: Created: latency-svc-dsqmk
Jan 23 01:23:33.148: INFO: Got endpoints: latency-svc-7ncvl [744.56165ms]
Jan 23 01:23:33.171: INFO: Created: latency-svc-blw5h
Jan 23 01:23:33.198: INFO: Got endpoints: latency-svc-cpngq [748.141149ms]
Jan 23 01:23:33.209: INFO: Created: latency-svc-xf57w
Jan 23 01:23:33.249: INFO: Got endpoints: latency-svc-79hnh [748.549452ms]
Jan 23 01:23:33.260: INFO: Created: latency-svc-bsbjx
Jan 23 01:23:33.298: INFO: Got endpoints: latency-svc-hnhpv [749.96904ms]
Jan 23 01:23:33.323: INFO: Created: latency-svc-4qlsg
Jan 23 01:23:33.349: INFO: Got endpoints: latency-svc-gsqsg [739.033772ms]
Jan 23 01:23:33.360: INFO: Created: latency-svc-mz75k
Jan 23 01:23:33.398: INFO: Got endpoints: latency-svc-pwc5f [749.962578ms]
Jan 23 01:23:33.413: INFO: Created: latency-svc-fq2jc
Jan 23 01:23:33.449: INFO: Got endpoints: latency-svc-wbqdv [750.836746ms]
Jan 23 01:23:33.459: INFO: Created: latency-svc-qsm87
Jan 23 01:23:33.505: INFO: Got endpoints: latency-svc-57r4q [756.220404ms]
Jan 23 01:23:33.514: INFO: Created: latency-svc-gwmwr
Jan 23 01:23:33.548: INFO: Got endpoints: latency-svc-ft76b [748.458889ms]
Jan 23 01:23:33.560: INFO: Created: latency-svc-4s7mx
Jan 23 01:23:33.598: INFO: Got endpoints: latency-svc-zh2vc [748.265215ms]
Jan 23 01:23:33.630: INFO: Created: latency-svc-zrnjn
Jan 23 01:23:33.650: INFO: Got endpoints: latency-svc-gcvzk [751.399732ms]
Jan 23 01:23:33.660: INFO: Created: latency-svc-hmc52
Jan 23 01:23:33.699: INFO: Got endpoints: latency-svc-pd56w [750.360808ms]
Jan 23 01:23:33.709: INFO: Created: latency-svc-6pllz
Jan 23 01:23:33.749: INFO: Got endpoints: latency-svc-jwl6l [750.480409ms]
Jan 23 01:23:33.760: INFO: Created: latency-svc-s4trn
Jan 23 01:23:33.799: INFO: Got endpoints: latency-svc-sm58z [744.703549ms]
Jan 23 01:23:33.812: INFO: Created: latency-svc-qcchk
Jan 23 01:23:33.848: INFO: Got endpoints: latency-svc-dsqmk [749.842334ms]
Jan 23 01:23:33.859: INFO: Created: latency-svc-8zgw2
Jan 23 01:23:33.898: INFO: Got endpoints: latency-svc-blw5h [749.984284ms]
Jan 23 01:23:33.911: INFO: Created: latency-svc-qcg8n
Jan 23 01:23:33.968: INFO: Got endpoints: latency-svc-xf57w [770.389793ms]
Jan 23 01:23:33.982: INFO: Created: latency-svc-c2kjn
Jan 23 01:23:33.998: INFO: Got endpoints: latency-svc-bsbjx [749.28689ms]
Jan 23 01:23:34.009: INFO: Created: latency-svc-ppmpp
Jan 23 01:23:34.048: INFO: Got endpoints: latency-svc-4qlsg [749.812904ms]
Jan 23 01:23:34.062: INFO: Created: latency-svc-5p8tr
Jan 23 01:23:34.098: INFO: Got endpoints: latency-svc-mz75k [749.73842ms]
Jan 23 01:23:34.108: INFO: Created: latency-svc-x8ljf
Jan 23 01:23:34.148: INFO: Got endpoints: latency-svc-fq2jc [749.688262ms]
Jan 23 01:23:34.164: INFO: Created: latency-svc-xdhfj
Jan 23 01:23:34.202: INFO: Got endpoints: latency-svc-qsm87 [753.818162ms]
Jan 23 01:23:34.213: INFO: Created: latency-svc-cxlz7
Jan 23 01:23:34.248: INFO: Got endpoints: latency-svc-gwmwr [743.779903ms]
Jan 23 01:23:34.259: INFO: Created: latency-svc-brnm9
Jan 23 01:23:34.306: INFO: Got endpoints: latency-svc-4s7mx [757.387003ms]
Jan 23 01:23:34.317: INFO: Created: latency-svc-lckfg
Jan 23 01:23:34.348: INFO: Got endpoints: latency-svc-zrnjn [750.153273ms]
Jan 23 01:23:34.359: INFO: Created: latency-svc-j9q4t
Jan 23 01:23:34.398: INFO: Got endpoints: latency-svc-hmc52 [748.726797ms]
Jan 23 01:23:34.418: INFO: Created: latency-svc-h4vsk
Jan 23 01:23:34.448: INFO: Got endpoints: latency-svc-6pllz [749.231632ms]
Jan 23 01:23:34.460: INFO: Created: latency-svc-vcz2r
Jan 23 01:23:34.498: INFO: Got endpoints: latency-svc-s4trn [749.472995ms]
Jan 23 01:23:34.515: INFO: Created: latency-svc-tc7s9
Jan 23 01:23:34.549: INFO: Got endpoints: latency-svc-qcchk [750.112465ms]
Jan 23 01:23:34.564: INFO: Created: latency-svc-22c8q
Jan 23 01:23:34.598: INFO: Got endpoints: latency-svc-8zgw2 [750.060961ms]
Jan 23 01:23:34.613: INFO: Created: latency-svc-2x77t
Jan 23 01:23:34.649: INFO: Got endpoints: latency-svc-qcg8n [750.212316ms]
Jan 23 01:23:34.661: INFO: Created: latency-svc-b79hw
Jan 23 01:23:34.698: INFO: Got endpoints: latency-svc-c2kjn [729.915517ms]
Jan 23 01:23:34.710: INFO: Created: latency-svc-gcqdb
Jan 23 01:23:34.749: INFO: Got endpoints: latency-svc-ppmpp [750.563183ms]
Jan 23 01:23:34.763: INFO: Created: latency-svc-8h6js
Jan 23 01:23:34.798: INFO: Got endpoints: latency-svc-5p8tr [749.418251ms]
Jan 23 01:23:34.931: INFO: Created: latency-svc-l5q27
Jan 23 01:23:34.931: INFO: Got endpoints: latency-svc-x8ljf [832.773376ms]
Jan 23 01:23:34.933: INFO: Got endpoints: latency-svc-xdhfj [785.369165ms]
Jan 23 01:23:34.950: INFO: Created: latency-svc-pwzm2
Jan 23 01:23:34.950: INFO: Got endpoints: latency-svc-cxlz7 [747.939565ms]
Jan 23 01:23:34.961: INFO: Created: latency-svc-v4whb
Jan 23 01:23:34.972: INFO: Created: latency-svc-zz5gn
Jan 23 01:23:34.998: INFO: Got endpoints: latency-svc-brnm9 [749.860006ms]
Jan 23 01:23:35.008: INFO: Created: latency-svc-d6st6
Jan 23 01:23:35.048: INFO: Got endpoints: latency-svc-lckfg [742.343951ms]
Jan 23 01:23:35.063: INFO: Created: latency-svc-rntlv
Jan 23 01:23:35.099: INFO: Got endpoints: latency-svc-j9q4t [750.644202ms]
Jan 23 01:23:35.116: INFO: Created: latency-svc-bqkgx
Jan 23 01:23:35.152: INFO: Got endpoints: latency-svc-h4vsk [753.994597ms]
Jan 23 01:23:35.164: INFO: Created: latency-svc-fbrmf
Jan 23 01:23:35.200: INFO: Got endpoints: latency-svc-vcz2r [751.174031ms]
Jan 23 01:23:35.212: INFO: Created: latency-svc-69gz6
Jan 23 01:23:35.250: INFO: Got endpoints: latency-svc-tc7s9 [751.346436ms]
Jan 23 01:23:35.267: INFO: Created: latency-svc-csctp
Jan 23 01:23:35.298: INFO: Got endpoints: latency-svc-22c8q [749.617383ms]
Jan 23 01:23:35.308: INFO: Created: latency-svc-kmxhg
Jan 23 01:23:35.348: INFO: Got endpoints: latency-svc-2x77t [749.871812ms]
Jan 23 01:23:35.359: INFO: Created: latency-svc-2pjmm
Jan 23 01:23:35.398: INFO: Got endpoints: latency-svc-b79hw [749.382864ms]
Jan 23 01:23:35.416: INFO: Created: latency-svc-jh57c
Jan 23 01:23:35.448: INFO: Got endpoints: latency-svc-gcqdb [749.842138ms]
Jan 23 01:23:35.460: INFO: Created: latency-svc-r5tb9
Jan 23 01:23:35.498: INFO: Got endpoints: latency-svc-8h6js [749.607472ms]
Jan 23 01:23:35.512: INFO: Created: latency-svc-bvndf
Jan 23 01:23:35.549: INFO: Got endpoints: latency-svc-l5q27 [750.642228ms]
Jan 23 01:23:35.564: INFO: Created: latency-svc-j2dv9
Jan 23 01:23:35.599: INFO: Got endpoints: latency-svc-pwzm2 [667.930034ms]
Jan 23 01:23:35.610: INFO: Created: latency-svc-sst5x
Jan 23 01:23:35.650: INFO: Got endpoints: latency-svc-v4whb [716.260045ms]
Jan 23 01:23:35.660: INFO: Created: latency-svc-7dg65
Jan 23 01:23:35.701: INFO: Got endpoints: latency-svc-zz5gn [750.772731ms]
Jan 23 01:23:35.713: INFO: Created: latency-svc-tb868
Jan 23 01:23:35.748: INFO: Got endpoints: latency-svc-d6st6 [749.896557ms]
Jan 23 01:23:35.758: INFO: Created: latency-svc-wdt7f
Jan 23 01:23:35.799: INFO: Got endpoints: latency-svc-rntlv [750.767865ms]
Jan 23 01:23:35.821: INFO: Created: latency-svc-jxnzc
Jan 23 01:23:35.848: INFO: Got endpoints: latency-svc-bqkgx [749.151281ms]
Jan 23 01:23:35.866: INFO: Created: latency-svc-gj5tq
Jan 23 01:23:35.899: INFO: Got endpoints: latency-svc-fbrmf [745.986519ms]
Jan 23 01:23:35.908: INFO: Created: latency-svc-r26xs
Jan 23 01:23:35.948: INFO: Got endpoints: latency-svc-69gz6 [748.854328ms]
Jan 23 01:23:35.958: INFO: Created: latency-svc-cmsvx
Jan 23 01:23:35.998: INFO: Got endpoints: latency-svc-csctp [748.621041ms]
Jan 23 01:23:36.010: INFO: Created: latency-svc-kcnpp
Jan 23 01:23:36.048: INFO: Got endpoints: latency-svc-kmxhg [749.831243ms]
Jan 23 01:23:36.059: INFO: Created: latency-svc-dtntt
Jan 23 01:23:36.098: INFO: Got endpoints: latency-svc-2pjmm [750.182156ms]
Jan 23 01:23:36.114: INFO: Created: latency-svc-kjq9w
Jan 23 01:23:36.148: INFO: Got endpoints: latency-svc-jh57c [749.798117ms]
Jan 23 01:23:36.158: INFO: Created: latency-svc-qksk5
Jan 23 01:23:36.198: INFO: Got endpoints: latency-svc-r5tb9 [750.007307ms]
Jan 23 01:23:36.208: INFO: Created: latency-svc-d7vnp
Jan 23 01:23:36.259: INFO: Got endpoints: latency-svc-bvndf [760.695166ms]
Jan 23 01:23:36.269: INFO: Created: latency-svc-ws6x2
Jan 23 01:23:36.299: INFO: Got endpoints: latency-svc-j2dv9 [750.148487ms]
Jan 23 01:23:36.313: INFO: Created: latency-svc-rd2tp
Jan 23 01:23:36.348: INFO: Got endpoints: latency-svc-sst5x [749.088304ms]
Jan 23 01:23:36.377: INFO: Created: latency-svc-m8xgl
Jan 23 01:23:36.398: INFO: Got endpoints: latency-svc-7dg65 [748.558481ms]
Jan 23 01:23:36.409: INFO: Created: latency-svc-ftw8b
Jan 23 01:23:36.448: INFO: Got endpoints: latency-svc-tb868 [747.123032ms]
Jan 23 01:23:36.459: INFO: Created: latency-svc-hj5sq
Jan 23 01:23:36.498: INFO: Got endpoints: latency-svc-wdt7f [750.045511ms]
Jan 23 01:23:36.526: INFO: Created: latency-svc-m8d5z
Jan 23 01:23:36.549: INFO: Got endpoints: latency-svc-jxnzc [749.941821ms]
Jan 23 01:23:36.558: INFO: Created: latency-svc-b8s4m
Jan 23 01:23:36.598: INFO: Got endpoints: latency-svc-gj5tq [749.978115ms]
Jan 23 01:23:36.622: INFO: Created: latency-svc-nvjss
Jan 23 01:23:36.648: INFO: Got endpoints: latency-svc-r26xs [749.756895ms]
Jan 23 01:23:36.658: INFO: Created: latency-svc-zq6ws
Jan 23 01:23:36.710: INFO: Got endpoints: latency-svc-cmsvx [761.196621ms]
Jan 23 01:23:36.737: INFO: Created: latency-svc-rzmmp
Jan 23 01:23:36.748: INFO: Got endpoints: latency-svc-kcnpp [749.728789ms]
Jan 23 01:23:36.758: INFO: Created: latency-svc-qnm98
Jan 23 01:23:36.798: INFO: Got endpoints: latency-svc-dtntt [749.913453ms]
Jan 23 01:23:36.808: INFO: Created: latency-svc-2tgxw
Jan 23 01:23:36.848: INFO: Got endpoints: latency-svc-kjq9w [749.701609ms]
Jan 23 01:23:36.860: INFO: Created: latency-svc-rfp7q
Jan 23 01:23:36.898: INFO: Got endpoints: latency-svc-qksk5 [750.292321ms]
Jan 23 01:23:36.908: INFO: Created: latency-svc-gkssm
Jan 23 01:23:36.949: INFO: Got endpoints: latency-svc-d7vnp [750.492583ms]
Jan 23 01:23:36.960: INFO: Created: latency-svc-8xn2j
Jan 23 01:23:36.999: INFO: Got endpoints: latency-svc-ws6x2 [739.695394ms]
Jan 23 01:23:37.013: INFO: Created: latency-svc-95jqg
Jan 23 01:23:37.048: INFO: Got endpoints: latency-svc-rd2tp [749.609359ms]
Jan 23 01:23:37.058: INFO: Created: latency-svc-5l7b6
Jan 23 01:23:37.098: INFO: Got endpoints: latency-svc-m8xgl [749.761663ms]
Jan 23 01:23:37.108: INFO: Created: latency-svc-qr6fq
Jan 23 01:23:37.150: INFO: Got endpoints: latency-svc-ftw8b [751.429518ms]
Jan 23 01:23:37.163: INFO: Created: latency-svc-wrdvt
Jan 23 01:23:37.198: INFO: Got endpoints: latency-svc-hj5sq [749.812597ms]
Jan 23 01:23:37.207: INFO: Created: latency-svc-7x52b
Jan 23 01:23:37.248: INFO: Got endpoints: latency-svc-m8d5z [749.851408ms]
Jan 23 01:23:37.265: INFO: Created: latency-svc-wbm7x
Jan 23 01:23:37.299: INFO: Got endpoints: latency-svc-b8s4m [749.942761ms]
Jan 23 01:23:37.311: INFO: Created: latency-svc-65xzq
Jan 23 01:23:37.348: INFO: Got endpoints: latency-svc-nvjss [749.944211ms]
Jan 23 01:23:37.381: INFO: Created: latency-svc-lg5rd
Jan 23 01:23:37.398: INFO: Got endpoints: latency-svc-zq6ws [749.918514ms]
Jan 23 01:23:37.409: INFO: Created: latency-svc-85c54
Jan 23 01:23:37.448: INFO: Got endpoints: latency-svc-rzmmp [738.634322ms]
Jan 23 01:23:37.458: INFO: Created: latency-svc-gz8sb
Jan 23 01:23:37.498: INFO: Got endpoints: latency-svc-qnm98 [750.049424ms]
Jan 23 01:23:37.509: INFO: Created: latency-svc-v5zsb
Jan 23 01:23:37.549: INFO: Got endpoints: latency-svc-2tgxw [750.86889ms]
Jan 23 01:23:37.560: INFO: Created: latency-svc-tkxwx
Jan 23 01:23:37.598: INFO: Got endpoints: latency-svc-rfp7q [749.80811ms]
Jan 23 01:23:37.608: INFO: Created: latency-svc-jf8vv
Jan 23 01:23:37.650: INFO: Got endpoints: latency-svc-gkssm [751.378383ms]
Jan 23 01:23:37.665: INFO: Created: latency-svc-qp5bz
Jan 23 01:23:37.717: INFO: Got endpoints: latency-svc-8xn2j [767.909533ms]
Jan 23 01:23:37.733: INFO: Created: latency-svc-v88hf
Jan 23 01:23:37.748: INFO: Got endpoints: latency-svc-95jqg [748.98985ms]
Jan 23 01:23:37.757: INFO: Created: latency-svc-7qdwz
Jan 23 01:23:37.798: INFO: Got endpoints: latency-svc-5l7b6 [749.817078ms]
Jan 23 01:23:37.809: INFO: Created: latency-svc-vbgkj
Jan 23 01:23:37.848: INFO: Got endpoints: latency-svc-qr6fq [749.831982ms]
Jan 23 01:23:37.867: INFO: Created: latency-svc-2xwpq
Jan 23 01:23:37.898: INFO: Got endpoints: latency-svc-wrdvt [748.491376ms]
Jan 23 01:23:37.917: INFO: Created: latency-svc-87dgq
Jan 23 01:23:37.952: INFO: Got endpoints: latency-svc-7x52b [753.642237ms]
Jan 23 01:23:37.968: INFO: Created: latency-svc-cfgjh
Jan 23 01:23:37.998: INFO: Got endpoints: latency-svc-wbm7x [749.861137ms]
Jan 23 01:23:38.009: INFO: Created: latency-svc-vf8kp
Jan 23 01:23:38.048: INFO: Got endpoints: latency-svc-65xzq [749.22355ms]
Jan 23 01:23:38.059: INFO: Created: latency-svc-hmcdd
Jan 23 01:23:38.098: INFO: Got endpoints: latency-svc-lg5rd [750.024493ms]
Jan 23 01:23:38.109: INFO: Created: latency-svc-gnp87
Jan 23 01:23:38.159: INFO: Got endpoints: latency-svc-85c54 [760.958643ms]
Jan 23 01:23:38.172: INFO: Created: latency-svc-qxnkx
Jan 23 01:23:38.199: INFO: Got endpoints: latency-svc-gz8sb [750.38823ms]
Jan 23 01:23:38.212: INFO: Created: latency-svc-7c5p2
Jan 23 01:23:38.249: INFO: Got endpoints: latency-svc-v5zsb [750.31935ms]
Jan 23 01:23:38.279: INFO: Created: latency-svc-4bv55
Jan 23 01:23:38.299: INFO: Got endpoints: latency-svc-tkxwx [749.956188ms]
Jan 23 01:23:38.312: INFO: Created: latency-svc-6frnp
Jan 23 01:23:38.348: INFO: Got endpoints: latency-svc-jf8vv [749.995178ms]
Jan 23 01:23:38.360: INFO: Created: latency-svc-f8v8x
Jan 23 01:23:38.398: INFO: Got endpoints: latency-svc-qp5bz [748.723046ms]
Jan 23 01:23:38.409: INFO: Created: latency-svc-rfnjk
Jan 23 01:23:38.449: INFO: Got endpoints: latency-svc-v88hf [731.781493ms]
Jan 23 01:23:38.459: INFO: Created: latency-svc-g7qxg
Jan 23 01:23:38.498: INFO: Got endpoints: latency-svc-7qdwz [750.270979ms]
Jan 23 01:23:38.509: INFO: Created: latency-svc-tl85h
Jan 23 01:23:38.548: INFO: Got endpoints: latency-svc-vbgkj [750.201063ms]
Jan 23 01:23:38.558: INFO: Created: latency-svc-2ktqt
Jan 23 01:23:38.603: INFO: Got endpoints: latency-svc-2xwpq [754.738589ms]
Jan 23 01:23:38.614: INFO: Created: latency-svc-j2fct
Jan 23 01:23:38.648: INFO: Got endpoints: latency-svc-87dgq [749.699199ms]
Jan 23 01:23:38.662: INFO: Created: latency-svc-p6kxm
Jan 23 01:23:38.699: INFO: Got endpoints: latency-svc-cfgjh [746.523468ms]
Jan 23 01:23:38.718: INFO: Created: latency-svc-sn82n
Jan 23 01:23:38.750: INFO: Got endpoints: latency-svc-vf8kp [751.436801ms]
Jan 23 01:23:38.762: INFO: Created: latency-svc-p7xm5
Jan 23 01:23:38.799: INFO: Got endpoints: latency-svc-hmcdd [750.987941ms]
Jan 23 01:23:38.808: INFO: Created: latency-svc-8lh45
Jan 23 01:23:38.848: INFO: Got endpoints: latency-svc-gnp87 [750.05643ms]
Jan 23 01:23:38.863: INFO: Created: latency-svc-8m4wd
Jan 23 01:23:38.900: INFO: Got endpoints: latency-svc-qxnkx [740.422773ms]
Jan 23 01:23:38.918: INFO: Created: latency-svc-zqks5
Jan 23 01:23:38.949: INFO: Got endpoints: latency-svc-7c5p2 [749.976503ms]
Jan 23 01:23:38.961: INFO: Created: latency-svc-gl26c
Jan 23 01:23:39.001: INFO: Got endpoints: latency-svc-4bv55 [752.565832ms]
Jan 23 01:23:39.010: INFO: Created: latency-svc-9cshs
Jan 23 01:23:39.048: INFO: Got endpoints: latency-svc-6frnp [748.89528ms]
Jan 23 01:23:39.058: INFO: Created: latency-svc-xrbgh
Jan 23 01:23:39.099: INFO: Got endpoints: latency-svc-f8v8x [750.549957ms]
Jan 23 01:23:39.113: INFO: Created: latency-svc-bvtkn
Jan 23 01:23:39.158: INFO: Got endpoints: latency-svc-rfnjk [759.484313ms]
Jan 23 01:23:39.173: INFO: Created: latency-svc-4kjg9
Jan 23 01:23:39.199: INFO: Got endpoints: latency-svc-g7qxg [750.293616ms]
Jan 23 01:23:39.209: INFO: Created: latency-svc-vztjv
Jan 23 01:23:39.248: INFO: Got endpoints: latency-svc-tl85h [749.683651ms]
Jan 23 01:23:39.273: INFO: Created: latency-svc-4vdsw
Jan 23 01:23:39.298: INFO: Got endpoints: latency-svc-2ktqt [749.65957ms]
Jan 23 01:23:39.309: INFO: Created: latency-svc-74s72
Jan 23 01:23:39.348: INFO: Got endpoints: latency-svc-j2fct [745.364868ms]
Jan 23 01:23:39.362: INFO: Created: latency-svc-zw5vv
Jan 23 01:23:39.398: INFO: Got endpoints: latency-svc-p6kxm [749.764859ms]
Jan 23 01:23:39.448: INFO: Got endpoints: latency-svc-sn82n [749.863621ms]
Jan 23 01:23:39.498: INFO: Got endpoints: latency-svc-p7xm5 [748.69434ms]
Jan 23 01:23:39.548: INFO: Got endpoints: latency-svc-8lh45 [749.147854ms]
Jan 23 01:23:39.598: INFO: Got endpoints: latency-svc-8m4wd [750.129603ms]
Jan 23 01:23:39.648: INFO: Got endpoints: latency-svc-zqks5 [748.483938ms]
Jan 23 01:23:39.699: INFO: Got endpoints: latency-svc-gl26c [749.558861ms]
Jan 23 01:23:39.748: INFO: Got endpoints: latency-svc-9cshs [747.325732ms]
Jan 23 01:23:39.799: INFO: Got endpoints: latency-svc-xrbgh [750.878673ms]
Jan 23 01:23:39.848: INFO: Got endpoints: latency-svc-bvtkn [748.91125ms]
Jan 23 01:23:39.898: INFO: Got endpoints: latency-svc-4kjg9 [740.259507ms]
Jan 23 01:23:39.948: INFO: Got endpoints: latency-svc-vztjv [749.153083ms]
Jan 23 01:23:39.999: INFO: Got endpoints: latency-svc-4vdsw [750.852264ms]
Jan 23 01:23:40.048: INFO: Got endpoints: latency-svc-74s72 [749.975496ms]
Jan 23 01:23:40.099: INFO: Got endpoints: latency-svc-zw5vv [750.273824ms]
Jan 23 01:23:40.099: INFO: Latencies: [20.942702ms 28.292176ms 30.823552ms 46.539224ms 62.174872ms 71.823747ms 85.965289ms 114.371777ms 118.671987ms 134.884865ms 147.417566ms 153.941204ms 172.765132ms 177.47849ms 181.111086ms 181.740533ms 183.671261ms 184.922923ms 188.234646ms 188.235898ms 188.977064ms 189.258979ms 193.708538ms 198.94645ms 204.259702ms 231.616304ms 237.446123ms 243.058982ms 250.486397ms 266.77241ms 273.670718ms 275.104705ms 278.098779ms 307.53481ms 310.365887ms 312.240132ms 315.104349ms 315.412427ms 315.89439ms 318.884745ms 321.041001ms 321.28522ms 322.34941ms 324.327048ms 328.944095ms 330.530168ms 354.435914ms 397.075968ms 413.925395ms 470.811631ms 495.033542ms 536.655953ms 574.504523ms 607.039991ms 652.401496ms 667.930034ms 689.354538ms 716.260045ms 729.587379ms 729.915517ms 731.781493ms 738.634322ms 739.033772ms 739.695394ms 740.259507ms 740.422773ms 742.343951ms 743.166981ms 743.779903ms 744.56165ms 744.703549ms 745.364868ms 745.986519ms 746.523468ms 747.123032ms 747.325732ms 747.939565ms 748.141149ms 748.265215ms 748.458889ms 748.483938ms 748.491376ms 748.549452ms 748.558481ms 748.621041ms 748.69434ms 748.723046ms 748.726797ms 748.854328ms 748.89528ms 748.91125ms 748.98985ms 749.088304ms 749.147854ms 749.151281ms 749.153083ms 749.22355ms 749.231632ms 749.28689ms 749.382864ms 749.418251ms 749.472995ms 749.558861ms 749.599066ms 749.607472ms 749.609359ms 749.617383ms 749.65957ms 749.683651ms 749.688262ms 749.699199ms 749.701609ms 749.728789ms 749.73842ms 749.756895ms 749.761663ms 749.764859ms 749.798117ms 749.80811ms 749.812597ms 749.812904ms 749.817078ms 749.831243ms 749.831982ms 749.842138ms 749.842334ms 749.851408ms 749.860006ms 749.861137ms 749.863621ms 749.871812ms 749.896557ms 749.913453ms 749.918514ms 749.941821ms 749.942761ms 749.944211ms 749.956188ms 749.962578ms 749.96904ms 749.975496ms 749.976503ms 749.978115ms 749.984284ms 749.995178ms 750.007307ms 750.024493ms 750.045511ms 750.049424ms 750.05643ms 750.060961ms 750.112465ms 750.129603ms 750.148487ms 750.153273ms 750.182156ms 750.201063ms 750.212316ms 750.270979ms 750.273824ms 750.292321ms 750.293616ms 750.31935ms 750.360808ms 750.38823ms 750.437438ms 750.480409ms 750.492583ms 750.549957ms 750.563183ms 750.642228ms 750.644202ms 750.767865ms 750.772731ms 750.836746ms 750.852264ms 750.86889ms 750.878673ms 750.987941ms 751.174031ms 751.346436ms 751.378383ms 751.399732ms 751.429518ms 751.436801ms 752.565832ms 753.642237ms 753.818162ms 753.994597ms 754.738589ms 756.220404ms 757.387003ms 759.484313ms 760.695166ms 760.958643ms 761.196621ms 767.909533ms 770.389793ms 785.369165ms 832.773376ms]
Jan 23 01:23:40.099: INFO: 50 %ile: 749.418251ms
Jan 23 01:23:40.099: INFO: 90 %ile: 751.346436ms
Jan 23 01:23:40.099: INFO: 99 %ile: 785.369165ms
Jan 23 01:23:40.099: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:23:40.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-c2r59" for this suite.
Jan 23 01:23:56.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:23:56.158: INFO: namespace: e2e-tests-svc-latency-c2r59, resource: bindings, ignored listing per whitelist
Jan 23 01:23:56.172: INFO: namespace e2e-tests-svc-latency-c2r59 deletion completed in 16.070979288s

• [SLOW TEST:26.852 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:23:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 23 01:23:56.242: INFO: Waiting up to 5m0s for pod "pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-wgnk2" to be "success or failure"
Jan 23 01:23:56.250: INFO: Pod "pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.812654ms
Jan 23 01:23:58.254: INFO: Pod "pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219213s
Jan 23 01:24:00.258: INFO: Pod "pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015958798s
STEP: Saw pod success
Jan 23 01:24:00.258: INFO: Pod "pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:24:00.261: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:24:00.276: INFO: Waiting for pod pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:24:00.280: INFO: Pod pod-8d6c8054-1ead-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:24:00.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wgnk2" for this suite.
Jan 23 01:24:06.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:24:06.298: INFO: namespace: e2e-tests-emptydir-wgnk2, resource: bindings, ignored listing per whitelist
Jan 23 01:24:06.352: INFO: namespace e2e-tests-emptydir-wgnk2 deletion completed in 6.069665845s

• [SLOW TEST:10.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:24:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 01:24:06.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-b9rdn'
Jan 23 01:24:06.481: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 01:24:06.481: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Jan 23 01:24:08.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-b9rdn'
Jan 23 01:24:08.574: INFO: stderr: ""
Jan 23 01:24:08.574: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:24:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b9rdn" for this suite.
Jan 23 01:25:26.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:25:26.604: INFO: namespace: e2e-tests-kubectl-b9rdn, resource: bindings, ignored listing per whitelist
Jan 23 01:25:26.650: INFO: namespace e2e-tests-kubectl-b9rdn deletion completed in 1m18.07322056s

• [SLOW TEST:80.298 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:25:26.650: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-c3591dd9-1ead-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:25:26.718: INFO: Waiting up to 5m0s for pod "pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-nl8gj" to be "success or failure"
Jan 23 01:25:26.720: INFO: Pod "pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.961781ms
Jan 23 01:25:28.723: INFO: Pod "pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005095232s
STEP: Saw pod success
Jan 23 01:25:28.723: INFO: Pod "pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:25:28.725: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:25:28.743: INFO: Waiting for pod pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:25:28.746: INFO: Pod pod-secrets-c359625f-1ead-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:25:28.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nl8gj" for this suite.
Jan 23 01:25:34.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:25:34.778: INFO: namespace: e2e-tests-secrets-nl8gj, resource: bindings, ignored listing per whitelist
Jan 23 01:25:34.819: INFO: namespace e2e-tests-secrets-nl8gj deletion completed in 6.071353415s

• [SLOW TEST:8.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:25:34.820: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 23 01:25:37.398: INFO: Successfully updated pod "pod-update-c8377305-1ead-11e9-b567-9e07e353f0d4"
STEP: verifying the updated pod is in kubernetes
Jan 23 01:25:37.404: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:25:37.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-clfx4" for this suite.
Jan 23 01:25:59.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:25:59.440: INFO: namespace: e2e-tests-pods-clfx4, resource: bindings, ignored listing per whitelist
Jan 23 01:25:59.478: INFO: namespace e2e-tests-pods-clfx4 deletion completed in 22.071030987s

• [SLOW TEST:24.659 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:25:59.479: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:25:59.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bbwm2" for this suite.
Jan 23 01:26:21.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:26:21.577: INFO: namespace: e2e-tests-pods-bbwm2, resource: bindings, ignored listing per whitelist
Jan 23 01:26:21.619: INFO: namespace e2e-tests-pods-bbwm2 deletion completed in 22.071060024s

• [SLOW TEST:22.141 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:26:21.619: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:26:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-n7n4r" for this suite.
Jan 23 01:26:27.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:26:27.742: INFO: namespace: e2e-tests-kubelet-test-n7n4r, resource: bindings, ignored listing per whitelist
Jan 23 01:26:27.762: INFO: namespace e2e-tests-kubelet-test-n7n4r deletion completed in 6.069086329s

• [SLOW TEST:6.143 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:26:27.762: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 23 01:26:27.821: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kjc5t,SelfLink:/api/v1/namespaces/e2e-tests-watch-kjc5t/configmaps/e2e-watch-test-watch-closed,UID:e7c553eb-1ead-11e9-9fdb-02424cc6ba99,ResourceVersion:16554,Generation:0,CreationTimestamp:2019-01-23 01:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 01:26:27.821: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kjc5t,SelfLink:/api/v1/namespaces/e2e-tests-watch-kjc5t/configmaps/e2e-watch-test-watch-closed,UID:e7c553eb-1ead-11e9-9fdb-02424cc6ba99,ResourceVersion:16555,Generation:0,CreationTimestamp:2019-01-23 01:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 23 01:26:27.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kjc5t,SelfLink:/api/v1/namespaces/e2e-tests-watch-kjc5t/configmaps/e2e-watch-test-watch-closed,UID:e7c553eb-1ead-11e9-9fdb-02424cc6ba99,ResourceVersion:16556,Generation:0,CreationTimestamp:2019-01-23 01:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 01:26:27.828: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-kjc5t,SelfLink:/api/v1/namespaces/e2e-tests-watch-kjc5t/configmaps/e2e-watch-test-watch-closed,UID:e7c553eb-1ead-11e9-9fdb-02424cc6ba99,ResourceVersion:16557,Generation:0,CreationTimestamp:2019-01-23 01:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:26:27.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kjc5t" for this suite.
Jan 23 01:26:33.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:26:33.873: INFO: namespace: e2e-tests-watch-kjc5t, resource: bindings, ignored listing per whitelist
Jan 23 01:26:33.899: INFO: namespace e2e-tests-watch-kjc5t deletion completed in 6.068922619s

• [SLOW TEST:6.136 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:26:33.899: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 23 01:26:33.961: INFO: Waiting up to 5m0s for pod "pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-sgx5v" to be "success or failure"
Jan 23 01:26:33.962: INFO: Pod "pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876251ms
Jan 23 01:26:35.965: INFO: Pod "pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004736531s
STEP: Saw pod success
Jan 23 01:26:35.965: INFO: Pod "pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:26:35.968: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:26:35.987: INFO: Waiting for pod pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:26:35.990: INFO: Pod pod-eb6e84e8-1ead-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:26:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sgx5v" for this suite.
Jan 23 01:26:42.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:26:42.022: INFO: namespace: e2e-tests-emptydir-sgx5v, resource: bindings, ignored listing per whitelist
Jan 23 01:26:42.061: INFO: namespace e2e-tests-emptydir-sgx5v deletion completed in 6.068432588s

• [SLOW TEST:8.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:26:42.061: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 23 01:26:44.645: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4"
Jan 23 01:26:44.645: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-pods-g8r9q" to be "terminated due to deadline exceeded"
Jan 23 01:26:44.648: INFO: Pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.642676ms
Jan 23 01:26:46.651: INFO: Pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006008884s
Jan 23 01:26:48.654: INFO: Pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009071111s
Jan 23 01:26:48.654: INFO: Pod "pod-update-activedeadlineseconds-f04be74f-1ead-11e9-b567-9e07e353f0d4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:26:48.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-g8r9q" for this suite.
Jan 23 01:26:54.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:26:54.725: INFO: namespace: e2e-tests-pods-g8r9q, resource: bindings, ignored listing per whitelist
Jan 23 01:26:54.732: INFO: namespace e2e-tests-pods-g8r9q deletion completed in 6.07521516s

• [SLOW TEST:12.671 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:26:54.732: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 23 01:26:54.792: INFO: Waiting up to 5m0s for pod "pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-t5g6w" to be "success or failure"
Jan 23 01:26:54.794: INFO: Pod "pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819153ms
Jan 23 01:26:56.797: INFO: Pod "pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004833233s
STEP: Saw pod success
Jan 23 01:26:56.797: INFO: Pod "pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:26:56.799: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:26:56.813: INFO: Waiting for pod pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:26:56.817: INFO: Pod pod-f7d91f6c-1ead-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:26:56.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t5g6w" for this suite.
Jan 23 01:27:02.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:27:02.859: INFO: namespace: e2e-tests-emptydir-t5g6w, resource: bindings, ignored listing per whitelist
Jan 23 01:27:02.875: INFO: namespace e2e-tests-emptydir-t5g6w deletion completed in 6.054995294s

• [SLOW TEST:8.142 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:27:02.875: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0123 01:27:42.935985      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 01:27:42.936: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:27:42.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x87wg" for this suite.
Jan 23 01:27:48.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:27:48.963: INFO: namespace: e2e-tests-gc-x87wg, resource: bindings, ignored listing per whitelist
Jan 23 01:27:49.015: INFO: namespace e2e-tests-gc-x87wg deletion completed in 6.075940498s

• [SLOW TEST:46.140 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:27:49.015: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 23 01:27:53.596: INFO: Successfully updated pod "labelsupdate1832f31d-1eae-11e9-b567-9e07e353f0d4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:27:55.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7tspk" for this suite.
Jan 23 01:28:17.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:28:17.667: INFO: namespace: e2e-tests-downward-api-7tspk, resource: bindings, ignored listing per whitelist
Jan 23 01:28:17.678: INFO: namespace e2e-tests-downward-api-7tspk deletion completed in 22.064287461s

• [SLOW TEST:28.663 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:28:17.679: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 23 01:28:25.765: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 01:28:25.768: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 01:28:27.768: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 01:28:27.772: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 01:28:29.768: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 01:28:29.771: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:28:29.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-xtsxd" for this suite.
Jan 23 01:28:51.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:28:51.802: INFO: namespace: e2e-tests-container-lifecycle-hook-xtsxd, resource: bindings, ignored listing per whitelist
Jan 23 01:28:51.840: INFO: namespace e2e-tests-container-lifecycle-hook-xtsxd deletion completed in 22.064798s

• [SLOW TEST:34.161 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:28:51.841: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 23 01:28:53.913: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-3da6db14-1eae-11e9-b567-9e07e353f0d4,GenerateName:,Namespace:e2e-tests-events-xc658,SelfLink:/api/v1/namespaces/e2e-tests-events-xc658/pods/send-events-3da6db14-1eae-11e9-b567-9e07e353f0d4,UID:3da722f8-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17205,Generation:0,CreationTimestamp:2019-01-23 01:28:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 899884801,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cj2r4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cj2r4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cj2r4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002396100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002396120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:28:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:28:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:28:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:28:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.4,StartTime:2019-01-23 01:28:51 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-01-23 01:28:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://2c21dd93a406cbe56256ceb04ffb0ac7538fa8d90c836f26b02a77acda38b9e6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 23 01:28:55.916: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 23 01:28:57.920: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:28:57.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-xc658" for this suite.
Jan 23 01:29:35.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:29:35.975: INFO: namespace: e2e-tests-events-xc658, resource: bindings, ignored listing per whitelist
Jan 23 01:29:35.991: INFO: namespace e2e-tests-events-xc658 deletion completed in 38.059544736s

• [SLOW TEST:44.151 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:29:35.992: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:29:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 version'
Jan 23 01:29:36.095: INFO: stderr: ""
Jan 23 01:29:36.095: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-15T23:29:42Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:29:36.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8ghvj" for this suite.
Jan 23 01:29:42.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:29:42.143: INFO: namespace: e2e-tests-kubectl-8ghvj, resource: bindings, ignored listing per whitelist
Jan 23 01:29:42.151: INFO: namespace e2e-tests-kubectl-8ghvj deletion completed in 6.053139147s

• [SLOW TEST:6.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:29:42.151: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-5ba1f9d6-1eae-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:29:42.207: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-scqhk" to be "success or failure"
Jan 23 01:29:42.208: INFO: Pod "pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757809ms
Jan 23 01:29:44.211: INFO: Pod "pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004864157s
Jan 23 01:29:46.215: INFO: Pod "pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008188723s
STEP: Saw pod success
Jan 23 01:29:46.215: INFO: Pod "pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:29:46.217: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:29:46.236: INFO: Waiting for pod pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:29:46.238: INFO: Pod pod-configmaps-5ba251e0-1eae-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:29:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-scqhk" for this suite.
Jan 23 01:29:52.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:29:52.271: INFO: namespace: e2e-tests-configmap-scqhk, resource: bindings, ignored listing per whitelist
Jan 23 01:29:52.306: INFO: namespace e2e-tests-configmap-scqhk deletion completed in 6.065463704s

• [SLOW TEST:10.155 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:29:52.306: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-61af7598-1eae-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:29:52.358: INFO: Waiting up to 5m0s for pod "pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-n6wdp" to be "success or failure"
Jan 23 01:29:52.361: INFO: Pod "pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612453ms
Jan 23 01:29:54.364: INFO: Pod "pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711887s
Jan 23 01:29:56.368: INFO: Pod "pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010056418s
STEP: Saw pod success
Jan 23 01:29:56.368: INFO: Pod "pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:29:56.371: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4 container secret-env-test: <nil>
STEP: delete the pod
Jan 23 01:29:56.393: INFO: Waiting for pod pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:29:56.395: INFO: Pod pod-secrets-61afab70-1eae-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:29:56.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n6wdp" for this suite.
Jan 23 01:30:02.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:30:02.467: INFO: namespace: e2e-tests-secrets-n6wdp, resource: bindings, ignored listing per whitelist
Jan 23 01:30:02.470: INFO: namespace e2e-tests-secrets-n6wdp deletion completed in 6.072376004s

• [SLOW TEST:10.165 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:30:02.471: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-67bfa1ec-1eae-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:30:02.534: INFO: Waiting up to 5m0s for pod "pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-pjlvt" to be "success or failure"
Jan 23 01:30:02.536: INFO: Pod "pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.640525ms
Jan 23 01:30:04.539: INFO: Pod "pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004442766s
STEP: Saw pod success
Jan 23 01:30:04.539: INFO: Pod "pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:30:04.542: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:30:04.557: INFO: Waiting for pod pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:30:04.559: INFO: Pod pod-secrets-67c01dcd-1eae-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:30:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pjlvt" for this suite.
Jan 23 01:30:10.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:30:10.580: INFO: namespace: e2e-tests-secrets-pjlvt, resource: bindings, ignored listing per whitelist
Jan 23 01:30:10.633: INFO: namespace e2e-tests-secrets-pjlvt deletion completed in 6.072260458s

• [SLOW TEST:8.163 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:30:10.633: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 23 01:30:10.684: INFO: Waiting up to 5m0s for pod "downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-cnx4b" to be "success or failure"
Jan 23 01:30:10.687: INFO: Pod "downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886409ms
Jan 23 01:30:12.690: INFO: Pod "downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005524944s
Jan 23 01:30:14.693: INFO: Pod "downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008903144s
STEP: Saw pod success
Jan 23 01:30:14.693: INFO: Pod "downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:30:14.696: INFO: Trying to get logs from node kind-conformance-worker1 pod downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 01:30:14.714: INFO: Waiting for pod downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:30:14.716: INFO: Pod downward-api-6c9ba96b-1eae-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:30:14.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cnx4b" for this suite.
Jan 23 01:30:20.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:30:20.745: INFO: namespace: e2e-tests-downward-api-cnx4b, resource: bindings, ignored listing per whitelist
Jan 23 01:30:20.774: INFO: namespace e2e-tests-downward-api-cnx4b deletion completed in 6.056599514s

• [SLOW TEST:10.141 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:30:20.775: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:30:20.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-6rx94" to be "success or failure"
Jan 23 01:30:20.824: INFO: Pod "downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643103ms
Jan 23 01:30:22.826: INFO: Pod "downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004400266s
STEP: Saw pod success
Jan 23 01:30:22.827: INFO: Pod "downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:30:22.829: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:30:22.847: INFO: Waiting for pod downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:30:22.849: INFO: Pod downwardapi-volume-72a69c43-1eae-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:30:22.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6rx94" for this suite.
Jan 23 01:30:28.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:30:28.870: INFO: namespace: e2e-tests-downward-api-6rx94, resource: bindings, ignored listing per whitelist
Jan 23 01:30:28.906: INFO: namespace e2e-tests-downward-api-6rx94 deletion completed in 6.054324799s

• [SLOW TEST:8.131 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:30:28.906: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:30:28.958: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 23 01:30:33.962: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 01:30:33.962: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 23 01:30:35.965: INFO: Creating deployment "test-rollover-deployment"
Jan 23 01:30:35.975: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 23 01:30:37.980: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 23 01:30:37.986: INFO: Ensure that both replica sets have 1 created replica
Jan 23 01:30:37.992: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 23 01:30:37.996: INFO: Updating deployment test-rollover-deployment
Jan 23 01:30:37.996: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 23 01:30:40.002: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 23 01:30:40.008: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 23 01:30:40.014: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:40.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803838, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:42.021: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:42.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803840, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:44.020: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:44.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803840, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:46.021: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:46.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803840, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:48.020: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:48.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803840, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:50.019: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 01:30:50.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803840, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683803835, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 01:30:52.020: INFO: 
Jan 23 01:30:52.020: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 23 01:30:52.027: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-rpwr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rpwr9/deployments/test-rollover-deployment,UID:7bae2d89-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17635,Generation:2,CreationTimestamp:2019-01-23 01:30:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-23 01:30:35 +0000 UTC 2019-01-23 01:30:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-23 01:30:50 +0000 UTC 2019-01-23 01:30:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 01:30:52.031: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-rpwr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rpwr9/replicasets/test-rollover-deployment-6b7f9d6597,UID:7ce41667-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17626,Generation:2,CreationTimestamp:2019-01-23 01:30:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7bae2d89-1eae-11e9-9fdb-02424cc6ba99 0xc001e9c4f7 0xc001e9c4f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 23 01:30:52.031: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 23 01:30:52.031: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-rpwr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rpwr9/replicasets/test-rollover-controller,UID:77803827-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17634,Generation:2,CreationTimestamp:2019-01-23 01:30:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7bae2d89-1eae-11e9-9fdb-02424cc6ba99 0xc001e9c367 0xc001e9c368}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 01:30:52.031: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-rpwr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rpwr9/replicasets/test-rollover-deployment-6586df867b,UID:7bb0ae4f-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17591,Generation:2,CreationTimestamp:2019-01-23 01:30:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7bae2d89-1eae-11e9-9fdb-02424cc6ba99 0xc001e9c427 0xc001e9c428}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 23 01:30:52.034: INFO: Pod "test-rollover-deployment-6b7f9d6597-pr9w5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-pr9w5,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-rpwr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rpwr9/pods/test-rollover-deployment-6b7f9d6597-pr9w5,UID:7ce719d1-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:17608,Generation:0,CreationTimestamp:2019-01-23 01:30:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 7ce41667-1eae-11e9-9fdb-02424cc6ba99 0xc001ffb8f7 0xc001ffb8f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wljlc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wljlc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wljlc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-conformance-worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ffb960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ffb990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:30:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:30:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:30:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-23 01:30:38 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.40.0.5,StartTime:2019-01-23 01:30:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-23 01:30:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2f0611cfb52dfa0ff43ab1cb9ae59b4b43f3739c21e74322c2a5b293b5eb481f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:30:52.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-rpwr9" for this suite.
Jan 23 01:30:58.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:30:58.060: INFO: namespace: e2e-tests-deployment-rpwr9, resource: bindings, ignored listing per whitelist
Jan 23 01:30:58.103: INFO: namespace e2e-tests-deployment-rpwr9 deletion completed in 6.066210792s

• [SLOW TEST:29.197 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:30:58.103: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-tsp6t
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 01:30:58.147: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 01:31:20.197: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.32.0.4 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-tsp6t PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 01:31:20.197: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 01:31:21.321: INFO: Found all expected endpoints: [netserver-0]
Jan 23 01:31:21.324: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.40.0.4 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-tsp6t PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 01:31:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 01:31:22.425: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:31:22.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-tsp6t" for this suite.
Jan 23 01:31:44.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:31:44.467: INFO: namespace: e2e-tests-pod-network-test-tsp6t, resource: bindings, ignored listing per whitelist
Jan 23 01:31:44.502: INFO: namespace e2e-tests-pod-network-test-tsp6t deletion completed in 22.073366723s

• [SLOW TEST:46.399 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:31:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-j6fzk
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Jan 23 01:31:44.563: INFO: Found 0 stateful pods, waiting for 3
Jan 23 01:31:54.567: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 01:31:54.567: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 01:31:54.567: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 01:31:54.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-j6fzk ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 01:31:54.765: INFO: stderr: ""
Jan 23 01:31:54.765: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 01:31:54.765: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 23 01:32:04.793: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 23 01:32:14.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-j6fzk ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 01:32:15.005: INFO: stderr: ""
Jan 23 01:32:15.005: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 01:32:15.006: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 01:32:25.023: INFO: Waiting for StatefulSet e2e-tests-statefulset-j6fzk/ss2 to complete update
Jan 23 01:32:25.023: INFO: Waiting for Pod e2e-tests-statefulset-j6fzk/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 23 01:32:25.023: INFO: Waiting for Pod e2e-tests-statefulset-j6fzk/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 23 01:32:35.028: INFO: Waiting for StatefulSet e2e-tests-statefulset-j6fzk/ss2 to complete update
Jan 23 01:32:35.028: INFO: Waiting for Pod e2e-tests-statefulset-j6fzk/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 23 01:32:45.029: INFO: Waiting for StatefulSet e2e-tests-statefulset-j6fzk/ss2 to complete update
STEP: Rolling back to a previous revision
Jan 23 01:32:55.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-j6fzk ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 23 01:32:55.211: INFO: stderr: ""
Jan 23 01:32:55.211: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 23 01:32:55.211: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 23 01:33:05.238: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 23 01:33:15.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 exec --namespace=e2e-tests-statefulset-j6fzk ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 23 01:33:15.453: INFO: stderr: ""
Jan 23 01:33:15.453: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 23 01:33:15.453: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 23 01:33:35.469: INFO: Waiting for StatefulSet e2e-tests-statefulset-j6fzk/ss2 to complete update
Jan 23 01:33:35.469: INFO: Waiting for Pod e2e-tests-statefulset-j6fzk/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 23 01:33:45.477: INFO: Deleting all statefulset in ns e2e-tests-statefulset-j6fzk
Jan 23 01:33:45.481: INFO: Scaling statefulset ss2 to 0
Jan 23 01:34:05.492: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 01:34:05.494: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:34:05.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-j6fzk" for this suite.
Jan 23 01:34:11.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:34:11.563: INFO: namespace: e2e-tests-statefulset-j6fzk, resource: bindings, ignored listing per whitelist
Jan 23 01:34:11.582: INFO: namespace e2e-tests-statefulset-j6fzk deletion completed in 6.074205989s

• [SLOW TEST:147.079 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:34:11.582: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 23 01:34:11.648: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18422,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 01:34:11.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18423,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 23 01:34:11.648: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18424,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 23 01:34:21.677: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18441,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 01:34:21.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18442,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 23 01:34:21.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-gfgnl,SelfLink:/api/v1/namespaces/e2e-tests-watch-gfgnl/configmaps/e2e-watch-test-label-changed,UID:fc3a83b7-1eae-11e9-9fdb-02424cc6ba99,ResourceVersion:18443,Generation:0,CreationTimestamp:2019-01-23 01:34:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:34:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-gfgnl" for this suite.
Jan 23 01:34:27.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:34:27.714: INFO: namespace: e2e-tests-watch-gfgnl, resource: bindings, ignored listing per whitelist
Jan 23 01:34:27.743: INFO: namespace e2e-tests-watch-gfgnl deletion completed in 6.063538309s

• [SLOW TEST:16.161 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:34:27.743: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 23 01:34:27.801: INFO: Waiting up to 5m0s for pod "pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-84fgf" to be "success or failure"
Jan 23 01:34:27.805: INFO: Pod "pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690786ms
Jan 23 01:34:29.808: INFO: Pod "pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007085754s
STEP: Saw pod success
Jan 23 01:34:29.808: INFO: Pod "pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:34:29.811: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:34:29.827: INFO: Waiting for pod pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:34:29.830: INFO: Pod pod-05dcc909-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:34:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-84fgf" for this suite.
Jan 23 01:34:35.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:34:35.851: INFO: namespace: e2e-tests-emptydir-84fgf, resource: bindings, ignored listing per whitelist
Jan 23 01:34:35.904: INFO: namespace e2e-tests-emptydir-84fgf deletion completed in 6.071868432s

• [SLOW TEST:8.161 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:34:35.904: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:34:35.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-xwdwr" to be "success or failure"
Jan 23 01:34:35.970: INFO: Pod "downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.902941ms
Jan 23 01:34:37.973: INFO: Pod "downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005056744s
STEP: Saw pod success
Jan 23 01:34:37.973: INFO: Pod "downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:34:37.976: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:34:37.990: INFO: Waiting for pod downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:34:37.992: INFO: Pod downwardapi-volume-0abae0df-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:34:37.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xwdwr" for this suite.
Jan 23 01:34:44.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:34:44.057: INFO: namespace: e2e-tests-projected-xwdwr, resource: bindings, ignored listing per whitelist
Jan 23 01:34:44.077: INFO: namespace e2e-tests-projected-xwdwr deletion completed in 6.08073713s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:34:44.078: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-0f9995ca-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:34:44.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-72dl7" to be "success or failure"
Jan 23 01:34:44.143: INFO: Pod "pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314634ms
Jan 23 01:34:46.146: INFO: Pod "pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616327s
Jan 23 01:34:48.149: INFO: Pod "pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009647448s
STEP: Saw pod success
Jan 23 01:34:48.149: INFO: Pod "pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:34:48.151: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:34:48.168: INFO: Waiting for pod pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:34:48.170: INFO: Pod pod-projected-secrets-0f99e100-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:34:48.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-72dl7" for this suite.
Jan 23 01:34:54.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:34:54.221: INFO: namespace: e2e-tests-projected-72dl7, resource: bindings, ignored listing per whitelist
Jan 23 01:34:54.233: INFO: namespace e2e-tests-projected-72dl7 deletion completed in 6.061179461s

• [SLOW TEST:10.156 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:34:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 01:34:54.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:34:54.687: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 01:34:54.687: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 23 01:34:54.694: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 23 01:34:54.696: INFO: scanned /root for discovery docs: <nil>
Jan 23 01:34:54.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:35:10.430: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 23 01:35:10.430: INFO: stdout: "Created e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd\nScaling up e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 23 01:35:10.430: INFO: stdout: "Created e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd\nScaling up e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 23 01:35:10.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:35:10.505: INFO: stderr: ""
Jan 23 01:35:10.505: INFO: stdout: "e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd-vzhch "
Jan 23 01:35:10.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd-vzhch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:35:10.582: INFO: stderr: ""
Jan 23 01:35:10.582: INFO: stdout: "true"
Jan 23 01:35:10.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd-vzhch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:35:10.655: INFO: stderr: ""
Jan 23 01:35:10.655: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jan 23 01:35:10.655: INFO: e2e-test-nginx-rc-6cd2f8ed5bc7739412f516e9439e27fd-vzhch is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Jan 23 01:35:10.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z74bz'
Jan 23 01:35:10.733: INFO: stderr: ""
Jan 23 01:35:10.733: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:35:10.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z74bz" for this suite.
Jan 23 01:35:26.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:35:26.800: INFO: namespace: e2e-tests-kubectl-z74bz, resource: bindings, ignored listing per whitelist
Jan 23 01:35:26.807: INFO: namespace e2e-tests-kubectl-z74bz deletion completed in 16.06695987s

• [SLOW TEST:32.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:35:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-29117b47-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:35:26.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-swsjk" to be "success or failure"
Jan 23 01:35:26.872: INFO: Pod "pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.777414ms
Jan 23 01:35:28.875: INFO: Pod "pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004318417s
STEP: Saw pod success
Jan 23 01:35:28.875: INFO: Pod "pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:35:28.876: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:35:28.890: INFO: Waiting for pod pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:35:28.892: INFO: Pod pod-projected-secrets-29120f3d-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:35:28.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-swsjk" for this suite.
Jan 23 01:35:34.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:35:34.911: INFO: namespace: e2e-tests-projected-swsjk, resource: bindings, ignored listing per whitelist
Jan 23 01:35:34.963: INFO: namespace e2e-tests-projected-swsjk deletion completed in 6.068901251s

• [SLOW TEST:8.156 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:35:34.963: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-2dece4f9-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:35:35.017: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-rjk9r" to be "success or failure"
Jan 23 01:35:35.023: INFO: Pod "pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.214352ms
Jan 23 01:35:37.026: INFO: Pod "pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008737065s
Jan 23 01:35:39.029: INFO: Pod "pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011757332s
STEP: Saw pod success
Jan 23 01:35:39.029: INFO: Pod "pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:35:39.032: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:35:39.050: INFO: Waiting for pod pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:35:39.051: INFO: Pod pod-projected-secrets-2ded3bde-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:35:39.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rjk9r" for this suite.
Jan 23 01:35:45.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:35:45.114: INFO: namespace: e2e-tests-projected-rjk9r, resource: bindings, ignored listing per whitelist
Jan 23 01:35:45.117: INFO: namespace e2e-tests-projected-rjk9r deletion completed in 6.063742697s

• [SLOW TEST:10.154 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:35:45.117: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Jan 23 01:35:45.182: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 23 01:35:45.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:45.330: INFO: stderr: ""
Jan 23 01:35:45.330: INFO: stdout: "service/redis-slave created\n"
Jan 23 01:35:45.331: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 23 01:35:45.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:45.482: INFO: stderr: ""
Jan 23 01:35:45.482: INFO: stdout: "service/redis-master created\n"
Jan 23 01:35:45.482: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 23 01:35:45.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:45.642: INFO: stderr: ""
Jan 23 01:35:45.642: INFO: stdout: "service/frontend created\n"
Jan 23 01:35:45.642: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 23 01:35:45.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:45.802: INFO: stderr: ""
Jan 23 01:35:45.802: INFO: stdout: "deployment.extensions/frontend created\n"
Jan 23 01:35:45.802: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 23 01:35:45.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:45.954: INFO: stderr: ""
Jan 23 01:35:45.954: INFO: stdout: "deployment.extensions/redis-master created\n"
Jan 23 01:35:45.954: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 23 01:35:45.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:35:46.106: INFO: stderr: ""
Jan 23 01:35:46.106: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Jan 23 01:35:46.106: INFO: Waiting for all frontend pods to be Running.
Jan 23 01:36:11.170: INFO: Waiting for frontend to serve content.
Jan 23 01:36:13.566: INFO: Trying to add a new entry to the guestbook.
Jan 23 01:36:13.579: INFO: Verifying that added entry can be retrieved.
Jan 23 01:36:13.587: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jan 23 01:36:18.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:18.684: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:18.684: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 01:36:18.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:18.769: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:18.769: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 01:36:18.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:18.868: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:18.868: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 01:36:18.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:18.943: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:18.943: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 01:36:18.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:19.022: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:19.022: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 01:36:19.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tltk2'
Jan 23 01:36:19.097: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:36:19.097: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:36:19.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tltk2" for this suite.
Jan 23 01:36:57.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:36:57.133: INFO: namespace: e2e-tests-kubectl-tltk2, resource: bindings, ignored listing per whitelist
Jan 23 01:36:57.162: INFO: namespace e2e-tests-kubectl-tltk2 deletion completed in 38.061517727s

• [SLOW TEST:72.045 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:36:57.162: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 23 01:36:57.206: INFO: Waiting up to 5m0s for pod "pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-emptydir-gwvv2" to be "success or failure"
Jan 23 01:36:57.208: INFO: Pod "pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027319ms
Jan 23 01:36:59.211: INFO: Pod "pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005136047s
STEP: Saw pod success
Jan 23 01:36:59.211: INFO: Pod "pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:36:59.213: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:36:59.226: INFO: Waiting for pod pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:36:59.228: INFO: Pod pod-5eea47e7-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:36:59.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gwvv2" for this suite.
Jan 23 01:37:05.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:37:05.275: INFO: namespace: e2e-tests-emptydir-gwvv2, resource: bindings, ignored listing per whitelist
Jan 23 01:37:05.289: INFO: namespace e2e-tests-emptydir-gwvv2 deletion completed in 6.058903619s

• [SLOW TEST:8.127 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:37:05.289: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-63c3f22b-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:37:05.347: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-zzvm4" to be "success or failure"
Jan 23 01:37:05.349: INFO: Pod "pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609329ms
Jan 23 01:37:07.352: INFO: Pod "pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005495688s
Jan 23 01:37:09.356: INFO: Pod "pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009013469s
STEP: Saw pod success
Jan 23 01:37:09.356: INFO: Pod "pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:37:09.359: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:37:09.379: INFO: Waiting for pod pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:37:09.381: INFO: Pod pod-projected-configmaps-63c46b81-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:37:09.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zzvm4" for this suite.
Jan 23 01:37:15.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:37:15.437: INFO: namespace: e2e-tests-projected-zzvm4, resource: bindings, ignored listing per whitelist
Jan 23 01:37:15.453: INFO: namespace e2e-tests-projected-zzvm4 deletion completed in 6.069875416s

• [SLOW TEST:10.164 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:37:15.453: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 23 01:37:21.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 01:37:21.530: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 01:37:23.530: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 01:37:23.535: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 01:37:25.530: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 01:37:25.533: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:37:25.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-xpkzq" for this suite.
Jan 23 01:37:47.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:37:47.578: INFO: namespace: e2e-tests-container-lifecycle-hook-xpkzq, resource: bindings, ignored listing per whitelist
Jan 23 01:37:47.610: INFO: namespace e2e-tests-container-lifecycle-hook-xpkzq deletion completed in 22.065975658s

• [SLOW TEST:32.157 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:37:47.610: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-j56px
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j56px to expose endpoints map[]
Jan 23 01:37:47.663: INFO: Get endpoints failed (3.149718ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 23 01:37:48.665: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j56px exposes endpoints map[] (1.005522437s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-j56px
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j56px to expose endpoints map[pod1:[100]]
Jan 23 01:37:51.706: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j56px exposes endpoints map[pod1:[100]] (3.03543898s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-j56px
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j56px to expose endpoints map[pod1:[100] pod2:[101]]
Jan 23 01:37:53.739: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j56px exposes endpoints map[pod1:[100] pod2:[101]] (2.029853146s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-j56px
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j56px to expose endpoints map[pod2:[101]]
Jan 23 01:37:54.758: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j56px exposes endpoints map[pod2:[101]] (1.014389218s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-j56px
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j56px to expose endpoints map[]
Jan 23 01:37:55.768: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j56px exposes endpoints map[] (1.005532436s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:37:55.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-j56px" for this suite.
Jan 23 01:38:17.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:38:17.822: INFO: namespace: e2e-tests-services-j56px, resource: bindings, ignored listing per whitelist
Jan 23 01:38:17.867: INFO: namespace e2e-tests-services-j56px deletion completed in 22.075666627s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:30.257 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:38:17.868: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:38:17.929: INFO: (0) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.142309ms)
Jan 23 01:38:17.931: INFO: (1) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.269959ms)
Jan 23 01:38:17.933: INFO: (2) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.174955ms)
Jan 23 01:38:17.935: INFO: (3) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.94903ms)
Jan 23 01:38:17.937: INFO: (4) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.095056ms)
Jan 23 01:38:17.939: INFO: (5) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.007957ms)
Jan 23 01:38:17.942: INFO: (6) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.323532ms)
Jan 23 01:38:17.944: INFO: (7) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.134231ms)
Jan 23 01:38:17.946: INFO: (8) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.139741ms)
Jan 23 01:38:17.948: INFO: (9) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.229857ms)
Jan 23 01:38:17.951: INFO: (10) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.438047ms)
Jan 23 01:38:17.953: INFO: (11) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.314541ms)
Jan 23 01:38:17.955: INFO: (12) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.237537ms)
Jan 23 01:38:17.957: INFO: (13) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.148316ms)
Jan 23 01:38:17.960: INFO: (14) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.219064ms)
Jan 23 01:38:17.962: INFO: (15) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.263332ms)
Jan 23 01:38:17.964: INFO: (16) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.495069ms)
Jan 23 01:38:17.967: INFO: (17) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.551952ms)
Jan 23 01:38:17.970: INFO: (18) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.554092ms)
Jan 23 01:38:17.972: INFO: (19) /api/v1/nodes/kind-conformance-worker1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.600428ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:38:17.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ttn6b" for this suite.
Jan 23 01:38:23.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:38:24.006: INFO: namespace: e2e-tests-proxy-ttn6b, resource: bindings, ignored listing per whitelist
Jan 23 01:38:24.045: INFO: namespace e2e-tests-proxy-ttn6b deletion completed in 6.070431082s

• [SLOW TEST:6.177 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:38:24.046: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 23 01:38:24.099: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 01:38:24.103: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 01:38:24.105: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker1 before test
Jan 23 01:38:24.109: INFO: kube-proxy-4tfgc from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 01:38:24.109: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-23 00:13:23 +0000 UTC (1 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 01:38:24.109: INFO: weave-net-6h6xx from kube-system started at 2019-01-23 00:10:21 +0000 UTC (2 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container weave ready: true, restart count 0
Jan 23 01:38:24.109: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 01:38:24.109: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-ghcfq from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 23 01:38:24.109: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 23 01:38:24.109: INFO: coredns-86c58d9df4-2wct2 from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container coredns ready: true, restart count 0
Jan 23 01:38:24.109: INFO: coredns-86c58d9df4-84xdw from kube-system started at 2019-01-23 00:10:21 +0000 UTC (1 container statuses recorded)
Jan 23 01:38:24.109: INFO: 	Container coredns ready: true, restart count 0
Jan 23 01:38:24.109: INFO: 
Logging pods the kubelet thinks is on node kind-conformance-worker2 before test
Jan 23 01:38:24.113: INFO: sonobuoy-systemd-logs-daemon-set-ba573f0aad3b41ff-6bn97 from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 01:38:24.113: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 23 01:38:24.113: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 23 01:38:24.113: INFO: kube-proxy-85kj4 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (1 container statuses recorded)
Jan 23 01:38:24.113: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 01:38:24.113: INFO: weave-net-29887 from kube-system started at 2019-01-23 00:10:23 +0000 UTC (2 container statuses recorded)
Jan 23 01:38:24.113: INFO: 	Container weave ready: true, restart count 0
Jan 23 01:38:24.113: INFO: 	Container weave-npc ready: true, restart count 0
Jan 23 01:38:24.113: INFO: sonobuoy-e2e-job-9907aed0bf024cec from heptio-sonobuoy started at 2019-01-23 00:13:33 +0000 UTC (2 container statuses recorded)
Jan 23 01:38:24.113: INFO: 	Container e2e ready: true, restart count 0
Jan 23 01:38:24.113: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.157c5685b1fd58fe], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:38:25.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-sxmnp" for this suite.
Jan 23 01:38:31.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:38:31.205: INFO: namespace: e2e-tests-sched-pred-sxmnp, resource: bindings, ignored listing per whitelist
Jan 23 01:38:31.207: INFO: namespace e2e-tests-sched-pred-sxmnp deletion completed in 6.075042164s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.161 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:38:31.207: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-96fa958a-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:38:31.267: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-vnsdp" to be "success or failure"
Jan 23 01:38:31.271: INFO: Pod "pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752843ms
Jan 23 01:38:33.275: INFO: Pod "pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00718776s
Jan 23 01:38:35.278: INFO: Pod "pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010633488s
STEP: Saw pod success
Jan 23 01:38:35.278: INFO: Pod "pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:38:35.281: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:38:35.297: INFO: Waiting for pod pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:38:35.299: INFO: Pod pod-projected-configmaps-96fada2f-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:38:35.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vnsdp" for this suite.
Jan 23 01:38:41.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:38:41.327: INFO: namespace: e2e-tests-projected-vnsdp, resource: bindings, ignored listing per whitelist
Jan 23 01:38:41.373: INFO: namespace e2e-tests-projected-vnsdp deletion completed in 6.070844906s

• [SLOW TEST:10.166 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:38:41.373: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Jan 23 01:38:41.432: INFO: Waiting up to 5m0s for pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-containers-2whzr" to be "success or failure"
Jan 23 01:38:41.433: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.700124ms
Jan 23 01:38:43.437: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004958616s
Jan 23 01:38:45.440: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008368187s
Jan 23 01:38:47.443: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011001053s
Jan 23 01:38:49.446: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014508131s
Jan 23 01:38:51.449: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017370397s
Jan 23 01:38:53.452: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020916709s
Jan 23 01:38:55.456: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.02451095s
Jan 23 01:38:57.459: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027901136s
Jan 23 01:38:59.463: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.031692845s
Jan 23 01:39:01.467: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.03542899s
STEP: Saw pod success
Jan 23 01:39:01.467: INFO: Pod "client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:39:01.470: INFO: Trying to get logs from node kind-conformance-worker1 pod client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4 container test-container: <nil>
STEP: delete the pod
Jan 23 01:39:01.487: INFO: Waiting for pod client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:39:01.488: INFO: Pod client-containers-9d09b60e-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:39:01.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2whzr" for this suite.
Jan 23 01:39:07.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:39:07.504: INFO: namespace: e2e-tests-containers-2whzr, resource: bindings, ignored listing per whitelist
Jan 23 01:39:07.558: INFO: namespace e2e-tests-containers-2whzr deletion completed in 6.067635405s

• [SLOW TEST:26.185 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:39:07.558: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 23 01:39:07.618: INFO: Waiting up to 5m0s for pod "downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-zk875" to be "success or failure"
Jan 23 01:39:07.620: INFO: Pod "downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061786ms
Jan 23 01:39:09.622: INFO: Pod "downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004445373s
Jan 23 01:39:11.625: INFO: Pod "downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00692673s
STEP: Saw pod success
Jan 23 01:39:11.625: INFO: Pod "downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:39:11.626: INFO: Trying to get logs from node kind-conformance-worker2 pod downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 01:39:11.640: INFO: Waiting for pod downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:39:11.641: INFO: Pod downward-api-aca53c88-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:39:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zk875" for this suite.
Jan 23 01:39:17.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:39:17.686: INFO: namespace: e2e-tests-downward-api-zk875, resource: bindings, ignored listing per whitelist
Jan 23 01:39:17.701: INFO: namespace e2e-tests-downward-api-zk875 deletion completed in 6.057250141s

• [SLOW TEST:10.143 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:39:17.701: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 23 01:39:17.756: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:39:21.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-lldgt" for this suite.
Jan 23 01:39:43.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:39:43.917: INFO: namespace: e2e-tests-init-container-lldgt, resource: bindings, ignored listing per whitelist
Jan 23 01:39:43.917: INFO: namespace e2e-tests-init-container-lldgt deletion completed in 22.064223503s

• [SLOW TEST:26.216 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:39:43.917: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 23 01:39:43.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-f47nc'
Jan 23 01:39:44.046: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 01:39:44.046: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 23 01:39:44.052: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-r56b8]
Jan 23 01:39:44.052: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-r56b8" in namespace "e2e-tests-kubectl-f47nc" to be "running and ready"
Jan 23 01:39:44.053: INFO: Pod "e2e-test-nginx-rc-r56b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.372597ms
Jan 23 01:39:46.056: INFO: Pod "e2e-test-nginx-rc-r56b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004299705s
Jan 23 01:39:46.056: INFO: Pod "e2e-test-nginx-rc-r56b8" satisfied condition "running and ready"
Jan 23 01:39:46.056: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-r56b8]
Jan 23 01:39:46.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-f47nc'
Jan 23 01:39:46.146: INFO: stderr: ""
Jan 23 01:39:46.146: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Jan 23 01:39:46.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-f47nc'
Jan 23 01:39:46.222: INFO: stderr: ""
Jan 23 01:39:46.222: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:39:46.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-f47nc" for this suite.
Jan 23 01:40:08.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:40:08.260: INFO: namespace: e2e-tests-kubectl-f47nc, resource: bindings, ignored listing per whitelist
Jan 23 01:40:08.296: INFO: namespace e2e-tests-kubectl-f47nc deletion completed in 22.070912595s

• [SLOW TEST:24.379 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:40:08.296: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-d0da971c-1eaf-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:40:08.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-rjrbg" to be "success or failure"
Jan 23 01:40:08.369: INFO: Pod "pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.468281ms
Jan 23 01:40:10.372: INFO: Pod "pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004902975s
Jan 23 01:40:12.374: INFO: Pod "pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007246637s
STEP: Saw pod success
Jan 23 01:40:12.374: INFO: Pod "pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:40:12.376: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:40:12.393: INFO: Waiting for pod pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:40:12.395: INFO: Pod pod-projected-configmaps-d0dad8a1-1eaf-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:40:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rjrbg" for this suite.
Jan 23 01:40:18.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:40:18.462: INFO: namespace: e2e-tests-projected-rjrbg, resource: bindings, ignored listing per whitelist
Jan 23 01:40:18.470: INFO: namespace e2e-tests-projected-rjrbg deletion completed in 6.07283261s

• [SLOW TEST:10.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:40:18.470: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:40:40.538: INFO: Container started at 2019-01-23 01:40:19 +0000 UTC, pod became ready at 2019-01-23 01:40:39 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:40:40.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-j962c" for this suite.
Jan 23 01:41:02.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:41:02.572: INFO: namespace: e2e-tests-container-probe-j962c, resource: bindings, ignored listing per whitelist
Jan 23 01:41:02.607: INFO: namespace e2e-tests-container-probe-j962c deletion completed in 22.06551837s

• [SLOW TEST:44.136 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:41:02.607: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:41:02.660: INFO: Creating ReplicaSet my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4
Jan 23 01:41:02.665: INFO: Pod name my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4: Found 0 pods out of 1
Jan 23 01:41:07.670: INFO: Pod name my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4: Found 1 pods out of 1
Jan 23 01:41:07.670: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4" is running
Jan 23 01:41:07.675: INFO: Pod "my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4-hr9tv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:41:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:41:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:41:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-23 01:41:02 +0000 UTC Reason: Message:}])
Jan 23 01:41:07.675: INFO: Trying to dial the pod
Jan 23 01:41:12.683: INFO: Controller my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4: Got expected result from replica 1 [my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4-hr9tv]: "my-hostname-basic-f1380bef-1eaf-11e9-b567-9e07e353f0d4-hr9tv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:41:12.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-6dwkk" for this suite.
Jan 23 01:41:18.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:41:18.717: INFO: namespace: e2e-tests-replicaset-6dwkk, resource: bindings, ignored listing per whitelist
Jan 23 01:41:18.741: INFO: namespace e2e-tests-replicaset-6dwkk deletion completed in 6.054081356s

• [SLOW TEST:16.134 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:41:18.741: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 23 01:41:21.317: INFO: Successfully updated pod "labelsupdatefad48b6e-1eaf-11e9-b567-9e07e353f0d4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:41:23.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r7pt2" for this suite.
Jan 23 01:41:45.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:41:45.404: INFO: namespace: e2e-tests-projected-r7pt2, resource: bindings, ignored listing per whitelist
Jan 23 01:41:45.434: INFO: namespace e2e-tests-projected-r7pt2 deletion completed in 22.086941668s

• [SLOW TEST:26.693 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:41:45.434: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0ac03e7a-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:41:45.583: INFO: Waiting up to 5m0s for pod "pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-lx47k" to be "success or failure"
Jan 23 01:41:45.585: INFO: Pod "pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981074ms
Jan 23 01:41:47.588: INFO: Pod "pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004450103s
STEP: Saw pod success
Jan 23 01:41:47.588: INFO: Pod "pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:41:47.590: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:41:47.602: INFO: Waiting for pod pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:41:47.605: INFO: Pod pod-secrets-0acd160b-1eb0-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:41:47.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lx47k" for this suite.
Jan 23 01:41:53.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:41:53.629: INFO: namespace: e2e-tests-secrets-lx47k, resource: bindings, ignored listing per whitelist
Jan 23 01:41:53.666: INFO: namespace e2e-tests-secrets-lx47k deletion completed in 6.058104414s
STEP: Destroying namespace "e2e-tests-secret-namespace-gjdxv" for this suite.
Jan 23 01:41:59.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:41:59.722: INFO: namespace: e2e-tests-secret-namespace-gjdxv, resource: bindings, ignored listing per whitelist
Jan 23 01:41:59.730: INFO: namespace e2e-tests-secret-namespace-gjdxv deletion completed in 6.064121836s

• [SLOW TEST:14.296 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:41:59.730: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-1344c166-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating secret with name s-test-opt-upd-1344c1c0-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1344c166-1eb0-11e9-b567-9e07e353f0d4
STEP: Updating secret s-test-opt-upd-1344c1c0-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating secret with name s-test-opt-create-1344c1d4-1eb0-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:43:28.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7sp6x" for this suite.
Jan 23 01:43:50.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:43:50.352: INFO: namespace: e2e-tests-secrets-7sp6x, resource: bindings, ignored listing per whitelist
Jan 23 01:43:50.368: INFO: namespace e2e-tests-secrets-7sp6x deletion completed in 22.065702927s

• [SLOW TEST:110.638 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:43:50.368: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-5535e87b-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:43:50.423: INFO: Waiting up to 5m0s for pod "pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-5l488" to be "success or failure"
Jan 23 01:43:50.428: INFO: Pod "pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382757ms
Jan 23 01:43:52.431: INFO: Pod "pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007803991s
STEP: Saw pod success
Jan 23 01:43:52.431: INFO: Pod "pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:43:52.434: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:43:52.449: INFO: Waiting for pod pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:43:52.454: INFO: Pod pod-configmaps-5536288a-1eb0-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:43:52.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5l488" for this suite.
Jan 23 01:43:58.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:43:58.484: INFO: namespace: e2e-tests-configmap-5l488, resource: bindings, ignored listing per whitelist
Jan 23 01:43:58.523: INFO: namespace e2e-tests-configmap-5l488 deletion completed in 6.065871622s

• [SLOW TEST:8.155 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:43:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Jan 23 01:43:58.577: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-311263258 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:43:58.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dzjsx" for this suite.
Jan 23 01:44:04.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:44:04.663: INFO: namespace: e2e-tests-kubectl-dzjsx, resource: bindings, ignored listing per whitelist
Jan 23 01:44:04.707: INFO: namespace e2e-tests-kubectl-dzjsx deletion completed in 6.068489148s

• [SLOW TEST:6.184 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:44:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-48hvx/secret-test-5dc3b421-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:44:04.775: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-48hvx" to be "success or failure"
Jan 23 01:44:04.778: INFO: Pod "pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.613022ms
Jan 23 01:44:06.781: INFO: Pod "pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006107196s
STEP: Saw pod success
Jan 23 01:44:06.781: INFO: Pod "pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:44:06.782: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4 container env-test: <nil>
STEP: delete the pod
Jan 23 01:44:06.799: INFO: Waiting for pod pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:44:06.802: INFO: Pod pod-configmaps-5dc3fdf5-1eb0-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:44:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-48hvx" for this suite.
Jan 23 01:44:12.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:44:12.829: INFO: namespace: e2e-tests-secrets-48hvx, resource: bindings, ignored listing per whitelist
Jan 23 01:44:12.865: INFO: namespace e2e-tests-secrets-48hvx deletion completed in 6.060502859s

• [SLOW TEST:8.158 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:44:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-629dc074-1eb0-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume configMaps
Jan 23 01:44:12.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-configmap-j8fmg" to be "success or failure"
Jan 23 01:44:12.919: INFO: Pod "pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.066914ms
Jan 23 01:44:14.922: INFO: Pod "pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008241615s
STEP: Saw pod success
Jan 23 01:44:14.922: INFO: Pod "pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:44:14.925: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 01:44:14.942: INFO: Waiting for pod pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:44:14.944: INFO: Pod pod-configmaps-629dfb26-1eb0-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:44:14.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-j8fmg" for this suite.
Jan 23 01:44:20.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:44:21.011: INFO: namespace: e2e-tests-configmap-j8fmg, resource: bindings, ignored listing per whitelist
Jan 23 01:44:21.019: INFO: namespace e2e-tests-configmap-j8fmg deletion completed in 6.072344023s

• [SLOW TEST:8.154 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:44:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 23 01:44:21.085: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"677ca1d2-1eb0-11e9-9fdb-02424cc6ba99", Controller:(*bool)(0xc000c0fa46), BlockOwnerDeletion:(*bool)(0xc000c0fa47)}}
Jan 23 01:44:21.091: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"677ba9d8-1eb0-11e9-9fdb-02424cc6ba99", Controller:(*bool)(0xc001e76ec6), BlockOwnerDeletion:(*bool)(0xc001e76ec7)}}
Jan 23 01:44:21.096: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"677bfcd3-1eb0-11e9-9fdb-02424cc6ba99", Controller:(*bool)(0xc000c0fc32), BlockOwnerDeletion:(*bool)(0xc000c0fc33)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:44:26.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x44xh" for this suite.
Jan 23 01:44:32.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:44:32.129: INFO: namespace: e2e-tests-gc-x44xh, resource: bindings, ignored listing per whitelist
Jan 23 01:44:32.181: INFO: namespace e2e-tests-gc-x44xh deletion completed in 6.070472006s

• [SLOW TEST:11.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:44:32.181: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cqxjd
Jan 23 01:44:34.247: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cqxjd
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 01:44:34.249: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:48:34.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cqxjd" for this suite.
Jan 23 01:48:40.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:48:40.713: INFO: namespace: e2e-tests-container-probe-cqxjd, resource: bindings, ignored listing per whitelist
Jan 23 01:48:40.745: INFO: namespace e2e-tests-container-probe-cqxjd deletion completed in 6.069725773s

• [SLOW TEST:248.564 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:48:40.745: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 23 01:48:40.807: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 23 01:48:45.811: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:48:46.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-vffnx" for this suite.
Jan 23 01:48:52.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:48:52.871: INFO: namespace: e2e-tests-replication-controller-vffnx, resource: bindings, ignored listing per whitelist
Jan 23 01:48:52.895: INFO: namespace e2e-tests-replication-controller-vffnx deletion completed in 6.063663199s

• [SLOW TEST:12.150 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:48:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-cn98t
Jan 23 01:49:00.946: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-cn98t
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 01:49:00.949: INFO: Initial restart count of pod liveness-http is 0
Jan 23 01:49:24.995: INFO: Restart count of pod e2e-tests-container-probe-cn98t/liveness-http is now 1 (24.045813805s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:49:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cn98t" for this suite.
Jan 23 01:49:31.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:49:31.029: INFO: namespace: e2e-tests-container-probe-cn98t, resource: bindings, ignored listing per whitelist
Jan 23 01:49:31.078: INFO: namespace e2e-tests-container-probe-cn98t deletion completed in 6.073913631s

• [SLOW TEST:38.183 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:49:31.078: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-204b85fd-1eb1-11e9-b567-9e07e353f0d4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-204b85fd-1eb1-11e9-b567-9e07e353f0d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:49:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-48cx7" for this suite.
Jan 23 01:49:57.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:49:57.238: INFO: namespace: e2e-tests-projected-48cx7, resource: bindings, ignored listing per whitelist
Jan 23 01:49:57.241: INFO: namespace e2e-tests-projected-48cx7 deletion completed in 22.054824651s

• [SLOW TEST:26.162 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:49:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 23 01:50:01.321: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:01.325: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:03.325: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:03.328: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:05.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:05.328: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:07.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:07.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:09.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:09.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:11.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:11.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:13.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:13.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:15.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:15.328: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:17.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:17.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:19.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:19.330: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:21.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:21.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:23.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:23.330: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:25.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:25.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:27.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:27.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:29.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:29.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:31.325: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:31.329: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 01:50:33.326: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 01:50:33.329: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:50:33.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-g5wf4" for this suite.
Jan 23 01:50:55.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:50:55.385: INFO: namespace: e2e-tests-container-lifecycle-hook-g5wf4, resource: bindings, ignored listing per whitelist
Jan 23 01:50:55.405: INFO: namespace e2e-tests-container-lifecycle-hook-g5wf4 deletion completed in 22.062012138s

• [SLOW TEST:58.164 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:50:55.405: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-528cb275-1eb1-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:50:55.461: INFO: Waiting up to 5m0s for pod "pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-t59rk" to be "success or failure"
Jan 23 01:50:55.463: INFO: Pod "pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407701ms
Jan 23 01:50:57.466: INFO: Pod "pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004466824s
STEP: Saw pod success
Jan 23 01:50:57.466: INFO: Pod "pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:50:57.468: INFO: Trying to get logs from node kind-conformance-worker2 pod pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:50:57.481: INFO: Waiting for pod pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:50:57.487: INFO: Pod pod-secrets-528cf87a-1eb1-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:50:57.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-t59rk" for this suite.
Jan 23 01:51:03.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:51:03.507: INFO: namespace: e2e-tests-secrets-t59rk, resource: bindings, ignored listing per whitelist
Jan 23 01:51:03.546: INFO: namespace e2e-tests-secrets-t59rk deletion completed in 6.056946913s

• [SLOW TEST:8.141 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:51:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-57671a0f-1eb1-11e9-b567-9e07e353f0d4
STEP: Creating a pod to test consume secrets
Jan 23 01:51:03.599: INFO: Waiting up to 5m0s for pod "pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-secrets-n6n54" to be "success or failure"
Jan 23 01:51:03.601: INFO: Pod "pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348497ms
Jan 23 01:51:05.604: INFO: Pod "pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004433579s
STEP: Saw pod success
Jan 23 01:51:05.604: INFO: Pod "pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:51:05.607: INFO: Trying to get logs from node kind-conformance-worker1 pod pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 01:51:05.624: INFO: Waiting for pod pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:51:05.626: INFO: Pod pod-secrets-57679493-1eb1-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:51:05.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n6n54" for this suite.
Jan 23 01:51:11.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:51:11.644: INFO: namespace: e2e-tests-secrets-n6n54, resource: bindings, ignored listing per whitelist
Jan 23 01:51:11.690: INFO: namespace e2e-tests-secrets-n6n54 deletion completed in 6.062524242s

• [SLOW TEST:8.144 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:51:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-r4flg
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 01:51:11.739: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 01:51:27.790: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=udp&host=10.32.0.3&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-r4flg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 01:51:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 01:51:27.889: INFO: Waiting for endpoints: map[]
Jan 23 01:51:27.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=udp&host=10.40.0.4&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-r4flg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 01:51:27.892: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
Jan 23 01:51:27.994: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:51:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-r4flg" for this suite.
Jan 23 01:51:50.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:51:50.067: INFO: namespace: e2e-tests-pod-network-test-r4flg, resource: bindings, ignored listing per whitelist
Jan 23 01:51:50.071: INFO: namespace e2e-tests-pod-network-test-r4flg deletion completed in 22.073996055s

• [SLOW TEST:38.381 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:51:50.071: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:51:50.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-downward-api-xzjxv" to be "success or failure"
Jan 23 01:51:50.134: INFO: Pod "downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.898406ms
Jan 23 01:51:52.137: INFO: Pod "downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00518631s
STEP: Saw pod success
Jan 23 01:51:52.137: INFO: Pod "downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:51:52.140: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:51:52.158: INFO: Waiting for pod downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:51:52.161: INFO: Pod downwardapi-volume-7323d3ba-1eb1-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:51:52.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xzjxv" for this suite.
Jan 23 01:51:58.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:51:58.178: INFO: namespace: e2e-tests-downward-api-xzjxv, resource: bindings, ignored listing per whitelist
Jan 23 01:51:58.235: INFO: namespace e2e-tests-downward-api-xzjxv deletion completed in 6.072414445s

• [SLOW TEST:8.164 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:51:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Jan 23 01:51:58.287: INFO: Waiting up to 5m0s for pod "var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-var-expansion-xxmvq" to be "success or failure"
Jan 23 01:51:58.291: INFO: Pod "var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.370955ms
Jan 23 01:52:00.294: INFO: Pod "var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006421764s
STEP: Saw pod success
Jan 23 01:52:00.294: INFO: Pod "var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:52:00.296: INFO: Trying to get logs from node kind-conformance-worker1 pod var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4 container dapi-container: <nil>
STEP: delete the pod
Jan 23 01:52:00.310: INFO: Waiting for pod var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:52:00.311: INFO: Pod var-expansion-780045ed-1eb1-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:52:00.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-xxmvq" for this suite.
Jan 23 01:52:06.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:52:06.351: INFO: namespace: e2e-tests-var-expansion-xxmvq, resource: bindings, ignored listing per whitelist
Jan 23 01:52:06.386: INFO: namespace e2e-tests-var-expansion-xxmvq deletion completed in 6.072043361s

• [SLOW TEST:8.150 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:52:06.386: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:52:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-zwtj5" for this suite.
Jan 23 01:52:18.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:52:18.571: INFO: namespace: e2e-tests-namespaces-zwtj5, resource: bindings, ignored listing per whitelist
Jan 23 01:52:18.575: INFO: namespace e2e-tests-namespaces-zwtj5 deletion completed in 6.071066546s
STEP: Destroying namespace "e2e-tests-nsdeletetest-4d29s" for this suite.
Jan 23 01:52:18.577: INFO: Namespace e2e-tests-nsdeletetest-4d29s was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-lxlvl" for this suite.
Jan 23 01:52:24.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:52:24.647: INFO: namespace: e2e-tests-nsdeletetest-lxlvl, resource: bindings, ignored listing per whitelist
Jan 23 01:52:24.651: INFO: namespace e2e-tests-nsdeletetest-lxlvl deletion completed in 6.07396102s

• [SLOW TEST:18.265 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:52:24.651: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 23 01:52:24.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4" in namespace "e2e-tests-projected-7g8l5" to be "success or failure"
Jan 23 01:52:24.712: INFO: Pod "downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.73512ms
Jan 23 01:52:26.716: INFO: Pod "downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004809301s
STEP: Saw pod success
Jan 23 01:52:26.716: INFO: Pod "downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4" satisfied condition "success or failure"
Jan 23 01:52:26.718: INFO: Trying to get logs from node kind-conformance-worker2 pod downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4 container client-container: <nil>
STEP: delete the pod
Jan 23 01:52:26.732: INFO: Waiting for pod downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4 to disappear
Jan 23 01:52:26.741: INFO: Pod downwardapi-volume-87c02c2b-1eb1-11e9-b567-9e07e353f0d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:52:26.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7g8l5" for this suite.
Jan 23 01:52:32.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:52:32.805: INFO: namespace: e2e-tests-projected-7g8l5, resource: bindings, ignored listing per whitelist
Jan 23 01:52:32.814: INFO: namespace e2e-tests-projected-7g8l5 deletion completed in 6.070992236s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:52:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 23 01:52:32.866: INFO: namespace e2e-tests-kubectl-hcnzt
Jan 23 01:52:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-hcnzt'
Jan 23 01:52:33.360: INFO: stderr: ""
Jan 23 01:52:33.360: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 23 01:52:34.365: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:52:34.365: INFO: Found 0 / 1
Jan 23 01:52:35.364: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:52:35.364: INFO: Found 1 / 1
Jan 23 01:52:35.364: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 01:52:35.367: INFO: Selector matched 1 pods for map[app:redis]
Jan 23 01:52:35.367: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 01:52:35.367: INFO: wait on redis-master startup in e2e-tests-kubectl-hcnzt 
Jan 23 01:52:35.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 logs redis-master-67l4k redis-master --namespace=e2e-tests-kubectl-hcnzt'
Jan 23 01:52:35.449: INFO: stderr: ""
Jan 23 01:52:35.449: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jan 01:52:34.458 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jan 01:52:34.458 # Server started, Redis version 3.2.12\n1:M 23 Jan 01:52:34.458 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jan 01:52:34.458 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 23 01:52:35.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-hcnzt'
Jan 23 01:52:35.550: INFO: stderr: ""
Jan 23 01:52:35.550: INFO: stdout: "service/rm2 exposed\n"
Jan 23 01:52:35.554: INFO: Service rm2 in namespace e2e-tests-kubectl-hcnzt found.
STEP: exposing service
Jan 23 01:52:37.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-hcnzt'
Jan 23 01:52:37.645: INFO: stderr: ""
Jan 23 01:52:37.645: INFO: stdout: "service/rm3 exposed\n"
Jan 23 01:52:37.649: INFO: Service rm3 in namespace e2e-tests-kubectl-hcnzt found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:52:39.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hcnzt" for this suite.
Jan 23 01:53:01.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:53:01.697: INFO: namespace: e2e-tests-kubectl-hcnzt, resource: bindings, ignored listing per whitelist
Jan 23 01:53:01.727: INFO: namespace e2e-tests-kubectl-hcnzt deletion completed in 22.068659432s

• [SLOW TEST:28.913 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:53:01.727: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0123 01:53:02.813343      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 01:53:02.813: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:53:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pdlbf" for this suite.
Jan 23 01:53:08.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:53:08.840: INFO: namespace: e2e-tests-gc-pdlbf, resource: bindings, ignored listing per whitelist
Jan 23 01:53:08.886: INFO: namespace e2e-tests-gc-pdlbf deletion completed in 6.070135921s

• [SLOW TEST:7.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 23 01:53:08.886: INFO: >>> kubeConfig: /tmp/kubeconfig-311263258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Jan 23 01:53:08.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 create -f - --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:09.107: INFO: stderr: ""
Jan 23 01:53:09.107: INFO: stdout: "pod/pause created\n"
Jan 23 01:53:09.107: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 23 01:53:09.107: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-j6psh" to be "running and ready"
Jan 23 01:53:09.110: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.574351ms
Jan 23 01:53:11.113: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005932336s
Jan 23 01:53:11.113: INFO: Pod "pause" satisfied condition "running and ready"
Jan 23 01:53:11.113: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 23 01:53:11.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.193: INFO: stderr: ""
Jan 23 01:53:11.193: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 23 01:53:11.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pod pause -L testing-label --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.270: INFO: stderr: ""
Jan 23 01:53:11.270: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 23 01:53:11.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 label pods pause testing-label- --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.339: INFO: stderr: ""
Jan 23 01:53:11.339: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 23 01:53:11.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pod pause -L testing-label --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.402: INFO: stderr: ""
Jan 23 01:53:11.402: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Jan 23 01:53:11.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.478: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 01:53:11.478: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 23 01:53:11.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-j6psh'
Jan 23 01:53:11.546: INFO: stderr: "No resources found.\n"
Jan 23 01:53:11.546: INFO: stdout: ""
Jan 23 01:53:11.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-311263258 get pods -l name=pause --namespace=e2e-tests-kubectl-j6psh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 01:53:11.611: INFO: stderr: ""
Jan 23 01:53:11.611: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 23 01:53:11.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j6psh" for this suite.
Jan 23 01:53:17.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 23 01:53:17.661: INFO: namespace: e2e-tests-kubectl-j6psh, resource: bindings, ignored listing per whitelist
Jan 23 01:53:17.679: INFO: namespace e2e-tests-kubectl-j6psh deletion completed in 6.065593984s

• [SLOW TEST:8.793 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSJan 23 01:53:17.680: INFO: Running AfterSuite actions on all nodes
Jan 23 01:53:17.680: INFO: Running AfterSuite actions on node 1
Jan 23 01:53:17.680: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5938.913 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h39m0.108584247s
Test Suite Passed
