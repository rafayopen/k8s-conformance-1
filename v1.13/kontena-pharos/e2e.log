I0201 06:20:54.900219      18 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-622717276
I0201 06:20:54.900508      18 e2e.go:224] Starting e2e run "870de310-25e9-11e9-b51c-8292ded0de80" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1549002053 - Will randomize all specs
Will run 201 of 1946 specs

Feb  1 06:20:55.270: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:20:55.272: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb  1 06:21:02.479: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb  1 06:21:02.522: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb  1 06:21:02.522: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb  1 06:21:02.522: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb  1 06:21:02.536: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb  1 06:21:02.536: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Feb  1 06:21:02.536: INFO: e2e test version: v1.13.0
Feb  1 06:21:02.538: INFO: kube-apiserver version: v1.13.2
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:21:02.539: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
Feb  1 06:21:02.638: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb  1 06:21:02.653: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vbqc6
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-8c9503a9-25e9-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:21:10.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vbqc6" for this suite.
Feb  1 06:21:32.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:21:32.879: INFO: namespace: e2e-tests-configmap-vbqc6, resource: bindings, ignored listing per whitelist
Feb  1 06:21:32.977: INFO: namespace e2e-tests-configmap-vbqc6 deletion completed in 22.153567175s

â€¢ [SLOW TEST:30.439 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:21:32.978: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-jx5pl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:21:33.205: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb  1 06:21:38.210: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  1 06:21:40.220: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  1 06:21:40.246: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-jx5pl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jx5pl/deployments/test-cleanup-deployment,UID:a2e57c8e-25e9-11e9-90b2-96000019ce9c,ResourceVersion:2293,Generation:1,CreationTimestamp:2019-02-01 06:21:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb  1 06:21:40.248: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb  1 06:21:40.248: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb  1 06:21:40.248: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-jx5pl,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jx5pl/replicasets/test-cleanup-controller,UID:9eb42f63-25e9-11e9-90b2-96000019ce9c,ResourceVersion:2294,Generation:1,CreationTimestamp:2019-02-01 06:21:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a2e57c8e-25e9-11e9-90b2-96000019ce9c 0xc001377fdf 0xc001377ff0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  1 06:21:40.251: INFO: Pod "test-cleanup-controller-x6xsz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-x6xsz,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-jx5pl,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jx5pl/pods/test-cleanup-controller-x6xsz,UID:9eb5e72e-25e9-11e9-90b2-96000019ce9c,ResourceVersion:2288,Generation:0,CreationTimestamp:2019-02-01 06:21:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 9eb42f63-25e9-11e9-90b2-96000019ce9c 0xc0006be59f 0xc0006be5b0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-khs9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-khs9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-khs9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0006be620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0006be640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:21:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:21:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:21:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:21:33 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.3,StartTime:2019-02-01 06:21:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:21:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://264f01d4c73ee692387f611b26b597ebb819a5ef35e34a6c614e4862569775e5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:21:40.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jx5pl" for this suite.
Feb  1 06:21:46.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:21:46.350: INFO: namespace: e2e-tests-deployment-jx5pl, resource: bindings, ignored listing per whitelist
Feb  1 06:21:46.421: INFO: namespace e2e-tests-deployment-jx5pl deletion completed in 6.160450637s

â€¢ [SLOW TEST:13.443 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:21:46.422: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hhjk2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-hhjk2/secret-test-a6bb5a62-25e9-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:21:46.652: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-hhjk2" to be "success or failure"
Feb  1 06:21:46.656: INFO: Pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386269ms
Feb  1 06:21:48.665: INFO: Pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012457461s
Feb  1 06:21:50.670: INFO: Pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017130856s
Feb  1 06:21:52.676: INFO: Pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023819423s
STEP: Saw pod success
Feb  1 06:21:52.676: INFO: Pod "pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:21:52.680: INFO: Trying to get logs from node pharos-worker-2 pod pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80 container env-test: <nil>
STEP: delete the pod
Feb  1 06:21:52.702: INFO: Waiting for pod pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:21:52.706: INFO: Pod pod-configmaps-a6bbee30-25e9-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:21:52.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hhjk2" for this suite.
Feb  1 06:21:58.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:21:58.753: INFO: namespace: e2e-tests-secrets-hhjk2, resource: bindings, ignored listing per whitelist
Feb  1 06:21:58.824: INFO: namespace e2e-tests-secrets-hhjk2 deletion completed in 6.114257212s

â€¢ [SLOW TEST:12.403 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:21:58.825: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-69rp5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb  1 06:22:13.078: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:13.078: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:13.340: INFO: Exec stderr: ""
Feb  1 06:22:13.340: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:13.340: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:13.507: INFO: Exec stderr: ""
Feb  1 06:22:13.508: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:13.508: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:13.680: INFO: Exec stderr: ""
Feb  1 06:22:13.680: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:13.680: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:13.848: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb  1 06:22:13.848: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:13.848: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.030: INFO: Exec stderr: ""
Feb  1 06:22:14.030: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:14.031: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.216: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb  1 06:22:14.217: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:14.217: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.331: INFO: Exec stderr: ""
Feb  1 06:22:14.331: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.433: INFO: Exec stderr: ""
Feb  1 06:22:14.433: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.523: INFO: Exec stderr: ""
Feb  1 06:22:14.523: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-69rp5 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:22:14.523: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:22:14.619: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:22:14.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-69rp5" for this suite.
Feb  1 06:23:04.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:23:04.736: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-69rp5, resource: bindings, ignored listing per whitelist
Feb  1 06:23:04.741: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-69rp5 deletion completed in 50.114766361s

â€¢ [SLOW TEST:65.915 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:23:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-4ggvr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:23:04.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4ggvr" for this suite.
Feb  1 06:23:10.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:23:11.008: INFO: namespace: e2e-tests-services-4ggvr, resource: bindings, ignored listing per whitelist
Feb  1 06:23:11.062: INFO: namespace e2e-tests-services-4ggvr deletion completed in 6.115289868s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.321 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:23:11.062: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-jkr9k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  1 06:23:19.286: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  1 06:23:19.290: INFO: Pod pod-with-prestop-http-hook still exists
Feb  1 06:23:21.290: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  1 06:23:21.294: INFO: Pod pod-with-prestop-http-hook still exists
Feb  1 06:23:23.290: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  1 06:23:23.295: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:23:23.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jkr9k" for this suite.
Feb  1 06:23:45.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:23:45.465: INFO: namespace: e2e-tests-container-lifecycle-hook-jkr9k, resource: bindings, ignored listing per whitelist
Feb  1 06:23:45.468: INFO: namespace e2e-tests-container-lifecycle-hook-jkr9k deletion completed in 22.141941658s

â€¢ [SLOW TEST:34.406 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:23:45.469: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mbk28
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-edac541e-25e9-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:23:45.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-mbk28" to be "success or failure"
Feb  1 06:23:45.683: INFO: Pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.247752ms
Feb  1 06:23:47.692: INFO: Pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011464804s
Feb  1 06:23:49.699: INFO: Pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018500129s
Feb  1 06:23:51.705: INFO: Pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025214212s
STEP: Saw pod success
Feb  1 06:23:51.705: INFO: Pod "pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:23:51.709: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  1 06:23:51.735: INFO: Waiting for pod pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:23:51.737: INFO: Pod pod-projected-secrets-edae1bd0-25e9-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:23:51.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mbk28" for this suite.
Feb  1 06:23:57.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:23:57.808: INFO: namespace: e2e-tests-projected-mbk28, resource: bindings, ignored listing per whitelist
Feb  1 06:23:57.864: INFO: namespace e2e-tests-projected-mbk28 deletion completed in 6.121007161s

â€¢ [SLOW TEST:12.396 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:23:57.865: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sxv29
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-f50f51fa-25e9-11e9-b51c-8292ded0de80
STEP: Creating secret with name secret-projected-all-test-volume-f50f51d5-25e9-11e9-b51c-8292ded0de80
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb  1 06:23:58.077: INFO: Waiting up to 5m0s for pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-sxv29" to be "success or failure"
Feb  1 06:23:58.081: INFO: Pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019151ms
Feb  1 06:24:00.087: INFO: Pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009948627s
Feb  1 06:24:02.092: INFO: Pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015101358s
Feb  1 06:24:04.099: INFO: Pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021529707s
STEP: Saw pod success
Feb  1 06:24:04.099: INFO: Pod "projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:24:04.109: INFO: Trying to get logs from node pharos-worker-2 pod projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb  1 06:24:04.132: INFO: Waiting for pod projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:24:04.135: INFO: Pod projected-volume-f50f5181-25e9-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:24:04.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sxv29" for this suite.
Feb  1 06:24:10.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:24:10.201: INFO: namespace: e2e-tests-projected-sxv29, resource: bindings, ignored listing per whitelist
Feb  1 06:24:10.255: INFO: namespace e2e-tests-projected-sxv29 deletion completed in 6.115171953s

â€¢ [SLOW TEST:12.390 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:24:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-5b6hs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-fc7472f4-25e9-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:24:10.476: INFO: Waiting up to 5m0s for pod "pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-5b6hs" to be "success or failure"
Feb  1 06:24:10.479: INFO: Pod "pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173583ms
Feb  1 06:24:12.483: INFO: Pod "pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007511289s
Feb  1 06:24:14.488: INFO: Pod "pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011832558s
STEP: Saw pod success
Feb  1 06:24:14.488: INFO: Pod "pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:24:14.491: INFO: Trying to get logs from node pharos-worker-0 pod pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 06:24:14.521: INFO: Waiting for pod pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:24:14.524: INFO: Pod pod-secrets-fc75457b-25e9-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:24:14.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-5b6hs" for this suite.
Feb  1 06:24:20.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:24:20.653: INFO: namespace: e2e-tests-secrets-5b6hs, resource: bindings, ignored listing per whitelist
Feb  1 06:24:20.656: INFO: namespace e2e-tests-secrets-5b6hs deletion completed in 6.128094972s

â€¢ [SLOW TEST:10.399 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:24:20.657: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-szvsm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0201 06:25:00.912752      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 06:25:00.913: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:25:00.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-szvsm" for this suite.
Feb  1 06:25:06.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:25:07.021: INFO: namespace: e2e-tests-gc-szvsm, resource: bindings, ignored listing per whitelist
Feb  1 06:25:07.071: INFO: namespace e2e-tests-gc-szvsm deletion completed in 6.151280191s

â€¢ [SLOW TEST:46.414 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:25:07.072: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bnht2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb  1 06:25:13.351: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-1e581192-25ea-11e9-b51c-8292ded0de80", GenerateName:"", Namespace:"e2e-tests-pods-bnht2", SelfLink:"/api/v1/namespaces/e2e-tests-pods-bnht2/pods/pod-submit-remove-1e581192-25ea-11e9-b51c-8292ded0de80", UID:"1e57caa8-25ea-11e9-90b2-96000019ce9c", ResourceVersion:"3160", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684599107, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"317415239"}, Annotations:map[string]string{"kubernetes.io/psp":"00-pharos-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9j4v4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c22200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9j4v4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0012311c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pharos-worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001a30300), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001231210)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001231230)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001231238), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00123123c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599107, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599113, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599113, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599107, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"95.216.203.233", PodIP:"10.43.0.2", StartTime:(*v1.Time)(0xc0015370c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001537100), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db", ContainerID:"docker://1870a280da45fe61716b380f6df922a979bb7dae4e6344c414bbe0877ba1103b"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb  1 06:25:18.383: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:25:18.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bnht2" for this suite.
Feb  1 06:25:24.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:25:24.459: INFO: namespace: e2e-tests-pods-bnht2, resource: bindings, ignored listing per whitelist
Feb  1 06:25:24.524: INFO: namespace e2e-tests-pods-bnht2 deletion completed in 6.131988196s

â€¢ [SLOW TEST:17.452 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:25:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zrn98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb  1 06:25:24.728: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-622717276 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:25:24.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zrn98" for this suite.
Feb  1 06:25:30.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:25:30.912: INFO: namespace: e2e-tests-kubectl-zrn98, resource: bindings, ignored listing per whitelist
Feb  1 06:25:30.954: INFO: namespace e2e-tests-kubectl-zrn98 deletion completed in 6.114554014s

â€¢ [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:25:30.954: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-w689q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:25:31.165: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb  1 06:25:31.174: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w689q/daemonsets","resourceVersion":"3224"},"items":null}

Feb  1 06:25:31.177: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w689q/pods","resourceVersion":"3224"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:25:31.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w689q" for this suite.
Feb  1 06:25:37.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:25:37.271: INFO: namespace: e2e-tests-daemonsets-w689q, resource: bindings, ignored listing per whitelist
Feb  1 06:25:37.335: INFO: namespace e2e-tests-daemonsets-w689q deletion completed in 6.136832116s

S [SKIPPING] [6.381 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb  1 06:25:31.165: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:25:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qz85b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qz85b
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  1 06:25:37.532: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  1 06:26:01.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=http&host=10.32.0.3&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qz85b PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:26:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:26:01.799: INFO: Waiting for endpoints: map[]
Feb  1 06:26:01.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=http&host=10.40.0.5&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qz85b PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:26:01.803: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:26:01.918: INFO: Waiting for endpoints: map[]
Feb  1 06:26:01.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=http&host=10.43.0.2&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-qz85b PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 06:26:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 06:26:02.264: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:26:02.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qz85b" for this suite.
Feb  1 06:26:24.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:26:24.335: INFO: namespace: e2e-tests-pod-network-test-qz85b, resource: bindings, ignored listing per whitelist
Feb  1 06:26:24.405: INFO: namespace e2e-tests-pod-network-test-qz85b deletion completed in 22.134842876s

â€¢ [SLOW TEST:47.070 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:26:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-m9zlh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb  1 06:26:24.650: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-m9zlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-m9zlh/configmaps/e2e-watch-test-watch-closed,UID:4c6a3721-25ea-11e9-90b2-96000019ce9c,ResourceVersion:3416,Generation:0,CreationTimestamp:2019-02-01 06:26:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  1 06:26:24.651: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-m9zlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-m9zlh/configmaps/e2e-watch-test-watch-closed,UID:4c6a3721-25ea-11e9-90b2-96000019ce9c,ResourceVersion:3417,Generation:0,CreationTimestamp:2019-02-01 06:26:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb  1 06:26:24.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-m9zlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-m9zlh/configmaps/e2e-watch-test-watch-closed,UID:4c6a3721-25ea-11e9-90b2-96000019ce9c,ResourceVersion:3418,Generation:0,CreationTimestamp:2019-02-01 06:26:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  1 06:26:24.671: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-m9zlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-m9zlh/configmaps/e2e-watch-test-watch-closed,UID:4c6a3721-25ea-11e9-90b2-96000019ce9c,ResourceVersion:3419,Generation:0,CreationTimestamp:2019-02-01 06:26:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:26:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-m9zlh" for this suite.
Feb  1 06:26:30.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:26:30.717: INFO: namespace: e2e-tests-watch-m9zlh, resource: bindings, ignored listing per whitelist
Feb  1 06:26:30.842: INFO: namespace e2e-tests-watch-m9zlh deletion completed in 6.16522858s

â€¢ [SLOW TEST:6.436 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:26:30.843: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-sm7qj
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb  1 06:26:31.091: INFO: Waiting up to 5m0s for pod "pod-50457e31-25ea-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-sm7qj" to be "success or failure"
Feb  1 06:26:31.105: INFO: Pod "pod-50457e31-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.88283ms
Feb  1 06:26:33.110: INFO: Pod "pod-50457e31-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018952641s
Feb  1 06:26:35.115: INFO: Pod "pod-50457e31-25ea-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024533059s
STEP: Saw pod success
Feb  1 06:26:35.116: INFO: Pod "pod-50457e31-25ea-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:26:35.119: INFO: Trying to get logs from node pharos-worker-0 pod pod-50457e31-25ea-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 06:26:35.140: INFO: Waiting for pod pod-50457e31-25ea-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:26:35.143: INFO: Pod pod-50457e31-25ea-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:26:35.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sm7qj" for this suite.
Feb  1 06:26:41.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:26:41.214: INFO: namespace: e2e-tests-emptydir-sm7qj, resource: bindings, ignored listing per whitelist
Feb  1 06:26:41.296: INFO: namespace e2e-tests-emptydir-sm7qj deletion completed in 6.137493295s

â€¢ [SLOW TEST:10.452 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:26:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8rxll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-567931b8-25ea-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:26:41.506: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-8rxll" to be "success or failure"
Feb  1 06:26:41.512: INFO: Pod "pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839661ms
Feb  1 06:26:43.517: INFO: Pod "pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010635956s
Feb  1 06:26:45.524: INFO: Pod "pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017855816s
STEP: Saw pod success
Feb  1 06:26:45.524: INFO: Pod "pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:26:45.529: INFO: Trying to get logs from node pharos-worker-2 pod pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:26:45.556: INFO: Waiting for pod pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:26:45.559: INFO: Pod pod-projected-configmaps-567a645b-25ea-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:26:45.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8rxll" for this suite.
Feb  1 06:26:51.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:26:51.698: INFO: namespace: e2e-tests-projected-8rxll, resource: bindings, ignored listing per whitelist
Feb  1 06:26:51.703: INFO: namespace e2e-tests-projected-8rxll deletion completed in 6.138001796s

â€¢ [SLOW TEST:10.406 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:26:51.704: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-ck6bs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-5cb42c0f-25ea-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:26:51.953: INFO: Waiting up to 5m0s for pod "pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-ck6bs" to be "success or failure"
Feb  1 06:26:51.956: INFO: Pod "pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.228817ms
Feb  1 06:26:53.962: INFO: Pod "pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009091284s
Feb  1 06:26:55.967: INFO: Pod "pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014050488s
STEP: Saw pod success
Feb  1 06:26:55.967: INFO: Pod "pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:26:55.970: INFO: Trying to get logs from node pharos-worker-0 pod pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 06:26:56.001: INFO: Waiting for pod pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:26:56.003: INFO: Pod pod-secrets-5cb4cf9e-25ea-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:26:56.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ck6bs" for this suite.
Feb  1 06:27:02.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:27:02.138: INFO: namespace: e2e-tests-secrets-ck6bs, resource: bindings, ignored listing per whitelist
Feb  1 06:27:02.147: INFO: namespace e2e-tests-secrets-ck6bs deletion completed in 6.139606127s

â€¢ [SLOW TEST:10.443 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:27:02.148: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fwhlk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:28:02.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fwhlk" for this suite.
Feb  1 06:28:24.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:28:24.526: INFO: namespace: e2e-tests-container-probe-fwhlk, resource: bindings, ignored listing per whitelist
Feb  1 06:28:24.551: INFO: namespace e2e-tests-container-probe-fwhlk deletion completed in 22.15755675s

â€¢ [SLOW TEST:82.403 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:28:24.552: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-r69nc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0201 06:28:34.781110      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 06:28:34.781: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:28:34.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-r69nc" for this suite.
Feb  1 06:28:40.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:28:40.833: INFO: namespace: e2e-tests-gc-r69nc, resource: bindings, ignored listing per whitelist
Feb  1 06:28:40.917: INFO: namespace e2e-tests-gc-r69nc deletion completed in 6.130306195s

â€¢ [SLOW TEST:16.365 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:28:40.918: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-b2rhl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:28:41.112: INFO: Creating ReplicaSet my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80
Feb  1 06:28:41.124: INFO: Pod name my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80: Found 0 pods out of 1
Feb  1 06:28:46.132: INFO: Pod name my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80: Found 1 pods out of 1
Feb  1 06:28:46.132: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80" is running
Feb  1 06:28:48.141: INFO: Pod "my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80-nsc5q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:28:41 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:28:41 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:28:41 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:28:41 +0000 UTC Reason: Message:}])
Feb  1 06:28:48.141: INFO: Trying to dial the pod
Feb  1 06:28:53.169: INFO: Controller my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80: Got expected result from replica 1 [my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80-nsc5q]: "my-hostname-basic-9dc6a6bb-25ea-11e9-b51c-8292ded0de80-nsc5q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:28:53.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-b2rhl" for this suite.
Feb  1 06:28:59.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:28:59.253: INFO: namespace: e2e-tests-replicaset-b2rhl, resource: bindings, ignored listing per whitelist
Feb  1 06:28:59.299: INFO: namespace e2e-tests-replicaset-b2rhl deletion completed in 6.120847876s

â€¢ [SLOW TEST:18.381 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:28:59.300: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-28fnc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  1 06:28:59.496: INFO: PodSpec: initContainers in spec.initContainers
Feb  1 06:29:59.897: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a8bbc61d-25ea-11e9-b51c-8292ded0de80", GenerateName:"", Namespace:"e2e-tests-init-container-28fnc", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-28fnc/pods/pod-init-a8bbc61d-25ea-11e9-b51c-8292ded0de80", UID:"a8bb8388-25ea-11e9-90b2-96000019ce9c", ResourceVersion:"4039", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684599339, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"496420029", "name":"foo"}, Annotations:map[string]string{"kubernetes.io/psp":"00-pharos-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rcbjs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001af2400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rcbjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rcbjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rcbjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001937f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pharos-worker-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001a34660), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001970010)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001970030)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001970038), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00197003c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599339, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599339, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599339, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599339, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"95.216.222.53", PodIP:"10.32.0.3", StartTime:(*v1.Time)(0xc00147b1a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00122a8c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00122a9a0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://8e92b71610cb87d463989164c70b34eaad3855d377d31205ee9f7f9229f436eb"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00147b2a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00147b240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:29:59.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-28fnc" for this suite.
Feb  1 06:30:21.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:30:22.025: INFO: namespace: e2e-tests-init-container-28fnc, resource: bindings, ignored listing per whitelist
Feb  1 06:30:22.053: INFO: namespace e2e-tests-init-container-28fnc deletion completed in 22.142648532s

â€¢ [SLOW TEST:82.754 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:30:22.054: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-f4ghd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0201 06:30:28.281542      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 06:30:28.281: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:30:28.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-f4ghd" for this suite.
Feb  1 06:30:34.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:30:34.355: INFO: namespace: e2e-tests-gc-f4ghd, resource: bindings, ignored listing per whitelist
Feb  1 06:30:34.411: INFO: namespace e2e-tests-gc-f4ghd deletion completed in 6.124088987s

â€¢ [SLOW TEST:12.357 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:30:34.412: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8827c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  1 06:30:39.150: INFO: Successfully updated pod "labelsupdatee16a301b-25ea-11e9-b51c-8292ded0de80"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:30:43.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8827c" for this suite.
Feb  1 06:31:05.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:31:05.275: INFO: namespace: e2e-tests-downward-api-8827c, resource: bindings, ignored listing per whitelist
Feb  1 06:31:05.368: INFO: namespace e2e-tests-downward-api-8827c deletion completed in 22.135171484s

â€¢ [SLOW TEST:30.956 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:31:05.370: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-28nd2
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f3e0dc3a-25ea-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f3e0dc3a-25ea-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:31:11.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-28nd2" for this suite.
Feb  1 06:31:33.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:31:33.744: INFO: namespace: e2e-tests-projected-28nd2, resource: bindings, ignored listing per whitelist
Feb  1 06:31:33.792: INFO: namespace e2e-tests-projected-28nd2 deletion completed in 22.12890618s

â€¢ [SLOW TEST:28.423 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:31:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-5wspn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5wspn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5wspn.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  1 06:32:02.040: INFO: Unable to read wheezy_udp@kubernetes.default from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.046: INFO: Unable to read wheezy_tcp@kubernetes.default from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.060: INFO: Unable to read wheezy_udp@kubernetes.default.svc from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.064: INFO: Unable to read wheezy_tcp@kubernetes.default.svc from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.070: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.076: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.082: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.089: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.095: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.100: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.105: INFO: Unable to read jessie_udp@kubernetes.default from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.110: INFO: Unable to read jessie_tcp@kubernetes.default from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.118: INFO: Unable to read jessie_udp@kubernetes.default.svc from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.123: INFO: Unable to read jessie_tcp@kubernetes.default.svc from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.127: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.132: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.136: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.141: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.146: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.152: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80)
Feb  1 06:32:02.152: INFO: Lookups using e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80 failed for: [wheezy_udp@kubernetes.default wheezy_tcp@kubernetes.default wheezy_udp@kubernetes.default.svc wheezy_tcp@kubernetes.default.svc wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default jessie_tcp@kubernetes.default jessie_udp@kubernetes.default.svc jessie_tcp@kubernetes.default.svc jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5wspn.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  1 06:32:07.283: INFO: DNS probes using e2e-tests-dns-5wspn/dns-test-04d34c15-25eb-11e9-b51c-8292ded0de80 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:32:07.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-5wspn" for this suite.
Feb  1 06:32:13.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:32:13.373: INFO: namespace: e2e-tests-dns-5wspn, resource: bindings, ignored listing per whitelist
Feb  1 06:32:13.450: INFO: namespace e2e-tests-dns-5wspn deletion completed in 6.147627492s

â€¢ [SLOW TEST:39.656 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:32:13.451: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-9wqzk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  1 06:32:13.655: INFO: Waiting up to 5m0s for pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-9wqzk" to be "success or failure"
Feb  1 06:32:13.659: INFO: Pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.268328ms
Feb  1 06:32:15.667: INFO: Pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011379204s
Feb  1 06:32:17.672: INFO: Pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016955723s
Feb  1 06:32:19.679: INFO: Pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023906141s
STEP: Saw pod success
Feb  1 06:32:19.679: INFO: Pod "downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:32:19.683: INFO: Trying to get logs from node pharos-worker-2 pod downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 06:32:19.717: INFO: Waiting for pod downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:32:19.719: INFO: Pod downward-api-1c74e8a6-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:32:19.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9wqzk" for this suite.
Feb  1 06:32:25.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:32:25.860: INFO: namespace: e2e-tests-downward-api-9wqzk, resource: bindings, ignored listing per whitelist
Feb  1 06:32:25.888: INFO: namespace e2e-tests-downward-api-9wqzk deletion completed in 6.161917931s

â€¢ [SLOW TEST:12.437 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:32:25.889: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4rt24
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 06:32:26.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-4rt24" to be "success or failure"
Feb  1 06:32:26.137: INFO: Pod "downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.430545ms
Feb  1 06:32:28.144: INFO: Pod "downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0093327s
Feb  1 06:32:30.151: INFO: Pod "downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016455691s
STEP: Saw pod success
Feb  1 06:32:30.151: INFO: Pod "downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:32:30.156: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 06:32:30.184: INFO: Waiting for pod downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:32:30.195: INFO: Pod downwardapi-volume-23e4f51b-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:32:30.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4rt24" for this suite.
Feb  1 06:32:36.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:32:36.305: INFO: namespace: e2e-tests-downward-api-4rt24, resource: bindings, ignored listing per whitelist
Feb  1 06:32:36.386: INFO: namespace e2e-tests-downward-api-4rt24 deletion completed in 6.176275931s

â€¢ [SLOW TEST:10.497 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:32:36.386: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-f7rn4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5plpp in namespace e2e-tests-proxy-f7rn4
I0201 06:32:36.614929      18 runners.go:184] Created replication controller with name: proxy-service-5plpp, namespace: e2e-tests-proxy-f7rn4, replica count: 1
I0201 06:32:37.665762      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 06:32:38.666153      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 06:32:39.666470      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 06:32:40.666901      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 06:32:41.667356      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:42.667725      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:43.668254      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:44.669586      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:45.670261      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:46.670628      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0201 06:32:47.671217      18 runners.go:184] proxy-service-5plpp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  1 06:32:47.677: INFO: setup took 11.074930164s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb  1 06:32:47.690: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 11.987181ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 13.430557ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 13.18938ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 12.217788ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 13.126191ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 12.548972ms)
Feb  1 06:32:47.691: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 13.102068ms)
Feb  1 06:32:47.694: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 16.274868ms)
Feb  1 06:32:47.696: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 18.249181ms)
Feb  1 06:32:47.699: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 20.092384ms)
Feb  1 06:32:47.699: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 20.791242ms)
Feb  1 06:32:47.700: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 22.814145ms)
Feb  1 06:32:47.701: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 22.839249ms)
Feb  1 06:32:47.703: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 24.710514ms)
Feb  1 06:32:47.897: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 219.647301ms)
Feb  1 06:32:47.897: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 219.093641ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 11.041305ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 10.77178ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 11.063734ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 11.049193ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 11.474533ms)
Feb  1 06:32:47.909: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 11.003598ms)
Feb  1 06:32:47.911: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 12.734246ms)
Feb  1 06:32:47.911: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 13.300712ms)
Feb  1 06:32:47.911: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 13.131126ms)
Feb  1 06:32:47.911: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 13.08404ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 13.565878ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 13.738156ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 14.30287ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 14.039592ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 13.57538ms)
Feb  1 06:32:47.912: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 13.804603ms)
Feb  1 06:32:47.922: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 10.224516ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.146077ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 10.206836ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 10.136803ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 10.300489ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 11.09402ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 10.620038ms)
Feb  1 06:32:47.923: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.877951ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 11.444685ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 11.653386ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 11.508413ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 11.526423ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 11.789478ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 11.75164ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 11.617285ms)
Feb  1 06:32:47.924: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 11.774748ms)
Feb  1 06:32:47.930: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 5.240114ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 5.864683ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 5.850236ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 5.982963ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 6.502192ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 6.331425ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 5.967997ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 6.398693ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 6.336243ms)
Feb  1 06:32:47.931: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 6.657072ms)
Feb  1 06:32:47.932: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 7.110704ms)
Feb  1 06:32:47.932: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 8.027972ms)
Feb  1 06:32:47.932: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 8.003525ms)
Feb  1 06:32:47.932: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 7.402793ms)
Feb  1 06:32:47.933: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 7.846309ms)
Feb  1 06:32:47.933: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 7.368161ms)
Feb  1 06:32:47.936: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 3.334544ms)
Feb  1 06:32:47.936: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 3.75196ms)
Feb  1 06:32:47.938: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 5.674653ms)
Feb  1 06:32:47.938: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 4.71499ms)
Feb  1 06:32:47.940: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 7.406218ms)
Feb  1 06:32:47.940: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 6.652082ms)
Feb  1 06:32:47.940: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 7.074979ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 7.525481ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 7.25382ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.223953ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.433325ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 7.602763ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 7.407733ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 7.058762ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 7.28664ms)
Feb  1 06:32:47.941: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 7.805169ms)
Feb  1 06:32:47.947: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 6.025323ms)
Feb  1 06:32:47.951: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.206455ms)
Feb  1 06:32:47.951: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 10.178213ms)
Feb  1 06:32:47.951: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.952849ms)
Feb  1 06:32:47.952: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.336008ms)
Feb  1 06:32:47.952: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 10.371362ms)
Feb  1 06:32:47.952: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 10.543715ms)
Feb  1 06:32:47.952: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 10.7532ms)
Feb  1 06:32:47.952: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.45812ms)
Feb  1 06:32:47.953: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 11.218284ms)
Feb  1 06:32:47.953: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 11.093309ms)
Feb  1 06:32:47.953: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 11.295726ms)
Feb  1 06:32:47.953: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 11.456168ms)
Feb  1 06:32:47.954: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 12.683927ms)
Feb  1 06:32:47.954: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 13.036018ms)
Feb  1 06:32:47.954: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 12.715511ms)
Feb  1 06:32:47.961: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 6.826725ms)
Feb  1 06:32:47.961: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 6.94785ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 6.997045ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 7.452979ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 7.414872ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.502768ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.3372ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 7.565674ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.58714ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 7.545492ms)
Feb  1 06:32:47.962: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.151129ms)
Feb  1 06:32:47.963: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 7.497581ms)
Feb  1 06:32:47.964: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 9.333685ms)
Feb  1 06:32:47.964: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 9.718408ms)
Feb  1 06:32:47.964: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 10.256523ms)
Feb  1 06:32:47.964: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 9.851528ms)
Feb  1 06:32:47.973: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 8.011589ms)
Feb  1 06:32:47.973: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 8.019104ms)
Feb  1 06:32:47.973: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.466021ms)
Feb  1 06:32:47.973: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 9.005166ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 8.999648ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 9.270073ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 9.26385ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 9.254744ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 9.06439ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 9.20661ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 9.138394ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 9.08513ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 9.44546ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.675111ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 9.231527ms)
Feb  1 06:32:47.974: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 9.328768ms)
Feb  1 06:32:47.981: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 6.132456ms)
Feb  1 06:32:47.988: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 13.2875ms)
Feb  1 06:32:47.988: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 12.91988ms)
Feb  1 06:32:47.988: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 13.32539ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 13.353042ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 13.560004ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 14.020385ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 14.050088ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 13.811712ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 14.303702ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 14.061444ms)
Feb  1 06:32:47.989: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 14.213868ms)
Feb  1 06:32:47.990: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 14.735988ms)
Feb  1 06:32:47.990: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 14.707748ms)
Feb  1 06:32:47.990: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 14.501339ms)
Feb  1 06:32:47.991: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 15.943161ms)
Feb  1 06:32:47.997: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 5.568579ms)
Feb  1 06:32:47.997: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 5.483825ms)
Feb  1 06:32:47.998: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 4.892305ms)
Feb  1 06:32:47.998: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 5.89762ms)
Feb  1 06:32:47.998: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 5.89516ms)
Feb  1 06:32:47.998: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 5.417336ms)
Feb  1 06:32:47.999: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 7.201663ms)
Feb  1 06:32:48.000: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 6.48653ms)
Feb  1 06:32:48.000: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 8.669839ms)
Feb  1 06:32:48.000: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 6.986763ms)
Feb  1 06:32:48.000: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 8.73501ms)
Feb  1 06:32:48.000: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.39161ms)
Feb  1 06:32:48.002: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 8.385929ms)
Feb  1 06:32:48.002: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.077072ms)
Feb  1 06:32:48.002: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 8.360412ms)
Feb  1 06:32:48.002: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 8.38055ms)
Feb  1 06:32:48.009: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.157615ms)
Feb  1 06:32:48.011: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 8.95419ms)
Feb  1 06:32:48.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 9.291014ms)
Feb  1 06:32:48.012: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 9.954451ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 10.2219ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 10.480752ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.596303ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 10.919511ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 10.695701ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 10.854259ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 11.141645ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 10.93885ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.978201ms)
Feb  1 06:32:48.013: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 10.788927ms)
Feb  1 06:32:48.014: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 11.505077ms)
Feb  1 06:32:48.015: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 12.255374ms)
Feb  1 06:32:48.022: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 6.015107ms)
Feb  1 06:32:48.022: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 5.792337ms)
Feb  1 06:32:48.022: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 6.151364ms)
Feb  1 06:32:48.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 6.58599ms)
Feb  1 06:32:48.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 7.541516ms)
Feb  1 06:32:48.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 7.50144ms)
Feb  1 06:32:48.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 6.578925ms)
Feb  1 06:32:48.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 6.335227ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 6.986329ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.401315ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 8.323332ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 8.518711ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 8.320653ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 8.485246ms)
Feb  1 06:32:48.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 8.021426ms)
Feb  1 06:32:48.026: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 9.811578ms)
Feb  1 06:32:48.034: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.348404ms)
Feb  1 06:32:48.034: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 8.576787ms)
Feb  1 06:32:48.035: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 8.709364ms)
Feb  1 06:32:48.035: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 8.650491ms)
Feb  1 06:32:48.035: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 8.966429ms)
Feb  1 06:32:48.035: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 8.723161ms)
Feb  1 06:32:48.037: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 11.006411ms)
Feb  1 06:32:48.037: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 11.196546ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 12.279859ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 12.433958ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 12.753242ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 13.172985ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 13.196395ms)
Feb  1 06:32:48.039: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 12.940435ms)
Feb  1 06:32:48.040: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 13.456346ms)
Feb  1 06:32:48.042: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 15.913111ms)
Feb  1 06:32:48.046: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 4.404392ms)
Feb  1 06:32:48.046: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 4.48547ms)
Feb  1 06:32:48.047: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 4.810275ms)
Feb  1 06:32:48.050: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 7.266155ms)
Feb  1 06:32:48.052: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 9.663459ms)
Feb  1 06:32:48.052: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 9.948364ms)
Feb  1 06:32:48.053: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 10.513121ms)
Feb  1 06:32:48.053: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 10.044722ms)
Feb  1 06:32:48.053: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.145944ms)
Feb  1 06:32:48.053: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 10.0232ms)
Feb  1 06:32:48.061: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 18.400628ms)
Feb  1 06:32:48.061: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 18.369519ms)
Feb  1 06:32:48.061: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 18.456757ms)
Feb  1 06:32:48.061: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 18.140317ms)
Feb  1 06:32:48.061: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 19.013608ms)
Feb  1 06:32:48.062: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 19.464238ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 16.004522ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 16.3911ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 15.782263ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 16.2478ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 15.898222ms)
Feb  1 06:32:48.079: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 16.660439ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 16.83272ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 16.457286ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 16.785764ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 16.752318ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 17.513172ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 16.688611ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 16.649983ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 17.452593ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 18.413884ms)
Feb  1 06:32:48.080: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 18.250723ms)
Feb  1 06:32:48.089: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.889401ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 8.576139ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 6.797239ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 8.985371ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 7.167449ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.837629ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 7.697448ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.428235ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 7.918384ms)
Feb  1 06:32:48.090: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.959918ms)
Feb  1 06:32:48.091: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 10.136217ms)
Feb  1 06:32:48.091: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 9.601345ms)
Feb  1 06:32:48.092: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 9.856865ms)
Feb  1 06:32:48.092: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.052124ms)
Feb  1 06:32:48.092: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 9.163371ms)
Feb  1 06:32:48.092: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 9.342692ms)
Feb  1 06:32:48.098: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 5.104984ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 6.121726ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 6.953865ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 6.342588ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 7.27599ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 6.74294ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 6.249724ms)
Feb  1 06:32:48.099: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 6.689023ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.211529ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 7.893884ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 6.693048ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 7.572281ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.218706ms)
Feb  1 06:32:48.100: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 8.200088ms)
Feb  1 06:32:48.101: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 8.465312ms)
Feb  1 06:32:48.101: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 8.42132ms)
Feb  1 06:32:48.110: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 9.448565ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 9.430886ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 9.666105ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.744586ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 9.96958ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 10.103904ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 9.932888ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.305063ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 10.150204ms)
Feb  1 06:32:48.111: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 10.183083ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 13.858384ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 13.753387ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 13.713697ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 13.81883ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 14.014609ms)
Feb  1 06:32:48.115: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 14.179795ms)
Feb  1 06:32:48.123: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 7.315493ms)
Feb  1 06:32:48.123: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 6.921592ms)
Feb  1 06:32:48.123: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 6.950097ms)
Feb  1 06:32:48.123: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 7.843393ms)
Feb  1 06:32:48.124: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 8.557221ms)
Feb  1 06:32:48.125: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.005521ms)
Feb  1 06:32:48.125: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 8.778529ms)
Feb  1 06:32:48.125: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 8.372626ms)
Feb  1 06:32:48.125: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 9.226557ms)
Feb  1 06:32:48.125: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 9.200073ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 11.029715ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.699904ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 10.526842ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 10.615837ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 10.753858ms)
Feb  1 06:32:48.127: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 11.535563ms)
Feb  1 06:32:48.136: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg/proxy/rewriteme"... (200; 7.291354ms)
Feb  1 06:32:48.136: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:1080/proxy/rewri... (200; 7.179516ms)
Feb  1 06:32:48.136: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.743288ms)
Feb  1 06:32:48.136: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:462/proxy/: tls qux (200; 9.189549ms)
Feb  1 06:32:48.136: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:162/proxy/: bar (200; 8.68741ms)
Feb  1 06:32:48.137: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:443/proxy/... (200; 8.700417ms)
Feb  1 06:32:48.137: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:1080/proxy/... (200; 9.710491ms)
Feb  1 06:32:48.137: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/http:proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 9.838733ms)
Feb  1 06:32:48.138: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname1/proxy/: tls baz (200; 9.207165ms)
Feb  1 06:32:48.138: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/https:proxy-service-5plpp-2vjcg:460/proxy/: tls baz (200; 9.451422ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/pods/proxy-service-5plpp-2vjcg:160/proxy/: foo (200; 9.42922ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname2/proxy/: bar (200; 10.415982ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/proxy-service-5plpp:portname1/proxy/: foo (200; 10.806642ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname2/proxy/: bar (200; 10.362464ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/https:proxy-service-5plpp:tlsportname2/proxy/: tls qux (200; 10.716995ms)
Feb  1 06:32:48.139: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-f7rn4/services/http:proxy-service-5plpp:portname1/proxy/: foo (200; 11.395727ms)
STEP: deleting ReplicationController proxy-service-5plpp in namespace e2e-tests-proxy-f7rn4, will wait for the garbage collector to delete the pods
Feb  1 06:32:48.206: INFO: Deleting ReplicationController proxy-service-5plpp took: 11.830248ms
Feb  1 06:32:48.306: INFO: Terminating ReplicationController proxy-service-5plpp pods took: 100.375729ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:32:52.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-f7rn4" for this suite.
Feb  1 06:32:58.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:32:58.340: INFO: namespace: e2e-tests-proxy-f7rn4, resource: bindings, ignored listing per whitelist
Feb  1 06:32:58.352: INFO: namespace e2e-tests-proxy-f7rn4 deletion completed in 6.139155592s

â€¢ [SLOW TEST:21.966 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:32:58.353: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-j8hs7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:33:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-j8hs7" for this suite.
Feb  1 06:33:37.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:33:37.748: INFO: namespace: e2e-tests-replication-controller-j8hs7, resource: bindings, ignored listing per whitelist
Feb  1 06:33:37.758: INFO: namespace e2e-tests-replication-controller-j8hs7 deletion completed in 22.149699672s

â€¢ [SLOW TEST:39.405 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:33:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-4r2nb
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:33:38.009: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:33:39.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-4r2nb" for this suite.
Feb  1 06:33:45.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:33:45.175: INFO: namespace: e2e-tests-custom-resource-definition-4r2nb, resource: bindings, ignored listing per whitelist
Feb  1 06:33:45.214: INFO: namespace e2e-tests-custom-resource-definition-4r2nb deletion completed in 6.139903921s

â€¢ [SLOW TEST:7.456 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:33:45.216: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-z9dp7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-z9dp7
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-z9dp7 to expose endpoints map[]
Feb  1 06:33:45.414: INFO: Get endpoints failed (3.082933ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb  1 06:33:46.418: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-z9dp7 exposes endpoints map[] (1.006970241s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-z9dp7
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-z9dp7 to expose endpoints map[pod1:[80]]
Feb  1 06:33:50.480: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-z9dp7 exposes endpoints map[pod1:[80]] (4.052363454s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-z9dp7
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-z9dp7 to expose endpoints map[pod1:[80] pod2:[80]]
Feb  1 06:33:54.609: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-z9dp7 exposes endpoints map[pod2:[80] pod1:[80]] (4.112117478s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-z9dp7
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-z9dp7 to expose endpoints map[pod2:[80]]
Feb  1 06:33:55.648: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-z9dp7 exposes endpoints map[pod2:[80]] (1.033609471s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-z9dp7
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-z9dp7 to expose endpoints map[]
Feb  1 06:33:56.664: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-z9dp7 exposes endpoints map[] (1.006184254s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:33:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-z9dp7" for this suite.
Feb  1 06:34:18.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:34:18.816: INFO: namespace: e2e-tests-services-z9dp7, resource: bindings, ignored listing per whitelist
Feb  1 06:34:18.851: INFO: namespace e2e-tests-services-z9dp7 deletion completed in 22.130068611s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:33.636 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:34:18.853: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-92t9l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  1 06:34:23.609: INFO: Successfully updated pod "labelsupdate673426b6-25eb-11e9-b51c-8292ded0de80"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:34:27.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-92t9l" for this suite.
Feb  1 06:34:49.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:34:49.742: INFO: namespace: e2e-tests-projected-92t9l, resource: bindings, ignored listing per whitelist
Feb  1 06:34:49.901: INFO: namespace e2e-tests-projected-92t9l deletion completed in 22.224832138s

â€¢ [SLOW TEST:31.048 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:34:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-pnj7f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-cm2m
STEP: Creating a pod to test atomic-volume-subpath
Feb  1 06:34:50.138: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cm2m" in namespace "e2e-tests-subpath-pnj7f" to be "success or failure"
Feb  1 06:34:50.143: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.53096ms
Feb  1 06:34:52.151: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012619445s
Feb  1 06:34:54.160: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021724724s
Feb  1 06:34:56.171: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033010672s
Feb  1 06:34:58.179: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 8.040630111s
Feb  1 06:35:00.184: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 10.045897538s
Feb  1 06:35:02.190: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 12.052171859s
Feb  1 06:35:04.196: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 14.057883115s
Feb  1 06:35:06.202: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 16.064006286s
Feb  1 06:35:08.208: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 18.069824006s
Feb  1 06:35:10.213: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 20.074602146s
Feb  1 06:35:12.218: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 22.080056573s
Feb  1 06:35:14.225: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 24.086708802s
Feb  1 06:35:16.235: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Running", Reason="", readiness=false. Elapsed: 26.096962709s
Feb  1 06:35:18.244: INFO: Pod "pod-subpath-test-secret-cm2m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.106208119s
STEP: Saw pod success
Feb  1 06:35:18.244: INFO: Pod "pod-subpath-test-secret-cm2m" satisfied condition "success or failure"
Feb  1 06:35:18.249: INFO: Trying to get logs from node pharos-worker-2 pod pod-subpath-test-secret-cm2m container test-container-subpath-secret-cm2m: <nil>
STEP: delete the pod
Feb  1 06:35:18.273: INFO: Waiting for pod pod-subpath-test-secret-cm2m to disappear
Feb  1 06:35:18.276: INFO: Pod pod-subpath-test-secret-cm2m no longer exists
STEP: Deleting pod pod-subpath-test-secret-cm2m
Feb  1 06:35:18.276: INFO: Deleting pod "pod-subpath-test-secret-cm2m" in namespace "e2e-tests-subpath-pnj7f"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:35:18.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-pnj7f" for this suite.
Feb  1 06:35:24.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:35:24.366: INFO: namespace: e2e-tests-subpath-pnj7f, resource: bindings, ignored listing per whitelist
Feb  1 06:35:24.433: INFO: namespace e2e-tests-subpath-pnj7f deletion completed in 6.148363718s

â€¢ [SLOW TEST:34.531 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:35:24.434: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qvlzs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:35:24.657: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:35:30.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qvlzs" for this suite.
Feb  1 06:36:16.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:36:16.760: INFO: namespace: e2e-tests-pods-qvlzs, resource: bindings, ignored listing per whitelist
Feb  1 06:36:16.896: INFO: namespace e2e-tests-pods-qvlzs deletion completed in 46.188785975s

â€¢ [SLOW TEST:52.462 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:36:16.898: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fzs75
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-ad91f452-25eb-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:36:17.126: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-fzs75" to be "success or failure"
Feb  1 06:36:17.129: INFO: Pod "pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007509ms
Feb  1 06:36:19.137: INFO: Pod "pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011075236s
Feb  1 06:36:21.145: INFO: Pod "pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019390807s
STEP: Saw pod success
Feb  1 06:36:21.146: INFO: Pod "pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:36:21.149: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  1 06:36:21.179: INFO: Waiting for pod pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:36:21.182: INFO: Pod pod-projected-secrets-ad92a8f2-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:36:21.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fzs75" for this suite.
Feb  1 06:36:27.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:36:27.370: INFO: namespace: e2e-tests-projected-fzs75, resource: bindings, ignored listing per whitelist
Feb  1 06:36:27.370: INFO: namespace e2e-tests-projected-fzs75 deletion completed in 6.180299577s

â€¢ [SLOW TEST:10.472 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:36:27.370: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-np6t8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 06:36:27.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-np6t8'
Feb  1 06:36:27.969: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  1 06:36:27.969: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb  1 06:36:29.980: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kc8dq]
Feb  1 06:36:29.980: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kc8dq" in namespace "e2e-tests-kubectl-np6t8" to be "running and ready"
Feb  1 06:36:29.988: INFO: Pod "e2e-test-nginx-rc-kc8dq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.709126ms
Feb  1 06:36:31.993: INFO: Pod "e2e-test-nginx-rc-kc8dq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012972751s
Feb  1 06:36:34.009: INFO: Pod "e2e-test-nginx-rc-kc8dq": Phase="Running", Reason="", readiness=true. Elapsed: 4.02928054s
Feb  1 06:36:34.009: INFO: Pod "e2e-test-nginx-rc-kc8dq" satisfied condition "running and ready"
Feb  1 06:36:34.010: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kc8dq]
Feb  1 06:36:34.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-np6t8'
Feb  1 06:36:34.163: INFO: stderr: ""
Feb  1 06:36:34.163: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb  1 06:36:34.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-np6t8'
Feb  1 06:36:34.262: INFO: stderr: ""
Feb  1 06:36:34.262: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:36:34.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-np6t8" for this suite.
Feb  1 06:36:56.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:36:56.371: INFO: namespace: e2e-tests-kubectl-np6t8, resource: bindings, ignored listing per whitelist
Feb  1 06:36:56.406: INFO: namespace e2e-tests-kubectl-np6t8 deletion completed in 22.137143575s

â€¢ [SLOW TEST:29.036 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:36:56.408: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-k6r7b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb  1 06:37:03.656: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:37:04.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-k6r7b" for this suite.
Feb  1 06:37:26.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:37:26.738: INFO: namespace: e2e-tests-replicaset-k6r7b, resource: bindings, ignored listing per whitelist
Feb  1 06:37:26.927: INFO: namespace e2e-tests-replicaset-k6r7b deletion completed in 22.231663784s

â€¢ [SLOW TEST:30.519 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:37:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xf2hp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:37:27.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xf2hp" for this suite.
Feb  1 06:37:49.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:37:49.244: INFO: namespace: e2e-tests-pods-xf2hp, resource: bindings, ignored listing per whitelist
Feb  1 06:37:49.325: INFO: namespace e2e-tests-pods-xf2hp deletion completed in 22.152028247s

â€¢ [SLOW TEST:22.397 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:37:49.327: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-d9rjf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:37:49.538: INFO: Creating deployment "test-recreate-deployment"
Feb  1 06:37:49.544: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb  1 06:37:49.551: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb  1 06:37:51.561: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb  1 06:37:51.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:37:53.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684599869, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:37:55.571: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb  1 06:37:55.581: INFO: Updating deployment test-recreate-deployment
Feb  1 06:37:55.581: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  1 06:37:55.629: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-d9rjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d9rjf/deployments/test-recreate-deployment,UID:e4a9240f-25eb-11e9-90b2-96000019ce9c,ResourceVersion:5633,Generation:2,CreationTimestamp:2019-02-01 06:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-01 06:37:55 +0000 UTC 2019-02-01 06:37:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-01 06:37:55 +0000 UTC 2019-02-01 06:37:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb  1 06:37:55.633: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-d9rjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d9rjf/replicasets/test-recreate-deployment-697fbf54bf,UID:e84664bd-25eb-11e9-90b2-96000019ce9c,ResourceVersion:5631,Generation:1,CreationTimestamp:2019-02-01 06:37:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e4a9240f-25eb-11e9-90b2-96000019ce9c 0xc002416957 0xc002416958}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 06:37:55.633: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb  1 06:37:55.633: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-d9rjf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d9rjf/replicasets/test-recreate-deployment-5dfdcc846d,UID:e4aac930-25eb-11e9-90b2-96000019ce9c,ResourceVersion:5622,Generation:2,CreationTimestamp:2019-02-01 06:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e4a9240f-25eb-11e9-90b2-96000019ce9c 0xc0024168a7 0xc0024168a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 06:37:55.637: INFO: Pod "test-recreate-deployment-697fbf54bf-7sdp7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-7sdp7,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-d9rjf,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d9rjf/pods/test-recreate-deployment-697fbf54bf-7sdp7,UID:e846c93a-25eb-11e9-90b2-96000019ce9c,ResourceVersion:5634,Generation:0,CreationTimestamp:2019-02-01 06:37:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf e84664bd-25eb-11e9-90b2-96000019ce9c 0xc0024171c7 0xc0024171c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-f5sxz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f5sxz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f5sxz true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002417230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002417250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:37:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:37:55 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:37:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:37:55.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-d9rjf" for this suite.
Feb  1 06:38:01.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:38:01.752: INFO: namespace: e2e-tests-deployment-d9rjf, resource: bindings, ignored listing per whitelist
Feb  1 06:38:01.810: INFO: namespace e2e-tests-deployment-d9rjf deletion completed in 6.168018908s

â€¢ [SLOW TEST:12.484 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:38:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4tx5z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  1 06:38:02.040: INFO: Waiting up to 5m0s for pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-4tx5z" to be "success or failure"
Feb  1 06:38:02.044: INFO: Pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.247514ms
Feb  1 06:38:04.050: INFO: Pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353879s
Feb  1 06:38:06.054: INFO: Pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013964959s
Feb  1 06:38:08.059: INFO: Pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018209929s
STEP: Saw pod success
Feb  1 06:38:08.059: INFO: Pod "downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:38:08.062: INFO: Trying to get logs from node pharos-worker-2 pod downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 06:38:08.102: INFO: Waiting for pod downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:38:08.105: INFO: Pod downward-api-ec1bd10d-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:38:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4tx5z" for this suite.
Feb  1 06:38:14.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:38:14.242: INFO: namespace: e2e-tests-downward-api-4tx5z, resource: bindings, ignored listing per whitelist
Feb  1 06:38:14.246: INFO: namespace e2e-tests-downward-api-4tx5z deletion completed in 6.135609376s

â€¢ [SLOW TEST:12.435 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:38:14.247: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dcfz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f3819bdd-25eb-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:38:14.456: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-dcfz9" to be "success or failure"
Feb  1 06:38:14.460: INFO: Pod "pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.424232ms
Feb  1 06:38:16.466: INFO: Pod "pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009521964s
Feb  1 06:38:18.472: INFO: Pod "pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01546828s
STEP: Saw pod success
Feb  1 06:38:18.472: INFO: Pod "pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:38:18.476: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:38:18.505: INFO: Waiting for pod pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:38:18.508: INFO: Pod pod-projected-configmaps-f3822d52-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:38:18.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dcfz9" for this suite.
Feb  1 06:38:24.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:38:24.707: INFO: namespace: e2e-tests-projected-dcfz9, resource: bindings, ignored listing per whitelist
Feb  1 06:38:24.768: INFO: namespace e2e-tests-projected-dcfz9 deletion completed in 6.253569535s

â€¢ [SLOW TEST:10.521 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:38:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-d4vhk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb  1 06:38:24.971: INFO: Waiting up to 5m0s for pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80" in namespace "e2e-tests-containers-d4vhk" to be "success or failure"
Feb  1 06:38:24.975: INFO: Pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684164ms
Feb  1 06:38:26.979: INFO: Pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007734715s
Feb  1 06:38:28.984: INFO: Pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012879377s
Feb  1 06:38:30.992: INFO: Pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021349345s
STEP: Saw pod success
Feb  1 06:38:30.993: INFO: Pod "client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:38:30.998: INFO: Trying to get logs from node pharos-worker-2 pod client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 06:38:31.017: INFO: Waiting for pod client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:38:31.021: INFO: Pod client-containers-f9c71265-25eb-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:38:31.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-d4vhk" for this suite.
Feb  1 06:38:37.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:38:37.116: INFO: namespace: e2e-tests-containers-d4vhk, resource: bindings, ignored listing per whitelist
Feb  1 06:38:37.181: INFO: namespace e2e-tests-containers-d4vhk deletion completed in 6.144719773s

â€¢ [SLOW TEST:12.413 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:38:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-k9tnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb  1 06:38:37.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:37.672: INFO: stderr: ""
Feb  1 06:38:37.672: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 06:38:37.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:37.783: INFO: stderr: ""
Feb  1 06:38:37.783: INFO: stdout: "update-demo-nautilus-nbft8 update-demo-nautilus-w5th9 "
Feb  1 06:38:37.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-nbft8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:37.877: INFO: stderr: ""
Feb  1 06:38:37.877: INFO: stdout: ""
Feb  1 06:38:37.877: INFO: update-demo-nautilus-nbft8 is created but not running
Feb  1 06:38:42.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:42.976: INFO: stderr: ""
Feb  1 06:38:42.976: INFO: stdout: "update-demo-nautilus-nbft8 update-demo-nautilus-w5th9 "
Feb  1 06:38:42.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-nbft8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:43.064: INFO: stderr: ""
Feb  1 06:38:43.064: INFO: stdout: "true"
Feb  1 06:38:43.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-nbft8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:43.176: INFO: stderr: ""
Feb  1 06:38:43.176: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 06:38:43.176: INFO: validating pod update-demo-nautilus-nbft8
Feb  1 06:38:43.185: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 06:38:43.185: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 06:38:43.185: INFO: update-demo-nautilus-nbft8 is verified up and running
Feb  1 06:38:43.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-w5th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:43.284: INFO: stderr: ""
Feb  1 06:38:43.284: INFO: stdout: "true"
Feb  1 06:38:43.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-w5th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:38:43.379: INFO: stderr: ""
Feb  1 06:38:43.379: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 06:38:43.379: INFO: validating pod update-demo-nautilus-w5th9
Feb  1 06:38:43.389: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 06:38:43.389: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 06:38:43.389: INFO: update-demo-nautilus-w5th9 is verified up and running
STEP: rolling-update to new replication controller
Feb  1 06:38:43.394: INFO: scanned /root for discovery docs: <nil>
Feb  1 06:38:43.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:06.914: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  1 06:39:06.915: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 06:39:06.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:07.012: INFO: stderr: ""
Feb  1 06:39:07.012: INFO: stdout: "update-demo-kitten-wjw58 update-demo-kitten-wsnz8 "
Feb  1 06:39:07.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-kitten-wjw58 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:07.102: INFO: stderr: ""
Feb  1 06:39:07.102: INFO: stdout: "true"
Feb  1 06:39:07.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-kitten-wjw58 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:07.201: INFO: stderr: ""
Feb  1 06:39:07.201: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  1 06:39:07.201: INFO: validating pod update-demo-kitten-wjw58
Feb  1 06:39:07.213: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  1 06:39:07.213: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  1 06:39:07.213: INFO: update-demo-kitten-wjw58 is verified up and running
Feb  1 06:39:07.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-kitten-wsnz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:07.305: INFO: stderr: ""
Feb  1 06:39:07.305: INFO: stdout: "true"
Feb  1 06:39:07.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-kitten-wsnz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k9tnq'
Feb  1 06:39:07.387: INFO: stderr: ""
Feb  1 06:39:07.387: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb  1 06:39:07.387: INFO: validating pod update-demo-kitten-wsnz8
Feb  1 06:39:07.397: INFO: got data: {
  "image": "kitten.jpg"
}

Feb  1 06:39:07.397: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb  1 06:39:07.397: INFO: update-demo-kitten-wsnz8 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:39:07.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k9tnq" for this suite.
Feb  1 06:39:29.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:39:29.441: INFO: namespace: e2e-tests-kubectl-k9tnq, resource: bindings, ignored listing per whitelist
Feb  1 06:39:29.554: INFO: namespace e2e-tests-kubectl-k9tnq deletion completed in 22.150243816s

â€¢ [SLOW TEST:52.372 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:39:29.555: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wmldm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2067079c-25ec-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:39:29.779: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-wmldm" to be "success or failure"
Feb  1 06:39:29.781: INFO: Pod "pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524915ms
Feb  1 06:39:31.791: INFO: Pod "pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011764847s
Feb  1 06:39:33.796: INFO: Pod "pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017289184s
STEP: Saw pod success
Feb  1 06:39:33.796: INFO: Pod "pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:39:33.801: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:39:33.828: INFO: Waiting for pod pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:39:33.831: INFO: Pod pod-projected-configmaps-2067d855-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:39:33.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wmldm" for this suite.
Feb  1 06:39:39.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:39:39.983: INFO: namespace: e2e-tests-projected-wmldm, resource: bindings, ignored listing per whitelist
Feb  1 06:39:40.012: INFO: namespace e2e-tests-projected-wmldm deletion completed in 6.173722634s

â€¢ [SLOW TEST:10.457 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:39:40.012: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-8nqh6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb  1 06:39:40.217: INFO: Waiting up to 5m0s for pod "client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-containers-8nqh6" to be "success or failure"
Feb  1 06:39:40.220: INFO: Pod "client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942863ms
Feb  1 06:39:42.225: INFO: Pod "client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007694292s
Feb  1 06:39:44.230: INFO: Pod "client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012623799s
STEP: Saw pod success
Feb  1 06:39:44.230: INFO: Pod "client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:39:44.235: INFO: Trying to get logs from node pharos-worker-2 pod client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 06:39:44.256: INFO: Waiting for pod client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:39:44.259: INFO: Pod client-containers-26a0264d-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:39:44.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-8nqh6" for this suite.
Feb  1 06:39:50.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:39:50.328: INFO: namespace: e2e-tests-containers-8nqh6, resource: bindings, ignored listing per whitelist
Feb  1 06:39:50.401: INFO: namespace e2e-tests-containers-8nqh6 deletion completed in 6.13760129s

â€¢ [SLOW TEST:10.389 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:39:50.403: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-tql26
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:39:56.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-tql26" for this suite.
Feb  1 06:40:46.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:40:46.764: INFO: namespace: e2e-tests-kubelet-test-tql26, resource: bindings, ignored listing per whitelist
Feb  1 06:40:46.847: INFO: namespace e2e-tests-kubelet-test-tql26 deletion completed in 50.182915101s

â€¢ [SLOW TEST:56.444 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:40:46.848: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-tdgzc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:40:55.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-tdgzc" for this suite.
Feb  1 06:41:01.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:41:01.342: INFO: namespace: e2e-tests-kubelet-test-tdgzc, resource: bindings, ignored listing per whitelist
Feb  1 06:41:01.361: INFO: namespace e2e-tests-kubelet-test-tdgzc deletion completed in 6.152523435s

â€¢ [SLOW TEST:14.513 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:41:01.362: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-hvbfx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:41:01.582: INFO: Creating deployment "nginx-deployment"
Feb  1 06:41:01.598: INFO: Waiting for observed generation 1
Feb  1 06:41:03.606: INFO: Waiting for all required pods to come up
Feb  1 06:41:03.614: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb  1 06:41:11.627: INFO: Waiting for deployment "nginx-deployment" to complete
Feb  1 06:41:11.636: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb  1 06:41:11.647: INFO: Updating deployment nginx-deployment
Feb  1 06:41:11.647: INFO: Waiting for observed generation 2
Feb  1 06:41:13.666: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb  1 06:41:13.671: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb  1 06:41:13.676: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  1 06:41:13.696: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb  1 06:41:13.697: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb  1 06:41:13.701: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb  1 06:41:13.706: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb  1 06:41:13.706: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb  1 06:41:13.715: INFO: Updating deployment nginx-deployment
Feb  1 06:41:13.716: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb  1 06:41:13.723: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb  1 06:41:15.742: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  1 06:41:15.756: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hvbfx/deployments/nginx-deployment,UID:5720b6ec-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6594,Generation:3,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-01 06:41:13 +0000 UTC 2019-02-01 06:41:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-01 06:41:13 +0000 UTC 2019-02-01 06:41:01 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb  1 06:41:15.766: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hvbfx/replicasets/nginx-deployment-65bbdb5f8,UID:5d212885-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6589,Generation:3,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5720b6ec-25ec-11e9-90b2-96000019ce9c 0xc002133b07 0xc002133b08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 06:41:15.766: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb  1 06:41:15.766: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hvbfx/replicasets/nginx-deployment-555b55d965,UID:5723df00-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6588,Generation:3,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5720b6ec-25ec-11e9-90b2-96000019ce9c 0xc002133a47 0xc002133a48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb  1 06:41:15.784: INFO: Pod "nginx-deployment-555b55d965-2klqs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2klqs,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-2klqs,UID:572ceaf1-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6421,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28440 0xc001a28441}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a284b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a284d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:10.40.0.7,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://57d192e9a4b7b711b418e95da0e8d762dea60f570ec5ec0f18cfc044086bf413}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.784: INFO: Pod "nginx-deployment-555b55d965-2kxvv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2kxvv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-2kxvv,UID:5e609639-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6591,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28590 0xc001a28591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.785: INFO: Pod "nginx-deployment-555b55d965-2thbz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2thbz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-2thbz,UID:572cb6d6-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6424,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a286d7 0xc001a286d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.4,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://b874acf95e80e5cfa06adfbde93b5b7e81fd56d510f952da0a66630083cd9992}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.785: INFO: Pod "nginx-deployment-555b55d965-44d6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-44d6x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-44d6x,UID:5e5f97ad-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6572,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28850 0xc001a28851}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a288d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a288f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.785: INFO: Pod "nginx-deployment-555b55d965-549m9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-549m9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-549m9,UID:572ab36d-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6439,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a289d7 0xc001a289d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.5,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://e8d9bece821fdf2bfa1f2e9acc446cb1baec55d76da2ef8110918332438ed64b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.786: INFO: Pod "nginx-deployment-555b55d965-89xm5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-89xm5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-89xm5,UID:5e5d9121-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6549,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28ba0 0xc001a28ba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.786: INFO: Pod "nginx-deployment-555b55d965-99nj8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-99nj8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-99nj8,UID:5e5d6d5d-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6553,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28cf7 0xc001a28cf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.786: INFO: Pod "nginx-deployment-555b55d965-9c295" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9c295,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-9c295,UID:5e6652fc-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6620,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a28ed7 0xc001a28ed8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a28f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a28fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.787: INFO: Pod "nginx-deployment-555b55d965-c8xrl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c8xrl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-c8xrl,UID:5e66326e-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6619,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29087 0xc001a29088}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a29100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a29120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.787: INFO: Pod "nginx-deployment-555b55d965-fdbhl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-fdbhl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-fdbhl,UID:5e665fdc-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6605,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29a37 0xc001a29a38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a29aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a29ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.787: INFO: Pod "nginx-deployment-555b55d965-j6c7w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j6c7w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-j6c7w,UID:5e5c4768-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6542,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29b77 0xc001a29b78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a29be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a29c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.787: INFO: Pod "nginx-deployment-555b55d965-jmldz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jmldz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-jmldz,UID:57278332-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6402,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29cb7 0xc001a29cb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a29d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a29d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.3,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://c67450dd133072e25f3d14a174f8d03597c2403ce3f2de228e1588ae7f51492a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.788: INFO: Pod "nginx-deployment-555b55d965-l9nxt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-l9nxt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-l9nxt,UID:572a8aef-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6418,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29e00 0xc001a29e01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a29e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a29e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:10.43.0.3,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://ef4f7d4e001eeaa1d726904b57c0ef10ddfd207f58d0740850a4b10c9320e9a3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.788: INFO: Pod "nginx-deployment-555b55d965-lffhd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lffhd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-lffhd,UID:5e636ba1-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6607,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc001a29f90 0xc001a29f91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b41b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.788: INFO: Pod "nginx-deployment-555b55d965-mbcgj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mbcgj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-mbcgj,UID:5727b5ce-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6389,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b4267 0xc0021b4268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b42d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b42f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:10.40.0.5,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://d36b7cccb0ae756660dd862c276f12ea6a22cac0ee296df50ce0e5ce30015564}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.788: INFO: Pod "nginx-deployment-555b55d965-r7jw5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-r7jw5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-r7jw5,UID:572af136-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6436,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b43b0 0xc0021b43b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:10.40.0.6,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://e9d211d83eeb36e6b72bdd9fc9c125cedd68163381286321b0d9557e9b35c2a7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.789: INFO: Pod "nginx-deployment-555b55d965-rrmhg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rrmhg,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-rrmhg,UID:5e663ed9-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6634,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b4500 0xc0021b4501}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.789: INFO: Pod "nginx-deployment-555b55d965-sjl7b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-sjl7b,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-sjl7b,UID:5e604681-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6562,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b4647 0xc0021b4648}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b46b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b46d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.789: INFO: Pod "nginx-deployment-555b55d965-v8tm7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v8tm7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-v8tm7,UID:5e6652bf-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6633,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b4787 0xc0021b4788}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b47f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.789: INFO: Pod "nginx-deployment-555b55d965-xz4dr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xz4dr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-555b55d965-xz4dr,UID:572673c3-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6399,Generation:0,CreationTimestamp:2019-02-01 06:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 5723df00-25ec-11e9-90b2-96000019ce9c 0xc0021b48c7 0xc0021b48c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:01 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:10.43.0.2,StartTime:2019-02-01 06:41:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-01 06:41:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db docker://b86cbeeffed093cdc8b9eb80b88cf4f7d6e0a520ccc6cba8a73347d8c6a35bb5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.790: INFO: Pod "nginx-deployment-65bbdb5f8-7j8g7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-7j8g7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-7j8g7,UID:5d2aa957-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6504,Generation:0,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b4a10 0xc0021b4a11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.790: INFO: Pod "nginx-deployment-65bbdb5f8-ftt49" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-ftt49,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-ftt49,UID:5e654fd7-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6598,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b4b70 0xc0021b4b71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.790: INFO: Pod "nginx-deployment-65bbdb5f8-hxnz6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hxnz6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-hxnz6,UID:5d2b5a12-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6503,Generation:0,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b4cd0 0xc0021b4cd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4d50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.791: INFO: Pod "nginx-deployment-65bbdb5f8-k28q2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-k28q2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-k28q2,UID:5e678ef5-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6638,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b4e30 0xc0021b4e31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b4eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b4ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.791: INFO: Pod "nginx-deployment-65bbdb5f8-l4hhp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-l4hhp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-l4hhp,UID:5e647641-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6592,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b4f90 0xc0021b4f91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.791: INFO: Pod "nginx-deployment-65bbdb5f8-lj77f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lj77f,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-lj77f,UID:5e6247d8-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6600,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b50f0 0xc0021b50f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.791: INFO: Pod "nginx-deployment-65bbdb5f8-p82x5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-p82x5,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-p82x5,UID:5e5d3011-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6543,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b5250 0xc0021b5251}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b52d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b52f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.792: INFO: Pod "nginx-deployment-65bbdb5f8-rzzs2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rzzs2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-rzzs2,UID:5d226f43-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6483,Generation:0,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b53b0 0xc0021b53b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.792: INFO: Pod "nginx-deployment-65bbdb5f8-sw88w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sw88w,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-sw88w,UID:5d23d4b7-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6489,Generation:0,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b5510 0xc0021b5511}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b55b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.792: INFO: Pod "nginx-deployment-65bbdb5f8-t6jp6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-t6jp6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-t6jp6,UID:5e62909b-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6590,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b5670 0xc0021b5671}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b56f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.795: INFO: Pod "nginx-deployment-65bbdb5f8-t7l2s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-t7l2s,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-t7l2s,UID:5d23da60-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6644,Generation:0,CreationTimestamp:2019-02-01 06:41:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b57d0 0xc0021b57d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:11 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.54,PodIP:10.40.0.8,StartTime:2019-02-01 06:41:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.796: INFO: Pod "nginx-deployment-65bbdb5f8-t9bj6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-t9bj6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-t9bj6,UID:5e6575fa-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6604,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b5950 0xc0021b5951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b59d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b59f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb  1 06:41:15.796: INFO: Pod "nginx-deployment-65bbdb5f8-xhjxk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xhjxk,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-hvbfx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hvbfx/pods/nginx-deployment-65bbdb5f8-xhjxk,UID:5e6591e3-25ec-11e9-90b2-96000019ce9c,ResourceVersion:6612,Generation:0,CreationTimestamp:2019-02-01 06:41:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 5d212885-25ec-11e9-90b2-96000019ce9c 0xc0021b5ab0 0xc0021b5ab1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xwcnf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xwcnf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xwcnf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021b5b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021b5b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:41:13 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:,StartTime:2019-02-01 06:41:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:41:15.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hvbfx" for this suite.
Feb  1 06:41:21.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:41:21.839: INFO: namespace: e2e-tests-deployment-hvbfx, resource: bindings, ignored listing per whitelist
Feb  1 06:41:21.958: INFO: namespace e2e-tests-deployment-hvbfx deletion completed in 6.15708428s

â€¢ [SLOW TEST:20.596 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:41:21.959: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-ncvbg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb  1 06:41:22.227: INFO: Waiting up to 5m0s for pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-var-expansion-ncvbg" to be "success or failure"
Feb  1 06:41:22.231: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.49907ms
Feb  1 06:41:24.236: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009380878s
Feb  1 06:41:26.244: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017389579s
Feb  1 06:41:28.253: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025681047s
Feb  1 06:41:30.262: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034892296s
Feb  1 06:41:32.267: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040368131s
Feb  1 06:41:34.273: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.045704528s
STEP: Saw pod success
Feb  1 06:41:34.273: INFO: Pod "var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:41:34.277: INFO: Trying to get logs from node pharos-worker-2 pod var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 06:41:34.298: INFO: Waiting for pod var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:41:34.302: INFO: Pod var-expansion-636dca8f-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:41:34.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-ncvbg" for this suite.
Feb  1 06:41:40.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:41:40.406: INFO: namespace: e2e-tests-var-expansion-ncvbg, resource: bindings, ignored listing per whitelist
Feb  1 06:41:40.467: INFO: namespace e2e-tests-var-expansion-ncvbg deletion completed in 6.157681697s

â€¢ [SLOW TEST:18.508 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:41:40.469: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pchfx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 06:41:40.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-pchfx" to be "success or failure"
Feb  1 06:41:40.709: INFO: Pod "downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.65766ms
Feb  1 06:41:42.718: INFO: Pod "downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013470231s
Feb  1 06:41:44.723: INFO: Pod "downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018175748s
STEP: Saw pod success
Feb  1 06:41:44.723: INFO: Pod "downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:41:44.727: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 06:41:44.759: INFO: Waiting for pod downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:41:44.761: INFO: Pod downwardapi-volume-6e705460-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:41:44.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pchfx" for this suite.
Feb  1 06:41:50.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:41:50.902: INFO: namespace: e2e-tests-projected-pchfx, resource: bindings, ignored listing per whitelist
Feb  1 06:41:50.902: INFO: namespace e2e-tests-projected-pchfx deletion completed in 6.135169526s

â€¢ [SLOW TEST:10.433 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:41:50.904: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-d896h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 06:41:51.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-d896h" to be "success or failure"
Feb  1 06:41:51.108: INFO: Pod "downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709907ms
Feb  1 06:41:53.112: INFO: Pod "downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008787402s
Feb  1 06:41:55.116: INFO: Pod "downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013231451s
STEP: Saw pod success
Feb  1 06:41:55.116: INFO: Pod "downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:41:55.119: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 06:41:55.172: INFO: Waiting for pod downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:41:55.179: INFO: Pod downwardapi-volume-74a41454-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:41:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d896h" for this suite.
Feb  1 06:42:01.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:42:01.243: INFO: namespace: e2e-tests-projected-d896h, resource: bindings, ignored listing per whitelist
Feb  1 06:42:01.358: INFO: namespace e2e-tests-projected-d896h deletion completed in 6.171782521s

â€¢ [SLOW TEST:10.454 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:42:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-fjr9x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-7ae3f343-25ec-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:42:01.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-fjr9x" to be "success or failure"
Feb  1 06:42:01.606: INFO: Pod "pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.352587ms
Feb  1 06:42:03.611: INFO: Pod "pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016317652s
Feb  1 06:42:05.615: INFO: Pod "pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020438039s
STEP: Saw pod success
Feb  1 06:42:05.615: INFO: Pod "pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:42:05.620: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:42:05.657: INFO: Waiting for pod pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:42:05.660: INFO: Pod pod-configmaps-7ae54352-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:42:05.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fjr9x" for this suite.
Feb  1 06:42:11.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:42:11.731: INFO: namespace: e2e-tests-configmap-fjr9x, resource: bindings, ignored listing per whitelist
Feb  1 06:42:11.802: INFO: namespace e2e-tests-configmap-fjr9x deletion completed in 6.136056434s

â€¢ [SLOW TEST:10.443 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:42:11.803: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-rt82c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  1 06:42:20.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:20.088: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:22.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:22.094: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:24.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:24.093: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:26.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:26.095: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:28.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:28.092: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:30.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:30.099: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:32.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:32.095: INFO: Pod pod-with-poststart-http-hook still exists
Feb  1 06:42:34.088: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  1 06:42:34.093: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:42:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-rt82c" for this suite.
Feb  1 06:42:56.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:42:56.233: INFO: namespace: e2e-tests-container-lifecycle-hook-rt82c, resource: bindings, ignored listing per whitelist
Feb  1 06:42:56.249: INFO: namespace e2e-tests-container-lifecycle-hook-rt82c deletion completed in 22.150768584s

â€¢ [SLOW TEST:44.447 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:42:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gd98b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 06:42:56.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-gd98b'
Feb  1 06:42:56.569: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  1 06:42:56.569: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb  1 06:42:56.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-gd98b'
Feb  1 06:42:56.714: INFO: stderr: ""
Feb  1 06:42:56.714: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:42:56.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gd98b" for this suite.
Feb  1 06:43:18.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:43:18.760: INFO: namespace: e2e-tests-kubectl-gd98b, resource: bindings, ignored listing per whitelist
Feb  1 06:43:18.870: INFO: namespace e2e-tests-kubectl-gd98b deletion completed in 22.150926057s

â€¢ [SLOW TEST:22.621 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:43:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-kwpq7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-kwpq7
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-kwpq7
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-kwpq7
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-kwpq7
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-kwpq7
Feb  1 06:43:25.166: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kwpq7, name: ss-0, uid: ac78bb95-25ec-11e9-90b2-96000019ce9c, status phase: Pending. Waiting for statefulset controller to delete.
Feb  1 06:43:25.356: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kwpq7, name: ss-0, uid: ac78bb95-25ec-11e9-90b2-96000019ce9c, status phase: Failed. Waiting for statefulset controller to delete.
Feb  1 06:43:25.361: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kwpq7, name: ss-0, uid: ac78bb95-25ec-11e9-90b2-96000019ce9c, status phase: Failed. Waiting for statefulset controller to delete.
Feb  1 06:43:25.365: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-kwpq7
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-kwpq7
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-kwpq7 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  1 06:43:31.397: INFO: Deleting all statefulset in ns e2e-tests-statefulset-kwpq7
Feb  1 06:43:31.401: INFO: Scaling statefulset ss to 0
Feb  1 06:43:41.425: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 06:43:41.436: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:43:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-kwpq7" for this suite.
Feb  1 06:43:47.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:43:47.523: INFO: namespace: e2e-tests-statefulset-kwpq7, resource: bindings, ignored listing per whitelist
Feb  1 06:43:47.581: INFO: namespace e2e-tests-statefulset-kwpq7 deletion completed in 6.126330631s

â€¢ [SLOW TEST:28.710 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:43:47.581: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bplbn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ba33a1a4-25ec-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:43:47.810: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-bplbn" to be "success or failure"
Feb  1 06:43:47.813: INFO: Pod "pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774746ms
Feb  1 06:43:49.818: INFO: Pod "pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007580128s
Feb  1 06:43:51.823: INFO: Pod "pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012740158s
STEP: Saw pod success
Feb  1 06:43:51.823: INFO: Pod "pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:43:51.828: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:43:51.852: INFO: Waiting for pod pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:43:51.854: INFO: Pod pod-configmaps-ba344bc8-25ec-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:43:51.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bplbn" for this suite.
Feb  1 06:43:57.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:43:57.971: INFO: namespace: e2e-tests-configmap-bplbn, resource: bindings, ignored listing per whitelist
Feb  1 06:43:57.995: INFO: namespace e2e-tests-configmap-bplbn deletion completed in 6.137142545s

â€¢ [SLOW TEST:10.414 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:43:57.998: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ljn9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb  1 06:43:58.214: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb  1 06:43:58.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:58.480: INFO: stderr: ""
Feb  1 06:43:58.481: INFO: stdout: "service/redis-slave created\n"
Feb  1 06:43:58.482: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb  1 06:43:58.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:58.729: INFO: stderr: ""
Feb  1 06:43:58.729: INFO: stdout: "service/redis-master created\n"
Feb  1 06:43:58.729: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb  1 06:43:58.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:58.949: INFO: stderr: ""
Feb  1 06:43:58.950: INFO: stdout: "service/frontend created\n"
Feb  1 06:43:58.950: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb  1 06:43:58.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:59.137: INFO: stderr: ""
Feb  1 06:43:59.137: INFO: stdout: "deployment.extensions/frontend created\n"
Feb  1 06:43:59.138: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  1 06:43:59.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:59.353: INFO: stderr: ""
Feb  1 06:43:59.353: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb  1 06:43:59.354: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb  1 06:43:59.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:43:59.585: INFO: stderr: ""
Feb  1 06:43:59.585: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb  1 06:43:59.585: INFO: Waiting for all frontend pods to be Running.
Feb  1 06:44:39.640: INFO: Waiting for frontend to serve content.
Feb  1 06:44:39.673: INFO: Trying to add a new entry to the guestbook.
Feb  1 06:44:44.707: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb  1 06:44:44.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:44.861: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:44.861: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb  1 06:44:44.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:44.954: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:44.954: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  1 06:44:44.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:45.060: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:45.060: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  1 06:44:45.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:45.146: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:45.146: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  1 06:44:45.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:45.250: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:45.250: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb  1 06:44:45.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ljn9x'
Feb  1 06:44:45.345: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 06:44:45.345: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:44:45.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ljn9x" for this suite.
Feb  1 06:46:47.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:46:47.397: INFO: namespace: e2e-tests-kubectl-ljn9x, resource: bindings, ignored listing per whitelist
Feb  1 06:46:47.512: INFO: namespace e2e-tests-kubectl-ljn9x deletion completed in 2m2.155758359s

â€¢ [SLOW TEST:169.515 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:46:47.513: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x6rft
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 06:46:47.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-x6rft'
Feb  1 06:46:48.070: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  1 06:46:48.070: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb  1 06:46:52.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-x6rft'
Feb  1 06:46:52.190: INFO: stderr: ""
Feb  1 06:46:52.190: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:46:52.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x6rft" for this suite.
Feb  1 06:48:44.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:48:44.260: INFO: namespace: e2e-tests-kubectl-x6rft, resource: bindings, ignored listing per whitelist
Feb  1 06:48:44.335: INFO: namespace e2e-tests-kubectl-x6rft deletion completed in 1m52.137970281s

â€¢ [SLOW TEST:116.822 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:48:44.335: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x7h4t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:48:44.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 version'
Feb  1 06:48:44.630: INFO: stderr: ""
Feb  1 06:48:44.630: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-10T23:28:14Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:48:44.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x7h4t" for this suite.
Feb  1 06:48:50.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:48:50.741: INFO: namespace: e2e-tests-kubectl-x7h4t, resource: bindings, ignored listing per whitelist
Feb  1 06:48:50.746: INFO: namespace e2e-tests-kubectl-x7h4t deletion completed in 6.111153674s

â€¢ [SLOW TEST:6.411 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:48:50.747: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-gvtjj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:48:50.987: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb  1 06:48:50.994: INFO: Number of nodes with available pods: 0
Feb  1 06:48:50.994: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb  1 06:48:51.008: INFO: Number of nodes with available pods: 0
Feb  1 06:48:51.008: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:52.013: INFO: Number of nodes with available pods: 0
Feb  1 06:48:52.013: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:53.013: INFO: Number of nodes with available pods: 0
Feb  1 06:48:53.013: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:54.013: INFO: Number of nodes with available pods: 0
Feb  1 06:48:54.013: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:55.013: INFO: Number of nodes with available pods: 1
Feb  1 06:48:55.013: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb  1 06:48:55.030: INFO: Number of nodes with available pods: 1
Feb  1 06:48:55.030: INFO: Number of running nodes: 0, number of available pods: 1
Feb  1 06:48:56.035: INFO: Number of nodes with available pods: 0
Feb  1 06:48:56.035: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb  1 06:48:56.046: INFO: Number of nodes with available pods: 0
Feb  1 06:48:56.046: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:57.050: INFO: Number of nodes with available pods: 0
Feb  1 06:48:57.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:58.051: INFO: Number of nodes with available pods: 0
Feb  1 06:48:58.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:48:59.057: INFO: Number of nodes with available pods: 0
Feb  1 06:48:59.057: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:00.056: INFO: Number of nodes with available pods: 0
Feb  1 06:49:00.056: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:01.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:01.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:02.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:02.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:03.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:03.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:04.060: INFO: Number of nodes with available pods: 0
Feb  1 06:49:04.060: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:05.053: INFO: Number of nodes with available pods: 0
Feb  1 06:49:05.053: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:06.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:06.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:07.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:07.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:08.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:08.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:09.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:09.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:10.056: INFO: Number of nodes with available pods: 0
Feb  1 06:49:10.056: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:11.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:11.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:12.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:12.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:13.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:13.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:14.059: INFO: Number of nodes with available pods: 0
Feb  1 06:49:14.059: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:15.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:15.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:16.056: INFO: Number of nodes with available pods: 0
Feb  1 06:49:16.056: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:17.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:17.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:18.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:18.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:19.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:19.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:20.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:20.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:21.052: INFO: Number of nodes with available pods: 0
Feb  1 06:49:21.052: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:22.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:22.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:23.053: INFO: Number of nodes with available pods: 0
Feb  1 06:49:23.053: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:24.059: INFO: Number of nodes with available pods: 0
Feb  1 06:49:24.059: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:25.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:25.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:26.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:26.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:27.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:27.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:28.055: INFO: Number of nodes with available pods: 0
Feb  1 06:49:28.055: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:29.053: INFO: Number of nodes with available pods: 0
Feb  1 06:49:29.053: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:30.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:30.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:31.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:31.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:32.057: INFO: Number of nodes with available pods: 0
Feb  1 06:49:32.057: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:33.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:33.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:34.050: INFO: Number of nodes with available pods: 0
Feb  1 06:49:34.050: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:35.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:35.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:36.051: INFO: Number of nodes with available pods: 0
Feb  1 06:49:36.051: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:49:37.051: INFO: Number of nodes with available pods: 1
Feb  1 06:49:37.051: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-gvtjj, will wait for the garbage collector to delete the pods
Feb  1 06:49:37.127: INFO: Deleting DaemonSet.extensions daemon-set took: 9.986102ms
Feb  1 06:49:37.227: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.489487ms
Feb  1 06:50:12.934: INFO: Number of nodes with available pods: 0
Feb  1 06:50:12.934: INFO: Number of running nodes: 0, number of available pods: 0
Feb  1 06:50:12.937: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gvtjj/daemonsets","resourceVersion":"8436"},"items":null}

Feb  1 06:50:12.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gvtjj/pods","resourceVersion":"8436"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:50:13.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gvtjj" for this suite.
Feb  1 06:50:19.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:50:19.087: INFO: namespace: e2e-tests-daemonsets-gvtjj, resource: bindings, ignored listing per whitelist
Feb  1 06:50:19.125: INFO: namespace e2e-tests-daemonsets-gvtjj deletion completed in 6.119987379s

â€¢ [SLOW TEST:88.378 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:50:19.125: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-6n2pd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-6n2pd
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-6n2pd
STEP: Deleting pre-stop pod
Feb  1 06:50:38.380: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:50:38.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-6n2pd" for this suite.
Feb  1 06:51:16.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:51:16.452: INFO: namespace: e2e-tests-prestop-6n2pd, resource: bindings, ignored listing per whitelist
Feb  1 06:51:16.535: INFO: namespace e2e-tests-prestop-6n2pd deletion completed in 38.131774286s

â€¢ [SLOW TEST:57.410 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:51:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cgn7b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c5cb2591-25ed-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 06:51:16.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-cgn7b" to be "success or failure"
Feb  1 06:51:16.754: INFO: Pod "pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690773ms
Feb  1 06:51:18.759: INFO: Pod "pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007156785s
Feb  1 06:51:20.766: INFO: Pod "pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014409783s
STEP: Saw pod success
Feb  1 06:51:20.766: INFO: Pod "pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:51:20.769: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 06:51:20.804: INFO: Waiting for pod pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:51:20.807: INFO: Pod pod-configmaps-c5cba858-25ed-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:51:20.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cgn7b" for this suite.
Feb  1 06:51:26.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:51:26.865: INFO: namespace: e2e-tests-configmap-cgn7b, resource: bindings, ignored listing per whitelist
Feb  1 06:51:26.937: INFO: namespace e2e-tests-configmap-cgn7b deletion completed in 6.125673742s

â€¢ [SLOW TEST:10.401 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:51:26.939: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-l5bcl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  1 06:51:27.239: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:27.244: INFO: Number of nodes with available pods: 0
Feb  1 06:51:27.244: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:51:28.253: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:28.257: INFO: Number of nodes with available pods: 0
Feb  1 06:51:28.257: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:51:29.254: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:29.260: INFO: Number of nodes with available pods: 0
Feb  1 06:51:29.260: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:51:30.251: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:30.262: INFO: Number of nodes with available pods: 0
Feb  1 06:51:30.262: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 06:51:31.251: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:31.255: INFO: Number of nodes with available pods: 1
Feb  1 06:51:31.255: INFO: Node pharos-worker-1 is running more than one daemon pod
Feb  1 06:51:32.255: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:32.260: INFO: Number of nodes with available pods: 3
Feb  1 06:51:32.260: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb  1 06:51:32.282: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:32.288: INFO: Number of nodes with available pods: 2
Feb  1 06:51:32.288: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:33.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:33.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:33.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:34.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:34.299: INFO: Number of nodes with available pods: 2
Feb  1 06:51:34.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:35.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:35.299: INFO: Number of nodes with available pods: 2
Feb  1 06:51:35.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:36.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:36.300: INFO: Number of nodes with available pods: 2
Feb  1 06:51:36.300: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:37.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:37.300: INFO: Number of nodes with available pods: 2
Feb  1 06:51:37.300: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:38.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:38.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:38.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:39.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:39.299: INFO: Number of nodes with available pods: 2
Feb  1 06:51:39.300: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:40.293: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:40.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:40.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:41.298: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:41.304: INFO: Number of nodes with available pods: 2
Feb  1 06:51:41.304: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:42.299: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:42.311: INFO: Number of nodes with available pods: 2
Feb  1 06:51:42.311: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:43.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:43.301: INFO: Number of nodes with available pods: 2
Feb  1 06:51:43.302: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:44.315: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:44.340: INFO: Number of nodes with available pods: 2
Feb  1 06:51:44.340: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:45.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:45.302: INFO: Number of nodes with available pods: 2
Feb  1 06:51:45.302: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:46.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:46.304: INFO: Number of nodes with available pods: 2
Feb  1 06:51:46.304: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:47.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:47.303: INFO: Number of nodes with available pods: 2
Feb  1 06:51:47.303: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:48.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:48.302: INFO: Number of nodes with available pods: 2
Feb  1 06:51:48.302: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:49.296: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:49.303: INFO: Number of nodes with available pods: 2
Feb  1 06:51:49.303: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:50.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:50.300: INFO: Number of nodes with available pods: 2
Feb  1 06:51:50.300: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:51.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:51.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:51.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:52.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:52.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:52.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:53.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:53.298: INFO: Number of nodes with available pods: 2
Feb  1 06:51:53.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:54.296: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:54.302: INFO: Number of nodes with available pods: 2
Feb  1 06:51:54.302: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:55.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:55.299: INFO: Number of nodes with available pods: 2
Feb  1 06:51:55.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:56.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:56.297: INFO: Number of nodes with available pods: 2
Feb  1 06:51:56.297: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:57.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:57.299: INFO: Number of nodes with available pods: 2
Feb  1 06:51:57.300: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:58.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:58.309: INFO: Number of nodes with available pods: 2
Feb  1 06:51:58.309: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:51:59.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:51:59.302: INFO: Number of nodes with available pods: 2
Feb  1 06:51:59.302: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:00.296: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:00.301: INFO: Number of nodes with available pods: 2
Feb  1 06:52:00.301: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:01.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:01.301: INFO: Number of nodes with available pods: 2
Feb  1 06:52:01.301: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:02.296: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:02.303: INFO: Number of nodes with available pods: 2
Feb  1 06:52:02.303: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:03.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:03.303: INFO: Number of nodes with available pods: 2
Feb  1 06:52:03.303: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:04.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:04.299: INFO: Number of nodes with available pods: 2
Feb  1 06:52:04.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:05.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:05.299: INFO: Number of nodes with available pods: 2
Feb  1 06:52:05.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:06.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:06.297: INFO: Number of nodes with available pods: 2
Feb  1 06:52:06.297: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:07.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:07.298: INFO: Number of nodes with available pods: 2
Feb  1 06:52:07.298: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:08.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:08.314: INFO: Number of nodes with available pods: 2
Feb  1 06:52:08.314: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:09.300: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:09.306: INFO: Number of nodes with available pods: 2
Feb  1 06:52:09.306: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:10.298: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:10.304: INFO: Number of nodes with available pods: 2
Feb  1 06:52:10.304: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:11.297: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:11.307: INFO: Number of nodes with available pods: 2
Feb  1 06:52:11.307: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:12.293: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:12.297: INFO: Number of nodes with available pods: 2
Feb  1 06:52:12.297: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:13.294: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:13.307: INFO: Number of nodes with available pods: 2
Feb  1 06:52:13.307: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:14.295: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:14.299: INFO: Number of nodes with available pods: 2
Feb  1 06:52:14.299: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 06:52:15.293: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 06:52:15.296: INFO: Number of nodes with available pods: 3
Feb  1 06:52:15.296: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-l5bcl, will wait for the garbage collector to delete the pods
Feb  1 06:52:15.373: INFO: Deleting DaemonSet.extensions daemon-set took: 6.112903ms
Feb  1 06:52:15.474: INFO: Terminating DaemonSet.extensions daemon-set pods took: 106.950471ms
Feb  1 06:52:52.882: INFO: Number of nodes with available pods: 0
Feb  1 06:52:52.882: INFO: Number of running nodes: 0, number of available pods: 0
Feb  1 06:52:52.887: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-l5bcl/daemonsets","resourceVersion":"8875"},"items":null}

Feb  1 06:52:52.891: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-l5bcl/pods","resourceVersion":"8875"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:52:52.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-l5bcl" for this suite.
Feb  1 06:52:58.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:52:59.035: INFO: namespace: e2e-tests-daemonsets-l5bcl, resource: bindings, ignored listing per whitelist
Feb  1 06:52:59.063: INFO: namespace e2e-tests-daemonsets-l5bcl deletion completed in 6.145943175s

â€¢ [SLOW TEST:92.125 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:52:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8j7pd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 06:52:59.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-8j7pd" to be "success or failure"
Feb  1 06:52:59.276: INFO: Pod "downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640227ms
Feb  1 06:53:01.280: INFO: Pod "downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006803014s
Feb  1 06:53:03.284: INFO: Pod "downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010741149s
STEP: Saw pod success
Feb  1 06:53:03.284: INFO: Pod "downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:53:03.288: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 06:53:03.312: INFO: Waiting for pod downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:53:03.315: INFO: Pod downwardapi-volume-02e71c10-25ee-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:53:03.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8j7pd" for this suite.
Feb  1 06:53:09.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:53:09.372: INFO: namespace: e2e-tests-projected-8j7pd, resource: bindings, ignored listing per whitelist
Feb  1 06:53:09.471: INFO: namespace e2e-tests-projected-8j7pd deletion completed in 6.15118317s

â€¢ [SLOW TEST:10.407 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:53:09.472: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-sm7m7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-091a8932-25ee-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 06:53:09.681: INFO: Waiting up to 5m0s for pod "pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-sm7m7" to be "success or failure"
Feb  1 06:53:09.683: INFO: Pod "pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800516ms
Feb  1 06:53:11.691: INFO: Pod "pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010630328s
Feb  1 06:53:13.696: INFO: Pod "pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015661253s
STEP: Saw pod success
Feb  1 06:53:13.696: INFO: Pod "pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:53:13.701: INFO: Trying to get logs from node pharos-worker-0 pod pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 06:53:13.730: INFO: Waiting for pod pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:53:13.733: INFO: Pod pod-secrets-091b2d98-25ee-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:53:13.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sm7m7" for this suite.
Feb  1 06:53:19.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:53:19.879: INFO: namespace: e2e-tests-secrets-sm7m7, resource: bindings, ignored listing per whitelist
Feb  1 06:53:19.882: INFO: namespace e2e-tests-secrets-sm7m7 deletion completed in 6.144048933s

â€¢ [SLOW TEST:10.410 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:53:19.883: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-v6mcs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:53:20.099: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb  1 06:53:25.106: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  1 06:53:27.119: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb  1 06:53:29.125: INFO: Creating deployment "test-rollover-deployment"
Feb  1 06:53:29.170: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb  1 06:53:31.179: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb  1 06:53:31.191: INFO: Ensure that both replica sets have 1 created replica
Feb  1 06:53:31.198: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb  1 06:53:31.219: INFO: Updating deployment test-rollover-deployment
Feb  1 06:53:31.219: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb  1 06:53:33.226: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb  1 06:53:33.233: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb  1 06:53:33.239: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:33.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600811, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:35.247: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:35.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600814, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:37.249: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:37.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600814, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:39.251: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:39.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600814, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:41.248: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:41.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600814, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:43.251: INFO: all replica sets need to contain the pod-template-hash label
Feb  1 06:53:43.251: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600814, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684600809, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 06:53:45.257: INFO: 
Feb  1 06:53:45.258: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  1 06:53:45.272: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-v6mcs,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v6mcs/deployments/test-rollover-deployment,UID:14b2ec8b-25ee-11e9-90b2-96000019ce9c,ResourceVersion:9127,Generation:2,CreationTimestamp:2019-02-01 06:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-01 06:53:29 +0000 UTC 2019-02-01 06:53:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-01 06:53:44 +0000 UTC 2019-02-01 06:53:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  1 06:53:45.277: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-v6mcs,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v6mcs/replicasets/test-rollover-deployment-6b7f9d6597,UID:15f1bec6-25ee-11e9-90b2-96000019ce9c,ResourceVersion:9118,Generation:2,CreationTimestamp:2019-02-01 06:53:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 14b2ec8b-25ee-11e9-90b2-96000019ce9c 0xc002315d57 0xc002315d58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  1 06:53:45.277: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb  1 06:53:45.277: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-v6mcs,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v6mcs/replicasets/test-rollover-controller,UID:0f4fbbbc-25ee-11e9-90b2-96000019ce9c,ResourceVersion:9126,Generation:2,CreationTimestamp:2019-02-01 06:53:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 14b2ec8b-25ee-11e9-90b2-96000019ce9c 0xc002315a2f 0xc002315a40}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 06:53:45.278: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-v6mcs,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v6mcs/replicasets/test-rollover-deployment-6586df867b,UID:14bad661-25ee-11e9-90b2-96000019ce9c,ResourceVersion:9086,Generation:2,CreationTimestamp:2019-02-01 06:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 14b2ec8b-25ee-11e9-90b2-96000019ce9c 0xc002315af7 0xc002315af8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 06:53:45.284: INFO: Pod "test-rollover-deployment-6b7f9d6597-6kwr6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-6kwr6,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-v6mcs,SelfLink:/api/v1/namespaces/e2e-tests-deployment-v6mcs/pods/test-rollover-deployment-6b7f9d6597-6kwr6,UID:15f4d05f-25ee-11e9-90b2-96000019ce9c,ResourceVersion:9099,Generation:0,CreationTimestamp:2019-02-01 06:53:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 15f1bec6-25ee-11e9-90b2-96000019ce9c 0xc001f688e7 0xc001f688e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-ffzxj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ffzxj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ffzxj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f68950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f68970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:53:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:53:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:53:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 06:53:31 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.4,StartTime:2019-02-01 06:53:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-01 06:53:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f455314ae86d3a516165556c768df872d991179d5a413c8a560c3b144c2ff438}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:53:45.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-v6mcs" for this suite.
Feb  1 06:53:51.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:53:51.376: INFO: namespace: e2e-tests-deployment-v6mcs, resource: bindings, ignored listing per whitelist
Feb  1 06:53:51.451: INFO: namespace e2e-tests-deployment-v6mcs deletion completed in 6.160583653s

â€¢ [SLOW TEST:31.568 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:53:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-ktbb6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  1 06:53:51.648: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:54:01.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-ktbb6" for this suite.
Feb  1 06:54:07.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:54:07.533: INFO: namespace: e2e-tests-init-container-ktbb6, resource: bindings, ignored listing per whitelist
Feb  1 06:54:07.558: INFO: namespace e2e-tests-init-container-ktbb6 deletion completed in 6.128996528s

â€¢ [SLOW TEST:16.106 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:54:07.559: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-w64h8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80
Feb  1 06:54:07.772: INFO: Pod name my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80: Found 0 pods out of 1
Feb  1 06:54:12.778: INFO: Pod name my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80: Found 1 pods out of 1
Feb  1 06:54:12.778: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80" are running
Feb  1 06:54:12.782: INFO: Pod "my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80-djmgq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:54:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:54:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:54:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-01 06:54:07 +0000 UTC Reason: Message:}])
Feb  1 06:54:12.783: INFO: Trying to dial the pod
Feb  1 06:54:17.805: INFO: Controller my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80: Got expected result from replica 1 [my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80-djmgq]: "my-hostname-basic-2bba7fb4-25ee-11e9-b51c-8292ded0de80-djmgq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:54:17.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-w64h8" for this suite.
Feb  1 06:54:23.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:54:23.934: INFO: namespace: e2e-tests-replication-controller-w64h8, resource: bindings, ignored listing per whitelist
Feb  1 06:54:23.934: INFO: namespace e2e-tests-replication-controller-w64h8 deletion completed in 6.120642808s

â€¢ [SLOW TEST:16.375 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:54:23.936: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-ksnz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 06:54:24.155: INFO: (0) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.08125ms)
Feb  1 06:54:24.161: INFO: (1) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.720085ms)
Feb  1 06:54:24.166: INFO: (2) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.019882ms)
Feb  1 06:54:24.173: INFO: (3) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.983221ms)
Feb  1 06:54:24.180: INFO: (4) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.487814ms)
Feb  1 06:54:24.185: INFO: (5) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.1588ms)
Feb  1 06:54:24.190: INFO: (6) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.939898ms)
Feb  1 06:54:24.197: INFO: (7) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.264624ms)
Feb  1 06:54:24.202: INFO: (8) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.53933ms)
Feb  1 06:54:24.208: INFO: (9) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.787274ms)
Feb  1 06:54:24.216: INFO: (10) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.543846ms)
Feb  1 06:54:24.222: INFO: (11) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.510386ms)
Feb  1 06:54:24.229: INFO: (12) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.960498ms)
Feb  1 06:54:24.235: INFO: (13) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.157783ms)
Feb  1 06:54:24.241: INFO: (14) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.923689ms)
Feb  1 06:54:24.247: INFO: (15) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.506897ms)
Feb  1 06:54:24.255: INFO: (16) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.387185ms)
Feb  1 06:54:24.261: INFO: (17) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.442367ms)
Feb  1 06:54:24.268: INFO: (18) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.489671ms)
Feb  1 06:54:24.275: INFO: (19) /api/v1/nodes/pharos-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.718557ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:54:24.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ksnz9" for this suite.
Feb  1 06:54:30.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:54:30.339: INFO: namespace: e2e-tests-proxy-ksnz9, resource: bindings, ignored listing per whitelist
Feb  1 06:54:30.436: INFO: namespace e2e-tests-proxy-ksnz9 deletion completed in 6.15603388s

â€¢ [SLOW TEST:6.501 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:54:30.438: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gxxq9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 06:54:30.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-gxxq9" to be "success or failure"
Feb  1 06:54:30.667: INFO: Pod "downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253472ms
Feb  1 06:54:32.676: INFO: Pod "downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011423567s
Feb  1 06:54:34.681: INFO: Pod "downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017021802s
STEP: Saw pod success
Feb  1 06:54:34.681: INFO: Pod "downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:54:34.686: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 06:54:34.714: INFO: Waiting for pod downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:54:34.717: INFO: Pod downwardapi-volume-395fdbd3-25ee-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:54:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gxxq9" for this suite.
Feb  1 06:54:40.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:54:40.869: INFO: namespace: e2e-tests-downward-api-gxxq9, resource: bindings, ignored listing per whitelist
Feb  1 06:54:40.873: INFO: namespace e2e-tests-downward-api-gxxq9 deletion completed in 6.147841156s

â€¢ [SLOW TEST:10.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:54:40.874: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bklx7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-bklx7
Feb  1 06:54:47.088: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-bklx7
STEP: checking the pod's current state and verifying that restartCount is present
Feb  1 06:54:47.091: INFO: Initial restart count of pod liveness-http is 0
Feb  1 06:55:05.151: INFO: Restart count of pod e2e-tests-container-probe-bklx7/liveness-http is now 1 (18.060328399s elapsed)
Feb  1 06:55:23.204: INFO: Restart count of pod e2e-tests-container-probe-bklx7/liveness-http is now 2 (36.113272888s elapsed)
Feb  1 06:55:43.269: INFO: Restart count of pod e2e-tests-container-probe-bklx7/liveness-http is now 3 (56.178454343s elapsed)
Feb  1 06:56:03.336: INFO: Restart count of pod e2e-tests-container-probe-bklx7/liveness-http is now 4 (1m16.244810022s elapsed)
Feb  1 06:57:03.520: INFO: Restart count of pod e2e-tests-container-probe-bklx7/liveness-http is now 5 (2m16.428584113s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:57:03.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bklx7" for this suite.
Feb  1 06:57:09.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:57:09.631: INFO: namespace: e2e-tests-container-probe-bklx7, resource: bindings, ignored listing per whitelist
Feb  1 06:57:09.668: INFO: namespace e2e-tests-container-probe-bklx7 deletion completed in 6.126872879s

â€¢ [SLOW TEST:148.794 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:57:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-zfw27
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:57:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-zfw27" for this suite.
Feb  1 06:57:53.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:57:54.013: INFO: namespace: e2e-tests-kubelet-test-zfw27, resource: bindings, ignored listing per whitelist
Feb  1 06:57:54.072: INFO: namespace e2e-tests-kubelet-test-zfw27 deletion completed in 38.137510479s

â€¢ [SLOW TEST:44.403 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:57:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-t6gtj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  1 06:57:54.279: INFO: Waiting up to 5m0s for pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-t6gtj" to be "success or failure"
Feb  1 06:57:54.283: INFO: Pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745543ms
Feb  1 06:57:56.289: INFO: Pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009689382s
Feb  1 06:57:58.299: INFO: Pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020320627s
Feb  1 06:58:00.305: INFO: Pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025682268s
STEP: Saw pod success
Feb  1 06:58:00.305: INFO: Pod "pod-b2bd5249-25ee-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 06:58:00.307: INFO: Trying to get logs from node pharos-worker-2 pod pod-b2bd5249-25ee-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 06:58:00.332: INFO: Waiting for pod pod-b2bd5249-25ee-11e9-b51c-8292ded0de80 to disappear
Feb  1 06:58:00.334: INFO: Pod pod-b2bd5249-25ee-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:58:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t6gtj" for this suite.
Feb  1 06:58:06.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:58:06.383: INFO: namespace: e2e-tests-emptydir-t6gtj, resource: bindings, ignored listing per whitelist
Feb  1 06:58:06.479: INFO: namespace e2e-tests-emptydir-t6gtj deletion completed in 6.140024046s

â€¢ [SLOW TEST:12.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:58:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-kmtjs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:58:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-kmtjs" for this suite.
Feb  1 06:58:56.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:58:56.784: INFO: namespace: e2e-tests-kubelet-test-kmtjs, resource: bindings, ignored listing per whitelist
Feb  1 06:58:56.887: INFO: namespace e2e-tests-kubelet-test-kmtjs deletion completed in 44.151336725s

â€¢ [SLOW TEST:50.408 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:58:56.888: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-g2r5l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb  1 06:58:57.094: INFO: Pod name pod-release: Found 0 pods out of 1
Feb  1 06:59:02.099: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 06:59:03.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-g2r5l" for this suite.
Feb  1 06:59:09.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 06:59:09.174: INFO: namespace: e2e-tests-replication-controller-g2r5l, resource: bindings, ignored listing per whitelist
Feb  1 06:59:09.256: INFO: namespace e2e-tests-replication-controller-g2r5l deletion completed in 6.122905976s

â€¢ [SLOW TEST:12.368 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 06:59:09.257: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-wjvxh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wjvxh
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb  1 06:59:09.493: INFO: Found 0 stateful pods, waiting for 3
Feb  1 06:59:19.498: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 06:59:19.498: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 06:59:19.498: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb  1 06:59:29.503: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 06:59:29.503: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 06:59:29.503: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  1 06:59:29.536: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb  1 06:59:39.578: INFO: Updating stateful set ss2
Feb  1 06:59:39.593: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 06:59:49.612: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb  1 06:59:59.643: INFO: Found 2 stateful pods, waiting for 3
Feb  1 07:00:09.653: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:00:09.654: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:00:09.654: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb  1 07:00:09.687: INFO: Updating stateful set ss2
Feb  1 07:00:09.697: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 07:00:19.712: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 07:00:29.736: INFO: Updating stateful set ss2
Feb  1 07:00:29.745: INFO: Waiting for StatefulSet e2e-tests-statefulset-wjvxh/ss2 to complete update
Feb  1 07:00:29.746: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 07:00:39.758: INFO: Waiting for StatefulSet e2e-tests-statefulset-wjvxh/ss2 to complete update
Feb  1 07:00:39.758: INFO: Waiting for Pod e2e-tests-statefulset-wjvxh/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  1 07:00:49.760: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wjvxh
Feb  1 07:00:49.763: INFO: Scaling statefulset ss2 to 0
Feb  1 07:01:19.783: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:01:19.787: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:01:19.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wjvxh" for this suite.
Feb  1 07:01:25.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:01:25.842: INFO: namespace: e2e-tests-statefulset-wjvxh, resource: bindings, ignored listing per whitelist
Feb  1 07:01:25.972: INFO: namespace e2e-tests-statefulset-wjvxh deletion completed in 6.164458632s

â€¢ [SLOW TEST:136.715 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:01:25.974: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-xk96b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  1 07:01:26.182: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  1 07:01:26.191: INFO: Waiting for terminating namespaces to be deleted...
Feb  1 07:01:26.194: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-0 before test
Feb  1 07:01:26.208: INFO: pharos-proxy-pharos-worker-0 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:01:26.208: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-01 06:19:57 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  1 07:01:26.208: INFO: weave-net-lm92h from kube-system started at 2019-02-01 06:08:43 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:01:26.208: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:01:26.208: INFO: pharos-telemetry-1549004400-mqq2t from kube-system started at 2019-02-01 07:00:00 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container agent ready: false, restart count 0
Feb  1 07:01:26.208: INFO: kube-proxy-xnlj8 from kube-system started at 2019-02-01 06:08:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:01:26.208: INFO: nginx-ingress-controller-mcdm5 from ingress-nginx started at 2019-02-01 06:09:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:01:26.208: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-w94p8 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.208: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  1 07:01:26.208: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:01:26.208: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-1 before test
Feb  1 07:01:26.223: INFO: nginx-ingress-controller-ptwhf from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:01:26.223: INFO: sonobuoy-e2e-job-52af506353674b35 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container e2e ready: true, restart count 0
Feb  1 07:01:26.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:01:26.223: INFO: pharos-proxy-pharos-worker-1 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:01:26.223: INFO: weave-net-b6kjz from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container weave ready: true, restart count 0
Feb  1 07:01:26.223: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:01:26.223: INFO: default-http-backend-6cf7bc5976-g6fn7 from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:01:26.223: INFO: coredns-5567f4d9b9-4h2rl from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container coredns ready: true, restart count 0
Feb  1 07:01:26.223: INFO: kube-proxy-xrxnk from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:01:26.223: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-7gmgs from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  1 07:01:26.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:01:26.223: INFO: metrics-server-8549f8c7bf-frntn from kube-system started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.223: INFO: 	Container metrics-server ready: true, restart count 0
Feb  1 07:01:26.223: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-2 before test
Feb  1 07:01:26.235: INFO: default-http-backend-6cf7bc5976-kvnrt from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.235: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:01:26.235: INFO: pharos-proxy-pharos-worker-2 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:01:26.235: INFO: weave-net-crgqw from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.235: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:01:26.235: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:01:26.235: INFO: nginx-ingress-controller-2p9l6 from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.235: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:01:26.235: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-nxrpt from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:01:26.235: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Feb  1 07:01:26.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:01:26.235: INFO: kube-proxy-xnpc2 from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:01:26.235: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node pharos-worker-0
STEP: verifying the node has the label node pharos-worker-1
STEP: verifying the node has the label node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod sonobuoy requesting resource cpu=0m on Node pharos-worker-0
Feb  1 07:01:26.289: INFO: Pod sonobuoy-e2e-job-52af506353674b35 requesting resource cpu=0m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff006445a7294224-7gmgs requesting resource cpu=0m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff006445a7294224-nxrpt requesting resource cpu=0m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff006445a7294224-w94p8 requesting resource cpu=0m on Node pharos-worker-0
Feb  1 07:01:26.289: INFO: Pod default-http-backend-6cf7bc5976-g6fn7 requesting resource cpu=10m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod default-http-backend-6cf7bc5976-kvnrt requesting resource cpu=10m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod nginx-ingress-controller-2p9l6 requesting resource cpu=0m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod nginx-ingress-controller-mcdm5 requesting resource cpu=0m on Node pharos-worker-0
Feb  1 07:01:26.289: INFO: Pod nginx-ingress-controller-ptwhf requesting resource cpu=0m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod coredns-5567f4d9b9-4h2rl requesting resource cpu=100m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod kube-proxy-xnlj8 requesting resource cpu=0m on Node pharos-worker-0
Feb  1 07:01:26.289: INFO: Pod kube-proxy-xnpc2 requesting resource cpu=0m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod kube-proxy-xrxnk requesting resource cpu=0m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod metrics-server-8549f8c7bf-frntn requesting resource cpu=10m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod pharos-proxy-pharos-worker-0 requesting resource cpu=0m on Node pharos-worker-0
Feb  1 07:01:26.289: INFO: Pod pharos-proxy-pharos-worker-1 requesting resource cpu=0m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod pharos-proxy-pharos-worker-2 requesting resource cpu=0m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod weave-net-b6kjz requesting resource cpu=20m on Node pharos-worker-1
Feb  1 07:01:26.289: INFO: Pod weave-net-crgqw requesting resource cpu=20m on Node pharos-worker-2
Feb  1 07:01:26.289: INFO: Pod weave-net-lm92h requesting resource cpu=20m on Node pharos-worker-0
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80.157f2b5f915e4d73], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-xk96b/filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80 to pharos-worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80.157f2b6005f69ed1], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80.157f2b603fee67e5], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80.157f2b6047485d01], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311cc69a-25ef-11e9-b51c-8292ded0de80.157f2b605947ae6e], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80.157f2b5f918e96a8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-xk96b/filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80 to pharos-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80.157f2b5fddb21736], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80.157f2b601b2984c4], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80.157f2b601f9f2c5e], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311e59d0-25ef-11e9-b51c-8292ded0de80.157f2b602cd61917], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80.157f2b5f91fe6f11], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-xk96b/filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80 to pharos-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80.157f2b5fe87b8061], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80.157f2b6020bb5478], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80.157f2b60257f7730], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-311f52fc-25ef-11e9-b51c-8292ded0de80.157f2b6034842c13], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.157f2b608188e107], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node pharos-worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node pharos-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node pharos-worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:01:31.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-xk96b" for this suite.
Feb  1 07:01:37.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:01:37.428: INFO: namespace: e2e-tests-sched-pred-xk96b, resource: bindings, ignored listing per whitelist
Feb  1 07:01:37.513: INFO: namespace e2e-tests-sched-pred-xk96b deletion completed in 6.127927459s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:11.540 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:01:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-z444j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:01:37.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-z444j" to be "success or failure"
Feb  1 07:01:37.738: INFO: Pod "downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.847984ms
Feb  1 07:01:39.746: INFO: Pod "downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010844831s
Feb  1 07:01:41.751: INFO: Pod "downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016144899s
STEP: Saw pod success
Feb  1 07:01:41.751: INFO: Pod "downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:01:41.755: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:01:41.786: INFO: Waiting for pod downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:01:41.789: INFO: Pod downwardapi-volume-37edc9f3-25ef-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:01:41.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z444j" for this suite.
Feb  1 07:01:47.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:01:47.866: INFO: namespace: e2e-tests-projected-z444j, resource: bindings, ignored listing per whitelist
Feb  1 07:01:47.911: INFO: namespace e2e-tests-projected-z444j deletion completed in 6.116779492s

â€¢ [SLOW TEST:10.398 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:01:47.913: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-llkcm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:01:48.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-llkcm" to be "success or failure"
Feb  1 07:01:48.151: INFO: Pod "downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.144728ms
Feb  1 07:01:50.156: INFO: Pod "downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007798557s
Feb  1 07:01:52.166: INFO: Pod "downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017868235s
STEP: Saw pod success
Feb  1 07:01:52.167: INFO: Pod "downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:01:52.172: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:01:52.204: INFO: Waiting for pod downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:01:52.207: INFO: Pod downwardapi-volume-3e223f15-25ef-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:01:52.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-llkcm" for this suite.
Feb  1 07:01:58.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:01:58.282: INFO: namespace: e2e-tests-projected-llkcm, resource: bindings, ignored listing per whitelist
Feb  1 07:01:58.356: INFO: namespace e2e-tests-projected-llkcm deletion completed in 6.141524022s

â€¢ [SLOW TEST:10.443 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:01:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-qt9bl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-qt9bl
Feb  1 07:02:02.587: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-qt9bl
STEP: checking the pod's current state and verifying that restartCount is present
Feb  1 07:02:02.596: INFO: Initial restart count of pod liveness-http is 0
Feb  1 07:02:22.652: INFO: Restart count of pod e2e-tests-container-probe-qt9bl/liveness-http is now 1 (20.056134693s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:02:22.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qt9bl" for this suite.
Feb  1 07:02:28.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:02:28.727: INFO: namespace: e2e-tests-container-probe-qt9bl, resource: bindings, ignored listing per whitelist
Feb  1 07:02:28.830: INFO: namespace e2e-tests-container-probe-qt9bl deletion completed in 6.156794777s

â€¢ [SLOW TEST:30.474 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:02:28.832: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t8hhm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-56866289-25ef-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:02:29.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-t8hhm" to be "success or failure"
Feb  1 07:02:29.074: INFO: Pod "pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154289ms
Feb  1 07:02:31.079: INFO: Pod "pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009153812s
Feb  1 07:02:33.090: INFO: Pod "pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020236279s
STEP: Saw pod success
Feb  1 07:02:33.090: INFO: Pod "pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:02:33.093: INFO: Trying to get logs from node pharos-worker-2 pod pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:02:33.117: INFO: Waiting for pod pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:02:33.121: INFO: Pod pod-projected-configmaps-5687353c-25ef-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:02:33.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t8hhm" for this suite.
Feb  1 07:02:39.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:02:39.203: INFO: namespace: e2e-tests-projected-t8hhm, resource: bindings, ignored listing per whitelist
Feb  1 07:02:39.253: INFO: namespace e2e-tests-projected-t8hhm deletion completed in 6.128080592s

â€¢ [SLOW TEST:10.421 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:02:39.254: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4mb4n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  1 07:02:39.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:39.967: INFO: stderr: ""
Feb  1 07:02:39.967: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 07:02:39.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:40.062: INFO: stderr: ""
Feb  1 07:02:40.062: INFO: stdout: "update-demo-nautilus-m4pqg update-demo-nautilus-v5vsr "
Feb  1 07:02:40.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:40.156: INFO: stderr: ""
Feb  1 07:02:40.156: INFO: stdout: ""
Feb  1 07:02:40.156: INFO: update-demo-nautilus-m4pqg is created but not running
Feb  1 07:02:45.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:45.256: INFO: stderr: ""
Feb  1 07:02:45.256: INFO: stdout: "update-demo-nautilus-m4pqg update-demo-nautilus-v5vsr "
Feb  1 07:02:45.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:45.346: INFO: stderr: ""
Feb  1 07:02:45.346: INFO: stdout: "true"
Feb  1 07:02:45.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:45.428: INFO: stderr: ""
Feb  1 07:02:45.428: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:02:45.428: INFO: validating pod update-demo-nautilus-m4pqg
Feb  1 07:02:45.459: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:02:45.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:02:45.459: INFO: update-demo-nautilus-m4pqg is verified up and running
Feb  1 07:02:45.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-v5vsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:45.564: INFO: stderr: ""
Feb  1 07:02:45.565: INFO: stdout: "true"
Feb  1 07:02:45.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-v5vsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:45.649: INFO: stderr: ""
Feb  1 07:02:45.649: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:02:45.649: INFO: validating pod update-demo-nautilus-v5vsr
Feb  1 07:02:45.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:02:45.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:02:45.661: INFO: update-demo-nautilus-v5vsr is verified up and running
STEP: scaling down the replication controller
Feb  1 07:02:45.666: INFO: scanned /root for discovery docs: <nil>
Feb  1 07:02:45.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:46.782: INFO: stderr: ""
Feb  1 07:02:46.782: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 07:02:46.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:46.878: INFO: stderr: ""
Feb  1 07:02:46.878: INFO: stdout: "update-demo-nautilus-m4pqg update-demo-nautilus-v5vsr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  1 07:02:51.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:51.983: INFO: stderr: ""
Feb  1 07:02:51.983: INFO: stdout: "update-demo-nautilus-m4pqg "
Feb  1 07:02:51.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:52.067: INFO: stderr: ""
Feb  1 07:02:52.067: INFO: stdout: "true"
Feb  1 07:02:52.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:52.147: INFO: stderr: ""
Feb  1 07:02:52.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:02:52.147: INFO: validating pod update-demo-nautilus-m4pqg
Feb  1 07:02:52.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:02:52.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:02:52.156: INFO: update-demo-nautilus-m4pqg is verified up and running
STEP: scaling up the replication controller
Feb  1 07:02:52.159: INFO: scanned /root for discovery docs: <nil>
Feb  1 07:02:52.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:53.431: INFO: stderr: ""
Feb  1 07:02:53.431: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 07:02:53.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:53.512: INFO: stderr: ""
Feb  1 07:02:53.512: INFO: stdout: "update-demo-nautilus-8fq2f update-demo-nautilus-m4pqg "
Feb  1 07:02:53.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-8fq2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:53.611: INFO: stderr: ""
Feb  1 07:02:53.611: INFO: stdout: ""
Feb  1 07:02:53.611: INFO: update-demo-nautilus-8fq2f is created but not running
Feb  1 07:02:58.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:58.712: INFO: stderr: ""
Feb  1 07:02:58.712: INFO: stdout: "update-demo-nautilus-8fq2f update-demo-nautilus-m4pqg "
Feb  1 07:02:58.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-8fq2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:58.801: INFO: stderr: ""
Feb  1 07:02:58.801: INFO: stdout: "true"
Feb  1 07:02:58.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-8fq2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:58.910: INFO: stderr: ""
Feb  1 07:02:58.910: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:02:58.910: INFO: validating pod update-demo-nautilus-8fq2f
Feb  1 07:02:58.925: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:02:58.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:02:58.925: INFO: update-demo-nautilus-8fq2f is verified up and running
Feb  1 07:02:58.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:59.027: INFO: stderr: ""
Feb  1 07:02:59.027: INFO: stdout: "true"
Feb  1 07:02:59.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-m4pqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:59.106: INFO: stderr: ""
Feb  1 07:02:59.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:02:59.106: INFO: validating pod update-demo-nautilus-m4pqg
Feb  1 07:02:59.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:02:59.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:02:59.112: INFO: update-demo-nautilus-m4pqg is verified up and running
STEP: using delete to clean up resources
Feb  1 07:02:59.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:59.205: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 07:02:59.205: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  1 07:02:59.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-4mb4n'
Feb  1 07:02:59.302: INFO: stderr: "No resources found.\n"
Feb  1 07:02:59.302: INFO: stdout: ""
Feb  1 07:02:59.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -l name=update-demo --namespace=e2e-tests-kubectl-4mb4n -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  1 07:02:59.412: INFO: stderr: ""
Feb  1 07:02:59.412: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:02:59.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4mb4n" for this suite.
Feb  1 07:03:21.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:03:21.506: INFO: namespace: e2e-tests-kubectl-4mb4n, resource: bindings, ignored listing per whitelist
Feb  1 07:03:21.536: INFO: namespace e2e-tests-kubectl-4mb4n deletion completed in 22.116076116s

â€¢ [SLOW TEST:42.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:03:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5tzkq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb  1 07:03:21.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-5tzkq'
Feb  1 07:03:21.979: INFO: stderr: ""
Feb  1 07:03:21.980: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb  1 07:03:22.988: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:03:22.988: INFO: Found 0 / 1
Feb  1 07:03:23.986: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:03:23.986: INFO: Found 0 / 1
Feb  1 07:03:24.985: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:03:24.985: INFO: Found 0 / 1
Feb  1 07:03:25.990: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:03:25.990: INFO: Found 1 / 1
Feb  1 07:03:25.990: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  1 07:03:25.994: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:03:25.994: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb  1 07:03:25.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 logs redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq'
Feb  1 07:03:26.099: INFO: stderr: ""
Feb  1 07:03:26.100: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Feb 07:03:24.641 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Feb 07:03:24.641 # Server started, Redis version 3.2.12\n1:M 01 Feb 07:03:24.642 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Feb 07:03:24.642 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb  1 07:03:26.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 log redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq --tail=1'
Feb  1 07:03:26.206: INFO: stderr: ""
Feb  1 07:03:26.206: INFO: stdout: "1:M 01 Feb 07:03:24.642 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb  1 07:03:26.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 log redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq --limit-bytes=1'
Feb  1 07:03:26.340: INFO: stderr: ""
Feb  1 07:03:26.340: INFO: stdout: " "
STEP: exposing timestamps
Feb  1 07:03:26.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 log redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq --tail=1 --timestamps'
Feb  1 07:03:26.448: INFO: stderr: ""
Feb  1 07:03:26.448: INFO: stdout: "2019-02-01T07:03:24.642621836Z 1:M 01 Feb 07:03:24.642 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb  1 07:03:28.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 log redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq --since=1s'
Feb  1 07:03:29.060: INFO: stderr: ""
Feb  1 07:03:29.060: INFO: stdout: ""
Feb  1 07:03:29.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 log redis-master-s2l4w redis-master --namespace=e2e-tests-kubectl-5tzkq --since=24h'
Feb  1 07:03:29.181: INFO: stderr: ""
Feb  1 07:03:29.181: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Feb 07:03:24.641 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Feb 07:03:24.641 # Server started, Redis version 3.2.12\n1:M 01 Feb 07:03:24.642 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Feb 07:03:24.642 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb  1 07:03:29.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5tzkq'
Feb  1 07:03:29.290: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 07:03:29.290: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb  1 07:03:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-5tzkq'
Feb  1 07:03:29.378: INFO: stderr: "No resources found.\n"
Feb  1 07:03:29.379: INFO: stdout: ""
Feb  1 07:03:29.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -l name=nginx --namespace=e2e-tests-kubectl-5tzkq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  1 07:03:29.468: INFO: stderr: ""
Feb  1 07:03:29.468: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:03:29.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5tzkq" for this suite.
Feb  1 07:03:45.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:03:45.554: INFO: namespace: e2e-tests-kubectl-5tzkq, resource: bindings, ignored listing per whitelist
Feb  1 07:03:45.624: INFO: namespace e2e-tests-kubectl-5tzkq deletion completed in 16.149366266s

â€¢ [SLOW TEST:24.088 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:03:45.626: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sdhnn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb  1 07:03:45.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:46.120: INFO: stderr: ""
Feb  1 07:03:46.120: INFO: stdout: "pod/pause created\n"
Feb  1 07:03:46.120: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb  1 07:03:46.120: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-sdhnn" to be "running and ready"
Feb  1 07:03:46.127: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200058ms
Feb  1 07:03:48.132: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011800145s
Feb  1 07:03:50.138: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.017295175s
Feb  1 07:03:50.138: INFO: Pod "pause" satisfied condition "running and ready"
Feb  1 07:03:50.138: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb  1 07:03:50.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.249: INFO: stderr: ""
Feb  1 07:03:50.249: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb  1 07:03:50.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pod pause -L testing-label --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.340: INFO: stderr: ""
Feb  1 07:03:50.340: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb  1 07:03:50.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 label pods pause testing-label- --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.451: INFO: stderr: ""
Feb  1 07:03:50.451: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb  1 07:03:50.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pod pause -L testing-label --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.554: INFO: stderr: ""
Feb  1 07:03:50.554: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb  1 07:03:50.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.645: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 07:03:50.645: INFO: stdout: "pod \"pause\" force deleted\n"
Feb  1 07:03:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-sdhnn'
Feb  1 07:03:50.728: INFO: stderr: "No resources found.\n"
Feb  1 07:03:50.728: INFO: stdout: ""
Feb  1 07:03:50.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -l name=pause --namespace=e2e-tests-kubectl-sdhnn -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  1 07:03:50.826: INFO: stderr: ""
Feb  1 07:03:50.826: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:03:50.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sdhnn" for this suite.
Feb  1 07:03:56.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:03:56.881: INFO: namespace: e2e-tests-kubectl-sdhnn, resource: bindings, ignored listing per whitelist
Feb  1 07:03:56.984: INFO: namespace e2e-tests-kubectl-sdhnn deletion completed in 6.151543894s

â€¢ [SLOW TEST:11.358 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:03:56.985: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-l5lns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb  1 07:03:57.721: INFO: created pod pod-service-account-defaultsa
Feb  1 07:03:57.721: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb  1 07:03:57.767: INFO: created pod pod-service-account-mountsa
Feb  1 07:03:57.768: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb  1 07:03:57.773: INFO: created pod pod-service-account-nomountsa
Feb  1 07:03:57.773: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb  1 07:03:57.779: INFO: created pod pod-service-account-defaultsa-mountspec
Feb  1 07:03:57.780: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb  1 07:03:57.786: INFO: created pod pod-service-account-mountsa-mountspec
Feb  1 07:03:57.786: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb  1 07:03:57.795: INFO: created pod pod-service-account-nomountsa-mountspec
Feb  1 07:03:57.795: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb  1 07:03:57.804: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb  1 07:03:57.804: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb  1 07:03:57.826: INFO: created pod pod-service-account-mountsa-nomountspec
Feb  1 07:03:57.826: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb  1 07:03:57.836: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb  1 07:03:57.836: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:03:57.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-l5lns" for this suite.
Feb  1 07:04:19.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:04:20.020: INFO: namespace: e2e-tests-svcaccounts-l5lns, resource: bindings, ignored listing per whitelist
Feb  1 07:04:20.091: INFO: namespace e2e-tests-svcaccounts-l5lns deletion completed in 22.187036014s

â€¢ [SLOW TEST:23.106 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:04:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-v4p9v
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb  1 07:04:20.305: INFO: Waiting up to 5m0s for pod "pod-98d46062-25ef-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-v4p9v" to be "success or failure"
Feb  1 07:04:20.308: INFO: Pod "pod-98d46062-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.096038ms
Feb  1 07:04:22.313: INFO: Pod "pod-98d46062-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007944601s
Feb  1 07:04:24.319: INFO: Pod "pod-98d46062-25ef-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013843275s
STEP: Saw pod success
Feb  1 07:04:24.319: INFO: Pod "pod-98d46062-25ef-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:04:24.322: INFO: Trying to get logs from node pharos-worker-0 pod pod-98d46062-25ef-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:04:24.362: INFO: Waiting for pod pod-98d46062-25ef-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:04:24.364: INFO: Pod pod-98d46062-25ef-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:04:24.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-v4p9v" for this suite.
Feb  1 07:04:30.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:04:30.467: INFO: namespace: e2e-tests-emptydir-v4p9v, resource: bindings, ignored listing per whitelist
Feb  1 07:04:30.499: INFO: namespace e2e-tests-emptydir-v4p9v deletion completed in 6.129904144s

â€¢ [SLOW TEST:10.408 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:04:30.499: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pfsv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-9f07f117-25ef-11e9-b51c-8292ded0de80
STEP: Creating secret with name s-test-opt-upd-9f07f1d5-25ef-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9f07f117-25ef-11e9-b51c-8292ded0de80
STEP: Updating secret s-test-opt-upd-9f07f1d5-25ef-11e9-b51c-8292ded0de80
STEP: Creating secret with name s-test-opt-create-9f07f240-25ef-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:06:01.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pfsv6" for this suite.
Feb  1 07:06:23.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:06:23.494: INFO: namespace: e2e-tests-projected-pfsv6, resource: bindings, ignored listing per whitelist
Feb  1 07:06:23.535: INFO: namespace e2e-tests-projected-pfsv6 deletion completed in 22.135515589s

â€¢ [SLOW TEST:113.036 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:06:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-xh299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:06:23.779: INFO: (0) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.594939ms)
Feb  1 07:06:23.789: INFO: (1) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.827051ms)
Feb  1 07:06:23.794: INFO: (2) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.321549ms)
Feb  1 07:06:23.800: INFO: (3) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.991033ms)
Feb  1 07:06:23.807: INFO: (4) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.07491ms)
Feb  1 07:06:23.813: INFO: (5) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.489044ms)
Feb  1 07:06:23.819: INFO: (6) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.160025ms)
Feb  1 07:06:23.825: INFO: (7) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.108196ms)
Feb  1 07:06:23.831: INFO: (8) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.470511ms)
Feb  1 07:06:23.836: INFO: (9) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.83864ms)
Feb  1 07:06:23.858: INFO: (10) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.884738ms)
Feb  1 07:06:23.867: INFO: (11) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.075336ms)
Feb  1 07:06:23.873: INFO: (12) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.988694ms)
Feb  1 07:06:23.880: INFO: (13) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.036872ms)
Feb  1 07:06:23.886: INFO: (14) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.016726ms)
Feb  1 07:06:23.893: INFO: (15) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.707639ms)
Feb  1 07:06:23.898: INFO: (16) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.643813ms)
Feb  1 07:06:23.905: INFO: (17) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.823586ms)
Feb  1 07:06:23.912: INFO: (18) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.223683ms)
Feb  1 07:06:23.918: INFO: (19) /api/v1/nodes/pharos-worker-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.880684ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:06:23.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-xh299" for this suite.
Feb  1 07:06:29.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:06:29.965: INFO: namespace: e2e-tests-proxy-xh299, resource: bindings, ignored listing per whitelist
Feb  1 07:06:30.089: INFO: namespace e2e-tests-proxy-xh299 deletion completed in 6.165328329s

â€¢ [SLOW TEST:6.553 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:06:30.090: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hmjlz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-e6501404-25ef-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:06:30.306: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-hmjlz" to be "success or failure"
Feb  1 07:06:30.309: INFO: Pod "pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793261ms
Feb  1 07:06:32.314: INFO: Pod "pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007620728s
Feb  1 07:06:34.323: INFO: Pod "pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016962868s
STEP: Saw pod success
Feb  1 07:06:34.323: INFO: Pod "pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:06:34.328: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:06:34.369: INFO: Waiting for pod pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:06:34.372: INFO: Pod pod-projected-secrets-e650d004-25ef-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:06:34.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hmjlz" for this suite.
Feb  1 07:06:40.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:06:40.439: INFO: namespace: e2e-tests-projected-hmjlz, resource: bindings, ignored listing per whitelist
Feb  1 07:06:40.520: INFO: namespace e2e-tests-projected-hmjlz deletion completed in 6.142585547s

â€¢ [SLOW TEST:10.430 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:06:40.521: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9767q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 07:06:40.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:40.860: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  1 07:06:40.860: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Feb  1 07:06:40.871: INFO: scanned /root for discovery docs: <nil>
Feb  1 07:06:40.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:57.715: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb  1 07:06:57.716: INFO: stdout: "Created e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe\nScaling up e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb  1 07:06:57.716: INFO: stdout: "Created e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe\nScaling up e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb  1 07:06:57.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:57.828: INFO: stderr: ""
Feb  1 07:06:57.828: INFO: stdout: "e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe-j4qz8 "
Feb  1 07:06:57.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe-j4qz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:57.921: INFO: stderr: ""
Feb  1 07:06:57.921: INFO: stdout: "true"
Feb  1 07:06:57.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe-j4qz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:58.015: INFO: stderr: ""
Feb  1 07:06:58.015: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb  1 07:06:58.016: INFO: e2e-test-nginx-rc-93e8d4e0bebab2e725e0c24901f339fe-j4qz8 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb  1 07:06:58.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9767q'
Feb  1 07:06:58.115: INFO: stderr: ""
Feb  1 07:06:58.115: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:06:58.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9767q" for this suite.
Feb  1 07:07:04.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:07:04.179: INFO: namespace: e2e-tests-kubectl-9767q, resource: bindings, ignored listing per whitelist
Feb  1 07:07:04.265: INFO: namespace e2e-tests-kubectl-9767q deletion completed in 6.14114439s

â€¢ [SLOW TEST:23.744 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:07:04.266: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-nvwb9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb  1 07:07:08.486: INFO: Pod pod-hostip-faae0495-25ef-11e9-b51c-8292ded0de80 has hostIP: 95.216.222.53
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:07:08.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nvwb9" for this suite.
Feb  1 07:07:30.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:07:30.608: INFO: namespace: e2e-tests-pods-nvwb9, resource: bindings, ignored listing per whitelist
Feb  1 07:07:30.636: INFO: namespace e2e-tests-pods-nvwb9 deletion completed in 22.137741793s

â€¢ [SLOW TEST:26.371 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:07:30.637: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-gv2kj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-5d2k
STEP: Creating a pod to test atomic-volume-subpath
Feb  1 07:07:30.869: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-5d2k" in namespace "e2e-tests-subpath-gv2kj" to be "success or failure"
Feb  1 07:07:30.872: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.861393ms
Feb  1 07:07:32.876: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007236747s
Feb  1 07:07:34.883: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013665354s
Feb  1 07:07:36.888: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01902286s
Feb  1 07:07:38.893: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 8.024120662s
Feb  1 07:07:40.897: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 10.028289778s
Feb  1 07:07:42.903: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 12.033701227s
Feb  1 07:07:44.909: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 14.040095041s
Feb  1 07:07:46.914: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 16.045099166s
Feb  1 07:07:48.921: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 18.051752603s
Feb  1 07:07:50.927: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 20.058018432s
Feb  1 07:07:52.933: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 22.06387384s
Feb  1 07:07:54.939: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 24.070114243s
Feb  1 07:07:56.946: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Running", Reason="", readiness=false. Elapsed: 26.076589493s
Feb  1 07:07:58.952: INFO: Pod "pod-subpath-test-downwardapi-5d2k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.082844013s
STEP: Saw pod success
Feb  1 07:07:58.952: INFO: Pod "pod-subpath-test-downwardapi-5d2k" satisfied condition "success or failure"
Feb  1 07:07:58.956: INFO: Trying to get logs from node pharos-worker-2 pod pod-subpath-test-downwardapi-5d2k container test-container-subpath-downwardapi-5d2k: <nil>
STEP: delete the pod
Feb  1 07:07:58.998: INFO: Waiting for pod pod-subpath-test-downwardapi-5d2k to disappear
Feb  1 07:07:59.001: INFO: Pod pod-subpath-test-downwardapi-5d2k no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-5d2k
Feb  1 07:07:59.001: INFO: Deleting pod "pod-subpath-test-downwardapi-5d2k" in namespace "e2e-tests-subpath-gv2kj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:07:59.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-gv2kj" for this suite.
Feb  1 07:08:05.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:08:05.109: INFO: namespace: e2e-tests-subpath-gv2kj, resource: bindings, ignored listing per whitelist
Feb  1 07:08:05.155: INFO: namespace e2e-tests-subpath-gv2kj deletion completed in 6.145511802s

â€¢ [SLOW TEST:34.519 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:08:05.158: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hf9xc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  1 07:08:05.685: INFO: Waiting up to 5m0s for pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-hf9xc" to be "success or failure"
Feb  1 07:08:05.689: INFO: Pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.98743ms
Feb  1 07:08:07.697: INFO: Pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011550346s
Feb  1 07:08:09.702: INFO: Pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016586204s
Feb  1 07:08:11.708: INFO: Pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022583018s
STEP: Saw pod success
Feb  1 07:08:11.708: INFO: Pod "downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:08:11.712: INFO: Trying to get logs from node pharos-worker-2 pod downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 07:08:11.737: INFO: Waiting for pod downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:08:11.741: INFO: Pod downward-api-1f29e7cd-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:08:11.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hf9xc" for this suite.
Feb  1 07:08:17.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:08:17.807: INFO: namespace: e2e-tests-downward-api-hf9xc, resource: bindings, ignored listing per whitelist
Feb  1 07:08:17.908: INFO: namespace e2e-tests-downward-api-hf9xc deletion completed in 6.161730298s

â€¢ [SLOW TEST:12.751 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:08:17.909: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-llmpf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-llmpf
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-llmpf
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-llmpf
Feb  1 07:08:18.111: INFO: Found 0 stateful pods, waiting for 1
Feb  1 07:08:28.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb  1 07:08:28.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:08:28.506: INFO: stderr: ""
Feb  1 07:08:28.506: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:08:28.507: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:08:28.511: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  1 07:08:38.518: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:08:38.518: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:08:38.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999994s
Feb  1 07:08:39.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995771016s
Feb  1 07:08:40.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988560508s
Feb  1 07:08:41.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981713507s
Feb  1 07:08:42.564: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974447596s
Feb  1 07:08:43.569: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.968822777s
Feb  1 07:08:44.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963704881s
Feb  1 07:08:45.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.957429507s
Feb  1 07:08:46.587: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95134246s
Feb  1 07:08:47.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 945.138826ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-llmpf
Feb  1 07:08:48.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:08:48.969: INFO: stderr: ""
Feb  1 07:08:48.969: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:08:48.969: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:08:48.974: INFO: Found 1 stateful pods, waiting for 3
Feb  1 07:08:58.986: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:08:58.986: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:08:58.986: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=false
Feb  1 07:09:08.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:09:08.982: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:09:08.982: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb  1 07:09:08.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:09:09.358: INFO: stderr: ""
Feb  1 07:09:09.358: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:09:09.358: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:09:09.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:09:09.597: INFO: stderr: ""
Feb  1 07:09:09.598: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:09:09.598: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:09:09.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:09:09.874: INFO: stderr: ""
Feb  1 07:09:09.874: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:09:09.874: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:09:09.874: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:09:09.878: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb  1 07:09:19.889: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:09:19.889: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:09:19.889: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:09:19.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999318s
Feb  1 07:09:20.913: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995185188s
Feb  1 07:09:21.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989220899s
Feb  1 07:09:22.924: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98329871s
Feb  1 07:09:23.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977702093s
Feb  1 07:09:24.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972055741s
Feb  1 07:09:25.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966545987s
Feb  1 07:09:26.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95780046s
Feb  1 07:09:27.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950545806s
Feb  1 07:09:28.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.762055ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-llmpf
Feb  1 07:09:29.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:09:30.347: INFO: stderr: ""
Feb  1 07:09:30.347: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:09:30.347: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:09:30.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:09:30.582: INFO: stderr: ""
Feb  1 07:09:30.582: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:09:30.582: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:09:30.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-llmpf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:09:30.840: INFO: stderr: ""
Feb  1 07:09:30.840: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:09:30.840: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:09:30.840: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  1 07:10:10.862: INFO: Deleting all statefulset in ns e2e-tests-statefulset-llmpf
Feb  1 07:10:10.867: INFO: Scaling statefulset ss to 0
Feb  1 07:10:10.880: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:10:10.883: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:10:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-llmpf" for this suite.
Feb  1 07:10:16.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:10:16.938: INFO: namespace: e2e-tests-statefulset-llmpf, resource: bindings, ignored listing per whitelist
Feb  1 07:10:17.031: INFO: namespace e2e-tests-statefulset-llmpf deletion completed in 6.128999313s

â€¢ [SLOW TEST:119.123 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:10:17.032: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-7dp9n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  1 07:10:17.226: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:10:27.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-7dp9n" for this suite.
Feb  1 07:10:33.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:10:33.890: INFO: namespace: e2e-tests-init-container-7dp9n, resource: bindings, ignored listing per whitelist
Feb  1 07:10:33.972: INFO: namespace e2e-tests-init-container-7dp9n deletion completed in 6.13341788s

â€¢ [SLOW TEST:16.940 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:10:33.973: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-b88p9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-77ac2346-25f0-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:10:34.178: INFO: Waiting up to 5m0s for pod "pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-b88p9" to be "success or failure"
Feb  1 07:10:34.181: INFO: Pod "pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85981ms
Feb  1 07:10:36.185: INFO: Pod "pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007266597s
Feb  1 07:10:38.195: INFO: Pod "pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017132372s
STEP: Saw pod success
Feb  1 07:10:38.195: INFO: Pod "pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:10:38.200: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:10:38.232: INFO: Waiting for pod pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:10:38.236: INFO: Pod pod-configmaps-77acc7eb-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:10:38.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b88p9" for this suite.
Feb  1 07:10:44.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:10:44.311: INFO: namespace: e2e-tests-configmap-b88p9, resource: bindings, ignored listing per whitelist
Feb  1 07:10:44.394: INFO: namespace e2e-tests-configmap-b88p9 deletion completed in 6.152291616s

â€¢ [SLOW TEST:10.421 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:10:44.395: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-zdkfn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:10:44.610: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7de3506d-25f0-11e9-90b2-96000019ce9c", Controller:(*bool)(0xc002505d96), BlockOwnerDeletion:(*bool)(0xc002505d97)}}
Feb  1 07:10:44.613: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7de22c2b-25f0-11e9-90b2-96000019ce9c", Controller:(*bool)(0xc0006be05e), BlockOwnerDeletion:(*bool)(0xc0006be05f)}}
Feb  1 07:10:44.619: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7de2c464-25f0-11e9-90b2-96000019ce9c", Controller:(*bool)(0xc000ebaa8e), BlockOwnerDeletion:(*bool)(0xc000ebaa8f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:10:49.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zdkfn" for this suite.
Feb  1 07:10:55.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:10:55.739: INFO: namespace: e2e-tests-gc-zdkfn, resource: bindings, ignored listing per whitelist
Feb  1 07:10:55.808: INFO: namespace e2e-tests-gc-zdkfn deletion completed in 6.168440279s

â€¢ [SLOW TEST:11.414 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:10:55.810: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8zhfh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-84b9276a-25f0-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:10:56.074: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-8zhfh" to be "success or failure"
Feb  1 07:10:56.078: INFO: Pod "pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215678ms
Feb  1 07:10:58.082: INFO: Pod "pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008555107s
Feb  1 07:11:00.089: INFO: Pod "pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015299645s
STEP: Saw pod success
Feb  1 07:11:00.089: INFO: Pod "pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:11:00.095: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:11:00.119: INFO: Waiting for pod pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:11:00.122: INFO: Pod pod-projected-secrets-84b9d36e-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:11:00.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8zhfh" for this suite.
Feb  1 07:11:06.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:11:06.197: INFO: namespace: e2e-tests-projected-8zhfh, resource: bindings, ignored listing per whitelist
Feb  1 07:11:06.267: INFO: namespace e2e-tests-projected-8zhfh deletion completed in 6.139074136s

â€¢ [SLOW TEST:10.458 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:11:06.268: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dp7mz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:11:06.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-dp7mz" to be "success or failure"
Feb  1 07:11:06.476: INFO: Pod "downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723415ms
Feb  1 07:11:08.481: INFO: Pod "downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008958634s
Feb  1 07:11:10.489: INFO: Pod "downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017168573s
STEP: Saw pod success
Feb  1 07:11:10.490: INFO: Pod "downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:11:10.494: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:11:10.514: INFO: Waiting for pod downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:11:10.521: INFO: Pod downwardapi-volume-8aec5cdf-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:11:10.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dp7mz" for this suite.
Feb  1 07:11:16.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:11:16.705: INFO: namespace: e2e-tests-projected-dp7mz, resource: bindings, ignored listing per whitelist
Feb  1 07:11:16.747: INFO: namespace e2e-tests-projected-dp7mz deletion completed in 6.221081501s

â€¢ [SLOW TEST:10.480 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:11:16.748: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-wx5m4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-wx5m4
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  1 07:11:16.955: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  1 07:11:39.054: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.32.0.3 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wx5m4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:11:39.055: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:11:40.197: INFO: Found all expected endpoints: [netserver-0]
Feb  1 07:11:40.201: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.43.0.2 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wx5m4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:11:40.201: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:11:41.328: INFO: Found all expected endpoints: [netserver-1]
Feb  1 07:11:41.333: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.40.0.5 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wx5m4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:11:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:11:42.457: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:11:42.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-wx5m4" for this suite.
Feb  1 07:12:04.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:12:04.587: INFO: namespace: e2e-tests-pod-network-test-wx5m4, resource: bindings, ignored listing per whitelist
Feb  1 07:12:04.600: INFO: namespace e2e-tests-pod-network-test-wx5m4 deletion completed in 22.132267325s

â€¢ [SLOW TEST:47.852 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:12:04.601: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-j27pk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:12:04.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-j27pk" to be "success or failure"
Feb  1 07:12:04.839: INFO: Pod "downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.810279ms
Feb  1 07:12:06.845: INFO: Pod "downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012459586s
Feb  1 07:12:08.849: INFO: Pod "downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016797158s
STEP: Saw pod success
Feb  1 07:12:08.849: INFO: Pod "downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:12:08.858: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:12:08.876: INFO: Waiting for pod downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:12:08.879: INFO: Pod downwardapi-volume-adb5667a-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:12:08.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j27pk" for this suite.
Feb  1 07:12:14.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:12:14.949: INFO: namespace: e2e-tests-downward-api-j27pk, resource: bindings, ignored listing per whitelist
Feb  1 07:12:15.045: INFO: namespace e2e-tests-downward-api-j27pk deletion completed in 6.161353378s

â€¢ [SLOW TEST:10.445 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:12:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-g4ss5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0201 07:12:25.357980      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 07:12:25.358: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:12:25.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-g4ss5" for this suite.
Feb  1 07:12:31.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:12:31.431: INFO: namespace: e2e-tests-gc-g4ss5, resource: bindings, ignored listing per whitelist
Feb  1 07:12:31.575: INFO: namespace e2e-tests-gc-g4ss5 deletion completed in 6.202682388s

â€¢ [SLOW TEST:16.529 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:12:31.576: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jl6dz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:12:31.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 version --client'
Feb  1 07:12:31.884: INFO: stderr: ""
Feb  1 07:12:31.884: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb  1 07:12:31.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-jl6dz'
Feb  1 07:12:32.120: INFO: stderr: ""
Feb  1 07:12:32.120: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb  1 07:12:32.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-jl6dz'
Feb  1 07:12:32.345: INFO: stderr: ""
Feb  1 07:12:32.345: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  1 07:12:33.353: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:12:33.353: INFO: Found 0 / 1
Feb  1 07:12:34.350: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:12:34.350: INFO: Found 0 / 1
Feb  1 07:12:35.351: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:12:35.351: INFO: Found 1 / 1
Feb  1 07:12:35.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  1 07:12:35.361: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:12:35.361: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  1 07:12:35.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 describe pod redis-master-q4wwm --namespace=e2e-tests-kubectl-jl6dz'
Feb  1 07:12:35.482: INFO: stderr: ""
Feb  1 07:12:35.483: INFO: stdout: "Name:               redis-master-q4wwm\nNamespace:          e2e-tests-kubectl-jl6dz\nPriority:           0\nPriorityClassName:  <none>\nNode:               pharos-worker-2/95.216.222.53\nStart Time:         Fri, 01 Feb 2019 07:12:32 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: 00-pharos-privileged\nStatus:             Running\nIP:                 10.32.0.3\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9f41a7c0a4badf6f55e27a1e1ab933afe4d1bfe7cbd5ea0acc5f5437f4d09ac6\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 01 Feb 2019 07:12:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bs8s9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-bs8s9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-bs8s9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                      Message\n  ----    ------     ----  ----                      -------\n  Normal  Scheduled  3s    default-scheduler         Successfully assigned e2e-tests-kubectl-jl6dz/redis-master-q4wwm to pharos-worker-2\n  Normal  Pulling    2s    kubelet, pharos-worker-2  pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, pharos-worker-2  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, pharos-worker-2  Created container\n  Normal  Started    1s    kubelet, pharos-worker-2  Started container\n"
Feb  1 07:12:35.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 describe rc redis-master --namespace=e2e-tests-kubectl-jl6dz'
Feb  1 07:12:35.619: INFO: stderr: ""
Feb  1 07:12:35.619: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-jl6dz\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-q4wwm\n"
Feb  1 07:12:35.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 describe service redis-master --namespace=e2e-tests-kubectl-jl6dz'
Feb  1 07:12:35.738: INFO: stderr: ""
Feb  1 07:12:35.738: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-jl6dz\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.103.37.243\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.32.0.3:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb  1 07:12:35.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 describe node pharos-master-0'
Feb  1 07:12:35.894: INFO: stderr: ""
Feb  1 07:12:35.894: INFO: stdout: "Name:               pharos-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=cx41\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=hel1\n                    kubernetes.io/hostname=pharos-master-0\n                    node-address.kontena.io/external-ip=95.216.203.230\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 01 Feb 2019 06:08:21 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 01 Feb 2019 06:09:23 +0000   Fri, 01 Feb 2019 06:09:23 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Fri, 01 Feb 2019 07:12:28 +0000   Fri, 01 Feb 2019 06:08:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 01 Feb 2019 07:12:28 +0000   Fri, 01 Feb 2019 06:08:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 01 Feb 2019 07:12:28 +0000   Fri, 01 Feb 2019 06:08:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 01 Feb 2019 07:12:28 +0000   Fri, 01 Feb 2019 06:08:31 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  95.216.203.230\n  Hostname:    pharos-master-0\nCapacity:\n cpu:                4\n ephemeral-storage:  157482440Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16038340Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  145135816464\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             15935940Ki\n pods:               110\nSystem Info:\n Machine ID:                 f728d96166e54391907b2edf0e269b37\n System UUID:                F728D961-66E5-4391-907B-2EDF0E269B37\n Boot ID:                    f219843e-97c3-412a-9118-d1c3d37038b9\n Kernel Version:             4.15.0-42-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     10.32.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-ff006445a7294224-hb4t2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  kube-system                coredns-5567f4d9b9-qtntp                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     63m\n  kube-system                etcd-pharos-master-0                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                kube-apiserver-pharos-master-0                             250m (6%)     0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                kube-controller-manager-pharos-master-0                    200m (5%)     0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                kube-proxy-44z9f                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                kube-scheduler-pharos-master-0                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                kubelet-rubber-stamp-d5cb864b-jdqml                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                weave-net-nrjj8                                            20m (0%)      0 (0%)      0 (0%)           0 (0%)         63m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                670m (16%)  0 (0%)\n  memory             70Mi (0%)   170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Feb  1 07:12:35.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 describe namespace e2e-tests-kubectl-jl6dz'
Feb  1 07:12:36.007: INFO: stderr: ""
Feb  1 07:12:36.007: INFO: stdout: "Name:         e2e-tests-kubectl-jl6dz\nLabels:       e2e-framework=kubectl\n              e2e-run=870de310-25e9-11e9-b51c-8292ded0de80\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:12:36.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jl6dz" for this suite.
Feb  1 07:12:58.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:12:58.187: INFO: namespace: e2e-tests-kubectl-jl6dz, resource: bindings, ignored listing per whitelist
Feb  1 07:12:58.195: INFO: namespace e2e-tests-kubectl-jl6dz deletion completed in 22.178588276s

â€¢ [SLOW TEST:26.619 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:12:58.196: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-xmdtz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:13:02.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-xmdtz" for this suite.
Feb  1 07:13:08.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:13:08.517: INFO: namespace: e2e-tests-emptydir-wrapper-xmdtz, resource: bindings, ignored listing per whitelist
Feb  1 07:13:08.581: INFO: namespace e2e-tests-emptydir-wrapper-xmdtz deletion completed in 6.127446926s

â€¢ [SLOW TEST:10.386 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:13:08.582: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pldzk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-s8ps6
STEP: Creating secret with name secret-test-d3d57c22-25f0-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:13:08.940: INFO: Waiting up to 5m0s for pod "pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-pldzk" to be "success or failure"
Feb  1 07:13:08.949: INFO: Pod "pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.194772ms
Feb  1 07:13:10.957: INFO: Pod "pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016603553s
Feb  1 07:13:12.962: INFO: Pod "pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022173373s
STEP: Saw pod success
Feb  1 07:13:12.963: INFO: Pod "pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:13:12.966: INFO: Trying to get logs from node pharos-worker-2 pod pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:13:12.995: INFO: Waiting for pod pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:13:12.998: INFO: Pod pod-secrets-d3eb31a5-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:13:12.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pldzk" for this suite.
Feb  1 07:13:19.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:13:19.091: INFO: namespace: e2e-tests-secrets-pldzk, resource: bindings, ignored listing per whitelist
Feb  1 07:13:19.128: INFO: namespace e2e-tests-secrets-pldzk deletion completed in 6.125468105s
STEP: Destroying namespace "e2e-tests-secret-namespace-s8ps6" for this suite.
Feb  1 07:13:25.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:13:25.161: INFO: namespace: e2e-tests-secret-namespace-s8ps6, resource: bindings, ignored listing per whitelist
Feb  1 07:13:25.270: INFO: namespace e2e-tests-secret-namespace-s8ps6 deletion completed in 6.141779491s

â€¢ [SLOW TEST:16.688 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:13:25.271: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pt7hp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:13:25.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-pt7hp" to be "success or failure"
Feb  1 07:13:25.483: INFO: Pod "downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682337ms
Feb  1 07:13:27.487: INFO: Pod "downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007515304s
Feb  1 07:13:29.493: INFO: Pod "downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013263863s
STEP: Saw pod success
Feb  1 07:13:29.493: INFO: Pod "downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:13:29.497: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:13:29.524: INFO: Waiting for pod downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:13:29.527: INFO: Pod downwardapi-volume-ddc6f274-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:13:29.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pt7hp" for this suite.
Feb  1 07:13:35.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:13:35.597: INFO: namespace: e2e-tests-projected-pt7hp, resource: bindings, ignored listing per whitelist
Feb  1 07:13:35.654: INFO: namespace e2e-tests-projected-pt7hp deletion completed in 6.122236669s

â€¢ [SLOW TEST:10.383 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:13:35.655: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-pxtll
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:13:35.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-pxtll" to be "success or failure"
Feb  1 07:13:35.867: INFO: Pod "downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598392ms
Feb  1 07:13:37.875: INFO: Pod "downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015709218s
Feb  1 07:13:39.883: INFO: Pod "downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024277692s
STEP: Saw pod success
Feb  1 07:13:39.884: INFO: Pod "downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:13:39.888: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:13:39.913: INFO: Waiting for pod downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:13:39.915: INFO: Pod downwardapi-volume-e3f72dcd-25f0-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:13:39.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pxtll" for this suite.
Feb  1 07:13:45.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:13:46.016: INFO: namespace: e2e-tests-downward-api-pxtll, resource: bindings, ignored listing per whitelist
Feb  1 07:13:46.068: INFO: namespace e2e-tests-downward-api-pxtll deletion completed in 6.144813626s

â€¢ [SLOW TEST:10.414 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:13:46.069: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-zcdlh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  1 07:13:55.033: INFO: Successfully updated pod "pod-update-ea53a67f-25f0-11e9-b51c-8292ded0de80"
STEP: verifying the updated pod is in kubernetes
Feb  1 07:13:55.040: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:13:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zcdlh" for this suite.
Feb  1 07:14:17.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:14:17.116: INFO: namespace: e2e-tests-pods-zcdlh, resource: bindings, ignored listing per whitelist
Feb  1 07:14:17.164: INFO: namespace e2e-tests-pods-zcdlh deletion completed in 22.11797991s

â€¢ [SLOW TEST:31.096 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:14:17.166: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ck9zr
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-fcb5e964-25f0-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fcb5e964-25f0-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:14:23.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ck9zr" for this suite.
Feb  1 07:14:45.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:14:45.553: INFO: namespace: e2e-tests-configmap-ck9zr, resource: bindings, ignored listing per whitelist
Feb  1 07:14:45.611: INFO: namespace e2e-tests-configmap-ck9zr deletion completed in 22.124794193s

â€¢ [SLOW TEST:28.445 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:14:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sdhk6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb  1 07:14:45.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 api-versions'
Feb  1 07:14:45.930: INFO: stderr: ""
Feb  1 07:14:45.930: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:14:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sdhk6" for this suite.
Feb  1 07:14:51.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:14:52.055: INFO: namespace: e2e-tests-kubectl-sdhk6, resource: bindings, ignored listing per whitelist
Feb  1 07:14:52.065: INFO: namespace e2e-tests-kubectl-sdhk6 deletion completed in 6.121197903s

â€¢ [SLOW TEST:6.454 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:14:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-n6hh5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb  1 07:14:52.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-n6hh5,SelfLink:/api/v1/namespaces/e2e-tests-watch-n6hh5/configmaps/e2e-watch-test-resource-version,UID:118167ad-25f1-11e9-90b2-96000019ce9c,ResourceVersion:13581,Generation:0,CreationTimestamp:2019-02-01 07:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  1 07:14:52.283: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-n6hh5,SelfLink:/api/v1/namespaces/e2e-tests-watch-n6hh5/configmaps/e2e-watch-test-resource-version,UID:118167ad-25f1-11e9-90b2-96000019ce9c,ResourceVersion:13582,Generation:0,CreationTimestamp:2019-02-01 07:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:14:52.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-n6hh5" for this suite.
Feb  1 07:14:58.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:14:58.398: INFO: namespace: e2e-tests-watch-n6hh5, resource: bindings, ignored listing per whitelist
Feb  1 07:14:58.424: INFO: namespace e2e-tests-watch-n6hh5 deletion completed in 6.135074794s

â€¢ [SLOW TEST:6.359 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:14:58.424: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-9d68l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-9d68l
I0201 07:14:58.629812      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-9d68l, replica count: 1
I0201 07:14:59.680967      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 07:15:00.681363      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 07:15:01.681736      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0201 07:15:02.682158      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  1 07:15:02.793: INFO: Created: latency-svc-7xv45
Feb  1 07:15:02.803: INFO: Got endpoints: latency-svc-7xv45 [20.66906ms]
Feb  1 07:15:02.820: INFO: Created: latency-svc-xzpxs
Feb  1 07:15:02.823: INFO: Got endpoints: latency-svc-xzpxs [20.378973ms]
Feb  1 07:15:02.833: INFO: Created: latency-svc-lbr6b
Feb  1 07:15:02.836: INFO: Got endpoints: latency-svc-lbr6b [32.12405ms]
Feb  1 07:15:02.838: INFO: Created: latency-svc-vgcws
Feb  1 07:15:02.842: INFO: Got endpoints: latency-svc-vgcws [38.256667ms]
Feb  1 07:15:02.848: INFO: Created: latency-svc-2mgpl
Feb  1 07:15:02.851: INFO: Got endpoints: latency-svc-2mgpl [47.531009ms]
Feb  1 07:15:02.861: INFO: Created: latency-svc-2rgtp
Feb  1 07:15:02.861: INFO: Got endpoints: latency-svc-2rgtp [56.513746ms]
Feb  1 07:15:02.871: INFO: Created: latency-svc-rqwrj
Feb  1 07:15:02.871: INFO: Got endpoints: latency-svc-rqwrj [67.062275ms]
Feb  1 07:15:02.875: INFO: Created: latency-svc-7pxrr
Feb  1 07:15:02.879: INFO: Got endpoints: latency-svc-7pxrr [18.165928ms]
Feb  1 07:15:02.892: INFO: Created: latency-svc-59ml2
Feb  1 07:15:02.897: INFO: Got endpoints: latency-svc-59ml2 [93.422248ms]
Feb  1 07:15:02.911: INFO: Created: latency-svc-4v2g5
Feb  1 07:15:02.913: INFO: Got endpoints: latency-svc-4v2g5 [109.207ms]
Feb  1 07:15:02.919: INFO: Created: latency-svc-lwhlz
Feb  1 07:15:02.921: INFO: Got endpoints: latency-svc-lwhlz [116.630053ms]
Feb  1 07:15:02.997: INFO: Created: latency-svc-r6s5d
Feb  1 07:15:03.000: INFO: Got endpoints: latency-svc-r6s5d [196.041415ms]
Feb  1 07:15:03.005: INFO: Created: latency-svc-8q285
Feb  1 07:15:03.009: INFO: Got endpoints: latency-svc-8q285 [205.164955ms]
Feb  1 07:15:03.015: INFO: Created: latency-svc-tkqrf
Feb  1 07:15:03.016: INFO: Got endpoints: latency-svc-tkqrf [212.434658ms]
Feb  1 07:15:03.032: INFO: Created: latency-svc-sqkts
Feb  1 07:15:03.035: INFO: Got endpoints: latency-svc-sqkts [230.837401ms]
Feb  1 07:15:03.054: INFO: Created: latency-svc-n6wzb
Feb  1 07:15:03.058: INFO: Got endpoints: latency-svc-n6wzb [254.069587ms]
Feb  1 07:15:03.063: INFO: Created: latency-svc-gv5v6
Feb  1 07:15:03.065: INFO: Got endpoints: latency-svc-gv5v6 [260.502572ms]
Feb  1 07:15:03.140: INFO: Created: latency-svc-qhrxb
Feb  1 07:15:03.142: INFO: Got endpoints: latency-svc-qhrxb [318.237325ms]
Feb  1 07:15:03.148: INFO: Created: latency-svc-84plc
Feb  1 07:15:03.153: INFO: Got endpoints: latency-svc-84plc [317.516311ms]
Feb  1 07:15:03.189: INFO: Created: latency-svc-wbsgs
Feb  1 07:15:03.191: INFO: Got endpoints: latency-svc-wbsgs [349.440252ms]
Feb  1 07:15:03.230: INFO: Created: latency-svc-h4zwq
Feb  1 07:15:03.269: INFO: Got endpoints: latency-svc-h4zwq [418.191814ms]
Feb  1 07:15:03.274: INFO: Created: latency-svc-ts5wd
Feb  1 07:15:03.276: INFO: Got endpoints: latency-svc-ts5wd [405.016552ms]
Feb  1 07:15:03.314: INFO: Created: latency-svc-gs7x8
Feb  1 07:15:03.315: INFO: Got endpoints: latency-svc-gs7x8 [435.583156ms]
Feb  1 07:15:03.346: INFO: Created: latency-svc-cpqg2
Feb  1 07:15:03.352: INFO: Got endpoints: latency-svc-cpqg2 [454.663429ms]
Feb  1 07:15:03.362: INFO: Created: latency-svc-sbsrv
Feb  1 07:15:03.370: INFO: Got endpoints: latency-svc-sbsrv [456.6634ms]
Feb  1 07:15:03.426: INFO: Created: latency-svc-rvhn7
Feb  1 07:15:03.431: INFO: Got endpoints: latency-svc-rvhn7 [510.273138ms]
Feb  1 07:15:03.439: INFO: Created: latency-svc-7czgg
Feb  1 07:15:03.442: INFO: Got endpoints: latency-svc-7czgg [442.099829ms]
Feb  1 07:15:03.475: INFO: Created: latency-svc-grvq4
Feb  1 07:15:03.477: INFO: Got endpoints: latency-svc-grvq4 [467.89146ms]
Feb  1 07:15:03.509: INFO: Created: latency-svc-tg7q5
Feb  1 07:15:03.511: INFO: Got endpoints: latency-svc-tg7q5 [494.383337ms]
Feb  1 07:15:03.527: INFO: Created: latency-svc-d2wbd
Feb  1 07:15:03.559: INFO: Got endpoints: latency-svc-d2wbd [523.317675ms]
Feb  1 07:15:03.561: INFO: Created: latency-svc-s68lx
Feb  1 07:15:03.573: INFO: Got endpoints: latency-svc-s68lx [515.18441ms]
Feb  1 07:15:03.645: INFO: Created: latency-svc-5dk5f
Feb  1 07:15:03.652: INFO: Got endpoints: latency-svc-5dk5f [587.485549ms]
Feb  1 07:15:03.666: INFO: Created: latency-svc-rk8p9
Feb  1 07:15:03.668: INFO: Got endpoints: latency-svc-rk8p9 [526.016611ms]
Feb  1 07:15:03.703: INFO: Created: latency-svc-khlbm
Feb  1 07:15:03.703: INFO: Got endpoints: latency-svc-khlbm [549.988182ms]
Feb  1 07:15:03.710: INFO: Created: latency-svc-8t5nj
Feb  1 07:15:03.712: INFO: Got endpoints: latency-svc-8t5nj [520.581618ms]
Feb  1 07:15:03.716: INFO: Created: latency-svc-r76nn
Feb  1 07:15:03.720: INFO: Got endpoints: latency-svc-r76nn [450.355294ms]
Feb  1 07:15:03.727: INFO: Created: latency-svc-m4h8r
Feb  1 07:15:03.729: INFO: Got endpoints: latency-svc-m4h8r [452.599183ms]
Feb  1 07:15:03.735: INFO: Created: latency-svc-4w2c5
Feb  1 07:15:03.735: INFO: Got endpoints: latency-svc-4w2c5 [420.548763ms]
Feb  1 07:15:03.741: INFO: Created: latency-svc-7d86q
Feb  1 07:15:03.743: INFO: Got endpoints: latency-svc-7d86q [391.441239ms]
Feb  1 07:15:03.749: INFO: Created: latency-svc-dpq7n
Feb  1 07:15:03.752: INFO: Got endpoints: latency-svc-dpq7n [381.793668ms]
Feb  1 07:15:03.755: INFO: Created: latency-svc-wzlsw
Feb  1 07:15:03.757: INFO: Got endpoints: latency-svc-wzlsw [325.640119ms]
Feb  1 07:15:03.764: INFO: Created: latency-svc-nrjdn
Feb  1 07:15:03.801: INFO: Created: latency-svc-q4lw5
Feb  1 07:15:03.809: INFO: Created: latency-svc-skb7d
Feb  1 07:15:03.811: INFO: Got endpoints: latency-svc-skb7d [299.636962ms]
Feb  1 07:15:03.811: INFO: Got endpoints: latency-svc-nrjdn [368.808449ms]
Feb  1 07:15:03.811: INFO: Got endpoints: latency-svc-q4lw5 [333.911874ms]
Feb  1 07:15:03.814: INFO: Created: latency-svc-hmsln
Feb  1 07:15:03.821: INFO: Got endpoints: latency-svc-hmsln [262.678531ms]
Feb  1 07:15:03.825: INFO: Created: latency-svc-ktqjj
Feb  1 07:15:03.827: INFO: Got endpoints: latency-svc-ktqjj [253.149228ms]
Feb  1 07:15:03.834: INFO: Created: latency-svc-rh4ld
Feb  1 07:15:03.834: INFO: Got endpoints: latency-svc-rh4ld [182.071495ms]
Feb  1 07:15:03.838: INFO: Created: latency-svc-4h2l8
Feb  1 07:15:03.839: INFO: Got endpoints: latency-svc-4h2l8 [171.435592ms]
Feb  1 07:15:03.855: INFO: Created: latency-svc-mmkgh
Feb  1 07:15:03.856: INFO: Got endpoints: latency-svc-mmkgh [152.829601ms]
Feb  1 07:15:03.864: INFO: Created: latency-svc-pbj56
Feb  1 07:15:03.866: INFO: Got endpoints: latency-svc-pbj56 [153.432725ms]
Feb  1 07:15:03.985: INFO: Created: latency-svc-mw9r2
Feb  1 07:15:03.987: INFO: Got endpoints: latency-svc-mw9r2 [267.559696ms]
Feb  1 07:15:03.991: INFO: Created: latency-svc-ll4j4
Feb  1 07:15:03.993: INFO: Got endpoints: latency-svc-ll4j4 [263.880525ms]
Feb  1 07:15:04.037: INFO: Created: latency-svc-d8b4f
Feb  1 07:15:04.039: INFO: Got endpoints: latency-svc-d8b4f [303.805544ms]
Feb  1 07:15:04.053: INFO: Created: latency-svc-mqtjt
Feb  1 07:15:04.054: INFO: Got endpoints: latency-svc-mqtjt [310.979816ms]
Feb  1 07:15:04.114: INFO: Created: latency-svc-s2xlg
Feb  1 07:15:04.124: INFO: Got endpoints: latency-svc-s2xlg [372.133085ms]
Feb  1 07:15:04.130: INFO: Created: latency-svc-ztnbs
Feb  1 07:15:04.131: INFO: Got endpoints: latency-svc-ztnbs [374.317794ms]
Feb  1 07:15:04.135: INFO: Created: latency-svc-fr29w
Feb  1 07:15:04.142: INFO: Created: latency-svc-ppf9v
Feb  1 07:15:04.146: INFO: Got endpoints: latency-svc-fr29w [334.551031ms]
Feb  1 07:15:04.151: INFO: Created: latency-svc-dnv2b
Feb  1 07:15:04.158: INFO: Created: latency-svc-w28q2
Feb  1 07:15:04.164: INFO: Created: latency-svc-d2z7l
Feb  1 07:15:04.170: INFO: Created: latency-svc-9kgcn
Feb  1 07:15:04.206: INFO: Got endpoints: latency-svc-ppf9v [394.288939ms]
Feb  1 07:15:04.206: INFO: Created: latency-svc-cqm82
Feb  1 07:15:04.210: INFO: Created: latency-svc-n9ml8
Feb  1 07:15:04.218: INFO: Created: latency-svc-8cgwx
Feb  1 07:15:04.223: INFO: Created: latency-svc-8j5n9
Feb  1 07:15:04.239: INFO: Created: latency-svc-k847z
Feb  1 07:15:04.273: INFO: Got endpoints: latency-svc-dnv2b [461.995185ms]
Feb  1 07:15:04.273: INFO: Created: latency-svc-82h5s
Feb  1 07:15:04.281: INFO: Created: latency-svc-clp5w
Feb  1 07:15:04.288: INFO: Created: latency-svc-9w9lv
Feb  1 07:15:04.297: INFO: Got endpoints: latency-svc-w28q2 [475.379402ms]
Feb  1 07:15:04.297: INFO: Created: latency-svc-6mskd
Feb  1 07:15:04.302: INFO: Created: latency-svc-z5mr7
Feb  1 07:15:04.305: INFO: Created: latency-svc-79z9v
Feb  1 07:15:04.318: INFO: Created: latency-svc-lf8pr
Feb  1 07:15:04.322: INFO: Created: latency-svc-tcqfz
Feb  1 07:15:04.370: INFO: Got endpoints: latency-svc-d2z7l [543.623066ms]
Feb  1 07:15:04.383: INFO: Created: latency-svc-24gmp
Feb  1 07:15:04.396: INFO: Got endpoints: latency-svc-9kgcn [561.779319ms]
Feb  1 07:15:04.448: INFO: Got endpoints: latency-svc-cqm82 [608.470164ms]
Feb  1 07:15:04.459: INFO: Created: latency-svc-5n959
Feb  1 07:15:04.568: INFO: Got endpoints: latency-svc-8cgwx [702.48056ms]
Feb  1 07:15:04.569: INFO: Created: latency-svc-g6mjz
Feb  1 07:15:04.569: INFO: Got endpoints: latency-svc-n9ml8 [712.189794ms]
Feb  1 07:15:04.581: INFO: Created: latency-svc-cvsnk
Feb  1 07:15:04.591: INFO: Created: latency-svc-fvw6h
Feb  1 07:15:04.596: INFO: Got endpoints: latency-svc-8j5n9 [608.157469ms]
Feb  1 07:15:04.607: INFO: Created: latency-svc-rmkbx
Feb  1 07:15:04.647: INFO: Got endpoints: latency-svc-k847z [654.16021ms]
Feb  1 07:15:04.696: INFO: Got endpoints: latency-svc-82h5s [657.072668ms]
Feb  1 07:15:04.699: INFO: Created: latency-svc-fjwdh
Feb  1 07:15:04.718: INFO: Created: latency-svc-q4s6f
Feb  1 07:15:04.747: INFO: Got endpoints: latency-svc-clp5w [692.898414ms]
Feb  1 07:15:04.880: INFO: Got endpoints: latency-svc-9w9lv [755.97554ms]
Feb  1 07:15:04.881: INFO: Got endpoints: latency-svc-6mskd [750.111203ms]
Feb  1 07:15:04.883: INFO: Created: latency-svc-77x2d
Feb  1 07:15:04.896: INFO: Created: latency-svc-lqvw6
Feb  1 07:15:04.897: INFO: Got endpoints: latency-svc-z5mr7 [750.801251ms]
Feb  1 07:15:04.907: INFO: Created: latency-svc-nhjv7
Feb  1 07:15:04.915: INFO: Created: latency-svc-6fpvq
Feb  1 07:15:04.947: INFO: Got endpoints: latency-svc-79z9v [740.487897ms]
Feb  1 07:15:04.959: INFO: Created: latency-svc-m4x8m
Feb  1 07:15:05.002: INFO: Got endpoints: latency-svc-lf8pr [729.02605ms]
Feb  1 07:15:05.016: INFO: Created: latency-svc-7lsxr
Feb  1 07:15:05.047: INFO: Got endpoints: latency-svc-tcqfz [749.788898ms]
Feb  1 07:15:05.060: INFO: Created: latency-svc-gtmdf
Feb  1 07:15:05.098: INFO: Got endpoints: latency-svc-24gmp [727.615152ms]
Feb  1 07:15:05.149: INFO: Got endpoints: latency-svc-5n959 [752.616385ms]
Feb  1 07:15:05.153: INFO: Created: latency-svc-x8cjj
Feb  1 07:15:05.209: INFO: Got endpoints: latency-svc-g6mjz [761.251716ms]
Feb  1 07:15:05.248: INFO: Got endpoints: latency-svc-cvsnk [679.547497ms]
Feb  1 07:15:05.249: INFO: Created: latency-svc-4m5mr
Feb  1 07:15:05.285: INFO: Created: latency-svc-2zttz
Feb  1 07:15:05.314: INFO: Got endpoints: latency-svc-fvw6h [744.851482ms]
Feb  1 07:15:05.318: INFO: Created: latency-svc-lvgbh
Feb  1 07:15:05.350: INFO: Got endpoints: latency-svc-rmkbx [754.349617ms]
Feb  1 07:15:05.352: INFO: Created: latency-svc-29q9x
Feb  1 07:15:05.362: INFO: Created: latency-svc-nkvq2
Feb  1 07:15:05.397: INFO: Got endpoints: latency-svc-fjwdh [749.401775ms]
Feb  1 07:15:05.461: INFO: Got endpoints: latency-svc-q4s6f [764.944352ms]
Feb  1 07:15:05.465: INFO: Created: latency-svc-vfvq8
Feb  1 07:15:05.497: INFO: Got endpoints: latency-svc-77x2d [749.589818ms]
Feb  1 07:15:05.530: INFO: Created: latency-svc-vdb5v
Feb  1 07:15:05.548: INFO: Got endpoints: latency-svc-lqvw6 [666.178619ms]
Feb  1 07:15:05.557: INFO: Created: latency-svc-p2mds
Feb  1 07:15:05.594: INFO: Created: latency-svc-j8pt7
Feb  1 07:15:05.597: INFO: Got endpoints: latency-svc-nhjv7 [716.882435ms]
Feb  1 07:15:05.634: INFO: Created: latency-svc-gkmhp
Feb  1 07:15:05.646: INFO: Got endpoints: latency-svc-6fpvq [749.134106ms]
Feb  1 07:15:05.697: INFO: Created: latency-svc-p8tbp
Feb  1 07:15:05.700: INFO: Got endpoints: latency-svc-m4x8m [753.17163ms]
Feb  1 07:15:05.747: INFO: Got endpoints: latency-svc-7lsxr [745.079625ms]
Feb  1 07:15:05.791: INFO: Created: latency-svc-bf5ln
Feb  1 07:15:05.847: INFO: Got endpoints: latency-svc-x8cjj [748.987061ms]
Feb  1 07:15:05.860: INFO: Got endpoints: latency-svc-gtmdf [812.964801ms]
Feb  1 07:15:05.865: INFO: Created: latency-svc-pgfgc
Feb  1 07:15:05.873: INFO: Created: latency-svc-5sv6g
Feb  1 07:15:05.903: INFO: Created: latency-svc-drklb
Feb  1 07:15:05.909: INFO: Got endpoints: latency-svc-4m5mr [759.806777ms]
Feb  1 07:15:05.954: INFO: Got endpoints: latency-svc-2zttz [743.837101ms]
Feb  1 07:15:05.955: INFO: Created: latency-svc-dwmgm
Feb  1 07:15:06.083: INFO: Got endpoints: latency-svc-29q9x [768.962694ms]
Feb  1 07:15:06.084: INFO: Got endpoints: latency-svc-lvgbh [835.262408ms]
Feb  1 07:15:06.087: INFO: Created: latency-svc-7krzc
Feb  1 07:15:06.095: INFO: Got endpoints: latency-svc-nkvq2 [744.686712ms]
Feb  1 07:15:06.101: INFO: Created: latency-svc-mfnhg
Feb  1 07:15:06.108: INFO: Created: latency-svc-tfs64
Feb  1 07:15:06.116: INFO: Created: latency-svc-xk5nd
Feb  1 07:15:06.146: INFO: Got endpoints: latency-svc-vfvq8 [749.63751ms]
Feb  1 07:15:06.155: INFO: Created: latency-svc-9hfkn
Feb  1 07:15:06.220: INFO: Got endpoints: latency-svc-vdb5v [758.873355ms]
Feb  1 07:15:06.232: INFO: Created: latency-svc-v42pn
Feb  1 07:15:06.247: INFO: Got endpoints: latency-svc-p2mds [749.516488ms]
Feb  1 07:15:06.257: INFO: Created: latency-svc-bxf79
Feb  1 07:15:06.298: INFO: Got endpoints: latency-svc-j8pt7 [750.586501ms]
Feb  1 07:15:06.313: INFO: Created: latency-svc-rphj5
Feb  1 07:15:06.347: INFO: Got endpoints: latency-svc-gkmhp [749.955641ms]
Feb  1 07:15:06.358: INFO: Created: latency-svc-k56vl
Feb  1 07:15:06.397: INFO: Got endpoints: latency-svc-p8tbp [750.524057ms]
Feb  1 07:15:06.408: INFO: Created: latency-svc-hklbx
Feb  1 07:15:06.446: INFO: Got endpoints: latency-svc-bf5ln [746.62115ms]
Feb  1 07:15:06.456: INFO: Created: latency-svc-rn92j
Feb  1 07:15:06.502: INFO: Got endpoints: latency-svc-pgfgc [754.359227ms]
Feb  1 07:15:06.513: INFO: Created: latency-svc-sgrgl
Feb  1 07:15:06.549: INFO: Got endpoints: latency-svc-5sv6g [702.398739ms]
Feb  1 07:15:06.560: INFO: Created: latency-svc-g6nzd
Feb  1 07:15:06.597: INFO: Got endpoints: latency-svc-drklb [737.071576ms]
Feb  1 07:15:06.612: INFO: Created: latency-svc-h4xwd
Feb  1 07:15:06.647: INFO: Got endpoints: latency-svc-dwmgm [737.562099ms]
Feb  1 07:15:06.657: INFO: Created: latency-svc-d97hq
Feb  1 07:15:06.696: INFO: Got endpoints: latency-svc-7krzc [742.350511ms]
Feb  1 07:15:06.776: INFO: Got endpoints: latency-svc-mfnhg [692.709893ms]
Feb  1 07:15:06.778: INFO: Created: latency-svc-qscpb
Feb  1 07:15:06.799: INFO: Got endpoints: latency-svc-tfs64 [714.980096ms]
Feb  1 07:15:06.814: INFO: Created: latency-svc-kths2
Feb  1 07:15:06.853: INFO: Got endpoints: latency-svc-xk5nd [758.088406ms]
Feb  1 07:15:06.854: INFO: Created: latency-svc-7fdvk
Feb  1 07:15:06.924: INFO: Got endpoints: latency-svc-9hfkn [777.881531ms]
Feb  1 07:15:06.924: INFO: Created: latency-svc-s9xfj
Feb  1 07:15:06.967: INFO: Created: latency-svc-msmh8
Feb  1 07:15:06.969: INFO: Got endpoints: latency-svc-v42pn [748.837285ms]
Feb  1 07:15:06.988: INFO: Created: latency-svc-tcpbx
Feb  1 07:15:06.996: INFO: Got endpoints: latency-svc-bxf79 [749.146344ms]
Feb  1 07:15:07.074: INFO: Created: latency-svc-g6kqq
Feb  1 07:15:07.075: INFO: Got endpoints: latency-svc-rphj5 [776.538274ms]
Feb  1 07:15:07.087: INFO: Created: latency-svc-vqtl2
Feb  1 07:15:07.096: INFO: Got endpoints: latency-svc-k56vl [749.568052ms]
Feb  1 07:15:07.111: INFO: Created: latency-svc-wxt7z
Feb  1 07:15:07.147: INFO: Got endpoints: latency-svc-hklbx [749.356756ms]
Feb  1 07:15:07.159: INFO: Created: latency-svc-lpdbz
Feb  1 07:15:07.215: INFO: Got endpoints: latency-svc-rn92j [768.427756ms]
Feb  1 07:15:07.229: INFO: Created: latency-svc-tb5vg
Feb  1 07:15:07.247: INFO: Got endpoints: latency-svc-sgrgl [745.195911ms]
Feb  1 07:15:07.260: INFO: Created: latency-svc-bmkw8
Feb  1 07:15:07.296: INFO: Got endpoints: latency-svc-g6nzd [746.535249ms]
Feb  1 07:15:07.308: INFO: Created: latency-svc-tvrgg
Feb  1 07:15:07.347: INFO: Got endpoints: latency-svc-h4xwd [749.766128ms]
Feb  1 07:15:07.360: INFO: Created: latency-svc-bgnqz
Feb  1 07:15:07.397: INFO: Got endpoints: latency-svc-d97hq [749.906228ms]
Feb  1 07:15:07.410: INFO: Created: latency-svc-4f7nn
Feb  1 07:15:07.447: INFO: Got endpoints: latency-svc-qscpb [750.365581ms]
Feb  1 07:15:07.457: INFO: Created: latency-svc-d2dl5
Feb  1 07:15:07.497: INFO: Got endpoints: latency-svc-kths2 [720.923479ms]
Feb  1 07:15:07.510: INFO: Created: latency-svc-7vrst
Feb  1 07:15:07.547: INFO: Got endpoints: latency-svc-7fdvk [747.166839ms]
Feb  1 07:15:07.558: INFO: Created: latency-svc-7sbl7
Feb  1 07:15:07.598: INFO: Got endpoints: latency-svc-s9xfj [744.301467ms]
Feb  1 07:15:07.610: INFO: Created: latency-svc-b6xxd
Feb  1 07:15:07.647: INFO: Got endpoints: latency-svc-msmh8 [721.345853ms]
Feb  1 07:15:07.657: INFO: Created: latency-svc-5s6b2
Feb  1 07:15:07.697: INFO: Got endpoints: latency-svc-tcpbx [727.157058ms]
Feb  1 07:15:07.707: INFO: Created: latency-svc-69z49
Feb  1 07:15:07.747: INFO: Got endpoints: latency-svc-g6kqq [751.086612ms]
Feb  1 07:15:07.761: INFO: Created: latency-svc-89j6m
Feb  1 07:15:07.797: INFO: Got endpoints: latency-svc-vqtl2 [722.054449ms]
Feb  1 07:15:07.814: INFO: Created: latency-svc-sxdg4
Feb  1 07:15:07.849: INFO: Got endpoints: latency-svc-wxt7z [752.225913ms]
Feb  1 07:15:07.863: INFO: Created: latency-svc-82xtf
Feb  1 07:15:07.897: INFO: Got endpoints: latency-svc-lpdbz [750.001099ms]
Feb  1 07:15:07.906: INFO: Created: latency-svc-hwspp
Feb  1 07:15:07.951: INFO: Got endpoints: latency-svc-tb5vg [736.316852ms]
Feb  1 07:15:07.962: INFO: Created: latency-svc-pddch
Feb  1 07:15:07.997: INFO: Got endpoints: latency-svc-bmkw8 [749.963814ms]
Feb  1 07:15:08.020: INFO: Created: latency-svc-smxx4
Feb  1 07:15:08.047: INFO: Got endpoints: latency-svc-tvrgg [750.583639ms]
Feb  1 07:15:08.064: INFO: Created: latency-svc-xbzkk
Feb  1 07:15:08.105: INFO: Got endpoints: latency-svc-bgnqz [758.53112ms]
Feb  1 07:15:08.119: INFO: Created: latency-svc-qnn7r
Feb  1 07:15:08.150: INFO: Got endpoints: latency-svc-4f7nn [752.850736ms]
Feb  1 07:15:08.179: INFO: Created: latency-svc-fp5hd
Feb  1 07:15:08.198: INFO: Got endpoints: latency-svc-d2dl5 [750.988856ms]
Feb  1 07:15:08.215: INFO: Created: latency-svc-qf852
Feb  1 07:15:08.247: INFO: Got endpoints: latency-svc-7vrst [749.693473ms]
Feb  1 07:15:08.269: INFO: Created: latency-svc-78kl7
Feb  1 07:15:08.309: INFO: Got endpoints: latency-svc-7sbl7 [762.813855ms]
Feb  1 07:15:08.325: INFO: Created: latency-svc-bppmf
Feb  1 07:15:08.347: INFO: Got endpoints: latency-svc-b6xxd [749.253888ms]
Feb  1 07:15:08.393: INFO: Created: latency-svc-4wrvc
Feb  1 07:15:08.396: INFO: Got endpoints: latency-svc-5s6b2 [749.844208ms]
Feb  1 07:15:08.440: INFO: Created: latency-svc-s94tj
Feb  1 07:15:08.451: INFO: Got endpoints: latency-svc-69z49 [754.392447ms]
Feb  1 07:15:08.461: INFO: Created: latency-svc-m6nw2
Feb  1 07:15:08.497: INFO: Got endpoints: latency-svc-89j6m [749.467766ms]
Feb  1 07:15:08.514: INFO: Created: latency-svc-bp7bl
Feb  1 07:15:08.566: INFO: Got endpoints: latency-svc-sxdg4 [768.481821ms]
Feb  1 07:15:08.580: INFO: Created: latency-svc-cv2fb
Feb  1 07:15:08.596: INFO: Got endpoints: latency-svc-82xtf [747.103061ms]
Feb  1 07:15:08.620: INFO: Created: latency-svc-558pp
Feb  1 07:15:08.647: INFO: Got endpoints: latency-svc-hwspp [749.79957ms]
Feb  1 07:15:08.657: INFO: Created: latency-svc-qqld8
Feb  1 07:15:08.696: INFO: Got endpoints: latency-svc-pddch [744.794516ms]
Feb  1 07:15:08.707: INFO: Created: latency-svc-rzsj5
Feb  1 07:15:08.759: INFO: Got endpoints: latency-svc-smxx4 [761.728207ms]
Feb  1 07:15:08.771: INFO: Created: latency-svc-65h5z
Feb  1 07:15:08.797: INFO: Got endpoints: latency-svc-xbzkk [750.187737ms]
Feb  1 07:15:08.810: INFO: Created: latency-svc-pq4tz
Feb  1 07:15:08.846: INFO: Got endpoints: latency-svc-qnn7r [740.876548ms]
Feb  1 07:15:08.857: INFO: Created: latency-svc-42rxb
Feb  1 07:15:08.896: INFO: Got endpoints: latency-svc-fp5hd [746.183156ms]
Feb  1 07:15:08.906: INFO: Created: latency-svc-jghnb
Feb  1 07:15:08.946: INFO: Got endpoints: latency-svc-qf852 [748.122866ms]
Feb  1 07:15:08.955: INFO: Created: latency-svc-nbcnv
Feb  1 07:15:09.006: INFO: Got endpoints: latency-svc-78kl7 [758.286548ms]
Feb  1 07:15:09.017: INFO: Created: latency-svc-5mzgs
Feb  1 07:15:09.047: INFO: Got endpoints: latency-svc-bppmf [737.782187ms]
Feb  1 07:15:09.063: INFO: Created: latency-svc-rjtr4
Feb  1 07:15:09.097: INFO: Got endpoints: latency-svc-4wrvc [749.87393ms]
Feb  1 07:15:09.109: INFO: Created: latency-svc-4t28w
Feb  1 07:15:09.151: INFO: Got endpoints: latency-svc-s94tj [754.957245ms]
Feb  1 07:15:09.166: INFO: Created: latency-svc-phmws
Feb  1 07:15:09.206: INFO: Got endpoints: latency-svc-m6nw2 [754.824871ms]
Feb  1 07:15:09.219: INFO: Created: latency-svc-stglw
Feb  1 07:15:09.247: INFO: Got endpoints: latency-svc-bp7bl [750.417697ms]
Feb  1 07:15:09.255: INFO: Created: latency-svc-f2pj2
Feb  1 07:15:09.297: INFO: Got endpoints: latency-svc-cv2fb [730.976627ms]
Feb  1 07:15:09.317: INFO: Created: latency-svc-k7qtc
Feb  1 07:15:09.346: INFO: Got endpoints: latency-svc-558pp [749.895897ms]
Feb  1 07:15:09.373: INFO: Created: latency-svc-hwpmm
Feb  1 07:15:09.398: INFO: Got endpoints: latency-svc-qqld8 [751.005747ms]
Feb  1 07:15:09.414: INFO: Created: latency-svc-dlxsr
Feb  1 07:15:09.446: INFO: Got endpoints: latency-svc-rzsj5 [749.687625ms]
Feb  1 07:15:09.458: INFO: Created: latency-svc-jsljd
Feb  1 07:15:09.497: INFO: Got endpoints: latency-svc-65h5z [738.272053ms]
Feb  1 07:15:09.508: INFO: Created: latency-svc-fw7bd
Feb  1 07:15:09.547: INFO: Got endpoints: latency-svc-pq4tz [749.762197ms]
Feb  1 07:15:09.555: INFO: Created: latency-svc-jswnk
Feb  1 07:15:09.597: INFO: Got endpoints: latency-svc-42rxb [750.286885ms]
Feb  1 07:15:09.653: INFO: Created: latency-svc-jbw64
Feb  1 07:15:09.653: INFO: Got endpoints: latency-svc-jghnb [757.359883ms]
Feb  1 07:15:09.663: INFO: Created: latency-svc-6bkln
Feb  1 07:15:09.696: INFO: Got endpoints: latency-svc-nbcnv [749.594461ms]
Feb  1 07:15:09.704: INFO: Created: latency-svc-lblkn
Feb  1 07:15:09.747: INFO: Got endpoints: latency-svc-5mzgs [741.325148ms]
Feb  1 07:15:09.839: INFO: Got endpoints: latency-svc-rjtr4 [791.265429ms]
Feb  1 07:15:09.841: INFO: Created: latency-svc-25bcf
Feb  1 07:15:09.870: INFO: Got endpoints: latency-svc-4t28w [772.766343ms]
Feb  1 07:15:09.880: INFO: Created: latency-svc-n2wvs
Feb  1 07:15:09.884: INFO: Created: latency-svc-tmncm
Feb  1 07:15:09.897: INFO: Got endpoints: latency-svc-phmws [745.184247ms]
Feb  1 07:15:09.969: INFO: Created: latency-svc-tx7tf
Feb  1 07:15:09.970: INFO: Got endpoints: latency-svc-stglw [762.94127ms]
Feb  1 07:15:09.989: INFO: Created: latency-svc-kfm4f
Feb  1 07:15:09.997: INFO: Got endpoints: latency-svc-f2pj2 [749.908251ms]
Feb  1 07:15:10.006: INFO: Created: latency-svc-d9g8k
Feb  1 07:15:10.047: INFO: Got endpoints: latency-svc-k7qtc [749.844814ms]
Feb  1 07:15:10.058: INFO: Created: latency-svc-8h2vb
Feb  1 07:15:10.096: INFO: Got endpoints: latency-svc-hwpmm [749.717129ms]
Feb  1 07:15:10.107: INFO: Created: latency-svc-7d2h6
Feb  1 07:15:10.147: INFO: Got endpoints: latency-svc-dlxsr [749.117241ms]
Feb  1 07:15:10.158: INFO: Created: latency-svc-chgdr
Feb  1 07:15:10.196: INFO: Got endpoints: latency-svc-jsljd [750.097566ms]
Feb  1 07:15:10.225: INFO: Created: latency-svc-52gxt
Feb  1 07:15:10.253: INFO: Got endpoints: latency-svc-fw7bd [755.634245ms]
Feb  1 07:15:10.280: INFO: Created: latency-svc-tfslg
Feb  1 07:15:10.296: INFO: Got endpoints: latency-svc-jswnk [749.151882ms]
Feb  1 07:15:10.308: INFO: Created: latency-svc-w4bbt
Feb  1 07:15:10.347: INFO: Got endpoints: latency-svc-jbw64 [750.089824ms]
Feb  1 07:15:10.358: INFO: Created: latency-svc-pxk7g
Feb  1 07:15:10.398: INFO: Got endpoints: latency-svc-6bkln [744.148328ms]
Feb  1 07:15:10.409: INFO: Created: latency-svc-ktdl7
Feb  1 07:15:10.446: INFO: Got endpoints: latency-svc-lblkn [750.35255ms]
Feb  1 07:15:10.455: INFO: Created: latency-svc-mzsln
Feb  1 07:15:10.497: INFO: Got endpoints: latency-svc-25bcf [749.623428ms]
Feb  1 07:15:10.505: INFO: Created: latency-svc-gmq7n
Feb  1 07:15:10.546: INFO: Got endpoints: latency-svc-n2wvs [707.517758ms]
Feb  1 07:15:10.559: INFO: Created: latency-svc-trksk
Feb  1 07:15:10.596: INFO: Got endpoints: latency-svc-tmncm [726.210868ms]
Feb  1 07:15:10.609: INFO: Created: latency-svc-rkghw
Feb  1 07:15:10.647: INFO: Got endpoints: latency-svc-tx7tf [749.775474ms]
Feb  1 07:15:10.696: INFO: Got endpoints: latency-svc-kfm4f [726.276371ms]
Feb  1 07:15:10.747: INFO: Got endpoints: latency-svc-d9g8k [749.622018ms]
Feb  1 07:15:10.797: INFO: Got endpoints: latency-svc-8h2vb [750.049169ms]
Feb  1 07:15:10.847: INFO: Got endpoints: latency-svc-7d2h6 [750.549914ms]
Feb  1 07:15:10.897: INFO: Got endpoints: latency-svc-chgdr [750.541377ms]
Feb  1 07:15:10.946: INFO: Got endpoints: latency-svc-52gxt [749.917508ms]
Feb  1 07:15:10.996: INFO: Got endpoints: latency-svc-tfslg [743.19053ms]
Feb  1 07:15:11.047: INFO: Got endpoints: latency-svc-w4bbt [750.60712ms]
Feb  1 07:15:11.096: INFO: Got endpoints: latency-svc-pxk7g [748.231647ms]
Feb  1 07:15:11.146: INFO: Got endpoints: latency-svc-ktdl7 [748.459996ms]
Feb  1 07:15:11.198: INFO: Got endpoints: latency-svc-mzsln [751.562533ms]
Feb  1 07:15:11.246: INFO: Got endpoints: latency-svc-gmq7n [749.294302ms]
Feb  1 07:15:11.296: INFO: Got endpoints: latency-svc-trksk [749.636448ms]
Feb  1 07:15:11.349: INFO: Got endpoints: latency-svc-rkghw [752.156556ms]
Feb  1 07:15:11.349: INFO: Latencies: [18.165928ms 20.378973ms 32.12405ms 38.256667ms 47.531009ms 56.513746ms 67.062275ms 93.422248ms 109.207ms 116.630053ms 152.829601ms 153.432725ms 171.435592ms 182.071495ms 196.041415ms 205.164955ms 212.434658ms 230.837401ms 253.149228ms 254.069587ms 260.502572ms 262.678531ms 263.880525ms 267.559696ms 299.636962ms 303.805544ms 310.979816ms 317.516311ms 318.237325ms 325.640119ms 333.911874ms 334.551031ms 349.440252ms 368.808449ms 372.133085ms 374.317794ms 381.793668ms 391.441239ms 394.288939ms 405.016552ms 418.191814ms 420.548763ms 435.583156ms 442.099829ms 450.355294ms 452.599183ms 454.663429ms 456.6634ms 461.995185ms 467.89146ms 475.379402ms 494.383337ms 510.273138ms 515.18441ms 520.581618ms 523.317675ms 526.016611ms 543.623066ms 549.988182ms 561.779319ms 587.485549ms 608.157469ms 608.470164ms 654.16021ms 657.072668ms 666.178619ms 679.547497ms 692.709893ms 692.898414ms 702.398739ms 702.48056ms 707.517758ms 712.189794ms 714.980096ms 716.882435ms 720.923479ms 721.345853ms 722.054449ms 726.210868ms 726.276371ms 727.157058ms 727.615152ms 729.02605ms 730.976627ms 736.316852ms 737.071576ms 737.562099ms 737.782187ms 738.272053ms 740.487897ms 740.876548ms 741.325148ms 742.350511ms 743.19053ms 743.837101ms 744.148328ms 744.301467ms 744.686712ms 744.794516ms 744.851482ms 745.079625ms 745.184247ms 745.195911ms 746.183156ms 746.535249ms 746.62115ms 747.103061ms 747.166839ms 748.122866ms 748.231647ms 748.459996ms 748.837285ms 748.987061ms 749.117241ms 749.134106ms 749.146344ms 749.151882ms 749.253888ms 749.294302ms 749.356756ms 749.401775ms 749.467766ms 749.516488ms 749.568052ms 749.589818ms 749.594461ms 749.622018ms 749.623428ms 749.636448ms 749.63751ms 749.687625ms 749.693473ms 749.717129ms 749.762197ms 749.766128ms 749.775474ms 749.788898ms 749.79957ms 749.844208ms 749.844814ms 749.87393ms 749.895897ms 749.906228ms 749.908251ms 749.917508ms 749.955641ms 749.963814ms 750.001099ms 750.049169ms 750.089824ms 750.097566ms 750.111203ms 750.187737ms 750.286885ms 750.35255ms 750.365581ms 750.417697ms 750.524057ms 750.541377ms 750.549914ms 750.583639ms 750.586501ms 750.60712ms 750.801251ms 750.988856ms 751.005747ms 751.086612ms 751.562533ms 752.156556ms 752.225913ms 752.616385ms 752.850736ms 753.17163ms 754.349617ms 754.359227ms 754.392447ms 754.824871ms 754.957245ms 755.634245ms 755.97554ms 757.359883ms 758.088406ms 758.286548ms 758.53112ms 758.873355ms 759.806777ms 761.251716ms 761.728207ms 762.813855ms 762.94127ms 764.944352ms 768.427756ms 768.481821ms 768.962694ms 772.766343ms 776.538274ms 777.881531ms 791.265429ms 812.964801ms 835.262408ms]
Feb  1 07:15:11.350: INFO: 50 %ile: 745.079625ms
Feb  1 07:15:11.350: INFO: 90 %ile: 757.359883ms
Feb  1 07:15:11.350: INFO: 99 %ile: 812.964801ms
Feb  1 07:15:11.350: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:15:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-9d68l" for this suite.
Feb  1 07:15:23.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:15:23.400: INFO: namespace: e2e-tests-svc-latency-9d68l, resource: bindings, ignored listing per whitelist
Feb  1 07:15:23.491: INFO: namespace e2e-tests-svc-latency-9d68l deletion completed in 12.135124047s

â€¢ [SLOW TEST:25.067 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:15:23.492: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-h5wl2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  1 07:15:28.250: INFO: Successfully updated pod "annotationupdate243da8de-25f1-11e9-b51c-8292ded0de80"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:15:32.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h5wl2" for this suite.
Feb  1 07:15:54.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:15:54.410: INFO: namespace: e2e-tests-projected-h5wl2, resource: bindings, ignored listing per whitelist
Feb  1 07:15:54.410: INFO: namespace e2e-tests-projected-h5wl2 deletion completed in 22.119256803s

â€¢ [SLOW TEST:30.919 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:15:54.411: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-nb7d8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  1 07:15:54.672: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:54.680: INFO: Number of nodes with available pods: 0
Feb  1 07:15:54.680: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:15:55.685: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:55.690: INFO: Number of nodes with available pods: 0
Feb  1 07:15:55.690: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:15:56.687: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:56.692: INFO: Number of nodes with available pods: 0
Feb  1 07:15:56.692: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:15:57.685: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:57.690: INFO: Number of nodes with available pods: 0
Feb  1 07:15:57.690: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:15:58.687: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:58.691: INFO: Number of nodes with available pods: 3
Feb  1 07:15:58.691: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb  1 07:15:58.714: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:58.719: INFO: Number of nodes with available pods: 2
Feb  1 07:15:58.719: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 07:15:59.726: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:15:59.729: INFO: Number of nodes with available pods: 2
Feb  1 07:15:59.729: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 07:16:00.727: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:16:00.731: INFO: Number of nodes with available pods: 2
Feb  1 07:16:00.732: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 07:16:01.727: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:16:01.731: INFO: Number of nodes with available pods: 2
Feb  1 07:16:01.731: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 07:16:02.728: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:16:02.732: INFO: Number of nodes with available pods: 3
Feb  1 07:16:02.732: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-nb7d8, will wait for the garbage collector to delete the pods
Feb  1 07:16:02.799: INFO: Deleting DaemonSet.extensions daemon-set took: 5.779703ms
Feb  1 07:16:02.899: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.519191ms
Feb  1 07:16:36.705: INFO: Number of nodes with available pods: 0
Feb  1 07:16:36.706: INFO: Number of running nodes: 0, number of available pods: 0
Feb  1 07:16:36.710: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-nb7d8/daemonsets","resourceVersion":"15144"},"items":null}

Feb  1 07:16:36.715: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-nb7d8/pods","resourceVersion":"15144"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:16:36.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-nb7d8" for this suite.
Feb  1 07:16:42.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:16:42.808: INFO: namespace: e2e-tests-daemonsets-nb7d8, resource: bindings, ignored listing per whitelist
Feb  1 07:16:42.900: INFO: namespace e2e-tests-daemonsets-nb7d8 deletion completed in 6.152642231s

â€¢ [SLOW TEST:48.490 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:16:42.901: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k9pkw
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-5396e15b-25f1-11e9-b51c-8292ded0de80
STEP: Creating configMap with name cm-test-opt-upd-5396e1ff-25f1-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5396e15b-25f1-11e9-b51c-8292ded0de80
STEP: Updating configmap cm-test-opt-upd-5396e1ff-25f1-11e9-b51c-8292ded0de80
STEP: Creating configMap with name cm-test-opt-create-5396e21e-25f1-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:18:08.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k9pkw" for this suite.
Feb  1 07:18:30.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:18:30.118: INFO: namespace: e2e-tests-projected-k9pkw, resource: bindings, ignored listing per whitelist
Feb  1 07:18:30.186: INFO: namespace e2e-tests-projected-k9pkw deletion completed in 22.139665713s

â€¢ [SLOW TEST:107.286 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:18:30.189: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4gsqf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  1 07:18:30.394: INFO: Waiting up to 5m0s for pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-4gsqf" to be "success or failure"
Feb  1 07:18:30.398: INFO: Pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475956ms
Feb  1 07:18:32.408: INFO: Pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013475809s
Feb  1 07:18:34.414: INFO: Pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020001647s
Feb  1 07:18:36.420: INFO: Pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025318541s
STEP: Saw pod success
Feb  1 07:18:36.420: INFO: Pod "downward-api-9385929f-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:18:36.424: INFO: Trying to get logs from node pharos-worker-2 pod downward-api-9385929f-25f1-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 07:18:36.450: INFO: Waiting for pod downward-api-9385929f-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:18:36.453: INFO: Pod downward-api-9385929f-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:18:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4gsqf" for this suite.
Feb  1 07:18:42.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:18:42.567: INFO: namespace: e2e-tests-downward-api-4gsqf, resource: bindings, ignored listing per whitelist
Feb  1 07:18:42.567: INFO: namespace e2e-tests-downward-api-4gsqf deletion completed in 6.107648803s

â€¢ [SLOW TEST:12.379 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:18:42.568: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-fzzdz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  1 07:18:42.759: INFO: Waiting up to 5m0s for pod "pod-9ae48935-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-fzzdz" to be "success or failure"
Feb  1 07:18:42.764: INFO: Pod "pod-9ae48935-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79098ms
Feb  1 07:18:44.769: INFO: Pod "pod-9ae48935-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010325922s
Feb  1 07:18:46.774: INFO: Pod "pod-9ae48935-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014940065s
STEP: Saw pod success
Feb  1 07:18:46.774: INFO: Pod "pod-9ae48935-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:18:46.777: INFO: Trying to get logs from node pharos-worker-2 pod pod-9ae48935-25f1-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:18:46.807: INFO: Waiting for pod pod-9ae48935-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:18:46.810: INFO: Pod pod-9ae48935-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:18:46.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fzzdz" for this suite.
Feb  1 07:18:52.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:18:52.892: INFO: namespace: e2e-tests-emptydir-fzzdz, resource: bindings, ignored listing per whitelist
Feb  1 07:18:52.950: INFO: namespace e2e-tests-emptydir-fzzdz deletion completed in 6.135107288s

â€¢ [SLOW TEST:10.383 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:18:52.951: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-nvsj5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:19:30.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-nvsj5" for this suite.
Feb  1 07:19:36.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:19:36.632: INFO: namespace: e2e-tests-container-runtime-nvsj5, resource: bindings, ignored listing per whitelist
Feb  1 07:19:36.699: INFO: namespace e2e-tests-container-runtime-nvsj5 deletion completed in 6.209472372s

â€¢ [SLOW TEST:43.749 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:19:36.700: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vcpzm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-bb29d04b-25f1-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:19:36.909: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-vcpzm" to be "success or failure"
Feb  1 07:19:36.912: INFO: Pod "pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511273ms
Feb  1 07:19:38.916: INFO: Pod "pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007029174s
Feb  1 07:19:40.921: INFO: Pod "pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011934584s
STEP: Saw pod success
Feb  1 07:19:40.922: INFO: Pod "pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:19:40.929: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:19:40.955: INFO: Waiting for pod pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:19:40.957: INFO: Pod pod-configmaps-bb2aa2aa-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:19:40.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vcpzm" for this suite.
Feb  1 07:19:46.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:19:47.110: INFO: namespace: e2e-tests-configmap-vcpzm, resource: bindings, ignored listing per whitelist
Feb  1 07:19:47.160: INFO: namespace e2e-tests-configmap-vcpzm deletion completed in 6.19741412s

â€¢ [SLOW TEST:10.460 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:19:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jzcz2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  1 07:19:47.377: INFO: Waiting up to 5m0s for pod "pod-c1686077-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-jzcz2" to be "success or failure"
Feb  1 07:19:47.385: INFO: Pod "pod-c1686077-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.63128ms
Feb  1 07:19:49.392: INFO: Pod "pod-c1686077-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014832243s
Feb  1 07:19:51.397: INFO: Pod "pod-c1686077-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020015135s
STEP: Saw pod success
Feb  1 07:19:51.397: INFO: Pod "pod-c1686077-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:19:51.401: INFO: Trying to get logs from node pharos-worker-2 pod pod-c1686077-25f1-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:19:51.421: INFO: Waiting for pod pod-c1686077-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:19:51.424: INFO: Pod pod-c1686077-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:19:51.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jzcz2" for this suite.
Feb  1 07:19:57.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:19:57.500: INFO: namespace: e2e-tests-emptydir-jzcz2, resource: bindings, ignored listing per whitelist
Feb  1 07:19:57.567: INFO: namespace e2e-tests-emptydir-jzcz2 deletion completed in 6.13675872s

â€¢ [SLOW TEST:10.406 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:19:57.568: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-96msf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-ppjgn
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-jkn2m
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:20:04.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-96msf" for this suite.
Feb  1 07:20:10.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:20:10.223: INFO: namespace: e2e-tests-namespaces-96msf, resource: bindings, ignored listing per whitelist
Feb  1 07:20:10.303: INFO: namespace e2e-tests-namespaces-96msf deletion completed in 6.161369445s
STEP: Destroying namespace "e2e-tests-nsdeletetest-ppjgn" for this suite.
Feb  1 07:20:10.307: INFO: Namespace e2e-tests-nsdeletetest-ppjgn was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-jkn2m" for this suite.
Feb  1 07:20:16.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:20:16.454: INFO: namespace: e2e-tests-nsdeletetest-jkn2m, resource: bindings, ignored listing per whitelist
Feb  1 07:20:16.454: INFO: namespace e2e-tests-nsdeletetest-jkn2m deletion completed in 6.144936851s

â€¢ [SLOW TEST:18.887 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:20:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-fxjrt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-68xjp
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Feb  1 07:20:20.830: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-9cbfd
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:20:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-fxjrt" for this suite.
Feb  1 07:20:51.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:20:51.110: INFO: namespace: e2e-tests-namespaces-fxjrt, resource: bindings, ignored listing per whitelist
Feb  1 07:20:51.120: INFO: namespace e2e-tests-namespaces-fxjrt deletion completed in 6.126460641s
STEP: Destroying namespace "e2e-tests-nsdeletetest-68xjp" for this suite.
Feb  1 07:20:51.122: INFO: Namespace e2e-tests-nsdeletetest-68xjp was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-9cbfd" for this suite.
Feb  1 07:20:57.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:20:57.200: INFO: namespace: e2e-tests-nsdeletetest-9cbfd, resource: bindings, ignored listing per whitelist
Feb  1 07:20:57.251: INFO: namespace e2e-tests-nsdeletetest-9cbfd deletion completed in 6.128357141s

â€¢ [SLOW TEST:40.796 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:20:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-wdbp6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-eb2c6bad-25f1-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:20:57.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-wdbp6" to be "success or failure"
Feb  1 07:20:57.456: INFO: Pod "pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126381ms
Feb  1 07:20:59.461: INFO: Pod "pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007949681s
Feb  1 07:21:01.465: INFO: Pod "pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011869593s
STEP: Saw pod success
Feb  1 07:21:01.465: INFO: Pod "pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:21:01.468: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:21:01.489: INFO: Waiting for pod pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:21:01.491: INFO: Pod pod-configmaps-eb2cfd75-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:21:01.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wdbp6" for this suite.
Feb  1 07:21:07.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:21:07.556: INFO: namespace: e2e-tests-configmap-wdbp6, resource: bindings, ignored listing per whitelist
Feb  1 07:21:07.624: INFO: namespace e2e-tests-configmap-wdbp6 deletion completed in 6.129245587s

â€¢ [SLOW TEST:10.372 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:21:07.625: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zwvmp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-zwvmp/configmap-test-f158f27b-25f1-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:21:07.811: INFO: Waiting up to 5m0s for pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-zwvmp" to be "success or failure"
Feb  1 07:21:07.814: INFO: Pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.134483ms
Feb  1 07:21:09.819: INFO: Pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00813997s
Feb  1 07:21:11.825: INFO: Pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014638579s
Feb  1 07:21:13.831: INFO: Pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020284483s
STEP: Saw pod success
Feb  1 07:21:13.831: INFO: Pod "pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:21:13.835: INFO: Trying to get logs from node pharos-worker-2 pod pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80 container env-test: <nil>
STEP: delete the pod
Feb  1 07:21:13.863: INFO: Waiting for pod pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:21:13.868: INFO: Pod pod-configmaps-f15986c1-25f1-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:21:13.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zwvmp" for this suite.
Feb  1 07:21:19.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:21:19.993: INFO: namespace: e2e-tests-configmap-zwvmp, resource: bindings, ignored listing per whitelist
Feb  1 07:21:20.024: INFO: namespace e2e-tests-configmap-zwvmp deletion completed in 6.14168902s

â€¢ [SLOW TEST:12.399 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:21:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-jxs9q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb  1 07:21:20.264: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-jxs9q" to be "success or failure"
Feb  1 07:21:20.267: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778782ms
Feb  1 07:21:22.277: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013000148s
Feb  1 07:21:24.288: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024464267s
Feb  1 07:21:26.297: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033358285s
STEP: Saw pod success
Feb  1 07:21:26.297: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb  1 07:21:26.302: INFO: Trying to get logs from node pharos-worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb  1 07:21:26.320: INFO: Waiting for pod pod-host-path-test to disappear
Feb  1 07:21:26.323: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:21:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-jxs9q" for this suite.
Feb  1 07:21:32.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:21:32.460: INFO: namespace: e2e-tests-hostpath-jxs9q, resource: bindings, ignored listing per whitelist
Feb  1 07:21:32.465: INFO: namespace e2e-tests-hostpath-jxs9q deletion completed in 6.131750648s

â€¢ [SLOW TEST:12.440 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:21:32.466: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-d7jmb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb  1 07:21:32.871: INFO: Waiting up to 5m0s for pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-containers-d7jmb" to be "success or failure"
Feb  1 07:21:32.874: INFO: Pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.554282ms
Feb  1 07:21:34.883: INFO: Pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012537947s
Feb  1 07:21:36.887: INFO: Pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016469901s
Feb  1 07:21:38.892: INFO: Pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021063927s
STEP: Saw pod success
Feb  1 07:21:38.892: INFO: Pod "client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:21:38.896: INFO: Trying to get logs from node pharos-worker-0 pod client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:21:38.939: INFO: Waiting for pod client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:21:38.943: INFO: Pod client-containers-002bdf74-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:21:38.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-d7jmb" for this suite.
Feb  1 07:21:44.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:21:44.982: INFO: namespace: e2e-tests-containers-d7jmb, resource: bindings, ignored listing per whitelist
Feb  1 07:21:45.096: INFO: namespace e2e-tests-containers-d7jmb deletion completed in 6.14899853s

â€¢ [SLOW TEST:12.630 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:21:45.097: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-gvth5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  1 07:21:49.860: INFO: Successfully updated pod "pod-update-activedeadlineseconds-07b4d801-25f2-11e9-b51c-8292ded0de80"
Feb  1 07:21:49.860: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-07b4d801-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-pods-gvth5" to be "terminated due to deadline exceeded"
Feb  1 07:21:49.879: INFO: Pod "pod-update-activedeadlineseconds-07b4d801-25f2-11e9-b51c-8292ded0de80": Phase="Running", Reason="", readiness=true. Elapsed: 18.008913ms
Feb  1 07:21:51.888: INFO: Pod "pod-update-activedeadlineseconds-07b4d801-25f2-11e9-b51c-8292ded0de80": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.027282106s
Feb  1 07:21:51.888: INFO: Pod "pod-update-activedeadlineseconds-07b4d801-25f2-11e9-b51c-8292ded0de80" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:21:51.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gvth5" for this suite.
Feb  1 07:21:57.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:21:57.927: INFO: namespace: e2e-tests-pods-gvth5, resource: bindings, ignored listing per whitelist
Feb  1 07:21:58.025: INFO: namespace e2e-tests-pods-gvth5 deletion completed in 6.130561135s

â€¢ [SLOW TEST:12.928 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:21:58.026: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-6bvn4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb  1 07:21:58.263: INFO: Waiting up to 5m0s for pod "client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-containers-6bvn4" to be "success or failure"
Feb  1 07:21:58.267: INFO: Pod "client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038323ms
Feb  1 07:22:00.274: INFO: Pod "client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010530686s
Feb  1 07:22:02.281: INFO: Pod "client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018072192s
STEP: Saw pod success
Feb  1 07:22:02.281: INFO: Pod "client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:22:02.285: INFO: Trying to get logs from node pharos-worker-2 pod client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:22:02.323: INFO: Waiting for pod client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:22:02.327: INFO: Pod client-containers-0f6bc0f3-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:22:02.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-6bvn4" for this suite.
Feb  1 07:22:08.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:22:08.459: INFO: namespace: e2e-tests-containers-6bvn4, resource: bindings, ignored listing per whitelist
Feb  1 07:22:08.489: INFO: namespace e2e-tests-containers-6bvn4 deletion completed in 6.150273877s

â€¢ [SLOW TEST:10.464 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:22:08.490: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tbxjc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-15a45604-25f2-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:22:08.747: INFO: Waiting up to 5m0s for pod "pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-tbxjc" to be "success or failure"
Feb  1 07:22:08.784: INFO: Pod "pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 36.408635ms
Feb  1 07:22:10.791: INFO: Pod "pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043828041s
Feb  1 07:22:12.799: INFO: Pod "pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051855998s
STEP: Saw pod success
Feb  1 07:22:12.799: INFO: Pod "pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:22:12.803: INFO: Trying to get logs from node pharos-worker-0 pod pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:22:12.855: INFO: Waiting for pod pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:22:12.857: INFO: Pod pod-secrets-15a50f06-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:22:12.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tbxjc" for this suite.
Feb  1 07:22:18.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:22:18.895: INFO: namespace: e2e-tests-secrets-tbxjc, resource: bindings, ignored listing per whitelist
Feb  1 07:22:18.989: INFO: namespace e2e-tests-secrets-tbxjc deletion completed in 6.127160977s

â€¢ [SLOW TEST:10.499 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:22:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pl9ht
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-1be3ec61-25f2-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:22:19.184: INFO: Waiting up to 5m0s for pod "pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-pl9ht" to be "success or failure"
Feb  1 07:22:19.187: INFO: Pod "pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.614746ms
Feb  1 07:22:21.193: INFO: Pod "pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009102305s
Feb  1 07:22:23.199: INFO: Pod "pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014513844s
STEP: Saw pod success
Feb  1 07:22:23.199: INFO: Pod "pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:22:23.203: INFO: Trying to get logs from node pharos-worker-2 pod pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:22:23.228: INFO: Waiting for pod pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:22:23.230: INFO: Pod pod-secrets-1be476dc-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:22:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pl9ht" for this suite.
Feb  1 07:22:29.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:22:29.282: INFO: namespace: e2e-tests-secrets-pl9ht, resource: bindings, ignored listing per whitelist
Feb  1 07:22:29.357: INFO: namespace e2e-tests-secrets-pl9ht deletion completed in 6.121786565s

â€¢ [SLOW TEST:10.367 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:22:29.358: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2hf4k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb  1 07:22:29.565: INFO: Waiting up to 5m0s for pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-2hf4k" to be "success or failure"
Feb  1 07:22:29.578: INFO: Pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.680617ms
Feb  1 07:22:31.582: INFO: Pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01731039s
Feb  1 07:22:33.589: INFO: Pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024598912s
Feb  1 07:22:35.596: INFO: Pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031608519s
STEP: Saw pod success
Feb  1 07:22:35.597: INFO: Pod "downward-api-22141e64-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:22:35.600: INFO: Trying to get logs from node pharos-worker-2 pod downward-api-22141e64-25f2-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 07:22:35.626: INFO: Waiting for pod downward-api-22141e64-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:22:35.629: INFO: Pod downward-api-22141e64-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:22:35.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2hf4k" for this suite.
Feb  1 07:22:41.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:22:41.668: INFO: namespace: e2e-tests-downward-api-2hf4k, resource: bindings, ignored listing per whitelist
Feb  1 07:22:41.765: INFO: namespace e2e-tests-downward-api-2hf4k deletion completed in 6.130037798s

â€¢ [SLOW TEST:12.407 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:22:41.766: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qjp4v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  1 07:22:41.975: INFO: namespace e2e-tests-kubectl-qjp4v
Feb  1 07:22:41.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-qjp4v'
Feb  1 07:22:42.455: INFO: stderr: ""
Feb  1 07:22:42.455: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  1 07:22:43.460: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:22:43.460: INFO: Found 0 / 1
Feb  1 07:22:44.460: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:22:44.460: INFO: Found 0 / 1
Feb  1 07:22:45.461: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:22:45.461: INFO: Found 0 / 1
Feb  1 07:22:46.463: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:22:46.463: INFO: Found 1 / 1
Feb  1 07:22:46.463: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  1 07:22:46.467: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:22:46.467: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  1 07:22:46.467: INFO: wait on redis-master startup in e2e-tests-kubectl-qjp4v 
Feb  1 07:22:46.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 logs redis-master-ngc8x redis-master --namespace=e2e-tests-kubectl-qjp4v'
Feb  1 07:22:46.584: INFO: stderr: ""
Feb  1 07:22:46.584: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Feb 07:22:45.118 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Feb 07:22:45.118 # Server started, Redis version 3.2.12\n1:M 01 Feb 07:22:45.118 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Feb 07:22:45.118 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb  1 07:22:46.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-qjp4v'
Feb  1 07:22:46.712: INFO: stderr: ""
Feb  1 07:22:46.712: INFO: stdout: "service/rm2 exposed\n"
Feb  1 07:22:46.715: INFO: Service rm2 in namespace e2e-tests-kubectl-qjp4v found.
STEP: exposing service
Feb  1 07:22:48.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-qjp4v'
Feb  1 07:22:48.827: INFO: stderr: ""
Feb  1 07:22:48.827: INFO: stdout: "service/rm3 exposed\n"
Feb  1 07:22:48.830: INFO: Service rm3 in namespace e2e-tests-kubectl-qjp4v found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:22:50.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qjp4v" for this suite.
Feb  1 07:23:12.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:23:12.973: INFO: namespace: e2e-tests-kubectl-qjp4v, resource: bindings, ignored listing per whitelist
Feb  1 07:23:12.998: INFO: namespace e2e-tests-kubectl-qjp4v deletion completed in 22.14233276s

â€¢ [SLOW TEST:31.232 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:23:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b8j8c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  1 07:23:13.217: INFO: Waiting up to 5m0s for pod "pod-3c19045f-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-b8j8c" to be "success or failure"
Feb  1 07:23:13.222: INFO: Pod "pod-3c19045f-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.099051ms
Feb  1 07:23:15.228: INFO: Pod "pod-3c19045f-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011082215s
Feb  1 07:23:17.232: INFO: Pod "pod-3c19045f-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015369306s
STEP: Saw pod success
Feb  1 07:23:17.232: INFO: Pod "pod-3c19045f-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:23:17.236: INFO: Trying to get logs from node pharos-worker-0 pod pod-3c19045f-25f2-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:23:17.268: INFO: Waiting for pod pod-3c19045f-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:23:17.271: INFO: Pod pod-3c19045f-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:23:17.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b8j8c" for this suite.
Feb  1 07:23:23.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:23:23.424: INFO: namespace: e2e-tests-emptydir-b8j8c, resource: bindings, ignored listing per whitelist
Feb  1 07:23:23.427: INFO: namespace e2e-tests-emptydir-b8j8c deletion completed in 6.149254461s

â€¢ [SLOW TEST:10.428 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:23:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-wt2gt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb  1 07:23:23.641: INFO: Waiting up to 5m0s for pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-var-expansion-wt2gt" to be "success or failure"
Feb  1 07:23:23.644: INFO: Pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273016ms
Feb  1 07:23:25.651: INFO: Pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00960346s
Feb  1 07:23:27.657: INFO: Pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015203654s
Feb  1 07:23:29.661: INFO: Pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019859172s
STEP: Saw pod success
Feb  1 07:23:29.662: INFO: Pod "var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:23:29.666: INFO: Trying to get logs from node pharos-worker-2 pod var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 07:23:29.691: INFO: Waiting for pod var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:23:29.693: INFO: Pod var-expansion-424fb722-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:23:29.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-wt2gt" for this suite.
Feb  1 07:23:35.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:23:35.807: INFO: namespace: e2e-tests-var-expansion-wt2gt, resource: bindings, ignored listing per whitelist
Feb  1 07:23:35.820: INFO: namespace e2e-tests-var-expansion-wt2gt deletion completed in 6.121852187s

â€¢ [SLOW TEST:12.390 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:23:35.822: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-m6wmg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-49b178b9-25f2-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:23:36.029: INFO: Waiting up to 5m0s for pod "pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-m6wmg" to be "success or failure"
Feb  1 07:23:36.031: INFO: Pod "pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469795ms
Feb  1 07:23:38.038: INFO: Pod "pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008911209s
Feb  1 07:23:40.045: INFO: Pod "pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015745365s
STEP: Saw pod success
Feb  1 07:23:40.045: INFO: Pod "pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:23:40.050: INFO: Trying to get logs from node pharos-worker-0 pod pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80 container secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:23:40.072: INFO: Waiting for pod pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:23:40.075: INFO: Pod pod-secrets-49b1ff88-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:23:40.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m6wmg" for this suite.
Feb  1 07:23:46.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:23:46.187: INFO: namespace: e2e-tests-secrets-m6wmg, resource: bindings, ignored listing per whitelist
Feb  1 07:23:46.223: INFO: namespace e2e-tests-secrets-m6wmg deletion completed in 6.142455932s

â€¢ [SLOW TEST:10.402 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:23:46.226: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-vcrbk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb  1 07:23:46.658: INFO: Pod name wrapped-volume-race-5003ae87-25f2-11e9-b51c-8292ded0de80: Found 0 pods out of 5
Feb  1 07:23:51.667: INFO: Pod name wrapped-volume-race-5003ae87-25f2-11e9-b51c-8292ded0de80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5003ae87-25f2-11e9-b51c-8292ded0de80 in namespace e2e-tests-emptydir-wrapper-vcrbk, will wait for the garbage collector to delete the pods
Feb  1 07:24:11.770: INFO: Deleting ReplicationController wrapped-volume-race-5003ae87-25f2-11e9-b51c-8292ded0de80 took: 8.380388ms
Feb  1 07:24:11.871: INFO: Terminating ReplicationController wrapped-volume-race-5003ae87-25f2-11e9-b51c-8292ded0de80 pods took: 100.547982ms
STEP: Creating RC which spawns configmap-volume pods
Feb  1 07:24:53.890: INFO: Pod name wrapped-volume-race-7818cb3a-25f2-11e9-b51c-8292ded0de80: Found 0 pods out of 5
Feb  1 07:24:58.903: INFO: Pod name wrapped-volume-race-7818cb3a-25f2-11e9-b51c-8292ded0de80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7818cb3a-25f2-11e9-b51c-8292ded0de80 in namespace e2e-tests-emptydir-wrapper-vcrbk, will wait for the garbage collector to delete the pods
Feb  1 07:25:19.044: INFO: Deleting ReplicationController wrapped-volume-race-7818cb3a-25f2-11e9-b51c-8292ded0de80 took: 22.021515ms
Feb  1 07:25:19.144: INFO: Terminating ReplicationController wrapped-volume-race-7818cb3a-25f2-11e9-b51c-8292ded0de80 pods took: 100.478988ms
STEP: Creating RC which spawns configmap-volume pods
Feb  1 07:26:02.967: INFO: Pod name wrapped-volume-race-a144a234-25f2-11e9-b51c-8292ded0de80: Found 0 pods out of 5
Feb  1 07:26:07.978: INFO: Pod name wrapped-volume-race-a144a234-25f2-11e9-b51c-8292ded0de80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a144a234-25f2-11e9-b51c-8292ded0de80 in namespace e2e-tests-emptydir-wrapper-vcrbk, will wait for the garbage collector to delete the pods
Feb  1 07:26:28.077: INFO: Deleting ReplicationController wrapped-volume-race-a144a234-25f2-11e9-b51c-8292ded0de80 took: 8.806198ms
Feb  1 07:26:28.177: INFO: Terminating ReplicationController wrapped-volume-race-a144a234-25f2-11e9-b51c-8292ded0de80 pods took: 100.467366ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:27:14.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-vcrbk" for this suite.
Feb  1 07:27:22.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:27:22.210: INFO: namespace: e2e-tests-emptydir-wrapper-vcrbk, resource: bindings, ignored listing per whitelist
Feb  1 07:27:22.262: INFO: namespace e2e-tests-emptydir-wrapper-vcrbk deletion completed in 8.129159026s

â€¢ [SLOW TEST:216.037 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:27:22.263: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zhmfv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 07:27:22.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-zhmfv'
Feb  1 07:27:22.580: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb  1 07:27:22.580: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb  1 07:27:24.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-zhmfv'
Feb  1 07:27:24.700: INFO: stderr: ""
Feb  1 07:27:24.700: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:27:24.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zhmfv" for this suite.
Feb  1 07:27:30.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:27:30.829: INFO: namespace: e2e-tests-kubectl-zhmfv, resource: bindings, ignored listing per whitelist
Feb  1 07:27:30.845: INFO: namespace e2e-tests-kubectl-zhmfv deletion completed in 6.140472565s

â€¢ [SLOW TEST:8.582 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:27:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7xphp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb  1 07:27:31.049: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-622717276 proxy --unix-socket=/tmp/kubectl-proxy-unix555357963/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:27:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7xphp" for this suite.
Feb  1 07:27:37.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:27:37.239: INFO: namespace: e2e-tests-kubectl-7xphp, resource: bindings, ignored listing per whitelist
Feb  1 07:27:37.288: INFO: namespace e2e-tests-kubectl-7xphp deletion completed in 6.156708501s

â€¢ [SLOW TEST:6.443 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:27:37.290: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bh8cg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d99f0313-25f2-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:27:37.503: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-bh8cg" to be "success or failure"
Feb  1 07:27:37.507: INFO: Pod "pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420653ms
Feb  1 07:27:39.515: INFO: Pod "pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011487712s
Feb  1 07:27:41.520: INFO: Pod "pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016169035s
STEP: Saw pod success
Feb  1 07:27:41.520: INFO: Pod "pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:27:41.524: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:27:41.556: INFO: Waiting for pod pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:27:41.560: INFO: Pod pod-projected-secrets-d99fbd3d-25f2-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:27:41.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bh8cg" for this suite.
Feb  1 07:27:47.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:27:47.684: INFO: namespace: e2e-tests-projected-bh8cg, resource: bindings, ignored listing per whitelist
Feb  1 07:27:47.713: INFO: namespace e2e-tests-projected-bh8cg deletion completed in 6.148387607s

â€¢ [SLOW TEST:10.423 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:27:47.714: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-kfpfm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:27:47.927: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb  1 07:27:47.937: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  1 07:27:52.945: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  1 07:27:52.945: INFO: Creating deployment "test-rolling-update-deployment"
Feb  1 07:27:52.954: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb  1 07:27:52.961: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb  1 07:27:54.970: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb  1 07:27:54.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684602872, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684602872, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684602872, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684602872, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  1 07:27:56.978: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb  1 07:27:56.989: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-kfpfm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kfpfm/deployments/test-rolling-update-deployment,UID:e2d5208d-25f2-11e9-90b2-96000019ce9c,ResourceVersion:18080,Generation:1,CreationTimestamp:2019-02-01 07:27:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-01 07:27:52 +0000 UTC 2019-02-01 07:27:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-01 07:27:56 +0000 UTC 2019-02-01 07:27:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb  1 07:27:56.993: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-kfpfm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kfpfm/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:e2d79fd0-25f2-11e9-90b2-96000019ce9c,ResourceVersion:18071,Generation:1,CreationTimestamp:2019-02-01 07:27:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e2d5208d-25f2-11e9-90b2-96000019ce9c 0xc0028cece7 0xc0028cece8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb  1 07:27:56.993: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb  1 07:27:56.993: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-kfpfm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-kfpfm/replicasets/test-rolling-update-controller,UID:dfd766bc-25f2-11e9-90b2-96000019ce9c,ResourceVersion:18079,Generation:2,CreationTimestamp:2019-02-01 07:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e2d5208d-25f2-11e9-90b2-96000019ce9c 0xc0028cec27 0xc0028cec28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb  1 07:27:56.997: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-swft5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-swft5,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-kfpfm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-kfpfm/pods/test-rolling-update-deployment-68b55d7bc6-swft5,UID:e2d80c52-25f2-11e9-90b2-96000019ce9c,ResourceVersion:18070,Generation:0,CreationTimestamp:2019-02-01 07:27:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 e2d79fd0-25f2-11e9-90b2-96000019ce9c 0xc0028cf907 0xc0028cf908}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-2n92p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2n92p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-2n92p true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028cf970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028cf990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:27:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:27:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:27:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:27:52 +0000 UTC  }],Message:,Reason:,HostIP:95.216.203.233,PodIP:10.43.0.2,StartTime:2019-02-01 07:27:52 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-01 07:27:56 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7e6a4258e48203ff81dfd7101a9b2fc44f64112f9be9fad53ea8eed0b0a69481}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:27:56.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-kfpfm" for this suite.
Feb  1 07:28:03.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:28:03.119: INFO: namespace: e2e-tests-deployment-kfpfm, resource: bindings, ignored listing per whitelist
Feb  1 07:28:03.129: INFO: namespace e2e-tests-deployment-kfpfm deletion completed in 6.126451425s

â€¢ [SLOW TEST:15.416 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:28:03.130: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9bv7m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 07:28:03.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-9bv7m'
Feb  1 07:28:03.551: INFO: stderr: ""
Feb  1 07:28:03.551: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb  1 07:28:08.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-9bv7m -o json'
Feb  1 07:28:08.716: INFO: stderr: ""
Feb  1 07:28:08.716: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"00-pharos-privileged\"\n        },\n        \"creationTimestamp\": \"2019-02-01T07:28:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-9bv7m\",\n        \"resourceVersion\": \"18142\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-9bv7m/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e923f2db-25f2-11e9-90b2-96000019ce9c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4zmjp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pharos-worker-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4zmjp\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4zmjp\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-01T07:28:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-01T07:28:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-01T07:28:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-01T07:28:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e70c2b6affd3fe3dff9c9e77261594a95d37b3fdb02558f95a361f27e4e2ff1e\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:4796839eb3bcdec13551d3dd7f711d3c78fac2c0d06f4f78f2741f554aee77db\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-01T07:28:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"95.216.222.53\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.0.3\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-01T07:28:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb  1 07:28:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 replace -f - --namespace=e2e-tests-kubectl-9bv7m'
Feb  1 07:28:08.923: INFO: stderr: ""
Feb  1 07:28:08.923: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb  1 07:28:08.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-9bv7m'
Feb  1 07:28:22.197: INFO: stderr: ""
Feb  1 07:28:22.197: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:28:22.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9bv7m" for this suite.
Feb  1 07:28:28.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:28:28.309: INFO: namespace: e2e-tests-kubectl-9bv7m, resource: bindings, ignored listing per whitelist
Feb  1 07:28:28.339: INFO: namespace e2e-tests-kubectl-9bv7m deletion completed in 6.136056546s

â€¢ [SLOW TEST:25.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:28:28.340: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-hb694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-hb694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hb694 to expose endpoints map[]
Feb  1 07:28:28.565: INFO: Get endpoints failed (6.034785ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb  1 07:28:29.571: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hb694 exposes endpoints map[] (1.011811201s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-hb694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hb694 to expose endpoints map[pod1:[100]]
Feb  1 07:28:33.635: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hb694 exposes endpoints map[pod1:[100]] (4.054788225s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-hb694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hb694 to expose endpoints map[pod1:[100] pod2:[101]]
Feb  1 07:28:37.707: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hb694 exposes endpoints map[pod1:[100] pod2:[101]] (4.064574978s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-hb694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hb694 to expose endpoints map[pod2:[101]]
Feb  1 07:28:38.739: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hb694 exposes endpoints map[pod2:[101]] (1.024915251s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-hb694
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hb694 to expose endpoints map[]
Feb  1 07:28:39.764: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hb694 exposes endpoints map[] (1.01938507s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:28:39.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-hb694" for this suite.
Feb  1 07:28:45.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:28:45.865: INFO: namespace: e2e-tests-services-hb694, resource: bindings, ignored listing per whitelist
Feb  1 07:28:45.952: INFO: namespace e2e-tests-services-hb694 deletion completed in 6.151535775s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:17.613 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:28:45.953: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5z2tq
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-02924e52-25f3-11e9-b51c-8292ded0de80
STEP: Creating configMap with name cm-test-opt-upd-02924edc-25f3-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-02924e52-25f3-11e9-b51c-8292ded0de80
STEP: Updating configmap cm-test-opt-upd-02924edc-25f3-11e9-b51c-8292ded0de80
STEP: Creating configMap with name cm-test-opt-create-02924f1e-25f3-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:30:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5z2tq" for this suite.
Feb  1 07:30:31.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:30:31.263: INFO: namespace: e2e-tests-configmap-5z2tq, resource: bindings, ignored listing per whitelist
Feb  1 07:30:31.271: INFO: namespace e2e-tests-configmap-5z2tq deletion completed in 22.13912667s

â€¢ [SLOW TEST:105.318 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:30:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-xzxsw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-4155d5d4-25f3-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:30:31.508: INFO: Waiting up to 5m0s for pod "pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-xzxsw" to be "success or failure"
Feb  1 07:30:31.512: INFO: Pod "pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685134ms
Feb  1 07:30:33.517: INFO: Pod "pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632497s
Feb  1 07:30:35.521: INFO: Pod "pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012870571s
STEP: Saw pod success
Feb  1 07:30:35.521: INFO: Pod "pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:30:35.525: INFO: Trying to get logs from node pharos-worker-2 pod pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:30:35.546: INFO: Waiting for pod pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:30:35.548: INFO: Pod pod-configmaps-41569080-25f3-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:30:35.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xzxsw" for this suite.
Feb  1 07:30:41.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:30:41.678: INFO: namespace: e2e-tests-configmap-xzxsw, resource: bindings, ignored listing per whitelist
Feb  1 07:30:41.687: INFO: namespace e2e-tests-configmap-xzxsw deletion completed in 6.13140505s

â€¢ [SLOW TEST:10.416 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:30:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cklwb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb  1 07:30:41.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cklwb'
Feb  1 07:30:42.010: INFO: stderr: ""
Feb  1 07:30:42.010: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb  1 07:30:42.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cklwb'
Feb  1 07:30:52.846: INFO: stderr: ""
Feb  1 07:30:52.847: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:30:52.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cklwb" for this suite.
Feb  1 07:30:58.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:30:58.945: INFO: namespace: e2e-tests-kubectl-cklwb, resource: bindings, ignored listing per whitelist
Feb  1 07:30:58.985: INFO: namespace e2e-tests-kubectl-cklwb deletion completed in 6.130513724s

â€¢ [SLOW TEST:17.297 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:30:58.987: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rf6hd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb  1 07:30:59.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:30:59.522: INFO: stderr: ""
Feb  1 07:30:59.522: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  1 07:30:59.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:30:59.624: INFO: stderr: ""
Feb  1 07:30:59.624: INFO: stdout: "update-demo-nautilus-j4m8z update-demo-nautilus-llw4b "
Feb  1 07:30:59.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-j4m8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:30:59.717: INFO: stderr: ""
Feb  1 07:30:59.718: INFO: stdout: ""
Feb  1 07:30:59.718: INFO: update-demo-nautilus-j4m8z is created but not running
Feb  1 07:31:04.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:04.816: INFO: stderr: ""
Feb  1 07:31:04.816: INFO: stdout: "update-demo-nautilus-j4m8z update-demo-nautilus-llw4b "
Feb  1 07:31:04.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-j4m8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:04.905: INFO: stderr: ""
Feb  1 07:31:04.905: INFO: stdout: "true"
Feb  1 07:31:04.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-j4m8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:05.002: INFO: stderr: ""
Feb  1 07:31:05.002: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:31:05.002: INFO: validating pod update-demo-nautilus-j4m8z
Feb  1 07:31:05.225: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:31:05.225: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:31:05.225: INFO: update-demo-nautilus-j4m8z is verified up and running
Feb  1 07:31:05.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-llw4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:05.312: INFO: stderr: ""
Feb  1 07:31:05.312: INFO: stdout: "true"
Feb  1 07:31:05.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods update-demo-nautilus-llw4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:05.417: INFO: stderr: ""
Feb  1 07:31:05.417: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  1 07:31:05.417: INFO: validating pod update-demo-nautilus-llw4b
Feb  1 07:31:05.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  1 07:31:05.425: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  1 07:31:05.425: INFO: update-demo-nautilus-llw4b is verified up and running
STEP: using delete to clean up resources
Feb  1 07:31:05.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:05.536: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  1 07:31:05.536: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  1 07:31:05.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-rf6hd'
Feb  1 07:31:05.639: INFO: stderr: "No resources found.\n"
Feb  1 07:31:05.639: INFO: stdout: ""
Feb  1 07:31:05.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 get pods -l name=update-demo --namespace=e2e-tests-kubectl-rf6hd -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  1 07:31:05.738: INFO: stderr: ""
Feb  1 07:31:05.738: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:31:05.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rf6hd" for this suite.
Feb  1 07:31:27.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:31:27.769: INFO: namespace: e2e-tests-kubectl-rf6hd, resource: bindings, ignored listing per whitelist
Feb  1 07:31:27.876: INFO: namespace e2e-tests-kubectl-rf6hd deletion completed in 22.131934963s

â€¢ [SLOW TEST:28.889 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:31:27.877: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-m2qjt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-j49d
STEP: Creating a pod to test atomic-volume-subpath
Feb  1 07:31:28.091: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j49d" in namespace "e2e-tests-subpath-m2qjt" to be "success or failure"
Feb  1 07:31:28.096: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709549ms
Feb  1 07:31:30.102: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010668838s
Feb  1 07:31:32.108: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017329922s
Feb  1 07:31:34.113: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021955071s
Feb  1 07:31:36.117: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 8.026431834s
Feb  1 07:31:38.122: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 10.031162881s
Feb  1 07:31:40.127: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 12.036086406s
Feb  1 07:31:42.132: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 14.041091397s
Feb  1 07:31:44.137: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 16.046071323s
Feb  1 07:31:46.143: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 18.052157789s
Feb  1 07:31:48.150: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 20.059509775s
Feb  1 07:31:50.159: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 22.067983951s
Feb  1 07:31:52.165: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 24.074100187s
Feb  1 07:31:54.170: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Running", Reason="", readiness=false. Elapsed: 26.079033049s
Feb  1 07:31:56.175: INFO: Pod "pod-subpath-test-configmap-j49d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.084046553s
STEP: Saw pod success
Feb  1 07:31:56.175: INFO: Pod "pod-subpath-test-configmap-j49d" satisfied condition "success or failure"
Feb  1 07:31:56.179: INFO: Trying to get logs from node pharos-worker-2 pod pod-subpath-test-configmap-j49d container test-container-subpath-configmap-j49d: <nil>
STEP: delete the pod
Feb  1 07:31:56.237: INFO: Waiting for pod pod-subpath-test-configmap-j49d to disappear
Feb  1 07:31:56.239: INFO: Pod pod-subpath-test-configmap-j49d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j49d
Feb  1 07:31:56.239: INFO: Deleting pod "pod-subpath-test-configmap-j49d" in namespace "e2e-tests-subpath-m2qjt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:31:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-m2qjt" for this suite.
Feb  1 07:32:02.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:32:02.356: INFO: namespace: e2e-tests-subpath-m2qjt, resource: bindings, ignored listing per whitelist
Feb  1 07:32:02.490: INFO: namespace e2e-tests-subpath-m2qjt deletion completed in 6.240794121s

â€¢ [SLOW TEST:34.613 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:32:02.490: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6m775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-77b8458b-25f3-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:32:02.750: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-6m775" to be "success or failure"
Feb  1 07:32:02.753: INFO: Pod "pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06439ms
Feb  1 07:32:04.757: INFO: Pod "pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006848285s
Feb  1 07:32:06.763: INFO: Pod "pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012688147s
STEP: Saw pod success
Feb  1 07:32:06.763: INFO: Pod "pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:32:06.766: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  1 07:32:06.818: INFO: Waiting for pod pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:32:06.821: INFO: Pod pod-projected-secrets-77b8d6bc-25f3-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:32:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6m775" for this suite.
Feb  1 07:32:12.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:32:12.898: INFO: namespace: e2e-tests-projected-6m775, resource: bindings, ignored listing per whitelist
Feb  1 07:32:13.009: INFO: namespace e2e-tests-projected-6m775 deletion completed in 6.183122486s

â€¢ [SLOW TEST:10.518 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:32:13.009: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bb7dg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:32:13.222: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:32:19.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bb7dg" for this suite.
Feb  1 07:32:57.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:32:57.502: INFO: namespace: e2e-tests-pods-bb7dg, resource: bindings, ignored listing per whitelist
Feb  1 07:32:57.533: INFO: namespace e2e-tests-pods-bb7dg deletion completed in 38.139873695s

â€¢ [SLOW TEST:44.524 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:32:57.535: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-cq9vm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-cq9vm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  1 07:32:57.726: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  1 07:33:21.826: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.3:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cq9vm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:33:21.827: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:33:21.963: INFO: Found all expected endpoints: [netserver-0]
Feb  1 07:33:21.967: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.5:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cq9vm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:33:21.968: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:33:22.079: INFO: Found all expected endpoints: [netserver-1]
Feb  1 07:33:22.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.43.0.2:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cq9vm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:33:22.083: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:33:22.393: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:33:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-cq9vm" for this suite.
Feb  1 07:33:44.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:33:44.508: INFO: namespace: e2e-tests-pod-network-test-cq9vm, resource: bindings, ignored listing per whitelist
Feb  1 07:33:44.538: INFO: namespace e2e-tests-pod-network-test-cq9vm deletion completed in 22.137598859s

â€¢ [SLOW TEST:47.004 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:33:44.539: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nlntd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:34:06.787: INFO: Container started at 2019-02-01 07:33:47 +0000 UTC, pod became ready at 2019-02-01 07:34:06 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:34:06.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nlntd" for this suite.
Feb  1 07:34:28.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:34:28.906: INFO: namespace: e2e-tests-container-probe-nlntd, resource: bindings, ignored listing per whitelist
Feb  1 07:34:28.925: INFO: namespace e2e-tests-container-probe-nlntd deletion completed in 22.13025408s

â€¢ [SLOW TEST:44.386 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:34:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-dsbhx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  1 07:34:39.201: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:39.214: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:41.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:41.223: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:43.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:43.219: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:45.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:45.220: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:47.215: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:47.222: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:49.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:49.221: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:51.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:51.220: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:53.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:53.219: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:55.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:55.221: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:57.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:57.218: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  1 07:34:59.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  1 07:34:59.219: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:34:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dsbhx" for this suite.
Feb  1 07:35:21.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:35:21.341: INFO: namespace: e2e-tests-container-lifecycle-hook-dsbhx, resource: bindings, ignored listing per whitelist
Feb  1 07:35:21.403: INFO: namespace e2e-tests-container-lifecycle-hook-dsbhx deletion completed in 22.162933262s

â€¢ [SLOW TEST:52.476 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:35:21.404: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nfckw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-ee436fd4-25f3-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:35:21.633: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-nfckw" to be "success or failure"
Feb  1 07:35:21.635: INFO: Pod "pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.702386ms
Feb  1 07:35:23.641: INFO: Pod "pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008279537s
Feb  1 07:35:25.647: INFO: Pod "pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013926759s
STEP: Saw pod success
Feb  1 07:35:25.647: INFO: Pod "pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:35:25.651: INFO: Trying to get logs from node pharos-worker-2 pod pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:35:25.681: INFO: Waiting for pod pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:35:25.684: INFO: Pod pod-projected-configmaps-ee441178-25f3-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:35:25.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nfckw" for this suite.
Feb  1 07:35:31.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:35:31.780: INFO: namespace: e2e-tests-projected-nfckw, resource: bindings, ignored listing per whitelist
Feb  1 07:35:31.836: INFO: namespace e2e-tests-projected-nfckw deletion completed in 6.144858743s

â€¢ [SLOW TEST:10.432 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:35:31.837: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-r2x6l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  1 07:35:32.049: INFO: Waiting up to 5m0s for pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-r2x6l" to be "success or failure"
Feb  1 07:35:32.053: INFO: Pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719723ms
Feb  1 07:35:34.060: INFO: Pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010611645s
Feb  1 07:35:36.066: INFO: Pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016787062s
Feb  1 07:35:38.073: INFO: Pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023766116s
STEP: Saw pod success
Feb  1 07:35:38.073: INFO: Pod "pod-f479ab9f-25f3-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:35:38.079: INFO: Trying to get logs from node pharos-worker-0 pod pod-f479ab9f-25f3-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:35:38.120: INFO: Waiting for pod pod-f479ab9f-25f3-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:35:38.122: INFO: Pod pod-f479ab9f-25f3-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:35:38.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r2x6l" for this suite.
Feb  1 07:35:44.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:35:44.248: INFO: namespace: e2e-tests-emptydir-r2x6l, resource: bindings, ignored listing per whitelist
Feb  1 07:35:44.264: INFO: namespace e2e-tests-emptydir-r2x6l deletion completed in 6.134876065s

â€¢ [SLOW TEST:12.427 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:35:44.265: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-n4fhj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:35:44.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-n4fhj" for this suite.
Feb  1 07:35:50.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:35:50.628: INFO: namespace: e2e-tests-kubelet-test-n4fhj, resource: bindings, ignored listing per whitelist
Feb  1 07:35:50.683: INFO: namespace e2e-tests-kubelet-test-n4fhj deletion completed in 6.150982502s

â€¢ [SLOW TEST:6.418 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:35:50.684: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-m6gfb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  1 07:35:50.956: INFO: Waiting up to 5m0s for pod "pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-m6gfb" to be "success or failure"
Feb  1 07:35:50.968: INFO: Pod "pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.467294ms
Feb  1 07:35:52.973: INFO: Pod "pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016683985s
Feb  1 07:35:54.978: INFO: Pod "pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021249571s
STEP: Saw pod success
Feb  1 07:35:54.978: INFO: Pod "pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:35:54.981: INFO: Trying to get logs from node pharos-worker-0 pod pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:35:55.037: INFO: Waiting for pod pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:35:55.040: INFO: Pod pod-ffbe89a6-25f3-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:35:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m6gfb" for this suite.
Feb  1 07:36:01.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:36:01.089: INFO: namespace: e2e-tests-emptydir-m6gfb, resource: bindings, ignored listing per whitelist
Feb  1 07:36:01.189: INFO: namespace e2e-tests-emptydir-m6gfb deletion completed in 6.144136816s

â€¢ [SLOW TEST:10.505 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:36:01.190: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qfz8v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-05fbedc8-25f4-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:36:01.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-qfz8v" to be "success or failure"
Feb  1 07:36:01.437: INFO: Pod "pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047386ms
Feb  1 07:36:03.455: INFO: Pod "pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021762458s
Feb  1 07:36:05.465: INFO: Pod "pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030934551s
STEP: Saw pod success
Feb  1 07:36:05.465: INFO: Pod "pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:36:05.484: INFO: Trying to get logs from node pharos-worker-2 pod pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:36:05.515: INFO: Waiting for pod pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:36:05.524: INFO: Pod pod-projected-configmaps-05fc7577-25f4-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:36:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qfz8v" for this suite.
Feb  1 07:36:11.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:36:11.614: INFO: namespace: e2e-tests-projected-qfz8v, resource: bindings, ignored listing per whitelist
Feb  1 07:36:11.664: INFO: namespace e2e-tests-projected-qfz8v deletion completed in 6.131287265s

â€¢ [SLOW TEST:10.475 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:36:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-qbkjs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb  1 07:36:11.883: INFO: Waiting up to 5m0s for pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80" in namespace "e2e-tests-var-expansion-qbkjs" to be "success or failure"
Feb  1 07:36:11.889: INFO: Pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085787ms
Feb  1 07:36:13.893: INFO: Pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009326481s
Feb  1 07:36:15.898: INFO: Pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014475031s
Feb  1 07:36:17.906: INFO: Pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022666463s
STEP: Saw pod success
Feb  1 07:36:17.907: INFO: Pod "var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:36:17.911: INFO: Trying to get logs from node pharos-worker-0 pod var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80 container dapi-container: <nil>
STEP: delete the pod
Feb  1 07:36:17.938: INFO: Waiting for pod var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:36:17.942: INFO: Pod var-expansion-0c368dd8-25f4-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:36:17.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qbkjs" for this suite.
Feb  1 07:36:23.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:36:24.097: INFO: namespace: e2e-tests-var-expansion-qbkjs, resource: bindings, ignored listing per whitelist
Feb  1 07:36:24.109: INFO: namespace e2e-tests-var-expansion-qbkjs deletion completed in 6.15748646s

â€¢ [SLOW TEST:12.445 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:36:24.110: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-rfdss
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  1 07:39:13.412: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:13.418: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:15.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:15.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:17.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:17.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:19.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:19.438: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:21.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:21.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:23.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:23.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:25.419: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:25.426: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:27.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:27.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:29.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:29.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:31.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:31.440: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:33.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:33.427: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:35.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:35.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:37.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:37.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:39.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:39.421: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:41.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:41.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:43.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:43.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:45.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:45.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:47.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:47.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:49.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:49.431: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:51.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:51.425: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:53.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:53.425: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:55.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:55.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:57.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:57.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:39:59.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:39:59.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:01.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:01.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:03.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:03.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:05.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:05.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:07.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:07.426: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:09.419: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:09.427: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:11.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:11.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:13.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:13.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:15.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:15.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:17.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:17.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:19.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:19.425: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:21.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:21.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:23.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:23.430: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:25.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:25.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:27.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:27.426: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:29.419: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:29.425: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:31.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:31.429: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:33.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:33.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:35.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:35.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:37.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:37.422: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:39.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:39.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:41.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:41.425: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:43.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:43.434: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:45.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:45.432: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:47.419: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:47.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:49.419: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:49.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:51.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:51.423: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:53.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:53.422: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:55.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:55.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:57.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:57.424: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:40:59.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:40:59.426: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:41:01.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:41:01.422: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  1 07:41:03.424: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  1 07:41:03.429: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:41:03.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-rfdss" for this suite.
Feb  1 07:41:25.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:41:25.477: INFO: namespace: e2e-tests-container-lifecycle-hook-rfdss, resource: bindings, ignored listing per whitelist
Feb  1 07:41:25.563: INFO: namespace e2e-tests-container-lifecycle-hook-rfdss deletion completed in 22.127203969s

â€¢ [SLOW TEST:301.453 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:41:25.564: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-pfg85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb  1 07:41:25.763: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:41:36.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-pfg85" for this suite.
Feb  1 07:41:58.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:41:58.475: INFO: namespace: e2e-tests-init-container-pfg85, resource: bindings, ignored listing per whitelist
Feb  1 07:41:58.570: INFO: namespace e2e-tests-init-container-pfg85 deletion completed in 22.156972265s

â€¢ [SLOW TEST:33.006 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:41:58.571: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-sxblg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  1 07:41:58.781: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  1 07:41:58.792: INFO: Waiting for terminating namespaces to be deleted...
Feb  1 07:41:58.796: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-0 before test
Feb  1 07:41:58.815: INFO: pharos-proxy-pharos-worker-0 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:41:58.815: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-01 06:19:57 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  1 07:41:58.815: INFO: weave-net-lm92h from kube-system started at 2019-02-01 06:08:43 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:41:58.815: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:41:58.815: INFO: pharos-telemetry-1549004400-mqq2t from kube-system started at 2019-02-01 07:00:00 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container agent ready: false, restart count 0
Feb  1 07:41:58.815: INFO: kube-proxy-xnlj8 from kube-system started at 2019-02-01 06:08:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:41:58.815: INFO: nginx-ingress-controller-mcdm5 from ingress-nginx started at 2019-02-01 06:09:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:41:58.815: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-w94p8 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.815: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:41:58.815: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  1 07:41:58.815: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-1 before test
Feb  1 07:41:58.833: INFO: pharos-proxy-pharos-worker-1 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:41:58.833: INFO: weave-net-b6kjz from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container weave ready: true, restart count 0
Feb  1 07:41:58.833: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:41:58.833: INFO: default-http-backend-6cf7bc5976-g6fn7 from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:41:58.833: INFO: coredns-5567f4d9b9-4h2rl from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container coredns ready: true, restart count 0
Feb  1 07:41:58.833: INFO: kube-proxy-xrxnk from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:41:58.833: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-7gmgs from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:41:58.833: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  1 07:41:58.833: INFO: metrics-server-8549f8c7bf-frntn from kube-system started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container metrics-server ready: true, restart count 0
Feb  1 07:41:58.833: INFO: nginx-ingress-controller-ptwhf from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:41:58.833: INFO: sonobuoy-e2e-job-52af506353674b35 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.833: INFO: 	Container e2e ready: true, restart count 0
Feb  1 07:41:58.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:41:58.833: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-2 before test
Feb  1 07:41:58.843: INFO: default-http-backend-6cf7bc5976-kvnrt from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.843: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:41:58.843: INFO: pharos-proxy-pharos-worker-2 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:41:58.843: INFO: kube-proxy-xnpc2 from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.843: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:41:58.843: INFO: weave-net-crgqw from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.843: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:41:58.843: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:41:58.843: INFO: nginx-ingress-controller-2p9l6 from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:41:58.843: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:41:58.843: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-nxrpt from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:41:58.843: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:41:58.843: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dd6d74b4-25f4-11e9-b51c-8292ded0de80 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-dd6d74b4-25f4-11e9-b51c-8292ded0de80 off the node pharos-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dd6d74b4-25f4-11e9-b51c-8292ded0de80
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:42:06.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-sxblg" for this suite.
Feb  1 07:42:24.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:42:25.045: INFO: namespace: e2e-tests-sched-pred-sxblg, resource: bindings, ignored listing per whitelist
Feb  1 07:42:25.054: INFO: namespace e2e-tests-sched-pred-sxblg deletion completed in 18.13504566s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:26.484 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:42:25.055: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jnn4d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb  1 07:42:25.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 cluster-info'
Feb  1 07:42:25.516: INFO: stderr: ""
Feb  1 07:42:25.516: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:42:25.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jnn4d" for this suite.
Feb  1 07:42:31.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:42:31.572: INFO: namespace: e2e-tests-kubectl-jnn4d, resource: bindings, ignored listing per whitelist
Feb  1 07:42:31.656: INFO: namespace e2e-tests-kubectl-jnn4d deletion completed in 6.133505013s

â€¢ [SLOW TEST:6.601 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:42:31.657: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9sqvl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  1 07:42:31.882: INFO: Waiting up to 5m0s for pod "pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-9sqvl" to be "success or failure"
Feb  1 07:42:31.885: INFO: Pod "pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327929ms
Feb  1 07:42:33.891: INFO: Pod "pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008713266s
Feb  1 07:42:35.895: INFO: Pod "pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013157966s
STEP: Saw pod success
Feb  1 07:42:35.895: INFO: Pod "pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:42:35.899: INFO: Trying to get logs from node pharos-worker-2 pod pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:42:35.917: INFO: Waiting for pod pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:42:35.919: INFO: Pod pod-eeb6c1f5-25f4-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:42:35.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9sqvl" for this suite.
Feb  1 07:42:41.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:42:42.003: INFO: namespace: e2e-tests-emptydir-9sqvl, resource: bindings, ignored listing per whitelist
Feb  1 07:42:42.056: INFO: namespace e2e-tests-emptydir-9sqvl deletion completed in 6.133371384s

â€¢ [SLOW TEST:10.399 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:42:42.057: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-6l6lc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-6l6lc
Feb  1 07:42:48.277: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-6l6lc
STEP: checking the pod's current state and verifying that restartCount is present
Feb  1 07:42:48.280: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:46:49.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-6l6lc" for this suite.
Feb  1 07:46:55.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:46:55.236: INFO: namespace: e2e-tests-container-probe-6l6lc, resource: bindings, ignored listing per whitelist
Feb  1 07:46:55.289: INFO: namespace e2e-tests-container-probe-6l6lc deletion completed in 6.173471549s

â€¢ [SLOW TEST:253.232 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:46:55.290: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kk4fw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:46:55.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-kk4fw" to be "success or failure"
Feb  1 07:46:55.517: INFO: Pod "downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510151ms
Feb  1 07:46:57.523: INFO: Pod "downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010046942s
Feb  1 07:46:59.529: INFO: Pod "downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015569775s
STEP: Saw pod success
Feb  1 07:46:59.529: INFO: Pod "downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:46:59.533: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:46:59.595: INFO: Waiting for pod downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:46:59.598: INFO: Pod downwardapi-volume-8bd9d2ae-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:46:59.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kk4fw" for this suite.
Feb  1 07:47:05.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:47:05.745: INFO: namespace: e2e-tests-projected-kk4fw, resource: bindings, ignored listing per whitelist
Feb  1 07:47:05.755: INFO: namespace e2e-tests-projected-kk4fw deletion completed in 6.145995906s

â€¢ [SLOW TEST:10.465 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:47:05.755: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-bsktc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb  1 07:47:05.949: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  1 07:47:05.963: INFO: Waiting for terminating namespaces to be deleted...
Feb  1 07:47:05.968: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-0 before test
Feb  1 07:47:05.981: INFO: pharos-telemetry-1549004400-mqq2t from kube-system started at 2019-02-01 07:00:00 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container agent ready: false, restart count 0
Feb  1 07:47:05.981: INFO: weave-net-lm92h from kube-system started at 2019-02-01 06:08:43 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:47:05.981: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:47:05.981: INFO: nginx-ingress-controller-mcdm5 from ingress-nginx started at 2019-02-01 06:09:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:47:05.981: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-w94p8 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:47:05.981: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  1 07:47:05.981: INFO: kube-proxy-xnlj8 from kube-system started at 2019-02-01 06:08:43 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:47:05.981: INFO: sonobuoy from heptio-sonobuoy started at 2019-02-01 06:19:57 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.981: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  1 07:47:05.981: INFO: pharos-proxy-pharos-worker-0 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:47:05.981: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-1 before test
Feb  1 07:47:05.993: INFO: pharos-proxy-pharos-worker-1 from kube-system started at <nil> (0 container statuses recorded)
Feb  1 07:47:05.993: INFO: weave-net-b6kjz from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container weave ready: true, restart count 0
Feb  1 07:47:05.993: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:47:05.993: INFO: default-http-backend-6cf7bc5976-g6fn7 from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:47:05.993: INFO: coredns-5567f4d9b9-4h2rl from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container coredns ready: true, restart count 0
Feb  1 07:47:05.993: INFO: kube-proxy-xrxnk from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:47:05.993: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-7gmgs from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:47:05.993: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  1 07:47:05.993: INFO: metrics-server-8549f8c7bf-frntn from kube-system started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container metrics-server ready: true, restart count 0
Feb  1 07:47:05.993: INFO: nginx-ingress-controller-ptwhf from ingress-nginx started at 2019-02-01 06:09:02 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:47:05.993: INFO: sonobuoy-e2e-job-52af506353674b35 from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:05.993: INFO: 	Container e2e ready: true, restart count 0
Feb  1 07:47:05.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  1 07:47:05.993: INFO: 
Logging pods the kubelet thinks is on node pharos-worker-2 before test
Feb  1 07:47:06.000: INFO: weave-net-crgqw from kube-system started at 2019-02-01 06:08:42 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:06.000: INFO: 	Container weave ready: true, restart count 1
Feb  1 07:47:06.000: INFO: 	Container weave-npc ready: true, restart count 0
Feb  1 07:47:06.000: INFO: nginx-ingress-controller-2p9l6 from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:06.000: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb  1 07:47:06.000: INFO: sonobuoy-systemd-logs-daemon-set-ff006445a7294224-nxrpt from heptio-sonobuoy started at 2019-02-01 06:20:05 +0000 UTC (2 container statuses recorded)
Feb  1 07:47:06.000: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Feb  1 07:47:06.000: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb  1 07:47:06.000: INFO: kube-proxy-xnpc2 from kube-system started at 2019-02-01 06:08:42 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:06.000: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  1 07:47:06.000: INFO: default-http-backend-6cf7bc5976-kvnrt from ingress-nginx started at 2019-02-01 06:09:32 +0000 UTC (1 container statuses recorded)
Feb  1 07:47:06.000: INFO: 	Container default-http-backend ready: true, restart count 0
Feb  1 07:47:06.000: INFO: pharos-proxy-pharos-worker-2 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.157f2ddd75d287eb], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:47:07.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-bsktc" for this suite.
Feb  1 07:47:13.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:47:13.157: INFO: namespace: e2e-tests-sched-pred-bsktc, resource: bindings, ignored listing per whitelist
Feb  1 07:47:13.177: INFO: namespace e2e-tests-sched-pred-bsktc deletion completed in 6.130825681s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.422 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:47:13.178: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zbx9d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb  1 07:47:13.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 create -f - --namespace=e2e-tests-kubectl-zbx9d'
Feb  1 07:47:13.675: INFO: stderr: ""
Feb  1 07:47:13.675: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb  1 07:47:14.681: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:14.681: INFO: Found 0 / 1
Feb  1 07:47:15.679: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:15.679: INFO: Found 0 / 1
Feb  1 07:47:16.680: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:16.680: INFO: Found 0 / 1
Feb  1 07:47:17.681: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:17.681: INFO: Found 1 / 1
Feb  1 07:47:17.681: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb  1 07:47:17.686: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:17.686: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  1 07:47:17.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 patch pod redis-master-qfjzs --namespace=e2e-tests-kubectl-zbx9d -p {"metadata":{"annotations":{"x":"y"}}}'
Feb  1 07:47:17.789: INFO: stderr: ""
Feb  1 07:47:17.789: INFO: stdout: "pod/redis-master-qfjzs patched\n"
STEP: checking annotations
Feb  1 07:47:17.793: INFO: Selector matched 1 pods for map[app:redis]
Feb  1 07:47:17.793: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:47:17.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zbx9d" for this suite.
Feb  1 07:47:39.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:47:39.877: INFO: namespace: e2e-tests-kubectl-zbx9d, resource: bindings, ignored listing per whitelist
Feb  1 07:47:39.915: INFO: namespace e2e-tests-kubectl-zbx9d deletion completed in 22.117699973s

â€¢ [SLOW TEST:26.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:47:39.916: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xh59p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  1 07:47:40.123: INFO: Waiting up to 5m0s for pod "pod-a67128ad-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-xh59p" to be "success or failure"
Feb  1 07:47:40.129: INFO: Pod "pod-a67128ad-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.26798ms
Feb  1 07:47:42.135: INFO: Pod "pod-a67128ad-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011865403s
Feb  1 07:47:44.139: INFO: Pod "pod-a67128ad-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016395424s
STEP: Saw pod success
Feb  1 07:47:44.139: INFO: Pod "pod-a67128ad-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:47:44.143: INFO: Trying to get logs from node pharos-worker-2 pod pod-a67128ad-25f5-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:47:44.164: INFO: Waiting for pod pod-a67128ad-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:47:44.167: INFO: Pod pod-a67128ad-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:47:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xh59p" for this suite.
Feb  1 07:47:50.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:47:50.254: INFO: namespace: e2e-tests-emptydir-xh59p, resource: bindings, ignored listing per whitelist
Feb  1 07:47:50.323: INFO: namespace e2e-tests-emptydir-xh59p deletion completed in 6.149736449s

â€¢ [SLOW TEST:10.407 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:47:50.325: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zjdrc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:47:50.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-zjdrc" to be "success or failure"
Feb  1 07:47:50.535: INFO: Pod "downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588908ms
Feb  1 07:47:52.541: INFO: Pod "downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008962232s
Feb  1 07:47:54.546: INFO: Pod "downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014301139s
STEP: Saw pod success
Feb  1 07:47:54.546: INFO: Pod "downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:47:54.550: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:47:54.576: INFO: Waiting for pod downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:47:54.579: INFO: Pod downwardapi-volume-aca4ef96-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:47:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zjdrc" for this suite.
Feb  1 07:48:00.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:48:00.739: INFO: namespace: e2e-tests-downward-api-zjdrc, resource: bindings, ignored listing per whitelist
Feb  1 07:48:00.744: INFO: namespace e2e-tests-downward-api-zjdrc deletion completed in 6.159310133s

â€¢ [SLOW TEST:10.419 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:48:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-lwp8z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b2dbf189-25f5-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume secrets
Feb  1 07:48:00.966: INFO: Waiting up to 5m0s for pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-secrets-lwp8z" to be "success or failure"
Feb  1 07:48:00.969: INFO: Pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.172976ms
Feb  1 07:48:02.979: INFO: Pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013057964s
Feb  1 07:48:04.986: INFO: Pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019947259s
Feb  1 07:48:06.991: INFO: Pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025060172s
STEP: Saw pod success
Feb  1 07:48:06.991: INFO: Pod "pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:48:06.998: INFO: Trying to get logs from node pharos-worker-2 pod pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80 container secret-env-test: <nil>
STEP: delete the pod
Feb  1 07:48:07.023: INFO: Waiting for pod pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:48:07.026: INFO: Pod pod-secrets-b2dda6bc-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:48:07.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lwp8z" for this suite.
Feb  1 07:48:13.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:48:13.169: INFO: namespace: e2e-tests-secrets-lwp8z, resource: bindings, ignored listing per whitelist
Feb  1 07:48:13.184: INFO: namespace e2e-tests-secrets-lwp8z deletion completed in 6.151140799s

â€¢ [SLOW TEST:12.440 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:48:13.185: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-q9mbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:48:13.442: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-q9mbp" to be "success or failure"
Feb  1 07:48:13.445: INFO: Pod "downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021561ms
Feb  1 07:48:15.450: INFO: Pod "downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007691305s
Feb  1 07:48:17.456: INFO: Pod "downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013754955s
STEP: Saw pod success
Feb  1 07:48:17.456: INFO: Pod "downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:48:17.468: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:48:17.518: INFO: Waiting for pod downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:48:17.528: INFO: Pod downwardapi-volume-ba4d07e7-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:48:17.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q9mbp" for this suite.
Feb  1 07:48:23.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:48:23.645: INFO: namespace: e2e-tests-downward-api-q9mbp, resource: bindings, ignored listing per whitelist
Feb  1 07:48:23.690: INFO: namespace e2e-tests-downward-api-q9mbp deletion completed in 6.155203752s

â€¢ [SLOW TEST:10.505 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:48:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4k4n7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-8n4l
STEP: Creating a pod to test atomic-volume-subpath
Feb  1 07:48:24.006: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8n4l" in namespace "e2e-tests-subpath-4k4n7" to be "success or failure"
Feb  1 07:48:24.010: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Pending", Reason="", readiness=false. Elapsed: 3.059489ms
Feb  1 07:48:26.015: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008227251s
Feb  1 07:48:28.025: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018658723s
Feb  1 07:48:30.031: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024645757s
Feb  1 07:48:32.038: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 8.031355487s
Feb  1 07:48:34.044: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 10.03767336s
Feb  1 07:48:36.050: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 12.043090308s
Feb  1 07:48:38.056: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 14.049517206s
Feb  1 07:48:40.095: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 16.088894686s
Feb  1 07:48:42.100: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 18.093729755s
Feb  1 07:48:44.106: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 20.099737468s
Feb  1 07:48:46.114: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 22.107231055s
Feb  1 07:48:48.119: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Running", Reason="", readiness=false. Elapsed: 24.112911064s
Feb  1 07:48:50.125: INFO: Pod "pod-subpath-test-projected-8n4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.11808288s
STEP: Saw pod success
Feb  1 07:48:50.125: INFO: Pod "pod-subpath-test-projected-8n4l" satisfied condition "success or failure"
Feb  1 07:48:50.129: INFO: Trying to get logs from node pharos-worker-2 pod pod-subpath-test-projected-8n4l container test-container-subpath-projected-8n4l: <nil>
STEP: delete the pod
Feb  1 07:48:50.153: INFO: Waiting for pod pod-subpath-test-projected-8n4l to disappear
Feb  1 07:48:50.155: INFO: Pod pod-subpath-test-projected-8n4l no longer exists
STEP: Deleting pod pod-subpath-test-projected-8n4l
Feb  1 07:48:50.156: INFO: Deleting pod "pod-subpath-test-projected-8n4l" in namespace "e2e-tests-subpath-4k4n7"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:48:50.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4k4n7" for this suite.
Feb  1 07:48:56.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:48:56.234: INFO: namespace: e2e-tests-subpath-4k4n7, resource: bindings, ignored listing per whitelist
Feb  1 07:48:56.300: INFO: namespace e2e-tests-subpath-4k4n7 deletion completed in 6.132837202s

â€¢ [SLOW TEST:32.609 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:48:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-n2k6l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:48:56.543: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-n2k6l" to be "success or failure"
Feb  1 07:48:56.546: INFO: Pod "downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444773ms
Feb  1 07:48:58.551: INFO: Pod "downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008163781s
Feb  1 07:49:00.557: INFO: Pod "downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013825626s
STEP: Saw pod success
Feb  1 07:49:00.557: INFO: Pod "downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:49:00.560: INFO: Trying to get logs from node pharos-worker-0 pod downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:49:00.585: INFO: Waiting for pod downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:49:00.587: INFO: Pod downwardapi-volume-d3fdcab4-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:49:00.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n2k6l" for this suite.
Feb  1 07:49:06.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:49:06.643: INFO: namespace: e2e-tests-downward-api-n2k6l, resource: bindings, ignored listing per whitelist
Feb  1 07:49:06.737: INFO: namespace e2e-tests-downward-api-n2k6l deletion completed in 6.14420908s

â€¢ [SLOW TEST:10.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:49:06.737: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-c74hc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb  1 07:49:10.957: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-da2ecbcf-25f5-11e9-b51c-8292ded0de80,GenerateName:,Namespace:e2e-tests-events-c74hc,SelfLink:/api/v1/namespaces/e2e-tests-events-c74hc/pods/send-events-da2ecbcf-25f5-11e9-b51c-8292ded0de80,UID:da2ed944-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21413,Generation:0,CreationTimestamp:2019-02-01 07:49:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 922606238,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rf2z9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rf2z9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rf2z9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pharos-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6e080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6e0a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:49:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:49:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:49:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:49:06 +0000 UTC  }],Message:,Reason:,HostIP:95.216.222.53,PodIP:10.32.0.3,StartTime:2019-02-01 07:49:06 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-01 07:49:09 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://0401c4102b105aab68fd66da548ad776405e9501b0a91cc655bc422d903a8e33}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb  1 07:49:12.967: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb  1 07:49:14.973: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:49:14.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-c74hc" for this suite.
Feb  1 07:49:52.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:49:53.016: INFO: namespace: e2e-tests-events-c74hc, resource: bindings, ignored listing per whitelist
Feb  1 07:49:53.133: INFO: namespace e2e-tests-events-c74hc deletion completed in 38.150830332s

â€¢ [SLOW TEST:46.396 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:49:53.135: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5hpg7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f5dabe08-25f5-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 07:49:53.359: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-5hpg7" to be "success or failure"
Feb  1 07:49:53.362: INFO: Pod "pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.897177ms
Feb  1 07:49:55.369: INFO: Pod "pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009876541s
Feb  1 07:49:57.377: INFO: Pod "pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017623591s
STEP: Saw pod success
Feb  1 07:49:57.378: INFO: Pod "pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:49:57.381: INFO: Trying to get logs from node pharos-worker-0 pod pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  1 07:49:57.407: INFO: Waiting for pod pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:49:57.410: INFO: Pod pod-projected-configmaps-f5db4a23-25f5-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:49:57.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5hpg7" for this suite.
Feb  1 07:50:03.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:50:03.475: INFO: namespace: e2e-tests-projected-5hpg7, resource: bindings, ignored listing per whitelist
Feb  1 07:50:03.551: INFO: namespace e2e-tests-projected-5hpg7 deletion completed in 6.134802171s

â€¢ [SLOW TEST:10.417 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:50:03.552: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-wh52s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb  1 07:50:03.745: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21551,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  1 07:50:03.745: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21551,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb  1 07:50:13.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21569,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  1 07:50:13.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21569,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb  1 07:50:23.768: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21586,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  1 07:50:23.769: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21586,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb  1 07:50:33.778: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21603,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  1 07:50:33.778: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-a,UID:fc0c5a31-25f5-11e9-90b2-96000019ce9c,ResourceVersion:21603,Generation:0,CreationTimestamp:2019-02-01 07:50:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb  1 07:50:43.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-b,UID:13e9c809-25f6-11e9-90b2-96000019ce9c,ResourceVersion:21621,Generation:0,CreationTimestamp:2019-02-01 07:50:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  1 07:50:43.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-b,UID:13e9c809-25f6-11e9-90b2-96000019ce9c,ResourceVersion:21621,Generation:0,CreationTimestamp:2019-02-01 07:50:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb  1 07:50:53.830: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-b,UID:13e9c809-25f6-11e9-90b2-96000019ce9c,ResourceVersion:21638,Generation:0,CreationTimestamp:2019-02-01 07:50:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  1 07:50:53.830: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-wh52s,SelfLink:/api/v1/namespaces/e2e-tests-watch-wh52s/configmaps/e2e-watch-test-configmap-b,UID:13e9c809-25f6-11e9-90b2-96000019ce9c,ResourceVersion:21638,Generation:0,CreationTimestamp:2019-02-01 07:50:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:51:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-wh52s" for this suite.
Feb  1 07:51:09.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:51:09.948: INFO: namespace: e2e-tests-watch-wh52s, resource: bindings, ignored listing per whitelist
Feb  1 07:51:09.988: INFO: namespace e2e-tests-watch-wh52s deletion completed in 6.148379685s

â€¢ [SLOW TEST:66.437 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:51:09.989: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kb9xd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 07:51:10.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80" in namespace "e2e-tests-projected-kb9xd" to be "success or failure"
Feb  1 07:51:10.223: INFO: Pod "downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240983ms
Feb  1 07:51:12.231: INFO: Pod "downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012069739s
Feb  1 07:51:14.236: INFO: Pod "downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016855931s
STEP: Saw pod success
Feb  1 07:51:14.236: INFO: Pod "downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:51:14.240: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 07:51:14.261: INFO: Waiting for pod downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:51:14.263: INFO: Pod downwardapi-volume-23ab1707-25f6-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:51:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kb9xd" for this suite.
Feb  1 07:51:20.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:51:20.328: INFO: namespace: e2e-tests-projected-kb9xd, resource: bindings, ignored listing per whitelist
Feb  1 07:51:20.412: INFO: namespace e2e-tests-projected-kb9xd deletion completed in 6.143585058s

â€¢ [SLOW TEST:10.422 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:51:20.412: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-mjtk7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb  1 07:51:21.131: INFO: Waiting up to 5m0s for pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml" in namespace "e2e-tests-svcaccounts-mjtk7" to be "success or failure"
Feb  1 07:51:21.135: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942478ms
Feb  1 07:51:23.142: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011179626s
Feb  1 07:51:25.152: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021008081s
Feb  1 07:51:27.159: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027908017s
Feb  1 07:51:29.164: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03326844s
STEP: Saw pod success
Feb  1 07:51:29.164: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml" satisfied condition "success or failure"
Feb  1 07:51:29.168: INFO: Trying to get logs from node pharos-worker-0 pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml container token-test: <nil>
STEP: delete the pod
Feb  1 07:51:29.197: INFO: Waiting for pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml to disappear
Feb  1 07:51:29.203: INFO: Pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-g9mml no longer exists
STEP: Creating a pod to test consume service account root CA
Feb  1 07:51:29.217: INFO: Waiting up to 5m0s for pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh" in namespace "e2e-tests-svcaccounts-mjtk7" to be "success or failure"
Feb  1 07:51:29.225: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.468198ms
Feb  1 07:51:31.230: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013503746s
Feb  1 07:51:33.235: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017954443s
Feb  1 07:51:35.240: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0225979s
Feb  1 07:51:37.244: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027461167s
STEP: Saw pod success
Feb  1 07:51:37.244: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh" satisfied condition "success or failure"
Feb  1 07:51:37.247: INFO: Trying to get logs from node pharos-worker-2 pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh container root-ca-test: <nil>
STEP: delete the pod
Feb  1 07:51:37.265: INFO: Waiting for pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh to disappear
Feb  1 07:51:37.268: INFO: Pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-6qhzh no longer exists
STEP: Creating a pod to test consume service account namespace
Feb  1 07:51:37.277: INFO: Waiting up to 5m0s for pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs" in namespace "e2e-tests-svcaccounts-mjtk7" to be "success or failure"
Feb  1 07:51:37.281: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282079ms
Feb  1 07:51:39.286: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008982682s
Feb  1 07:51:41.292: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014497662s
Feb  1 07:51:43.298: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020708971s
Feb  1 07:51:45.305: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027457501s
STEP: Saw pod success
Feb  1 07:51:45.305: INFO: Pod "pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs" satisfied condition "success or failure"
Feb  1 07:51:45.311: INFO: Trying to get logs from node pharos-worker-0 pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs container namespace-test: <nil>
STEP: delete the pod
Feb  1 07:51:45.337: INFO: Waiting for pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs to disappear
Feb  1 07:51:45.340: INFO: Pod pod-service-account-2a2b7dc5-25f6-11e9-b51c-8292ded0de80-2gnjs no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:51:45.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-mjtk7" for this suite.
Feb  1 07:51:51.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:51:51.424: INFO: namespace: e2e-tests-svcaccounts-mjtk7, resource: bindings, ignored listing per whitelist
Feb  1 07:51:51.472: INFO: namespace e2e-tests-svcaccounts-mjtk7 deletion completed in 6.125455839s

â€¢ [SLOW TEST:31.061 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:51:51.474: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-m7pnm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-m7pnm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  1 07:51:51.664: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb  1 07:52:15.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=udp&host=10.40.0.5&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-m7pnm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:52:15.769: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:52:16.110: INFO: Waiting for endpoints: map[]
Feb  1 07:52:16.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=udp&host=10.32.0.3&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-m7pnm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:52:16.114: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:52:16.326: INFO: Waiting for endpoints: map[]
Feb  1 07:52:16.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.4:8080/dial?request=hostName&protocol=udp&host=10.43.0.2&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-m7pnm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb  1 07:52:16.333: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
Feb  1 07:52:16.578: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:52:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-m7pnm" for this suite.
Feb  1 07:52:38.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:52:38.622: INFO: namespace: e2e-tests-pod-network-test-m7pnm, resource: bindings, ignored listing per whitelist
Feb  1 07:52:38.715: INFO: namespace e2e-tests-pod-network-test-m7pnm deletion completed in 22.127000923s

â€¢ [SLOW TEST:47.242 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:52:38.717: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-849vj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb  1 07:52:38.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 --namespace=e2e-tests-kubectl-849vj run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb  1 07:52:44.077: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb  1 07:52:44.077: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:52:46.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-849vj" for this suite.
Feb  1 07:52:54.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:52:54.222: INFO: namespace: e2e-tests-kubectl-849vj, resource: bindings, ignored listing per whitelist
Feb  1 07:52:54.227: INFO: namespace e2e-tests-kubectl-849vj deletion completed in 8.131050599s

â€¢ [SLOW TEST:15.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:52:54.229: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7cvxj
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-61c9f944-25f6-11e9-b51c-8292ded0de80
STEP: Creating secret with name s-test-opt-upd-61c9fa1e-25f6-11e9-b51c-8292ded0de80
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-61c9f944-25f6-11e9-b51c-8292ded0de80
STEP: Updating secret s-test-opt-upd-61c9fa1e-25f6-11e9-b51c-8292ded0de80
STEP: Creating secret with name s-test-opt-create-61c9fa4e-25f6-11e9-b51c-8292ded0de80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:54:11.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7cvxj" for this suite.
Feb  1 07:54:33.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:54:33.389: INFO: namespace: e2e-tests-secrets-7cvxj, resource: bindings, ignored listing per whitelist
Feb  1 07:54:33.453: INFO: namespace e2e-tests-secrets-7cvxj deletion completed in 22.156002693s

â€¢ [SLOW TEST:99.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:54:33.455: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-jl589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-jl589
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-jl589
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-jl589
Feb  1 07:54:33.678: INFO: Found 0 stateful pods, waiting for 1
Feb  1 07:54:43.685: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb  1 07:54:43.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:54:43.929: INFO: stderr: ""
Feb  1 07:54:43.929: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:54:43.929: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:54:43.937: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  1 07:54:53.946: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:54:53.946: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:54:53.971: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:54:53.971: INFO: ss-0  pharos-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:54:53.971: INFO: ss-1                   Pending         []
Feb  1 07:54:53.971: INFO: 
Feb  1 07:54:53.971: INFO: StatefulSet ss has not reached scale 3, at 2
Feb  1 07:54:54.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994900945s
Feb  1 07:54:55.990: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98550933s
Feb  1 07:54:56.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976287562s
Feb  1 07:54:58.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968769055s
Feb  1 07:54:59.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963586783s
Feb  1 07:55:00.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958576486s
Feb  1 07:55:01.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94922784s
Feb  1 07:55:02.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944795638s
Feb  1 07:55:03.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.136766ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-jl589
Feb  1 07:55:04.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:55:04.305: INFO: stderr: ""
Feb  1 07:55:04.305: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:55:04.305: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:55:04.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:55:04.621: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  1 07:55:04.621: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:55:04.621: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:55:04.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:55:04.852: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb  1 07:55:04.852: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:55:04.852: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:55:04.860: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:04.860: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:04.860: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb  1 07:55:04.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:55:05.061: INFO: stderr: ""
Feb  1 07:55:05.061: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:55:05.061: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:55:05.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:55:05.363: INFO: stderr: ""
Feb  1 07:55:05.363: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:55:05.363: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:55:05.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-jl589 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:55:05.582: INFO: stderr: ""
Feb  1 07:55:05.582: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:55:05.582: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:55:05.582: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:55:05.585: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb  1 07:55:15.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:55:15.598: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:55:15.598: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  1 07:55:15.611: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:15.611: INFO: ss-0  pharos-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:15.612: INFO: ss-1  pharos-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:15.612: INFO: ss-2  pharos-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:15.612: INFO: 
Feb  1 07:55:15.612: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:16.618: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:16.618: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:16.618: INFO: ss-1  pharos-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:16.618: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:16.618: INFO: 
Feb  1 07:55:16.618: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:17.626: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:17.626: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:17.626: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:17.626: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:17.626: INFO: 
Feb  1 07:55:17.626: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:18.631: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:18.631: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:18.631: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:18.631: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:18.631: INFO: 
Feb  1 07:55:18.631: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:19.637: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:19.637: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:19.637: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:19.637: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:19.637: INFO: 
Feb  1 07:55:19.637: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:20.649: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:20.649: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:20.649: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:20.649: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:20.649: INFO: 
Feb  1 07:55:20.649: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:21.654: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:21.654: INFO: ss-0  pharos-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:33 +0000 UTC  }]
Feb  1 07:55:21.654: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:21.654: INFO: ss-2  pharos-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:21.654: INFO: 
Feb  1 07:55:21.654: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  1 07:55:22.659: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb  1 07:55:22.659: INFO: ss-1  pharos-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:55:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-01 07:54:53 +0000 UTC  }]
Feb  1 07:55:22.659: INFO: 
Feb  1 07:55:22.659: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  1 07:55:23.665: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.948435537s
Feb  1 07:55:24.671: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.3741ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-jl589
Feb  1 07:55:25.676: INFO: Scaling statefulset ss to 0
Feb  1 07:55:25.693: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  1 07:55:25.696: INFO: Deleting all statefulset in ns e2e-tests-statefulset-jl589
Feb  1 07:55:25.700: INFO: Scaling statefulset ss to 0
Feb  1 07:55:25.711: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:55:25.717: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:55:25.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-jl589" for this suite.
Feb  1 07:55:31.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:55:31.812: INFO: namespace: e2e-tests-statefulset-jl589, resource: bindings, ignored listing per whitelist
Feb  1 07:55:31.869: INFO: namespace e2e-tests-statefulset-jl589 deletion completed in 6.136555493s

â€¢ [SLOW TEST:58.415 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:55:31.872: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-6dpsd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-6dpsd
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb  1 07:55:32.092: INFO: Found 0 stateful pods, waiting for 3
Feb  1 07:55:42.099: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:42.099: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:42.099: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb  1 07:55:52.099: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:52.099: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:52.099: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb  1 07:55:52.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-6dpsd ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:55:52.370: INFO: stderr: ""
Feb  1 07:55:52.370: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:55:52.370: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb  1 07:56:02.426: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb  1 07:56:12.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-6dpsd ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:56:12.690: INFO: stderr: ""
Feb  1 07:56:12.690: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:56:12.690: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:56:22.720: INFO: Waiting for StatefulSet e2e-tests-statefulset-6dpsd/ss2 to complete update
Feb  1 07:56:22.721: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 07:56:22.721: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb  1 07:56:32.732: INFO: Waiting for StatefulSet e2e-tests-statefulset-6dpsd/ss2 to complete update
Feb  1 07:56:32.732: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb  1 07:56:42.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-6dpsd ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb  1 07:56:42.964: INFO: stderr: ""
Feb  1 07:56:42.964: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb  1 07:56:42.964: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb  1 07:56:53.005: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb  1 07:57:03.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-622717276 exec --namespace=e2e-tests-statefulset-6dpsd ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb  1 07:57:03.262: INFO: stderr: ""
Feb  1 07:57:03.262: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb  1 07:57:03.262: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb  1 07:57:03.298: INFO: Waiting for StatefulSet e2e-tests-statefulset-6dpsd/ss2 to complete update
Feb  1 07:57:03.298: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb  1 07:57:03.298: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb  1 07:57:03.298: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb  1 07:57:13.315: INFO: Waiting for StatefulSet e2e-tests-statefulset-6dpsd/ss2 to complete update
Feb  1 07:57:13.315: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb  1 07:57:13.315: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb  1 07:57:23.312: INFO: Waiting for StatefulSet e2e-tests-statefulset-6dpsd/ss2 to complete update
Feb  1 07:57:23.312: INFO: Waiting for Pod e2e-tests-statefulset-6dpsd/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb  1 07:57:33.312: INFO: Deleting all statefulset in ns e2e-tests-statefulset-6dpsd
Feb  1 07:57:33.316: INFO: Scaling statefulset ss2 to 0
Feb  1 07:58:03.340: INFO: Waiting for statefulset status.replicas updated to 0
Feb  1 07:58:03.348: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:58:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-6dpsd" for this suite.
Feb  1 07:58:09.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:58:09.475: INFO: namespace: e2e-tests-statefulset-6dpsd, resource: bindings, ignored listing per whitelist
Feb  1 07:58:09.497: INFO: namespace e2e-tests-statefulset-6dpsd deletion completed in 6.130101639s

â€¢ [SLOW TEST:157.625 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:58:09.498: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-w8btb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  1 07:58:09.714: INFO: Waiting up to 5m0s for pod "pod-1db4adfc-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-w8btb" to be "success or failure"
Feb  1 07:58:09.721: INFO: Pod "pod-1db4adfc-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.224091ms
Feb  1 07:58:11.731: INFO: Pod "pod-1db4adfc-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016636632s
Feb  1 07:58:13.735: INFO: Pod "pod-1db4adfc-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021346675s
STEP: Saw pod success
Feb  1 07:58:13.736: INFO: Pod "pod-1db4adfc-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:58:13.739: INFO: Trying to get logs from node pharos-worker-2 pod pod-1db4adfc-25f7-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 07:58:13.765: INFO: Waiting for pod pod-1db4adfc-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:58:13.772: INFO: Pod pod-1db4adfc-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:58:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-w8btb" for this suite.
Feb  1 07:58:19.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:58:19.883: INFO: namespace: e2e-tests-emptydir-w8btb, resource: bindings, ignored listing per whitelist
Feb  1 07:58:19.887: INFO: namespace e2e-tests-emptydir-w8btb deletion completed in 6.10921646s

â€¢ [SLOW TEST:10.390 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:58:19.888: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-zfmk8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0201 07:58:21.155992      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 07:58:21.156: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:58:21.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zfmk8" for this suite.
Feb  1 07:58:27.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:58:27.265: INFO: namespace: e2e-tests-gc-zfmk8, resource: bindings, ignored listing per whitelist
Feb  1 07:58:27.316: INFO: namespace e2e-tests-gc-zfmk8 deletion completed in 6.155156862s

â€¢ [SLOW TEST:7.429 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:58:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-mbx2q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:58:31.564: INFO: Waiting up to 5m0s for pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-pods-mbx2q" to be "success or failure"
Feb  1 07:58:31.584: INFO: Pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 19.325624ms
Feb  1 07:58:33.595: INFO: Pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030457432s
Feb  1 07:58:35.608: INFO: Pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043995738s
Feb  1 07:58:37.613: INFO: Pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048949002s
STEP: Saw pod success
Feb  1 07:58:37.613: INFO: Pod "client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 07:58:37.618: INFO: Trying to get logs from node pharos-worker-2 pod client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80 container env3cont: <nil>
STEP: delete the pod
Feb  1 07:58:37.638: INFO: Waiting for pod client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 07:58:37.641: INFO: Pod client-envvars-2abb6843-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 07:58:37.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mbx2q" for this suite.
Feb  1 07:59:17.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 07:59:17.703: INFO: namespace: e2e-tests-pods-mbx2q, resource: bindings, ignored listing per whitelist
Feb  1 07:59:17.808: INFO: namespace e2e-tests-pods-mbx2q deletion completed in 40.161669169s

â€¢ [SLOW TEST:50.490 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 07:59:17.809: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-q2s8k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb  1 07:59:18.033: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb  1 07:59:18.043: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:18.046: INFO: Number of nodes with available pods: 0
Feb  1 07:59:18.046: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:59:19.051: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:19.056: INFO: Number of nodes with available pods: 0
Feb  1 07:59:19.056: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:59:20.053: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:20.069: INFO: Number of nodes with available pods: 0
Feb  1 07:59:20.069: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:59:21.056: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:21.062: INFO: Number of nodes with available pods: 0
Feb  1 07:59:21.062: INFO: Node pharos-worker-0 is running more than one daemon pod
Feb  1 07:59:22.054: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:22.059: INFO: Number of nodes with available pods: 3
Feb  1 07:59:22.059: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb  1 07:59:22.088: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:22.088: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:22.088: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:22.096: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:23.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:23.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:23.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:23.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:24.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:24.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:24.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:24.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:25.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:25.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:25.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:25.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:26.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:26.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:26.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:26.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:27.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:27.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:27.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:27.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:28.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:28.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:28.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:28.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:29.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:29.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:29.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:29.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:30.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:30.105: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:30.105: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:30.113: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:31.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:31.106: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:31.106: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:31.113: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:32.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:32.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:32.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:32.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:33.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:33.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:33.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:33.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:34.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:34.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:34.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:34.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:35.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:35.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:35.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:35.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:36.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:36.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:36.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:36.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:37.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:37.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:37.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:37.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:38.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:38.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:38.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:38.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:39.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:39.104: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:39.104: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:39.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:40.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:40.105: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:40.105: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:40.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:41.107: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:41.107: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:41.107: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:41.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:42.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:42.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:42.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:42.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:43.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:43.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:43.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:43.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:44.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:44.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:44.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:44.113: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:45.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:45.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:45.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:45.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:46.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:46.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:46.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:46.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:47.115: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:47.115: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:47.115: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:47.119: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:48.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:48.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:48.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:48.105: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:49.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:49.102: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:49.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:49.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:50.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:50.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:50.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:50.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:51.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:51.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:51.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:51.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:52.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:52.101: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:52.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:52.114: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:53.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:53.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:53.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:53.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:54.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:54.103: INFO: Wrong image for pod: daemon-set-kndkj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:54.103: INFO: Pod daemon-set-kndkj is not available
Feb  1 07:59:54.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:54.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:55.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:55.105: INFO: Pod daemon-set-vp2ww is not available
Feb  1 07:59:55.105: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:55.129: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:56.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:56.103: INFO: Pod daemon-set-vp2ww is not available
Feb  1 07:59:56.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:56.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:57.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:57.102: INFO: Pod daemon-set-vp2ww is not available
Feb  1 07:59:57.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:57.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:58.100: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:58.100: INFO: Pod daemon-set-vp2ww is not available
Feb  1 07:59:58.100: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:58.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 07:59:59.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:59.104: INFO: Pod daemon-set-vp2ww is not available
Feb  1 07:59:59.104: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 07:59:59.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:00.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:00.105: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:00.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:01.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:01.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:01.105: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:02.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:02.105: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:02.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:03.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:03.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:03.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:04.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:04.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:04.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:05.107: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:05.107: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:05.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:06.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:06.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:06.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:07.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:07.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:07.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:08.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:08.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:08.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:09.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:09.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:09.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:10.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:10.104: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:10.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:11.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:11.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:11.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:12.111: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:12.111: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:12.116: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:13.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:13.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:13.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:14.100: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:14.100: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:14.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:15.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:15.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:15.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:16.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:16.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:16.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:17.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:17.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:17.112: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:18.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:18.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:18.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:19.111: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:19.111: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:19.117: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:20.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:20.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:20.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:21.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:21.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:21.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:22.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:22.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:22.105: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:23.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:23.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:23.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:24.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:24.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:24.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:25.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:25.104: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:25.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:26.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:26.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:26.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:27.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:27.103: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:27.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:28.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:28.104: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:28.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:29.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:29.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:29.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:30.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:30.102: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:30.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:31.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:31.101: INFO: Wrong image for pod: daemon-set-xrlvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:31.101: INFO: Pod daemon-set-xrlvc is not available
Feb  1 08:00:31.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:32.101: INFO: Pod daemon-set-8bbmf is not available
Feb  1 08:00:32.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:32.105: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:33.102: INFO: Pod daemon-set-8bbmf is not available
Feb  1 08:00:33.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:33.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:34.101: INFO: Pod daemon-set-8bbmf is not available
Feb  1 08:00:34.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:34.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:35.101: INFO: Pod daemon-set-8bbmf is not available
Feb  1 08:00:35.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:35.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:36.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:36.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:37.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:37.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:38.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:38.133: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:39.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:39.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:40.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:40.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:41.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:41.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:42.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:42.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:43.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:43.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:44.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:44.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:45.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:45.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:46.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:46.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:47.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:47.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:48.109: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:48.115: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:49.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:49.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:50.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:50.115: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:51.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:51.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:52.105: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:52.111: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:53.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:53.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:54.103: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:54.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:55.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:55.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:56.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:56.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:57.100: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:57.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:58.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:58.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:00:59.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:00:59.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:00.104: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:00.110: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:01.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:01.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:02.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:02.108: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:03.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:03.105: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:04.101: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:04.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:05.102: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:05.107: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:06.100: INFO: Wrong image for pod: daemon-set-jwlqf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb  1 08:01:06.100: INFO: Pod daemon-set-jwlqf is not available
Feb  1 08:01:06.106: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:07.105: INFO: Pod daemon-set-z7j66 is not available
Feb  1 08:01:07.109: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb  1 08:01:07.115: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:07.119: INFO: Number of nodes with available pods: 2
Feb  1 08:01:07.119: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 08:01:08.124: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:08.128: INFO: Number of nodes with available pods: 2
Feb  1 08:01:08.128: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 08:01:09.126: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:09.129: INFO: Number of nodes with available pods: 2
Feb  1 08:01:09.129: INFO: Node pharos-worker-2 is running more than one daemon pod
Feb  1 08:01:10.125: INFO: DaemonSet pods can't tolerate node pharos-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  1 08:01:10.132: INFO: Number of nodes with available pods: 3
Feb  1 08:01:10.132: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-q2s8k, will wait for the garbage collector to delete the pods
Feb  1 08:01:10.215: INFO: Deleting DaemonSet.extensions daemon-set took: 7.484952ms
Feb  1 08:01:10.316: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.360242ms
Feb  1 08:01:22.921: INFO: Number of nodes with available pods: 0
Feb  1 08:01:22.921: INFO: Number of running nodes: 0, number of available pods: 0
Feb  1 08:01:22.925: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-q2s8k/daemonsets","resourceVersion":"23777"},"items":null}

Feb  1 08:01:22.928: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-q2s8k/pods","resourceVersion":"23777"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:01:22.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-q2s8k" for this suite.
Feb  1 08:01:28.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:01:29.038: INFO: namespace: e2e-tests-daemonsets-q2s8k, resource: bindings, ignored listing per whitelist
Feb  1 08:01:29.127: INFO: namespace e2e-tests-daemonsets-q2s8k deletion completed in 6.175658077s

â€¢ [SLOW TEST:131.318 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:01:29.128: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-4wsbn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  1 08:01:29.379: INFO: Waiting up to 5m0s for pod "pod-94b771c4-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-4wsbn" to be "success or failure"
Feb  1 08:01:29.382: INFO: Pod "pod-94b771c4-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.972267ms
Feb  1 08:01:31.389: INFO: Pod "pod-94b771c4-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010503221s
Feb  1 08:01:33.395: INFO: Pod "pod-94b771c4-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01577429s
STEP: Saw pod success
Feb  1 08:01:33.395: INFO: Pod "pod-94b771c4-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:01:33.399: INFO: Trying to get logs from node pharos-worker-2 pod pod-94b771c4-25f7-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 08:01:33.417: INFO: Waiting for pod pod-94b771c4-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:01:33.420: INFO: Pod pod-94b771c4-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:01:33.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4wsbn" for this suite.
Feb  1 08:01:39.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:01:39.610: INFO: namespace: e2e-tests-emptydir-4wsbn, resource: bindings, ignored listing per whitelist
Feb  1 08:01:39.627: INFO: namespace e2e-tests-emptydir-4wsbn deletion completed in 6.201739858s

â€¢ [SLOW TEST:10.499 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:01:39.628: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-fwzj8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  1 08:01:39.830: INFO: Waiting up to 5m0s for pod "pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-fwzj8" to be "success or failure"
Feb  1 08:01:39.833: INFO: Pod "pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229166ms
Feb  1 08:01:41.837: INFO: Pod "pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007190703s
Feb  1 08:01:43.844: INFO: Pod "pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013573503s
STEP: Saw pod success
Feb  1 08:01:43.844: INFO: Pod "pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:01:43.847: INFO: Trying to get logs from node pharos-worker-0 pod pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 08:01:43.875: INFO: Waiting for pod pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:01:43.877: INFO: Pod pod-9af1e0ba-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:01:43.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fwzj8" for this suite.
Feb  1 08:01:49.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:01:49.917: INFO: namespace: e2e-tests-emptydir-fwzj8, resource: bindings, ignored listing per whitelist
Feb  1 08:01:50.019: INFO: namespace e2e-tests-emptydir-fwzj8 deletion completed in 6.137901712s

â€¢ [SLOW TEST:10.392 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:01:50.021: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6hcbf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  1 08:01:50.262: INFO: Waiting up to 5m0s for pod "pod-a1292cf8-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-emptydir-6hcbf" to be "success or failure"
Feb  1 08:01:50.266: INFO: Pod "pod-a1292cf8-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.895704ms
Feb  1 08:01:52.272: INFO: Pod "pod-a1292cf8-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009988328s
Feb  1 08:01:54.276: INFO: Pod "pod-a1292cf8-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014463207s
STEP: Saw pod success
Feb  1 08:01:54.276: INFO: Pod "pod-a1292cf8-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:01:54.280: INFO: Trying to get logs from node pharos-worker-2 pod pod-a1292cf8-25f7-11e9-b51c-8292ded0de80 container test-container: <nil>
STEP: delete the pod
Feb  1 08:01:54.297: INFO: Waiting for pod pod-a1292cf8-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:01:54.300: INFO: Pod pod-a1292cf8-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:01:54.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6hcbf" for this suite.
Feb  1 08:02:00.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:02:00.400: INFO: namespace: e2e-tests-emptydir-6hcbf, resource: bindings, ignored listing per whitelist
Feb  1 08:02:00.460: INFO: namespace e2e-tests-emptydir-6hcbf deletion completed in 6.154309793s

â€¢ [SLOW TEST:10.439 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:02:00.461: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xm9gw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 08:02:00.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-xm9gw" to be "success or failure"
Feb  1 08:02:00.674: INFO: Pod "downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579355ms
Feb  1 08:02:02.679: INFO: Pod "downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00999214s
Feb  1 08:02:04.687: INFO: Pod "downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017366178s
STEP: Saw pod success
Feb  1 08:02:04.687: INFO: Pod "downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:02:04.691: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 08:02:04.712: INFO: Waiting for pod downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:02:04.720: INFO: Pod downwardapi-volume-a75e07d1-25f7-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:02:04.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xm9gw" for this suite.
Feb  1 08:02:10.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:02:10.782: INFO: namespace: e2e-tests-downward-api-xm9gw, resource: bindings, ignored listing per whitelist
Feb  1 08:02:10.896: INFO: namespace e2e-tests-downward-api-xm9gw deletion completed in 6.166242311s

â€¢ [SLOW TEST:10.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:02:10.898: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-m72zq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-kr7s
STEP: Creating a pod to test atomic-volume-subpath
Feb  1 08:02:11.130: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kr7s" in namespace "e2e-tests-subpath-m72zq" to be "success or failure"
Feb  1 08:02:11.136: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888909ms
Feb  1 08:02:13.141: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010250165s
Feb  1 08:02:15.145: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015109647s
Feb  1 08:02:17.150: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019502811s
Feb  1 08:02:19.157: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 8.026708684s
Feb  1 08:02:21.163: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 10.033008281s
Feb  1 08:02:23.169: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 12.038464989s
Feb  1 08:02:25.173: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 14.043080586s
Feb  1 08:02:27.181: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 16.050480618s
Feb  1 08:02:29.186: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 18.055847422s
Feb  1 08:02:31.194: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 20.063937589s
Feb  1 08:02:33.199: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 22.069003766s
Feb  1 08:02:35.204: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 24.07408959s
Feb  1 08:02:37.209: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Running", Reason="", readiness=false. Elapsed: 26.078772714s
Feb  1 08:02:39.213: INFO: Pod "pod-subpath-test-configmap-kr7s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.08322175s
STEP: Saw pod success
Feb  1 08:02:39.214: INFO: Pod "pod-subpath-test-configmap-kr7s" satisfied condition "success or failure"
Feb  1 08:02:39.217: INFO: Trying to get logs from node pharos-worker-2 pod pod-subpath-test-configmap-kr7s container test-container-subpath-configmap-kr7s: <nil>
STEP: delete the pod
Feb  1 08:02:39.267: INFO: Waiting for pod pod-subpath-test-configmap-kr7s to disappear
Feb  1 08:02:39.270: INFO: Pod pod-subpath-test-configmap-kr7s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kr7s
Feb  1 08:02:39.270: INFO: Deleting pod "pod-subpath-test-configmap-kr7s" in namespace "e2e-tests-subpath-m72zq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:02:39.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-m72zq" for this suite.
Feb  1 08:02:45.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:02:45.308: INFO: namespace: e2e-tests-subpath-m72zq, resource: bindings, ignored listing per whitelist
Feb  1 08:02:45.429: INFO: namespace e2e-tests-subpath-m72zq deletion completed in 6.150960568s

â€¢ [SLOW TEST:34.532 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:02:45.431: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-vbmft
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-vbmft
Feb  1 08:02:51.660: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-vbmft
STEP: checking the pod's current state and verifying that restartCount is present
Feb  1 08:02:51.663: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:06:52.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vbmft" for this suite.
Feb  1 08:06:58.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:06:58.249: INFO: namespace: e2e-tests-container-probe-vbmft, resource: bindings, ignored listing per whitelist
Feb  1 08:06:58.271: INFO: namespace e2e-tests-container-probe-vbmft deletion completed in 6.127290749s

â€¢ [SLOW TEST:252.841 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:06:58.272: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-c2w5n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb  1 08:06:58.531: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24572,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb  1 08:06:58.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24573,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb  1 08:06:58.532: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24574,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb  1 08:07:08.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24593,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb  1 08:07:08.573: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24594,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb  1 08:07:08.574: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-c2w5n,SelfLink:/api/v1/namespaces/e2e-tests-watch-c2w5n/configmaps/e2e-watch-test-label-changed,UID:58e674ff-25f8-11e9-90b2-96000019ce9c,ResourceVersion:24595,Generation:0,CreationTimestamp:2019-02-01 08:06:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:07:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-c2w5n" for this suite.
Feb  1 08:07:14.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:07:14.698: INFO: namespace: e2e-tests-watch-c2w5n, resource: bindings, ignored listing per whitelist
Feb  1 08:07:14.707: INFO: namespace e2e-tests-watch-c2w5n deletion completed in 6.127271038s

â€¢ [SLOW TEST:16.435 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:07:14.710: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-xhj7g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-xhj7g.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 102.239.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.239.102_udp@PTR;check="$$(dig +tcp +noall +answer +search 102.239.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.239.102_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-xhj7g;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-xhj7g.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-xhj7g.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 102.239.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.239.102_udp@PTR;check="$$(dig +tcp +noall +answer +search 102.239.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.239.102_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  1 08:07:32.956: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.962: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.967: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.972: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.975: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.979: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.983: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.987: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.991: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:32.997: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.002: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.007: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.011: INFO: Unable to read 10.108.239.102_udp@PTR from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.015: INFO: Unable to read 10.108.239.102_tcp@PTR from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.020: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.024: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.039: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-xhj7g from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.044: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.048: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.053: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.064: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.075: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.081: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.087: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.092: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.100: INFO: Unable to read 10.108.239.102_udp@PTR from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.105: INFO: Unable to read 10.108.239.102_tcp@PTR from pod e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80: the server could not find the requested resource (get pods dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80)
Feb  1 08:07:33.105: INFO: Lookups using e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g wheezy_udp@dns-test-service.e2e-tests-dns-xhj7g.svc wheezy_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.108.239.102_udp@PTR 10.108.239.102_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-xhj7g jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g jessie_udp@dns-test-service.e2e-tests-dns-xhj7g.svc jessie_tcp@dns-test-service.e2e-tests-dns-xhj7g.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-xhj7g.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-xhj7g.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.108.239.102_udp@PTR 10.108.239.102_tcp@PTR]

Feb  1 08:07:38.272: INFO: DNS probes using e2e-tests-dns-xhj7g/dns-test-62ae63d1-25f8-11e9-b51c-8292ded0de80 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:07:38.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-xhj7g" for this suite.
Feb  1 08:07:44.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:07:44.386: INFO: namespace: e2e-tests-dns-xhj7g, resource: bindings, ignored listing per whitelist
Feb  1 08:07:44.498: INFO: namespace e2e-tests-dns-xhj7g deletion completed in 6.176677329s

â€¢ [SLOW TEST:29.789 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:07:44.500: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-jb44h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0201 08:08:15.324901      18 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb  1 08:08:15.325: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:08:15.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jb44h" for this suite.
Feb  1 08:08:21.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:08:21.460: INFO: namespace: e2e-tests-gc-jb44h, resource: bindings, ignored listing per whitelist
Feb  1 08:08:21.492: INFO: namespace e2e-tests-gc-jb44h deletion completed in 6.160341162s

â€¢ [SLOW TEST:36.992 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:08:21.493: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-qpc9m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qpc9m
Feb  1 08:08:27.724: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qpc9m
STEP: checking the pod's current state and verifying that restartCount is present
Feb  1 08:08:27.728: INFO: Initial restart count of pod liveness-exec is 0
Feb  1 08:09:21.903: INFO: Restart count of pod e2e-tests-container-probe-qpc9m/liveness-exec is now 1 (54.175537767s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:09:21.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qpc9m" for this suite.
Feb  1 08:09:27.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:09:28.041: INFO: namespace: e2e-tests-container-probe-qpc9m, resource: bindings, ignored listing per whitelist
Feb  1 08:09:28.046: INFO: namespace e2e-tests-container-probe-qpc9m deletion completed in 6.125612944s

â€¢ [SLOW TEST:66.552 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:09:28.046: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-h9l6d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb  1 08:09:32.787: INFO: Successfully updated pod "annotationupdateb224f6e3-25f8-11e9-b51c-8292ded0de80"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:09:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-h9l6d" for this suite.
Feb  1 08:09:56.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:09:56.942: INFO: namespace: e2e-tests-downward-api-h9l6d, resource: bindings, ignored listing per whitelist
Feb  1 08:09:57.015: INFO: namespace e2e-tests-downward-api-h9l6d deletion completed in 22.17105712s

â€¢ [SLOW TEST:28.969 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:09:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cqjst
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-cqjst/configmap-test-c36b6f27-25f8-11e9-b51c-8292ded0de80
STEP: Creating a pod to test consume configMaps
Feb  1 08:09:57.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80" in namespace "e2e-tests-configmap-cqjst" to be "success or failure"
Feb  1 08:09:57.245: INFO: Pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167099ms
Feb  1 08:09:59.254: INFO: Pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011778031s
Feb  1 08:10:01.258: INFO: Pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016107598s
Feb  1 08:10:03.265: INFO: Pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022784357s
STEP: Saw pod success
Feb  1 08:10:03.265: INFO: Pod "pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:10:03.269: INFO: Trying to get logs from node pharos-worker-0 pod pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80 container env-test: <nil>
STEP: delete the pod
Feb  1 08:10:03.298: INFO: Waiting for pod pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:10:03.303: INFO: Pod pod-configmaps-c36ce7fc-25f8-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:10:03.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cqjst" for this suite.
Feb  1 08:10:09.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:10:09.354: INFO: namespace: e2e-tests-configmap-cqjst, resource: bindings, ignored listing per whitelist
Feb  1 08:10:09.440: INFO: namespace e2e-tests-configmap-cqjst deletion completed in 6.132426479s

â€¢ [SLOW TEST:12.424 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb  1 08:10:09.441: INFO: >>> kubeConfig: /tmp/kubeconfig-622717276
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nflq9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb  1 08:10:09.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80" in namespace "e2e-tests-downward-api-nflq9" to be "success or failure"
Feb  1 08:10:09.655: INFO: Pod "downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.030034ms
Feb  1 08:10:11.665: INFO: Pod "downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020552424s
Feb  1 08:10:13.673: INFO: Pod "downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02917814s
STEP: Saw pod success
Feb  1 08:10:13.673: INFO: Pod "downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80" satisfied condition "success or failure"
Feb  1 08:10:13.679: INFO: Trying to get logs from node pharos-worker-2 pod downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80 container client-container: <nil>
STEP: delete the pod
Feb  1 08:10:13.701: INFO: Waiting for pod downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80 to disappear
Feb  1 08:10:13.704: INFO: Pod downwardapi-volume-cad1b1c7-25f8-11e9-b51c-8292ded0de80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb  1 08:10:13.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nflq9" for this suite.
Feb  1 08:10:19.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb  1 08:10:19.850: INFO: namespace: e2e-tests-downward-api-nflq9, resource: bindings, ignored listing per whitelist
Feb  1 08:10:19.855: INFO: namespace e2e-tests-downward-api-nflq9 deletion completed in 6.142728748s

â€¢ [SLOW TEST:10.414 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSFeb  1 08:10:19.857: INFO: Running AfterSuite actions on all nodes
Feb  1 08:10:19.857: INFO: Running AfterSuite actions on node 1
Feb  1 08:10:19.857: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 6564.588 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h49m26.458964706s
Test Suite Passed
